<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 82]
- [cs.CL](#cs.CL) [Total: 63]
- [cs.LG](#cs.LG) [Total: 95]
- [cs.AI](#cs.AI) [Total: 37]
- [cs.MA](#cs.MA) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Fourier-Based GAN Fingerprint Detection using ResNet50](https://arxiv.org/abs/2510.19840)
*Sai Teja Erukude,Viswa Chaitanya Marella,Suhasnadh Reddy Veluru*

Main category: cs.CV

TL;DR: Frequency-domain analysis with deep learning effectively distinguishes StyleGAN-generated images from real ones, achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of detecting GAN-generated images for digital forensics and industrial authenticity.

Method: Uses 2D DFT to transform images into Fourier domain, trains ResNet50 on transformed images to detect periodic artifacts.

Result: Model achieves 92.8% accuracy and AUC of 0.95, outperforming spatial-domain models.

Conclusion: GAN-generated images have unique frequency-domain fingerprints; combining signal processing and deep learning enhances forensics.

Abstract: The rapid rise of photorealistic images produced from Generative Adversarial
Networks (GANs) poses a serious challenge for image forensics and industrial
systems requiring reliable content authenticity. This paper uses
frequency-domain analysis combined with deep learning to solve the problem of
distinguishing StyleGAN-generated images from real ones. Specifically, a
two-dimensional Discrete Fourier Transform (2D DFT) was applied to transform
images into the Fourier domain, where subtle periodic artifacts become
detectable. A ResNet50 neural network is trained on these transformed images to
differentiate between real and synthetic ones. The experiments demonstrate that
the frequency-domain model achieves a 92.8 percent and an AUC of 0.95,
significantly outperforming the equivalent model trained on raw spatial-domain
images. These results indicate that the GAN-generated images have unique
frequency-domain signatures or "fingerprints". The method proposed highlights
the industrial potential of combining signal processing techniques and deep
learning to enhance digital forensics and strengthen the trustworthiness of
industrial AI systems.

</details>


### [2] [Transformed Multi-view 3D Shape Features with Contrastive Learning](https://arxiv.org/abs/2510.19955)
*Márcus Vinícius Lobo Costa,Sherlon Almeida da Silva,Bárbara Caroline Benato,Leo Sampaio Ferraz Ribeiro,Moacir Antonelli Ponti*

Main category: cs.CV

TL;DR: The paper explores using Vision Transformers (ViTs) with contrastive learning objectives to improve 3D shape feature representation, achieving high accuracy (~90.6%) on ModelNet10 and reducing reliance on labeled data.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of CNNs in capturing 3D shape relationships and the dependency on extensive labeled data.

Method: Utilizes ViTs paired with contrastive supervised and self-supervised learning objectives for multi-view 3D analysis.

Result: Supervised contrastive losses achieved ~90.6% accuracy on ModelNet10, demonstrating the effectiveness of ViTs and contrastive learning.

Conclusion: Combining ViTs with contrastive learning addresses the challenges of 3D shape representation, validated through extensive experiments.

Abstract: This paper addresses the challenges in representation learning of 3D shape
features by investigating state-of-the-art backbones paired with both
contrastive supervised and self-supervised learning objectives. Computer vision
methods struggle with recognizing 3D objects from 2D images, often requiring
extensive labeled data and relying on Convolutional Neural Networks (CNNs) that
may overlook crucial shape relationships. Our work demonstrates that Vision
Transformers (ViTs) based architectures, when paired with modern contrastive
objectives, achieve promising results in multi-view 3D analysis on our
downstream tasks, unifying contrastive and 3D shape understanding pipelines.
For example, supervised contrastive losses reached about 90.6% accuracy on
ModelNet10. The use of ViTs and contrastive learning, leveraging ViTs' ability
to understand overall shapes and contrastive learning's effectiveness,
overcomes the need for extensive labeled data and the limitations of CNNs in
capturing crucial shape relationships. The success stems from capturing global
shape semantics via ViTs and refining local discriminative features through
contrastive optimization. Importantly, our approach is empirical, as it is
grounded on extensive experimental evaluation to validate the effectiveness of
combining ViTs with contrastive objectives for 3D representation learning.

</details>


### [3] [FutrTrack: A Camera-LiDAR Fusion Transformer for 3D Multiple Object Tracking](https://arxiv.org/abs/2510.19981)
*Martha Teiko Teye,Ori Maoz,Matthias Rottmann*

Main category: cs.CV

TL;DR: FutrTrack is a camera-LiDAR multi-object tracking framework using transformer-based refinement and fusion-driven tracking for robust performance.


<details>
  <summary>Details</summary>
Motivation: To enhance 3D multi-object tracking by leveraging multimodal sensor data and transformer-based methods without explicit motion models.

Method: Introduces a transformer-based smoother and fusion tracker integrating camera and LiDAR features, refining trajectories and propagating identities using geometric and semantic cues.

Result: Achieves strong performance (74.7 aMOTA on nuScenes) with reduced identity switches, outperforming single-sensor approaches.

Conclusion: FutrTrack demonstrates the effectiveness of multimodal transformer-based tracking, offering competitive accuracy and robustness.

Abstract: We propose FutrTrack, a modular camera-LiDAR multi-object tracking framework
that builds on existing 3D detectors by introducing a transformer-based
smoother and a fusion-driven tracker. Inspired by query-based tracking
frameworks, FutrTrack employs a multimodal two-stage transformer refinement and
tracking pipeline. Our fusion tracker integrates bounding boxes with multimodal
bird's-eye-view (BEV) fusion features from multiple cameras and LiDAR without
the need for an explicit motion model. The tracker assigns and propagates
identities across frames, leveraging both geometric and semantic cues for
robust re-identification under occlusion and viewpoint changes. Prior to
tracking, we refine sequences of bounding boxes with a temporal smoother over a
moving window to refine trajectories, reduce jitter, and improve spatial
consistency. Evaluated on nuScenes and KITTI, FutrTrack demonstrates that
query-based transformer tracking methods benefit significantly from multimodal
sensor features compared with previous single-sensor approaches. With an aMOTA
of 74.7 on the nuScenes test set, FutrTrack achieves strong performance on 3D
MOT benchmarks, reducing identity switches while maintaining competitive
accuracy. Our approach provides an efficient framework for improving
transformer-based trackers to compete with other neural-network-based methods
even with limited data and without pretraining.

</details>


### [4] [Improving Predictive Confidence in Medical Imaging via Online Label Smoothing](https://arxiv.org/abs/2510.20011)
*Kushan Choudhury,Shubhrodeep Roy,Ankur Chanda,Shubhajit Biswas,Somenath Kuiry*

Main category: cs.CV

TL;DR: Online Label Smoothing (OLS) improves medical image classification accuracy and calibration by dynamically adjusting soft labels during training.


<details>
  <summary>Details</summary>
Motivation: Deep learning models in medical imaging often produce overconfident predictions, reducing their reliability in healthcare settings. Traditional label smoothing fails to account for class relationships.

Method: The study explores Online Label Smoothing (OLS), a dynamic approach adjusting soft labels based on model predictions during training. Evaluated on RadImageNet dataset using ResNet-50, MobileNetV2, and VGG-19 architectures.

Result: OLS consistently improves Top-1 and Top-5 accuracy compared to standard methods, enhances feature embeddings, and improves calibration.

Conclusion: OLS is a practical and effective solution for trustworthy AI systems in medical imaging, improving both performance and reliability.

Abstract: Deep learning models, especially convolutional neural networks, have achieved
impressive results in medical image classification. However, these models often
produce overconfident predictions, which can undermine their reliability in
critical healthcare settings. While traditional label smoothing offers a simple
way to reduce such overconfidence, it fails to consider relationships between
classes by treating all non-target classes equally. In this study, we explore
the use of Online Label Smoothing (OLS), a dynamic approach that adjusts soft
labels throughout training based on the model's own prediction patterns. We
evaluate OLS on the large-scale RadImageNet dataset using three widely used
architectures: ResNet-50, MobileNetV2, and VGG-19. Our results show that OLS
consistently improves both Top-1 and Top-5 classification accuracy compared to
standard training methods, including hard labels, conventional label smoothing,
and teacher-free knowledge distillation. In addition to accuracy gains, OLS
leads to more compact and well-separated feature embeddings, indicating
improved representation learning. These findings suggest that OLS not only
strengthens predictive performance but also enhances calibration, making it a
practical and effective solution for developing trustworthy AI systems in the
medical imaging domain.

</details>


### [5] [A Unified Detection Pipeline for Robust Object Detection in Fisheye-Based Traffic Surveillance](https://arxiv.org/abs/2510.20016)
*Neema Jakisa Owor,Joshua Kofi Asamoah,Tanner Wambui Muturi,Anneliese Jakisa Owor,Blessing Agyei Kyem,Andrews Danyo,Yaw Adu-Gyamfi,Armstrong Aboah*

Main category: cs.CV

TL;DR: A framework improves object detection in fisheye traffic imagery using preprocessing, post-processing, and ensemble models, achieving competitive results in a challenge.


<details>
  <summary>Details</summary>
Motivation: Fisheye cameras capture wide views but distort object appearances, especially near edges, challenging standard detectors.

Method: Uses preprocessing/postprocessing pipelines and ensembles state-of-the-art detectors trained on fisheye traffic data.

Result: Achieved an F1 score of 0.6366, ranking 8th among 62 teams in the 2025 AI City Challenge Track 4.

Conclusion: The framework effectively handles fisheye imagery challenges, enhancing detection accuracy.

Abstract: Fisheye cameras offer an efficient solution for wide-area traffic
surveillance by capturing large fields of view from a single vantage point.
However, the strong radial distortion and nonuniform resolution inherent in
fisheye imagery introduce substantial challenges for standard object detectors,
particularly near image boundaries where object appearance is severely
degraded. In this work, we present a detection framework designed to operate
robustly under these conditions. Our approach employs a simple yet effective
pre and post processing pipeline that enhances detection consistency across the
image, especially in regions affected by severe distortion. We train several
state-of-the-art detection models on the fisheye traffic imagery and combine
their outputs through an ensemble strategy to improve overall detection
accuracy. Our method achieves an F1 score of0.6366 on the 2025 AI City
Challenge Track 4, placing 8thoverall out of 62 teams. These results
demonstrate the effectiveness of our framework in addressing issues inherent to
fisheye imagery.

</details>


### [6] [Extreme Views: 3DGS Filter for Novel View Synthesis from Out-of-Distribution Camera Poses](https://arxiv.org/abs/2510.20027)
*Damian Bowness,Charalambos Poullis*

Main category: cs.CV

TL;DR: A novel real-time render-aware filtering method improves visual quality in 3D Gaussian Splatting models when navigating outside training viewpoints by targeting generative uncertainty.


<details>
  <summary>Details</summary>
Motivation: Visual noise occurs in 3DGS models when viewed outside the training data distribution due to uncertain predictions, necessitating a solution.

Method: The proposed method uses sensitivity scores from intermediate gradients to filter instabilities caused by anisotropic orientations, addressing generative uncertainty directly.

Result: The method enhances visual quality, realism, and consistency compared to NeRF-based approaches like BayesRays, integrating seamlessly into existing pipelines.

Conclusion: The filter effectively mitigates noise in extrapolated regions without requiring retraining, maintaining high visual fidelity in real-time.

Abstract: When viewing a 3D Gaussian Splatting (3DGS) model from camera positions
significantly outside the training data distribution, substantial visual noise
commonly occurs. These artifacts result from the lack of training data in these
extrapolated regions, leading to uncertain density, color, and geometry
predictions from the model.
  To address this issue, we propose a novel real-time render-aware filtering
method. Our approach leverages sensitivity scores derived from intermediate
gradients, explicitly targeting instabilities caused by anisotropic
orientations rather than isotropic variance. This filtering method directly
addresses the core issue of generative uncertainty, allowing 3D reconstruction
systems to maintain high visual fidelity even when users freely navigate
outside the original training viewpoints.
  Experimental evaluation demonstrates that our method substantially improves
visual quality, realism, and consistency compared to existing Neural Radiance
Field (NeRF)-based approaches such as BayesRays. Critically, our filter
seamlessly integrates into existing 3DGS rendering pipelines in real-time,
unlike methods that require extensive post-hoc retraining or fine-tuning.
  Code and results at https://damian-bowness.github.io/EV3DGS

</details>


### [7] [BrainPuzzle: Hybrid Physics and Data-Driven Reconstruction for Transcranial Ultrasound Tomography](https://arxiv.org/abs/2510.20029)
*Shengyu Chen,Shihang Feng,Yi Luo,Xiaowei Jia,Youzuo Lin*

Main category: cs.CV

TL;DR: BrainPuzzle combines physics-based modeling and machine learning to improve transcranial ultrasound speed-of-sound mapping, addressing limitations of traditional methods.


<details>
  <summary>Details</summary>
Motivation: Ultrasound brain imaging faces challenges due to skull-brain tissue differences and coupling difficulties. Existing methods struggle with weak signals and incomplete coverage.

Method: BrainPuzzle uses a hybrid two-stage approach: 1) Reverse time migration preserves structural details; 2) A transformer-based encoder-decoder fuses fragments into accurate speed-of-sound maps.

Result: Experiments show superior reconstruction accuracy and image completeness compared to traditional methods.

Conclusion: BrainPuzzle offers potential advancements in quantitative ultrasound brain imaging by overcoming key technical challenges.

Abstract: Ultrasound brain imaging remains challenging due to the large difference in
sound speed between the skull and brain tissues and the difficulty of coupling
large probes to the skull. This work aims to achieve quantitative transcranial
ultrasound by reconstructing an accurate speed-of-sound (SoS) map of the brain.
Traditional physics-based full-waveform inversion (FWI) is limited by weak
signals caused by skull-induced attenuation, mode conversion, and phase
aberration, as well as incomplete spatial coverage since full-aperture arrays
are clinically impractical. In contrast, purely data-driven methods that learn
directly from raw ultrasound data often fail to model the complex nonlinear and
nonlocal wave propagation through bone, leading to anatomically plausible but
quantitatively biased SoS maps under low signal-to-noise and sparse-aperture
conditions. To address these issues, we propose BrainPuzzle, a hybrid two-stage
framework that combines physical modeling with machine learning. In the first
stage, reverse time migration (time-reversal acoustics) is applied to
multi-angle acquisitions to produce migration fragments that preserve
structural details even under low SNR. In the second stage, a transformer-based
super-resolution encoder-decoder with a graph-based attention unit (GAU) fuses
these fragments into a coherent and quantitatively accurate SoS image. A
partial-array acquisition strategy using a movable low-count transducer set
improves feasibility and coupling, while the hybrid algorithm compensates for
the missing aperture. Experiments on two synthetic datasets show that
BrainPuzzle achieves superior SoS reconstruction accuracy and image
completeness, demonstrating its potential for advancing quantitative ultrasound
brain imaging.

</details>


### [8] [Exposing Blindspots: Cultural Bias Evaluation in Generative Image Models](https://arxiv.org/abs/2510.20042)
*Huichan Seo,Sieun Choi,Minki Hong,Yi Zhou,Junseo Kim,Lukman Ismaila,Naome Etori,Mehul Agarwal,Zhixuan Liu,Jihie Kim,Jean Oh*

Main category: cs.CV

TL;DR: The paper examines cultural bias in generative image models, focusing on both text-to-image (T2I) and image-to-image (I2I) systems. It reveals biases favoring Global-North depictions and superficial edits in I2I models, offering a reproducible benchmark for future improvements.


<details>
  <summary>Details</summary>
Motivation: The study addresses the lack of exploration into cultural bias in I2I editing systems, aiming to provide standardized diagnostics for such biases in generative models.

Method: The authors conducted a unified evaluation across six countries using an 8-category schema, era-aware prompts, and fixed settings. They combined automatic metrics, culture-aware VQA, and native expert judgments.

Result: Key findings include models defaulting to Global-North depictions, I2I edits eroding cultural fidelity, and superficial changes applied inconsistently.

Conclusion: The study highlights the unreliability of culture-sensitive edits in current systems and provides a benchmark for diagnosing and mitigating cultural bias in generative models.

Abstract: Generative image models produce striking visuals yet often misrepresent
culture. Prior work has examined cultural bias mainly in text-to-image (T2I)
systems, leaving image-to-image (I2I) editors underexplored. We bridge this gap
with a unified evaluation across six countries, an 8-category/36-subcategory
schema, and era-aware prompts, auditing both T2I generation and I2I editing
under a standardized protocol that yields comparable diagnostics. Using open
models with fixed settings, we derive cross-country, cross-era, and
cross-category evaluations. Our framework combines standard automatic metrics,
a culture-aware retrieval-augmented VQA, and expert human judgments collected
from native reviewers. To enable reproducibility, we release the complete image
corpus, prompts, and configurations. Our study reveals three findings: (1)
under country-agnostic prompts, models default to Global-North, modern-leaning
depictions that flatten cross-country distinctions; (2) iterative I2I editing
erodes cultural fidelity even when conventional metrics remain flat or improve;
and (3) I2I models apply superficial cues (palette shifts, generic props)
rather than era-consistent, context-aware changes, often retaining source
identity for Global-South targets. These results highlight that
culture-sensitive edits remain unreliable in current systems. By releasing
standardized data, prompts, and human evaluation protocols, we provide a
reproducible, culture-centered benchmark for diagnosing and tracking cultural
bias in generative image models.

</details>


### [9] [Filter-Based Reconstruction of Images from Events](https://arxiv.org/abs/2510.20071)
*Bernd Pfrommer*

Main category: cs.CV

TL;DR: A simpler, filter-based method (FIBAR) reconstructs intensity images from event camera data efficiently on CPUs, though noisier than neural networks.


<details>
  <summary>Details</summary>
Motivation: To provide a simpler, efficient alternative to neural network-based methods for reconstructing intensity images from event camera data.

Method: Uses temporal digital IIR filtering for intensity changes and a novel algorithm to detect stale pixels, followed by Gaussian filtering.

Result: Runs at 42(140) million events/s on CPUs, suitable for tasks like fiducial marker detection despite noise and ghost images.

Conclusion: FIBAR offers a practical, asynchronous solution with trade-offs in noise and ghosting compared to neural networks.

Abstract: Reconstructing an intensity image from the events of a moving event camera is
a challenging task that is typically approached with neural networks deployed
on graphics processing units. This paper presents a much simpler, FIlter Based
Asynchronous Reconstruction method (FIBAR). First, intensity changes signaled
by events are integrated with a temporal digital IIR filter. To reduce
reconstruction noise, stale pixels are detected by a novel algorithm that
regulates a window of recently updated pixels. Arguing that for a moving
camera, the absence of events at a pixel location likely implies a low image
gradient, stale pixels are then blurred with a Gaussian filter. In contrast to
most existing methods, FIBAR is asynchronous and permits image read-out at an
arbitrary time. It runs on a modern laptop CPU at about 42(140) million
events/s with (without) spatial filtering enabled. A few simple qualitative
experiments are presented that show the difference in image reconstruction
between FIBAR and a neural network-based approach (FireNet). FIBAR's
reconstruction is noisier than neural network-based methods and suffers from
ghost images. However, it is sufficient for certain tasks such as the detection
of fiducial markers. Code is available at
https://github.com/ros-event-camera/event_image_reconstruction_fibar

</details>


### [10] [Endoshare: A Source Available Solution to De-Identify and Manage Surgical Videos](https://arxiv.org/abs/2510.20087)
*Lorenzo Arboit,Dennis N. Schneider,Britty Baby,Vinkle Srivastav,Pietro Mascagni,Nicolas Padoy*

Main category: cs.CV

TL;DR: Endoshare is a cross-platform tool for standardizing and de-identifying endoscopic surgical videos, developed with iterative feedback and privacy-by-design principles. It showed high usability and acceptance among clinicians and computer scientists.


<details>
  <summary>Details</summary>
Motivation: Heterogeneous recording formats and privacy concerns limit widespread use of video-based surgical assessment and data science. Endoshare aims to address these challenges.

Method: Developed following the software development life cycle with iterative feedback. Used usability heuristics and Technology Acceptance Model constructs for evaluation. Conducted surveys and benchmarking across hardware configurations.

Result: High usability scores (4.68/5 clinicians, 4.03/5 computer scientists) and strong recommendations (9.20/10 surgeons). Processing time varied with mode, duration, and hardware power.

Conclusion: Endoshare offers a user-friendly, privacy-preserving solution for surgical video management. Future steps include compliance certification and interoperability validation.

Abstract: Video-based assessment and surgical data science can advance surgical
training, research, and quality improvement. However, widespread use remains
limited by heterogeneous recording formats and privacy concerns associated with
video sharing. We present Endoshare, a source-available, cross-platform
application for merging, standardizing, and de-identifying endoscopic videos in
minimally invasive surgery. Development followed the software development life
cycle with iterative, user-centered feedback. During the analysis phase, an
internal survey of clinicians and computer scientists based on ten usability
heuristics identified key requirements that guided a privacy-by-design
architecture. In the testing phase, an external clinician survey combined the
same heuristics with Technology Acceptance Model constructs to assess usability
and adoption, complemented by benchmarking across different hardware
configurations. Four clinicians and four computer scientists initially tested
the prototype, reporting high usability (4.68 +/- 0.40/5 and 4.03 +/- 0.51/5),
with the lowest score (4.00 +/- 0.93/5) relating to label clarity. After
refinement, the testing phase surveyed ten surgeons who reported high perceived
usefulness (5.07 +/- 1.75/7), ease of use (5.15 +/- 1.71/7), heuristic
usability (4.38 +/- 0.48/5), and strong recommendation (9.20 +/- 0.79/10).
Processing time varied with processing mode, video duration (both p <= 0.001),
and machine computational power (p = 0.041). Endoshare provides a transparent,
user-friendly pipeline for standardized, privacy-preserving surgical video
management. Compliance certification and broader interoperability validation
are needed to establish it as a deployable alternative to proprietary systems.
The software is available at https://camma-public.github.io/Endoshare/

</details>


### [11] [Attentive Convolution: Unifying the Expressivity of Self-Attention with Convolutional Efficiency](https://arxiv.org/abs/2510.20092)
*Hao Yu,Haoyu Chen,Yan Jiang,Wei Peng,Zhaodong Sun,Samuel Kaski,Guoying Zhao*

Main category: cs.CV

TL;DR: The paper proposes ATConv, a modified convolutional operator inspired by self-attention principles, achieving better performance than SA in vision tasks with lower complexity.


<details>
  <summary>Details</summary>
Motivation: To bridge the performance gap between self-attention (SA) and convolutions (Conv) by identifying and incorporating the intrinsic advantages of SA into Conv designs.

Method: Identifies two key principles of SA (adaptive routing and lateral inhibition) and integrates them into Conv via ATConv, a reformulated operator using $3\times3$ kernels.

Result: ATConv outperforms SA mechanisms in vision tasks, and the resulting AttNet achieves 84.4% ImageNet-1K accuracy with 27M parameters. It also improves diffusion-based image generation.

Conclusion: ATConv successfully captures SA's expressivity while retaining Conv's efficiency, offering a promising alternative for modern vision backbones.

Abstract: Self-attention (SA) has become the cornerstone of modern vision backbones for
its powerful expressivity over traditional Convolutions (Conv). However, its
quadratic complexity remains a critical bottleneck for practical applications.
Given that Conv offers linear complexity and strong visual priors, continuing
efforts have been made to promote the renaissance of Conv. However, a
persistent performance chasm remains, highlighting that these modernizations
have not yet captured the intrinsic expressivity that defines SA. In this
paper, we re-examine the design of the CNNs, directed by a key question: what
principles give SA its edge over Conv? As a result, we reveal two fundamental
insights that challenge the long-standing design intuitions in prior research
(e.g., Receptive field). The two findings are: (1) \textit{Adaptive routing}:
SA dynamically regulates positional information flow according to semantic
content, whereas Conv employs static kernels uniformly across all positions.
(2) \textit{Lateral inhibition}: SA induces score competition among token
weighting, effectively suppressing redundancy and sharpening representations,
whereas Conv filters lack such inhibitory dynamics and exhibit considerable
redundancy. Based on this, we propose \textit{Attentive Convolution} (ATConv),
a principled reformulation of the convolutional operator that intrinsically
injects these principles. Interestingly, with only $3\times3$ kernels, ATConv
consistently outperforms various SA mechanisms in fundamental vision tasks.
Building on ATConv, we introduce AttNet, a CNN family that can attain
\textbf{84.4\%} ImageNet-1K Top-1 accuracy with only 27M parameters. In
diffusion-based image generation, replacing all SA with the proposed $3\times
3$ ATConv in SiT-XL/2 reduces ImageNet FID by 0.15 in 400k steps with faster
sampling. Code is available at: github.com/price112/Attentive-Convolution.

</details>


### [12] [StableSketcher: Enhancing Diffusion Model for Pixel-based Sketch Generation via Visual Question Answering Feedback](https://arxiv.org/abs/2510.20093)
*Jiho Park,Sieun Choi,Jaeyoon Seo,Jihie Kim*

Main category: cs.CV

TL;DR: StableSketcher enhances diffusion models to generate high-fidelity hand-drawn sketches by optimizing latent decoding and introducing a new reward function for better text-image alignment.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in synthesizing human-drawn sketches using diffusion models and improving text-image alignment.

Method: Fine-tuning variational autoencoder for latent decoding and integrating a reinforcement learning reward function based on visual question answering.

Result: Improved stylistic fidelity and better alignment with prompts compared to Stable Diffusion.

Conclusion: StableSketcher successfully generates sketches with enhanced quality and introduces a novel dataset, SketchDUO, to advance research.

Abstract: Although recent advancements in diffusion models have significantly enriched
the quality of generated images, challenges remain in synthesizing pixel-based
human-drawn sketches, a representative example of abstract expression. To
combat these challenges, we propose StableSketcher, a novel framework that
empowers diffusion models to generate hand-drawn sketches with high prompt
fidelity. Within this framework, we fine-tune the variational autoencoder to
optimize latent decoding, enabling it to better capture the characteristics of
sketches. In parallel, we integrate a new reward function for reinforcement
learning based on visual question answering, which improves text-image
alignment and semantic consistency. Extensive experiments demonstrate that
StableSketcher generates sketches with improved stylistic fidelity, achieving
better alignment with prompts compared to the Stable Diffusion baseline.
Additionally, we introduce SketchDUO, to the best of our knowledge, the first
dataset comprising instance-level sketches paired with captions and
question-answer pairs, thereby addressing the limitations of existing datasets
that rely on image-label pairs. Our code and dataset will be made publicly
available upon acceptance.

</details>


### [13] [BIOCAP: Exploiting Synthetic Captions Beyond Labels in Biological Foundation Models](https://arxiv.org/abs/2510.20095)
*Ziheng Zhang,Xinyue Ma,Arpita Chowdhury,Elizabeth G. Campolongo,Matthew J. Thompson,Net Zhang,Samuel Stevens,Hilmar Lapp,Tanya Berger-Wolf,Yu Su,Wei-Lun Chao,Jianyang Gu*

Main category: cs.CV

TL;DR: The study integrates synthetic descriptive captions for biological images using MLLMs, enhancing multimodal foundation models like BIOCAP for improved species classification and text-image retrieval.


<details>
  <summary>Details</summary>
Motivation: To leverage captions as supervision for biological multimodal models, addressing the challenge of scaling instance-specific captions by generating synthetic ones.

Method: Generates synthetic captions with MLLMs using Wikipedia-derived visuals and taxon-tailored examples, then trains BIOCAP (BIOCLIP with Captions).

Result: BIOCAP shows strong performance in species classification and text-image retrieval, highlighting captions' value over labels.

Conclusion: Descriptive captions bridge biological images with multimodal models, outperforming traditional labels.

Abstract: This work investigates descriptive captions as an additional source of
supervision for biological multimodal foundation models. Images and captions
can be viewed as complementary samples from the latent morphospace of a
species, each capturing certain biological traits. Incorporating captions
during training encourages alignment with this shared latent structure,
emphasizing potentially diagnostic characters while suppressing spurious
correlations. The main challenge, however, lies in obtaining faithful,
instance-specific captions at scale. This requirement has limited the
utilization of natural language supervision in organismal biology compared with
many other scientific domains. We complement this gap by generating synthetic
captions with multimodal large language models (MLLMs), guided by
Wikipedia-derived visual information and taxon-tailored format examples. These
domain-specific contexts help reduce hallucination and yield accurate,
instance-based descriptive captions. Using these captions, we train BIOCAP
(i.e., BIOCLIP with Captions), a biological foundation model that captures rich
semantics and achieves strong performance in species classification and
text-image retrieval. These results demonstrate the value of descriptive
captions beyond labels in bridging biological images with multimodal foundation
models.

</details>


### [14] [Physics-Guided Fusion for Robust 3D Tracking of Fast Moving Small Objects](https://arxiv.org/abs/2510.20126)
*Prithvi Raj Singh,Raju Gottumukkala,Anthony S. Maida,Alan B. Barhorst,Vijaya Gopu*

Main category: cs.CV

TL;DR: The paper introduces a novel system combining deep learning and physics-based tracking to detect and track fast-moving tiny objects using RGB-D cameras, outperforming traditional methods by 70% in accuracy.


<details>
  <summary>Details</summary>
Motivation: The underexplored challenge of detecting and tracking rapidly moving small objects motivates the development of a more robust system leveraging RGB-D cameras.

Method: The system integrates deep learning-based detection with a physics-based tracking algorithm, incorporating kinematics motion equations and an outlier correction module.

Result: Testing on a custom racquetball dataset showed a 70% reduction in Average Displacement Error compared to Kalman filter-based trackers.

Conclusion: The hybrid approach of physics-based models and deep learning effectively improves real-time 3D detection and tracking for small, fast-moving objects, benefiting autonomous robotics.

Abstract: While computer vision has advanced considerably for general object detection
and tracking, the specific problem of fast-moving tiny objects remains
underexplored. This paper addresses the significant challenge of detecting and
tracking rapidly moving small objects using an RGB-D camera. Our novel system
combines deep learning-based detection with physics-based tracking to overcome
the limitations of existing approaches. Our contributions include: (1) a
comprehensive system design for object detection and tracking of fast-moving
small objects in 3D space, (2) an innovative physics-based tracking algorithm
that integrates kinematics motion equations to handle outliers and missed
detections, and (3) an outlier detection and correction module that
significantly improves tracking performance in challenging scenarios such as
occlusions and rapid direction changes. We evaluated our proposed system on a
custom racquetball dataset. Our evaluation shows our system surpassing kalman
filter based trackers with up to 70\% less Average Displacement Error. Our
system has significant applications for improving robot perception on
autonomous platforms and demonstrates the effectiveness of combining
physics-based models with deep learning approaches for real-time 3D detection
and tracking of challenging small objects.

</details>


### [15] [Inverse Image-Based Rendering for Light Field Generation from Single Images](https://arxiv.org/abs/2510.20132)
*Hyunjun Jung,Hae-Gon Jeon*

Main category: cs.CV

TL;DR: The paper proposes inverse image-based rendering, a method to synthesize novel views for light field generation from single images, avoiding computational costs or specialized hardware.


<details>
  <summary>Details</summary>
Motivation: Existing light field methods require high computational costs or specialized devices. The goal is to make light field generation more accessible by using single images.

Method: The method uses a neural rendering pipeline to reconstruct light flows from image pixels, employing cross-attention and iterative updates for occluded content.

Result: The approach works well on challenging datasets without retraining, outperforming state-of-the-art view synthesis methods.

Conclusion: Inverse image-based rendering is an effective solution for generating light fields from single images, offering broad applicability.

Abstract: A concept of light-fields computed from multiple view images on regular grids
has proven its benefit for scene representations, and supported realistic
renderings of novel views and photographic effects such as refocusing and
shallow depth of field. In spite of its effectiveness of light flow
computations, obtaining light fields requires either computational costs or
specialized devices like a bulky camera setup and a specialized microlens
array. In an effort to broaden its benefit and applicability, in this paper, we
propose a novel view synthesis method for light field generation from only
single images, named inverse image-based rendering. Unlike previous attempts to
implicitly rebuild 3D geometry or to explicitly represent objective scenes, our
method reconstructs light flows in a space from image pixels, which behaves in
the opposite way to image-based rendering. To accomplish this, we design a
neural rendering pipeline to render a target ray in an arbitrary viewpoint. Our
neural renderer first stores the light flow of source rays from the input
image, then computes the relationships among them through cross-attention, and
finally predicts the color of the target ray based on these relationships.
After the rendering pipeline generates the first novel view from a single input
image, the generated out-of-view contents are updated to the set of source
rays. This procedure is iteratively performed while ensuring the consistent
generation of occluded contents. We demonstrate that our inverse image-based
rendering works well with various challenging datasets without any retraining
or finetuning after once trained on synthetic dataset, and outperforms relevant
state-of-the-art novel view synthesis methods.

</details>


### [16] [Revisiting Logit Distributions for Reliable Out-of-Distribution Detection](https://arxiv.org/abs/2510.20134)
*Jiachen Liang,Ruibing Hou,Minyang Hu,Hong Chang,Shiguang Shan,Xilin Chen*

Main category: cs.CV

TL;DR: LogitGap is a novel post-hoc OOD detection method that leverages the logit space relationship to improve ID-OOD separability, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing post-hoc OOD detection methods often underexploit logit space information, limiting their effectiveness.

Method: LogitGap exploits the relationship between the maximum logit and remaining logits, refining it by focusing on informative subsets without training.

Result: LogitGap achieves superior performance in diverse OOD detection scenarios and benchmarks.

Conclusion: LogitGap is an effective, training-free OOD detection method that outperforms existing approaches.

Abstract: Out-of-distribution (OOD) detection is critical for ensuring the reliability
of deep learning models in open-world applications. While post-hoc methods are
favored for their efficiency and ease of deployment, existing approaches often
underexploit the rich information embedded in the model's logits space. In this
paper, we propose LogitGap, a novel post-hoc OOD detection method that
explicitly exploits the relationship between the maximum logit and the
remaining logits to enhance the separability between in-distribution (ID) and
OOD samples. To further improve its effectiveness, we refine LogitGap by
focusing on a more compact and informative subset of the logit space.
Specifically, we introduce a training-free strategy that automatically
identifies the most informative logits for scoring. We provide both theoretical
analysis and empirical evidence to validate the effectiveness of our approach.
Extensive experiments on both vision-language and vision-only models
demonstrate that LogitGap consistently achieves state-of-the-art performance
across diverse OOD detection scenarios and benchmarks. Code is available at
https://github.com/GIT-LJc/LogitGap.

</details>


### [17] [PartNeXt: A Next-Generation Dataset for Fine-Grained and Hierarchical 3D Part Understanding](https://arxiv.org/abs/2510.20155)
*Penghao Wang,Yiyang He,Xin Lv,Yukai Zhou,Lan Xu,Jingyi Yu,Jiayuan Gu*

Main category: cs.CV

TL;DR: PartNeXt introduces a high-quality, textured 3D dataset with hierarchical part labels, improving scalability and usability over existing datasets like PartNet. It benchmarks well on part segmentation and part-centric question answering, showing superior quality and diversity.


<details>
  <summary>Details</summary>
Motivation: Advancing 3D part understanding in computer vision, graphics, and robotics requires scalable, high-quality datasets. Existing datasets like PartNet have limitations in textures and annotation scalability.

Method: PartNeXt offers over 23,000 textured 3D models with hierarchical part labels across 50 categories. It benchmarks on class-agnostic part segmentation and introduces a new task for 3D-LLMs (part-centric question answering).

Result: PartNeXt outperforms PartNet in tasks like part segmentation and reveals gaps in open-vocabulary part grounding for 3D-LLMs. Training Point-SAM on PartNeXt shows substantial gains.

Conclusion: PartNeXt's scalable annotation, texture-aware labels, and multi-task evaluation enable new research opportunities in structured 3D understanding.

Abstract: Understanding objects at the level of their constituent parts is fundamental
to advancing computer vision, graphics, and robotics. While datasets like
PartNet have driven progress in 3D part understanding, their reliance on
untextured geometries and expert-dependent annotation limits scalability and
usability. We introduce PartNeXt, a next-generation dataset addressing these
gaps with over 23,000 high-quality, textured 3D models annotated with
fine-grained, hierarchical part labels across 50 categories. We benchmark
PartNeXt on two tasks: (1) class-agnostic part segmentation, where
state-of-the-art methods (e.g., PartField, SAMPart3D) struggle with
fine-grained and leaf-level parts, and (2) 3D part-centric question answering,
a new benchmark for 3D-LLMs that reveals significant gaps in open-vocabulary
part grounding. Additionally, training Point-SAM on PartNeXt yields substantial
gains over PartNet, underscoring the dataset's superior quality and diversity.
By combining scalable annotation, texture-aware labels, and multi-task
evaluation, PartNeXt opens new avenues for research in structured 3D
understanding.

</details>


### [18] [Monocular Visual 8D Pose Estimation for Articulated Bicycles and Cyclists](https://arxiv.org/abs/2510.20158)
*Eduardo R. Corral-Soto,Yang Liu,Yuan Ren,Bai Dongfeng,Liu Bingbing*

Main category: cs.CV

TL;DR: This paper introduces an 8D pose estimation method for articulated bicycles and cyclists using RGB images, improving on traditional 6D methods by estimating steering and pedal angles for better pose state and travel direction accuracy.


<details>
  <summary>Details</summary>
Motivation: Accurate pose estimation of cyclists is crucial for autonomous driving applications like intention classification and collision avoidance. Traditional 6D methods fall short for articulated bicycles due to varying parts like steering and pedals.

Method: The proposed method estimates an 8D pose (including steering and pedal angles) from a single RGB image, using synthetic and real data for training and jointly estimating pose and 3D keypoints.

Result: The method achieves competitive accuracy against state-of-the-art 6D pose estimators, demonstrating improved fine-grained pose estimation.

Conclusion: The 8D pose estimation method enhances bicycle pose accuracy, aiding autonomous driving systems in better understanding cyclist behavior and travel direction.

Abstract: In Autonomous Driving, cyclists belong to the safety-critical class of
Vulnerable Road Users (VRU), and accurate estimation of their pose is critical
for cyclist crossing intention classification, behavior prediction, and
collision avoidance. Unlike rigid objects, articulated bicycles are composed of
movable rigid parts linked by joints and constrained by a kinematic structure.
6D pose methods can estimate the 3D rotation and translation of rigid bicycles,
but 6D becomes insufficient when the steering/pedals angles of the bicycle
vary. That is because: 1) varying the articulated pose of the bicycle causes
its 3D bounding box to vary as well, and 2) the 3D box orientation is not
necessarily aligned to the orientation of the steering which determines the
actual intended travel direction. In this work, we introduce a method for
category-level 8D pose estimation for articulated bicycles and cyclists from a
single RGB image. Besides being able to estimate the 3D translation and
rotation of a bicycle from a single image, our method also estimates the
rotations of its steering handles and pedals with respect to the bicycle body
frame. These two new parameters enable the estimation of a more fine-grained
bicycle pose state and travel direction. Our proposed model jointly estimates
the 8D pose and the 3D Keypoints of articulated bicycles, and trains with a mix
of synthetic and real image data to generalize on real images. We include an
evaluation section where we evaluate the accuracy of our estimated 8D pose
parameters, and our method shows promising results by achieving competitive
scores when compared against state-of-the-art category-level 6D pose estimators
that use rigid canonical object templates for matching.

</details>


### [19] [TOMCAT: Test-time Comprehensive Knowledge Accumulation for Compositional Zero-Shot Learning](https://arxiv.org/abs/2510.20162)
*Xudong Yan,Songhe Feng*

Main category: cs.CV

TL;DR: The paper proposes TOMCAT, a novel approach for Compositional Zero-Shot Learning (CZSL) that leverages unsupervised data to update multimodal prototypes, adapts to distribution shifts, and aligns textual and visual knowledge, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing CZSL methods struggle with performance drops due to distribution shifts caused by unseen attribute-object compositions at test time.

Method: The approach updates multimodal prototypes using unsupervised data, employs adaptive weights for prototype adjustment, uses a dynamic priority queue for visual knowledge, and aligns textual and visual prototypes via multimodal learning.

Result: TOMCAT achieves state-of-the-art performance on four benchmark datasets in both closed-world and open-world settings.

Conclusion: The proposed method effectively addresses distribution shifts in CZSL and outperforms existing methods, with plans to release the code publicly.

Abstract: Compositional Zero-Shot Learning (CZSL) aims to recognize novel
attribute-object compositions based on the knowledge learned from seen ones.
Existing methods suffer from performance degradation caused by the distribution
shift of label space at test time, which stems from the inclusion of unseen
compositions recombined from attributes and objects. To overcome the challenge,
we propose a novel approach that accumulates comprehensive knowledge in both
textual and visual modalities from unsupervised data to update multimodal
prototypes at test time. Building on this, we further design an adaptive update
weight to control the degree of prototype adjustment, enabling the model to
flexibly adapt to distribution shift during testing. Moreover, a dynamic
priority queue is introduced that stores high-confidence images to acquire
visual knowledge from historical images for inference. Considering the semantic
consistency of multimodal knowledge, we align textual and visual prototypes by
multimodal collaborative representation learning. Extensive experiments
indicate that our approach achieves state-of-the-art performance on four
benchmark datasets under both closed-world and open-world settings. Code will
be available at https://github.com/xud-yan/TOMCAT .

</details>


### [20] [IB-GAN: Disentangled Representation Learning with Information Bottleneck Generative Adversarial Networks](https://arxiv.org/abs/2510.20165)
*Insu Jeon,Wonkwang Lee,Myeongjang Pyeon,Gunhee Kim*

Main category: cs.CV

TL;DR: IB-GAN is a new unsupervised GAN-based model leveraging Information Bottleneck framework for disentangled representation learning, outperforming InfoGAN and competing with β-VAEs.


<details>
  <summary>Details</summary>
Motivation: To improve disentangled representation learning in GANs by incorporating the Information Bottleneck framework.

Method: IB-GAN introduces an intermediate stochastic layer to constrain mutual information between input and output, trained jointly with the generator.

Result: Competitive disentanglement scores on dSprites and Color-dSprites, better FID scores on CelebA and 3D Chairs compared to β-VAEs and InfoGAN.

Conclusion: IB-GAN achieves better interpretability and performance in disentangled representation learning.

Abstract: We propose a new GAN-based unsupervised model for disentangled representation
learning. The new model is discovered in an attempt to utilize the Information
Bottleneck (IB) framework to the optimization of GAN, thereby named IB-GAN. The
architecture of IB-GAN is partially similar to that of InfoGAN but has a
critical difference; an intermediate layer of the generator is leveraged to
constrain the mutual information between the input and the generated output.
The intermediate stochastic layer can serve as a learnable latent distribution
that is trained with the generator jointly in an end-to-end fashion. As a
result, the generator of IB-GAN can harness the latent space in a disentangled
and interpretable manner. With the experiments on dSprites and Color-dSprites
dataset, we demonstrate that IB-GAN achieves competitive disentanglement scores
to those of state-of-the-art \b{eta}-VAEs and outperforms InfoGAN. Moreover,
the visual quality and the diversity of samples generated by IB-GAN are often
better than those by \b{eta}-VAEs and Info-GAN in terms of FID score on CelebA
and 3D Chairs dataset.

</details>


### [21] [PPMStereo: Pick-and-Play Memory Construction for Consistent Dynamic Stereo Matching](https://arxiv.org/abs/2510.20178)
*Yun Wang,Junjie Hu,Qiaole Dong,Yongjian Zhang,Yanwei Fu,Tin Lun Lam,Dapeng Wu*

Main category: cs.CV

TL;DR: PPMStereo introduces a memory buffer technique inspired by human decision-making to achieve efficient and consistent depth estimation from stereo video, outperforming previous methods in accuracy and computational cost.


<details>
  <summary>Details</summary>
Motivation: Ensuring temporally consistent depth estimation from stereo video is crucial for applications like augmented reality, but existing methods struggle with balancing temporal modeling and computational efficiency.

Method: The proposed PPMStereo uses a Pick-and-Play Memory (PPM) module, which involves picking relevant frames and playing them adaptively for aggregation, enabling efficient long-term spatio-temporal consistency.

Result: PPMStereo achieves state-of-the-art performance, with significant improvements in accuracy and temporal consistency (e.g., 17.3% and 9.02% over BiDAStereo) while reducing computational costs.

Conclusion: PPMStereo effectively addresses the trade-off between temporal consistency and computational efficiency, making it a practical solution for real-world stereo video applications.

Abstract: Temporally consistent depth estimation from stereo video is critical for
real-world applications such as augmented reality, where inconsistent depth
estimation disrupts the immersion of users. Despite its importance, this task
remains challenging due to the difficulty in modeling long-term temporal
consistency in a computationally efficient manner. Previous methods attempt to
address this by aggregating spatio-temporal information but face a fundamental
trade-off: limited temporal modeling provides only modest gains, whereas
capturing long-range dependencies significantly increases computational cost.
To address this limitation, we introduce a memory buffer for modeling
long-range spatio-temporal consistency while achieving efficient dynamic stereo
matching. Inspired by the two-stage decision-making process in humans, we
propose a \textbf{P}ick-and-\textbf{P}lay \textbf{M}emory (PPM) construction
module for dynamic \textbf{Stereo} matching, dubbed as \textbf{PPMStereo}. PPM
consists of a `pick' process that identifies the most relevant frames and a
`play' process that weights the selected frames adaptively for spatio-temporal
aggregation. This two-stage collaborative process maintains a compact yet
highly informative memory buffer while achieving temporally consistent
information aggregation. Extensive experiments validate the effectiveness of
PPMStereo, demonstrating state-of-the-art performance in both accuracy and
temporal consistency. % Notably, PPMStereo achieves 0.62/1.11 TEPE on the
Sintel clean/final (17.3\% \& 9.02\% improvements over BiDAStereo) with fewer
computational costs. Codes are available at
\textcolor{blue}{https://github.com/cocowy1/PPMStereo}.

</details>


### [22] [Evaluating Video Models as Simulators of Multi-Person Pedestrian Trajectories](https://arxiv.org/abs/2510.20182)
*Aaron Appelle,Jerome P. Lynch*

Main category: cs.CV

TL;DR: Benchmarking text-to-video and image-to-video models for simulating pedestrian dynamics reveals effective priors but identifies flaws like merging/disappearing people.


<details>
  <summary>Details</summary>
Motivation: To evaluate the plausibility of multi-agent dynamics in generated videos, addressing gaps in existing benchmarks.

Method: Proposes evaluation protocols for T2V/I2V models, using start frames from datasets for I2V and a prompt suite for T2V, including trajectory reconstruction from pixel-space.

Result: Leading models show effective priors for multi-agent behavior but exhibit failure modes (e.g., merging/disappearing people).

Conclusion: Despite promising priors, improvements are needed to address identified flaws in pedestrian dynamics simulation.

Abstract: Large-scale video generation models have demonstrated high visual realism in
diverse contexts, spurring interest in their potential as general-purpose world
simulators. Existing benchmarks focus on individual subjects rather than scenes
with multiple interacting people. However, the plausibility of multi-agent
dynamics in generated videos remains unverified. We propose a rigorous
evaluation protocol to benchmark text-to-video (T2V) and image-to-video (I2V)
models as implicit simulators of pedestrian dynamics. For I2V, we leverage
start frames from established datasets to enable comparison with a ground truth
video dataset. For T2V, we develop a prompt suite to explore diverse pedestrian
densities and interactions. A key component is a method to reconstruct 2D
bird's-eye view trajectories from pixel-space without known camera parameters.
Our analysis reveals that leading models have learned surprisingly effective
priors for plausible multi-agent behavior. However, failure modes like merging
and disappearing people highlight areas for future improvement.

</details>


### [23] [SPAN: Continuous Modeling of Suspicion Progression for Temporal Intention Localization](https://arxiv.org/abs/2510.20189)
*Xinyi Hu,Yuran Wang,Yue Li,Wenxuan Liu,Zheng Wang*

Main category: cs.CV

TL;DR: SPAN introduces continuous regression for Temporal Intention Localization, outperforming discrete methods by capturing fluctuating suspicious intentions with long-term dependencies and cumulative effects.


<details>
  <summary>Details</summary>
Motivation: Existing discrete classification methods fail to model the continuous nature of suspicious intentions, limiting early intervention and explainability.

Method: SPAN uses continuous regression, Suspicion Coefficient Modulation, and Concept-Anchored Mapping to model suspicion scores and link actions to predefined concepts.

Result: SPAN reduces MSE by 19.8% and improves average mAP by 1.78%, with a 2.74% gain in low-frequency cases.

Conclusion: SPAN's continuous modeling enables earlier detection, proactive intervention, and enhanced explainability in security applications.

Abstract: Temporal Intention Localization (TIL) is crucial for video surveillance,
focusing on identifying varying levels of suspicious intentions to improve
security monitoring. However, existing discrete classification methods fail to
capture the continuous nature of suspicious intentions, limiting early
intervention and explainability. In this paper, we propose the Suspicion
Progression Analysis Network (SPAN), which shifts from discrete classification
to continuous regression, enabling the capture of fluctuating and evolving
suspicious intentions. We reveal that suspicion exhibits long-term dependencies
and cumulative effects, similar to Temporal Point Process (TPP) theory. Based
on these insights, we define a suspicion score formula that models continuous
changes while accounting for temporal characteristics. We also introduce
Suspicion Coefficient Modulation, which adjusts suspicion coefficients using
multimodal information to reflect the varying impacts of suspicious actions.
Additionally, the Concept-Anchored Mapping method is proposed to link
suspicious actions to predefined intention concepts, offering insights into
both the actions and their potential underlying intentions. Extensive
experiments on the HAI dataset show that SPAN significantly outperforms
existing methods, reducing MSE by 19.8% and improving average mAP by 1.78%.
Notably, SPAN achieves a 2.74% mAP gain in low-frequency cases, demonstrating
its superior ability to capture subtle behavioral changes. Compared to discrete
classification systems, our continuous suspicion modeling approach enables
earlier detection and proactive intervention, greatly enhancing system
explainability and practical utility in security applications.

</details>


### [24] [A Structured Review and Quantitative Profiling of Public Brain MRI Datasets for Foundation Model Development](https://arxiv.org/abs/2510.20196)
*Minh Sao Khue Luu,Margaret V. Benedichuk,Ekaterina I. Roppert,Roman M. Kenzhin,Bair N. Tuchinov*

Main category: cs.CV

TL;DR: The study analyzes 54 brain MRI datasets (538,031 scans) to assess data scale, diversity, and consistency for foundation model development, highlighting imbalances and heterogeneity.


<details>
  <summary>Details</summary>
Motivation: To address the lack of systematic assessments of scale, diversity, and consistency in brain MRI datasets, which are critical for developing robust foundation models.

Method: Analyzed dataset-level metrics (modality, disease coverage, scale), image-level characteristics (voxel spacing, intensity distributions), preprocessing variability, and performed a feature-space case study with a 3D DenseNet121.

Result: Revealed strong imbalances between healthy and clinical cohorts, substantial image heterogeneity, residual preprocessing differences, and measurable covariate shift after standardization.

Conclusion: Findings underscore the need for preprocessing-aware and domain-adaptive strategies to improve generalizability in brain MRI foundation models.

Abstract: The development of foundation models for brain MRI depends critically on the
scale, diversity, and consistency of available data, yet systematic assessments
of these factors remain scarce. In this study, we analyze 54 publicly
accessible brain MRI datasets encompassing over 538,031 to provide a
structured, multi-level overview tailored to foundation model development. At
the dataset level, we characterize modality composition, disease coverage, and
dataset scale, revealing strong imbalances between large healthy cohorts and
smaller clinical populations. At the image level, we quantify voxel spacing,
orientation, and intensity distributions across 15 representative datasets,
demonstrating substantial heterogeneity that can influence representation
learning. We then perform a quantitative evaluation of preprocessing
variability, examining how intensity normalization, bias field correction,
skull stripping, spatial registration, and interpolation alter voxel statistics
and geometry. While these steps improve within-dataset consistency, residual
differences persist between datasets. Finally, feature-space case study using a
3D DenseNet121 shows measurable residual covariate shift after standardized
preprocessing, confirming that harmonization alone cannot eliminate
inter-dataset bias. Together, these analyses provide a unified characterization
of variability in public brain MRI resources and emphasize the need for
preprocessing-aware and domain-adaptive strategies in the design of
generalizable brain MRI foundation models.

</details>


### [25] [RAPO++: Cross-Stage Prompt Optimization for Text-to-Video Generation via Data Alignment and Test-Time Scaling](https://arxiv.org/abs/2510.20206)
*Bingjie Gao,Qianli Ma,Xiaoxue Wu,Shuai Yang,Guanzhou Lan,Haonan Zhao,Jiaxuan Chen,Qingyang Liu,Yu Qiao,Xinyuan Chen,Yaohui Wang,Li Niu*

Main category: cs.CV

TL;DR: RAPO++ is a cross-stage framework optimizing prompts for text-to-video generation via retrieval-augmented refinement, iterative feedback, and LLM fine-tuning, enhancing quality without modifying generative models.


<details>
  <summary>Details</summary>
Motivation: Short, unstructured user prompts limit T2V generation potential; RAPO++ addresses this by aligning prompts with training data and iterative optimization.

Method: Uses three stages: RAPO for retrieval-augmented refinement, SSPO for iterative feedback-based optimization, and LLM fine-tuning to internalize patterns.

Result: Significant improvements in semantic alignment, compositionality, temporal stability, and plausibility across benchmarks and models.

Conclusion: RAPO++ is a scalable, model-agnostic solution setting a new standard for prompt optimization in T2V generation.

Abstract: Prompt design plays a crucial role in text-to-video (T2V) generation, yet
user-provided prompts are often short, unstructured, and misaligned with
training data, limiting the generative potential of diffusion-based T2V models.
We present \textbf{RAPO++}, a cross-stage prompt optimization framework that
unifies training-data--aligned refinement, test-time iterative scaling, and
large language model (LLM) fine-tuning to substantially improve T2V generation
without modifying the underlying generative backbone. In \textbf{Stage 1},
Retrieval-Augmented Prompt Optimization (RAPO) enriches user prompts with
semantically relevant modifiers retrieved from a relation graph and refactors
them to match training distributions, enhancing compositionality and
multi-object fidelity. \textbf{Stage 2} introduces Sample-Specific Prompt
Optimization (SSPO), a closed-loop mechanism that iteratively refines prompts
using multi-source feedback -- including semantic alignment, spatial fidelity,
temporal coherence, and task-specific signals such as optical flow -- yielding
progressively improved video generation quality. \textbf{Stage 3} leverages
optimized prompt pairs from SSPO to fine-tune the rewriter LLM, internalizing
task-specific optimization patterns and enabling efficient, high-quality prompt
generation even before inference. Extensive experiments across five
state-of-the-art T2V models and five benchmarks demonstrate that RAPO++
achieves significant gains in semantic alignment, compositional reasoning,
temporal stability, and physical plausibility, outperforming existing methods
by large margins. Our results highlight RAPO++ as a model-agnostic,
cost-efficient, and scalable solution that sets a new standard for prompt
optimization in T2V generation. The code is available at
https://github.com/Vchitect/RAPO.

</details>


### [26] [FlowCycle: Pursuing Cycle-Consistent Flows for Text-based Editing](https://arxiv.org/abs/2510.20212)
*Yanghao Wang,Zhen Wang,Long Chen*

Main category: cs.CV

TL;DR: FlowCycle introduces a target-aware intermediate state for text-based image editing, improving editability and consistency by optimizing learnable noises through a cycle-consistent process.


<details>
  <summary>Details</summary>
Motivation: Current methods corrupt images in a target-agnostic way, leading to limited editability or inconsistency when modifications deviate from the source. FlowCycle aims to address this by creating a target-aware intermediate state.

Method: FlowCycle parameterizes corruption with learnable noises and optimizes them through a cycle-consistent process involving iterative editing and recovery with dual consistency constraints.

Result: The method achieves superior editing quality and consistency compared to state-of-the-art techniques.

Conclusion: FlowCycle offers a promising approach for text-based image editing by focusing on target-aware corruption, enhancing both editability and source consistency.

Abstract: Recent advances in pre-trained text-to-image flow models have enabled
remarkable progress in text-based image editing. Mainstream approaches always
adopt a corruption-then-restoration paradigm, where the source image is first
corrupted into an ``intermediate state'' and then restored to the target image
under the prompt guidance. However, current methods construct this intermediate
state in a target-agnostic manner, i.e., they primarily focus on realizing
source image reconstruction while neglecting the semantic gaps towards the
specific editing target. This design inherently results in limited editability
or inconsistency when the desired modifications substantially deviate from the
source. In this paper, we argue that the intermediate state should be
target-aware, i.e., selectively corrupting editing-relevant contents while
preserving editing-irrelevant ones. To this end, we propose FlowCycle, a novel
inversion-free and flow-based editing framework that parameterizes corruption
with learnable noises and optimizes them through a cycle-consistent process. By
iteratively editing the source to the target and recovering back to the source
with dual consistency constraints, FlowCycle learns to produce a target-aware
intermediate state, enabling faithful modifications while preserving source
consistency. Extensive ablations have demonstrated that FlowCycle achieves
superior editing quality and consistency over state-of-the-art methods.

</details>


### [27] [Towards Objective Obstetric Ultrasound Assessment: Contrastive Representation Learning for Fetal Movement Detection](https://arxiv.org/abs/2510.20214)
*Talha Ilyas,Duong Nhu,Allison Thomas,Arie Levin,Lim Wei Yap,Shu Gong,David Vera Anaya,Yiwen Jiang,Deval Mehta,Ritesh Warty,Vinayak Smith,Maya Reddy,Euan Wallace,Wenlong Cheng,Zongyuan Ge,Faezeh Marzbanrad*

Main category: cs.CV

TL;DR: CURL is a self-supervised learning framework using contrastive learning to detect fetal movements in ultrasound videos, achieving high sensitivity and AUROC scores.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for fetal movement detection are subjective and inaccurate, necessitating a reliable and objective solution.

Method: Proposes CURL with dual-contrastive loss (spatial and temporal) and task-specific sampling for robust motion representation learning.

Result: Achieves 78.01% sensitivity and 81.60% AUROC on a dataset of 92 subjects with 30-minute ultrasound sessions.

Conclusion: CURL shows promise for improving prenatal monitoring and clinical decision-making through self-supervised contrastive learning.

Abstract: Accurate fetal movement (FM) detection is essential for assessing prenatal
health, as abnormal movement patterns can indicate underlying complications
such as placental dysfunction or fetal distress. Traditional methods, including
maternal perception and cardiotocography (CTG), suffer from subjectivity and
limited accuracy. To address these challenges, we propose Contrastive
Ultrasound Video Representation Learning (CURL), a novel self-supervised
learning framework for FM detection from extended fetal ultrasound video
recordings. Our approach leverages a dual-contrastive loss, incorporating both
spatial and temporal contrastive learning, to learn robust motion
representations. Additionally, we introduce a task-specific sampling strategy,
ensuring the effective separation of movement and non-movement segments during
self-supervised training, while enabling flexible inference on arbitrarily long
ultrasound recordings through a probabilistic fine-tuning approach. Evaluated
on an in-house dataset of 92 subjects, each with 30-minute ultrasound sessions,
CURL achieves a sensitivity of 78.01% and an AUROC of 81.60%, demonstrating its
potential for reliable and objective FM analysis. These results highlight the
potential of self-supervised contrastive learning for fetal movement analysis,
paving the way for improved prenatal monitoring and clinical decision-making.

</details>


### [28] [EditInfinity: Image Editing with Binary-Quantized Generative Models](https://arxiv.org/abs/2510.20217)
*Jiahuan Wang,Yuxin Chen,Jun Yu,Guangming Lu,Wenjie Pei*

Main category: cs.CV

TL;DR: The paper introduces EditInfinity, a method for precise text-driven image editing using VQ-based generative models, addressing limitations of diffusion models by leveraging exact quantized representations for better inversion supervision.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based methods for text-driven image editing suffer from approximation errors during inversion due to lack of exact supervision in intermediate steps. This limits editing performance.

Method: The authors propose EditInfinity, adapting a binary-quantized generative model (Infinity) with an efficient inversion mechanism integrating text rectification and style preservation. A holistic smoothing strategy ensures high fidelity and semantic alignment.

Result: Extensive experiments on PIE-Bench show superior performance over diffusion-based baselines across add, change, and delete operations.

Conclusion: EditInfinity demonstrates that VQ-based models, with exact intermediate representations, offer more precise and effective solutions for text-driven image editing compared to diffusion models.

Abstract: Adapting pretrained diffusion-based generative models for text-driven image
editing with negligible tuning overhead has demonstrated remarkable potential.
A classical adaptation paradigm, as followed by these methods, first infers the
generative trajectory inversely for a given source image by image inversion,
then performs image editing along the inferred trajectory guided by the target
text prompts. However, the performance of image editing is heavily limited by
the approximation errors introduced during image inversion by diffusion models,
which arise from the absence of exact supervision in the intermediate
generative steps. To circumvent this issue, we investigate the
parameter-efficient adaptation of VQ-based generative models for image editing,
and leverage their inherent characteristic that the exact intermediate
quantized representations of a source image are attainable, enabling more
effective supervision for precise image inversion. Specifically, we propose
\emph{EditInfinity}, which adapts \emph{Infinity}, a binary-quantized
generative model, for image editing. We propose an efficient yet effective
image inversion mechanism that integrates text prompting rectification and
image style preservation, enabling precise image inversion. Furthermore, we
devise a holistic smoothing strategy which allows our \emph{EditInfinity} to
perform image editing with high fidelity to source images and precise semantic
alignment to the text prompts. Extensive experiments on the PIE-Bench benchmark
across "add", "change", and "delete" editing operations, demonstrate the
superior performance of our model compared to state-of-the-art diffusion-based
baselines. Code available at: https://github.com/yx-chen-ust/EditInfinity.

</details>


### [29] [Why LVLMs Are More Prone to Hallucinations in Longer Responses: The Role of Context](https://arxiv.org/abs/2510.20229)
*Ge Zheng,Jiaye Qian,Jiajin Tang,Sibei Yang*

Main category: cs.CV

TL;DR: The paper investigates hallucination issues in LVLMs, attributing them to context reliance in longer responses. It proposes an 'induce-detect-suppress' framework to mitigate hallucinations, showing consistent improvements across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Understanding whether hallucination in LVLMs is due to response length or deeper context reliance, aiming to improve model reliability.

Method: Proposes an 'induce-detect-suppress' framework: inducing hallucinations deliberately, detecting high-risk cases early, and suppressing them during decoding.

Result: The framework achieves significant improvements across benchmarks, validating its efficacy and the context-reliance hypothesis.

Conclusion: The study provides insights into LVLM hallucinations, emphasizing context reliance over length, and serves as a step toward deeper exploration.

Abstract: Large Vision-Language Models (LVLMs) have made significant progress in recent
years but are also prone to hallucination issues. They exhibit more
hallucinations in longer, free-form responses, often attributed to accumulated
uncertainties. In this paper, we ask: Does increased hallucination result
solely from length-induced errors, or is there a deeper underlying mechanism?
After a series of preliminary experiments and findings, we suggest that the
risk of hallucinations is not caused by length itself but by the increased
reliance on context for coherence and completeness in longer responses.
Building on these insights, we propose a novel "induce-detect-suppress"
framework that actively induces hallucinations through deliberately designed
contexts, leverages induced instances for early detection of high-risk cases,
and ultimately suppresses potential object-level hallucinations during actual
decoding. Our approach achieves consistent, significant improvements across all
benchmarks, demonstrating its efficacy. The strong detection and improved
hallucination mitigation not only validate our framework but, more importantly,
re-validate our hypothesis on context. Rather than solely pursuing performance
gains, this study aims to provide new insights and serves as a first step
toward a deeper exploration of hallucinations in LVLMs' longer responses.

</details>


### [30] [COS3D: Collaborative Open-Vocabulary 3D Segmentation](https://arxiv.org/abs/2510.20238)
*Runsong Zhu,Ka-Hei Hui,Zhengzhe Liu,Qianyi Wu,Weiliang Tang,Shi Qiu,Pheng-Ann Heng,Chi-Wing Fu*

Main category: cs.CV

TL;DR: COS3D introduces a collaborative prompt-segmentation framework integrating language and segmentation cues, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations of current Gaussian-splatting-based methods in open-vocabulary 3D segmentation.

Method: Uses collaborative fields (instance and language fields), feature mapping, and adaptive prompt refinement.

Result: Achieves leading performance on benchmarks and shows versatility in applications.

Conclusion: COS3D effectively integrates language and segmentation, offering superior results and broad applicability.

Abstract: Open-vocabulary 3D segmentation is a fundamental yet challenging task,
requiring a mutual understanding of both segmentation and language. However,
existing Gaussian-splatting-based methods rely either on a single 3D language
field, leading to inferior segmentation, or on pre-computed class-agnostic
segmentations, suffering from error accumulation. To address these limitations,
we present COS3D, a new collaborative prompt-segmentation framework that
contributes to effectively integrating complementary language and segmentation
cues throughout its entire pipeline. We first introduce the new concept of
collaborative field, comprising an instance field and a language field, as the
cornerstone for collaboration. During training, to effectively construct the
collaborative field, our key idea is to capture the intrinsic relationship
between the instance field and language field, through a novel
instance-to-language feature mapping and designing an efficient two-stage
training strategy. During inference, to bridge distinct characteristics of the
two fields, we further design an adaptive language-to-instance prompt
refinement, promoting high-quality prompt-segmentation inference. Extensive
experiments not only demonstrate COS3D's leading performance over existing
methods on two widely-used benchmarks but also show its high potential to
various applications,~\ie, novel image-based 3D segmentation, hierarchical
segmentation, and robotics. The code is publicly available at
\href{https://github.com/Runsong123/COS3D}{https://github.com/Runsong123/COS3D}.

</details>


### [31] [Empower Words: DualGround for Structured Phrase and Sentence-Level Temporal Grounding](https://arxiv.org/abs/2510.20244)
*Minseok Kang,Minhyeok Lee,Minjung Kim,Donghyeong Kim,Sangyoun Lee*

Main category: cs.CV

TL;DR: DualGround improves Video Temporal Grounding by separating global and local semantics through a dual-branch architecture, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing VTG models overly rely on [EOS]-driven global semantics and fail to utilize word-level signals effectively, limiting fine-grained temporal alignment.

Method: Proposes DualGround, a dual-branch architecture that separates global (sentence-level) and local (phrase-level) semantics, using token-role-aware cross-modal interaction and joint modeling.

Result: Achieves state-of-the-art performance on Moment Retrieval and Highlight Detection tasks across QVHighlights and Charades-STA benchmarks.

Conclusion: Disentangled semantic modeling enhances video-language alignment, capturing both coarse and localized semantics effectively.

Abstract: Video Temporal Grounding (VTG) aims to localize temporal segments in long,
untrimmed videos that align with a given natural language query. This task
typically comprises two subtasks: Moment Retrieval (MR) and Highlight Detection
(HD). While recent advances have been progressed by powerful pretrained
vision-language models such as CLIP and InternVideo2, existing approaches
commonly treat all text tokens uniformly during crossmodal attention,
disregarding their distinct semantic roles. To validate the limitations of this
approach, we conduct controlled experiments demonstrating that VTG models
overly rely on [EOS]-driven global semantics while failing to effectively
utilize word-level signals, which limits their ability to achieve fine-grained
temporal alignment. Motivated by this limitation, we propose DualGround, a
dual-branch architecture that explicitly separates global and local semantics
by routing the [EOS] token through a sentence-level path and clustering word
tokens into phrase-level units for localized grounding. Our method introduces
(1) tokenrole- aware cross modal interaction strategies that align video
features with sentence-level and phrase-level semantics in a structurally
disentangled manner, and (2) a joint modeling framework that not only improves
global sentence-level alignment but also enhances finegrained temporal
grounding by leveraging structured phrase-aware context. This design allows the
model to capture both coarse and localized semantics, enabling more expressive
and context-aware video grounding. DualGround achieves state-of-the-art
performance on both Moment Retrieval and Highlight Detection tasks across
QVHighlights and Charades- STA benchmarks, demonstrating the effectiveness of
disentangled semantic modeling in video-language alignment.

</details>


### [32] [Seeing the Unseen: Mask-Driven Positional Encoding and Strip-Convolution Context Modeling for Cross-View Object Geo-Localization](https://arxiv.org/abs/2510.20247)
*Shuhan Hu,Yiru Li,Yuanyuan Li,Yingying Zhu*

Main category: cs.CV

TL;DR: Proposes EDGeo for cross-view object geo-localization using mask-based positional encoding and context enhancement, improving accuracy by 3.39%.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack object shape awareness and struggle with large-span objects in satellite imagery.

Method: Introduces mask-based positional encoding (MPE) for spatial and shape data, and a context enhancement module (CEM) for long-range features.

Result: Achieves state-of-the-art performance, with a 3.39% accuracy boost on CVOGL and VIGOR-Building datasets.

Conclusion: EDGeo offers a robust framework for advancing cross-view geo-localization research.

Abstract: Cross-view object geo-localization enables high-precision object localization
through cross-view matching, with critical applications in autonomous driving,
urban management, and disaster response. However, existing methods rely on
keypoint-based positional encoding, which captures only 2D coordinates while
neglecting object shape information, resulting in sensitivity to annotation
shifts and limited cross-view matching capability. To address these
limitations, we propose a mask-based positional encoding scheme that leverages
segmentation masks to capture both spatial coordinates and object silhouettes,
thereby upgrading the model from "location-aware" to "object-aware."
Furthermore, to tackle the challenge of large-span objects (e.g., elongated
buildings) in satellite imagery, we design a context enhancement module. This
module employs horizontal and vertical strip convolutional kernels to extract
long-range contextual features, enhancing feature discrimination among
strip-like objects. Integrating MPE and CEM, we present EDGeo, an end-to-end
framework for robust cross-view object geo-localization. Extensive experiments
on two public datasets (CVOGL and VIGOR-Building) demonstrate that our method
achieves state-of-the-art performance, with a 3.39% improvement in localization
accuracy under challenging ground-to-satellite scenarios. This work provides a
robust positional encoding paradigm and a contextual modeling framework for
advancing cross-view geo-localization research.

</details>


### [33] [Calibrating Multimodal Consensus for Emotion Recognition](https://arxiv.org/abs/2510.20256)
*Guowei Zhong,Junjie Li,Huaiyu Zhu,Ruohong Huan,Yun Pan*

Main category: cs.CV

TL;DR: The paper proposes Calibrated Multimodal Consensus (CMC) to address semantic inconsistencies and text modality dominance in Multimodal Emotion Recognition (MER), achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing MER approaches often overlook semantic inconsistencies across modalities and are biased toward text dominance, reducing accuracy.

Method: CMC introduces a Pseudo Label Generation Module (PLGM) for self-supervised unimodal pretraining, followed by a Parameter-free Fusion Module (PFM) and Multimodal Consensus Router (MCR) for balanced multimodal finetuning.

Result: CMC matches or outperforms state-of-the-art methods on datasets CH-SIMS, CH-SIMS v2, CMU-MOSI, and CMU-MOSEI, particularly excelling in cases of semantic inconsistency.

Conclusion: CMC effectively mitigates text dominance and semantic inconsistencies, improving MER performance with publicly available implementation.

Abstract: In recent years, Multimodal Emotion Recognition (MER) has made substantial
progress. Nevertheless, most existing approaches neglect the semantic
inconsistencies that may arise across modalities, such as conflicting emotional
cues between text and visual inputs. Besides, current methods are often
dominated by the text modality due to its strong representational capacity,
which can compromise recognition accuracy. To address these challenges, we
propose a model termed Calibrated Multimodal Consensus (CMC). CMC introduces a
Pseudo Label Generation Module (PLGM) to produce pseudo unimodal labels,
enabling unimodal pretraining in a self-supervised fashion. It then employs a
Parameter-free Fusion Module (PFM) and a Multimodal Consensus Router (MCR) for
multimodal finetuning, thereby mitigating text dominance and guiding the fusion
process toward a more reliable consensus. Experimental results demonstrate that
CMC achieves performance on par with or superior to state-of-the-art methods
across four datasets, CH-SIMS, CH-SIMS v2, CMU-MOSI, and CMU-MOSEI, and
exhibits notable advantages in scenarios with semantic inconsistencies on
CH-SIMS and CH-SIMS v2. The implementation of this work is publicly accessible
at https://github.com/gw-zhong/CMC.

</details>


### [34] [Real-Time Currency Detection and Voice Feedback for Visually Impaired Individuals](https://arxiv.org/abs/2510.20267)
*Saraf Anzum Shreya,MD. Abu Ismail Siddique,Sharaf Tasnim*

Main category: cs.CV

TL;DR: A real-time currency detection system using smartphones and YOLOv8 nano model aids visually impaired individuals by identifying currency notes and coins with high accuracy and voice feedback.


<details>
  <summary>Details</summary>
Motivation: To assist visually impaired individuals in handling money independently by leveraging smartphone technology and machine learning.

Method: Uses a YOLOv8 nano model with a custom detection head featuring deep convolutional layers and Squeeze-and-Excitation blocks, trained on a dataset of 30 currency classes (USD, EUR, BDT).

Result: Achieves 97.73% accuracy, 95.23% recall, 95.85% f1-score, and 97.21% mAP50(B).

Conclusion: The system is practical and efficient, empowering visually impaired individuals to handle money independently with high accuracy and voice feedback.

Abstract: Technologies like smartphones have become an essential in our daily lives. It
has made accessible to everyone including visually impaired individuals. With
the use of smartphone cameras, image capturing and processing have become more
convenient. With the use of smartphones and machine learning, the life of
visually impaired can be made a little easier. Daily tasks such as handling
money without relying on someone can be troublesome for them. For that purpose
this paper presents a real-time currency detection system designed to assist
visually impaired individuals. The proposed model is trained on a dataset
containing 30 classes of notes and coins, representing 3 types of currency: US
dollar (USD), Euro (EUR), and Bangladeshi taka (BDT). Our approach uses a
YOLOv8 nano model with a custom detection head featuring deep convolutional
layers and Squeeze-and-Excitation blocks to enhance feature extraction and
detection accuracy. Our model has achieved a higher accuracy of 97.73%, recall
of 95.23%, f1-score of 95.85% and a mean Average Precision at IoU=0.5
(mAP50(B)) of 97.21\%. Using the voice feedback after the detection would help
the visually impaired to identify the currency. This paper aims to create a
practical and efficient currency detection system to empower visually impaired
individuals independent in handling money.

</details>


### [35] [GMFVAD: Using Grained Multi-modal Feature to Improve Video Anomaly Detection](https://arxiv.org/abs/2510.20268)
*Guangyu Dai,Dong Chen,Siliang Tang,Yueting Zhuang*

Main category: cs.CV

TL;DR: GMFVAD improves video anomaly detection by refining multi-modal features to reduce redundancy, achieving top performance on datasets.


<details>
  <summary>Details</summary>
Motivation: Existing methods incorporate text features coarsely, overlooking redundant information in video snippets.

Method: Proposes GMFVAD, generating finer multi-modal features using video snippets and captions to enhance visual features.

Result: Achieves state-of-the-art performance on four datasets; ablation confirms reduced redundancy boosts results.

Conclusion: GMFVAD effectively refines features and reduces redundancy, enhancing video anomaly detection.

Abstract: Video anomaly detection (VAD) is a challenging task that detects anomalous
frames in continuous surveillance videos. Most previous work utilizes the
spatio-temporal correlation of visual features to distinguish whether there are
abnormalities in video snippets. Recently, some works attempt to introduce
multi-modal information, like text feature, to enhance the results of video
anomaly detection. However, these works merely incorporate text features into
video snippets in a coarse manner, overlooking the significant amount of
redundant information that may exist within the video snippets. Therefore, we
propose to leverage the diversity among multi-modal information to further
refine the extracted features, reducing the redundancy in visual features, and
we propose Grained Multi-modal Feature for Video Anomaly Detection (GMFVAD).
Specifically, we generate more grained multi-modal feature based on the video
snippet, which summarizes the main content, and text features based on the
captions of original video will be introduced to further enhance the visual
features of highlighted portions. Experiments show that the proposed GMFVAD
achieves state-of-the-art performance on four mainly datasets. Ablation
experiments also validate that the improvement of GMFVAD is due to the
reduction of redundant information.

</details>


### [36] [Causal Debiasing for Visual Commonsense Reasoning](https://arxiv.org/abs/2510.20281)
*Jiayi Zou,Gengyun Jia,Bing-Kun Bao*

Main category: cs.CV

TL;DR: The paper addresses bias in Visual Commonsense Reasoning (VCR) datasets, introduces VCR-OOD datasets for evaluation, and proposes a debiasing method using backdoor adjustment.


<details>
  <summary>Details</summary>
Motivation: Existing VCR methods focus on prediction accuracy but ignore dataset biases, leading to unreliable generalization.

Method: Analyzes biases, introduces VCR-OOD datasets, and uses backdoor adjustment with a dictionary to remove prediction shortcuts.

Result: The debiasing method proves effective across datasets.

Conclusion: The proposed approach successfully mitigates bias in VCR, improving model generalization.

Abstract: Visual Commonsense Reasoning (VCR) refers to answering questions and
providing explanations based on images. While existing methods achieve high
prediction accuracy, they often overlook bias in datasets and lack debiasing
strategies. In this paper, our analysis reveals co-occurrence and statistical
biases in both textual and visual data. We introduce the VCR-OOD datasets,
comprising VCR-OOD-QA and VCR-OOD-VA subsets, which are designed to evaluate
the generalization capabilities of models across two modalities. Furthermore,
we analyze the causal graphs and prediction shortcuts in VCR and adopt a
backdoor adjustment method to remove bias. Specifically, we create a dictionary
based on the set of correct answers to eliminate prediction shortcuts.
Experiments demonstrate the effectiveness of our debiasing method across
different datasets.

</details>


### [37] [Knowledge-Informed Neural Network for Complex-Valued SAR Image Recognition](https://arxiv.org/abs/2510.20284)
*Haodong Yang,Zhongling Huang,Shaojie Guo,Zhe Zhang,Gong Cheng,Junwei Han*

Main category: cs.CV

TL;DR: The paper introduces KINN, a lightweight deep learning framework for CV-SAR image recognition, addressing the challenges of generalization, interpretability, and efficiency through physics-guided compression and aggregation.


<details>
  <summary>Details</summary>
Motivation: The authors aim to resolve the representation trilemma in CV-SAR image recognition by leveraging inherent electromagnetic scattering features, which are underutilized in conventional models.

Method: KINN uses a 'compression-aggregation-compression' architecture with physics-guided compression, adaptive dictionary processing, and self-distillation for compact classification.

Result: KINN achieves state-of-the-art performance in parameter-efficient recognition, demonstrating strong generalization in data-scarce and out-of-distribution scenarios.

Conclusion: KINN effectively addresses the representation trilemma, offering a trustworthy AI solution for SAR image analysis.

Abstract: Deep learning models for complex-valued Synthetic Aperture Radar (CV-SAR)
image recognition are fundamentally constrained by a representation trilemma
under data-limited and domain-shift scenarios: the concurrent, yet conflicting,
optimization of generalization, interpretability, and efficiency. Our work is
motivated by the premise that the rich electromagnetic scattering features
inherent in CV-SAR data hold the key to resolving this trilemma, yet they are
insufficiently harnessed by conventional data-driven models. To this end, we
introduce the Knowledge-Informed Neural Network (KINN), a lightweight framework
built upon a novel "compression-aggregation-compression" architecture. The
first stage performs a physics-guided compression, wherein a novel dictionary
processor adaptively embeds physical priors, enabling a compact unfolding
network to efficiently extract sparse, physically-grounded signatures. A
subsequent aggregation module enriches these representations, followed by a
final semantic compression stage that utilizes a compact classification head
with self-distillation to learn maximally task-relevant and discriminative
embeddings. We instantiate KINN in both CNN (0.7M) and Vision Transformer
(0.95M) variants. Extensive evaluations on five SAR benchmarks confirm that
KINN establishes a state-of-the-art in parameter-efficient recognition,
offering exceptional generalization in data-scarce and out-of-distribution
scenarios and tangible interpretability, thereby providing an effective
solution to the representation trilemma and offering a new path for trustworthy
AI in SAR image analysis.

</details>


### [38] [DMC$^3$: Dual-Modal Counterfactual Contrastive Construction for Egocentric Video Question Answering](https://arxiv.org/abs/2510.20285)
*Jiayi Zou,Chaofan Chen,Bing-Kun Bao,Changsheng Xu*

Main category: cs.CV

TL;DR: Proposes DMC³ framework for Egocentric VideoQA, addressing first-person challenges using counterfactual contrastive learning, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook first-person perspective challenges like multi-event understanding and hand-object interactions.

Method: Uses counterfactual sample construction (paraphrasing & interaction mining) and contrastive optimization.

Result: Achieves 52.51% (normal), 46.04% (indirect) on EgoTaskQA and 13.2% on QAEGO4D.

Conclusion: DMC³ effectively improves Egocentric VideoQA by leveraging counterfactual contrastive learning.

Abstract: Egocentric Video Question Answering (Egocentric VideoQA) plays an important
role in egocentric video understanding, which refers to answering questions
based on first-person videos. Although existing methods have made progress
through the paradigm of pre-training and fine-tuning, they ignore the unique
challenges posed by the first-person perspective, such as understanding
multiple events and recognizing hand-object interactions. To deal with these
challenges, we propose a Dual-Modal Counterfactual Contrastive Construction
(DMC$^3$) framework, which contains an egocentric videoqa baseline, a
counterfactual sample construction module and a counterfactual sample-involved
contrastive optimization. Specifically, We first develop a counterfactual
sample construction module to generate positive and negative samples for
textual and visual modalities through event description paraphrasing and core
interaction mining, respectively. Then, We feed these samples together with the
original samples into the baseline. Finally, in the counterfactual
sample-involved contrastive optimization module, we apply contrastive loss to
minimize the distance between the original sample features and the positive
sample features, while maximizing the distance from the negative samples.
Experiments show that our method achieve 52.51\% and 46.04\% on the
\textit{normal} and \textit{indirect} splits of EgoTaskQA, and 13.2\% on
QAEGO4D, both reaching the state-of-the-art performance.

</details>


### [39] [UI-Ins: Enhancing GUI Grounding with Multi-Perspective Instruction-as-Reasoning](https://arxiv.org/abs/2510.20286)
*Liangyu Chen,Hanzhang Zhou,Chenglin Cai,Jianan Zhang,Panrong Tong,Quyu Kong,Xu Zhang,Chen Liu,Yuqi Liu,Wenxuan Wang,Yue Wang,Qin Jin,Steven Hoi*

Main category: cs.CV

TL;DR: The paper introduces Instruction-as-Reasoning, a dynamic approach to GUI grounding that leverages instruction diversity, achieving SOTA results on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Prior works treat instructions statically, overlooking diversity's impact. The paper addresses this gap by dynamically analyzing instructions for better grounding.

Method: A two-stage training framework: SFT on diverse instructions, followed by RL for pathway optimization, producing models UI-Ins-7B and UI-Ins-32B.

Result: State-of-the-art performance on benchmarks (e.g., 87.3% on UI-I2E-Bench) and emergent reasoning, with strong agentic potential (74.1% success rate).

Conclusion: Dynamic instruction analysis improves grounding, mitigates policy collapse, and enables emergent reasoning, with public release of code/models.

Abstract: GUI grounding, which maps natural-language instructions to actionable UI
elements, is a core capability of GUI agents. Prior works largely treats
instructions as a static proxy for user intent, overlooking the impact of
instruction diversity and quality on grounding performance. Through a careful
investigation of existing grounding datasets, we find a 23.3% flaw rate in
their instructions and show that inference-time exploitation of instruction
diversity yields up to a substantial 76% relative performance improvement. In
this paper, we introduce the Instruction-as-Reasoning paradigm, treating
instructions as dynamic analytical pathways that offer distinct perspectives
and enabling the model to select the most effective pathway during reasoning.
To achieve this, we propose a two-stage training framework: supervised
fine-tuning (SFT) on synthesized, diverse instructions to instill
multi-perspective reasoning, followed by reinforcement learning (RL) to
optimize pathway selection and composition. Our resulting models, UI-Ins-7B and
UI-Ins-32B, achieve state-of-the-art results on five challenging grounding
benchmarks and exhibit emergent reasoning, selectively composing and
synthesizing novel instruction pathways at inference. In particular, UI-Ins-32B
attains the best grounding accuracy, scoring 87.3% on UI-I2E-Bench, 57.0% on
ScreenSpot-Pro, and 84.9% on MMBench-GUI L2. Furthermore, our model
demonstrates strong agentic potential, achieving a 74.1% success rate on
AndroidWorld using UI-Ins-7B as the executor. Our in-depth analysis reveals
additional insights such as how reasoning can be formulated to enhance rather
than hinder grounding performance, and how our method mitigates policy collapse
in the SFT+RL framework. All code and model checkpoints will be publicly
released in https://github.com/alibaba/UI-Ins.

</details>


### [40] [Breakdance Video classification in the age of Generative AI](https://arxiv.org/abs/2510.20287)
*Sauptik Dhar,Naveen Ramakrishnan,Michelle Munson*

Main category: cs.CV

TL;DR: The paper explores using modern video foundation models for breakdance, showing encoder models outperform video language models for prediction tasks.


<details>
  <summary>Details</summary>
Motivation: Breakdance is a niche but popular dance sport, yet understudied compared to mainstream sports like soccer or basketball.

Method: Analyzes both encoder and decoder video foundation models, focusing on their application for breakdance video classification.

Result: Video encoder models outperform state-of-the-art video language models for prediction tasks.

Conclusion: Provides insights on selecting encoder models and analyzes decoder model performance for breakdance classification.

Abstract: Large Vision Language models have seen huge application in several sports
use-cases recently. Most of these works have been targeted towards a limited
subset of popular sports like soccer, cricket, basketball etc; focusing on
generative tasks like visual question answering, highlight generation. This
work analyzes the applicability of the modern video foundation models (both
encoder and decoder) for a very niche but hugely popular dance sports -
breakdance. Our results show that Video Encoder models continue to outperform
state-of-the-art Video Language Models for prediction tasks. We provide
insights on how to choose the encoder model and provide a thorough analysis
into the workings of a finetuned decoder model for breakdance video
classification.

</details>


### [41] [A Parameter-Efficient Mixture-of-Experts Framework for Cross-Modal Geo-Localization](https://arxiv.org/abs/2510.20291)
*LinFeng Li,Jian Zhao,Zepeng Yang,Yuhang Song,Bojun Lin,Tianle Zhang,Yuchen Yuan,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: Winning solution for RoboSense 2025 Track 4 uses domain-aligned preprocessing and Mixture-of-Experts to address cross-modal drone navigation challenges.


<details>
  <summary>Details</summary>
Motivation: Severe inter-platform heterogeneity and domain gap between training descriptions and test queries.

Method: Domain-aligned preprocessing (platform-wise partitioning, satellite augmentation, orientation removal) and LLM-based caption refinement. Uses BGE-M3 (text) and EVA-CLIP (image) with progressive two-stage training.

Result: Tops the official leaderboard, demonstrating robust cross-modal geo-localization under heterogeneous viewpoints.

Conclusion: The proposed framework effectively mitigates platform-specific challenges for cross-modal retrieval.

Abstract: We present a winning solution to RoboSense 2025 Track 4: Cross-Modal Drone
Navigation. The task retrieves the most relevant geo-referenced image from a
large multi-platform corpus (satellite/drone/ground) given a natural-language
query. Two obstacles are severe inter-platform heterogeneity and a domain gap
between generic training descriptions and platform-specific test queries. We
mitigate these with a domain-aligned preprocessing pipeline and a
Mixture-of-Experts (MoE) framework: (i) platform-wise partitioning, satellite
augmentation, and removal of orientation words; (ii) an LLM-based caption
refinement pipeline to align textual semantics with the distinct visual
characteristics of each platform. Using BGE-M3 (text) and EVA-CLIP (image), we
train three platform experts using a progressive two-stage, hard-negative
mining strategy to enhance discriminative power, and fuse their scores at
inference. The system tops the official leaderboard, demonstrating robust
cross-modal geo-localization under heterogeneous viewpoints.

</details>


### [42] [HyperET: Efficient Training in Hyperbolic Space for Multi-modal Large Language Models](https://arxiv.org/abs/2510.20322)
*Zelin Peng,Zhengqin Xu,Qingyang Liu,Xiaokang Yang,Wei Shen*

Main category: cs.CV

TL;DR: HyperET introduces an efficient training paradigm for multi-modal large language models (MLLMs) using hyperbolic space to align visual and textual modalities at arbitrary granularity levels, reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs face inefficiency due to misaligned vision encoders (e.g., CLIP, SAM) lacking multi-granularity alignment with language. HyperET addresses this gap.

Method: HyperET leverages hyperbolic space for hierarchical alignment, using dynamic hyperbolic radius adjustment and learnable matrices with Möbius multiplication.

Result: HyperET improves existing MLLMs' pre-training and fine-tuning performance with less than 1% additional parameters.

Conclusion: HyperET offers a principled and efficient solution for multi-granularity alignment in MLLMs, demonstrating significant improvements.

Abstract: Multi-modal large language models (MLLMs) have emerged as a transformative
approach for aligning visual and textual understanding. They typically require
extremely high computational resources (e.g., thousands of GPUs) for training
to achieve cross-modal alignment at multi-granularity levels. We argue that a
key source of this inefficiency lies in the vision encoders they widely equip
with, e.g., CLIP and SAM, which lack the alignment with language at
multi-granularity levels. To address this issue, in this paper, we leverage
hyperbolic space, which inherently models hierarchical levels and thus provides
a principled framework for bridging the granularity gap between visual and
textual modalities at an arbitrary granularity level. Concretely, we propose an
efficient training paradigm for MLLMs, dubbed as HyperET, which can optimize
visual representations to align with their textual counterparts at an arbitrary
granularity level through dynamic hyperbolic radius adjustment in hyperbolic
space. HyperET employs learnable matrices with M\"{o}bius multiplication
operations, implemented via three effective configurations: diagonal scaling
matrices, block-diagonal matrices, and banded matrices, providing a flexible
yet efficient parametrization strategy. Comprehensive experiments across
multiple MLLM benchmarks demonstrate that HyperET consistently improves both
existing pre-training and fine-tuning MLLMs clearly with less than 1\%
additional parameters.

</details>


### [43] [AnyPcc: Compressing Any Point Cloud with a Single Universal Model](https://arxiv.org/abs/2510.20331)
*Kangli Wang,Qianxi Yi,Yuqi Ye,Shihao Li,Wei Gao*

Main category: cs.CV

TL;DR: AnyPcc introduces a universal framework for point cloud compression, addressing generalization challenges through robust context modeling and efficient handling of out-of-distribution data, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: The paper aims to tackle the generalization challenge in deep learning-based point cloud compression, focusing on improving robustness and handling out-of-distribution data.

Method: AnyPcc uses a Universal Context Model for robust dependencies and Instance-Adaptive Fine-Tuning (IAFT) to handle OOD data by fine-tuning a subset of network weights per instance.

Result: Extensive experiments on 15 datasets show AnyPcc achieves state-of-the-art performance in point cloud compression.

Conclusion: AnyPcc sets a new benchmark in point cloud compression, with plans to release code and datasets for reproducibility.

Abstract: Generalization remains a critical challenge for deep learning-based point
cloud geometry compression. We argue this stems from two key limitations: the
lack of robust context models and the inefficient handling of
out-of-distribution (OOD) data. To address both, we introduce AnyPcc, a
universal point cloud compression framework. AnyPcc first employs a Universal
Context Model that leverages priors from both spatial and channel-wise grouping
to capture robust contextual dependencies. Second, our novel Instance-Adaptive
Fine-Tuning (IAFT) strategy tackles OOD data by synergizing explicit and
implicit compression paradigms. It fine-tunes a small subset of network weights
for each instance and incorporates them into the bitstream, where the marginal
bit cost of the weights is dwarfed by the resulting savings in geometry
compression. Extensive experiments on a benchmark of 15 diverse datasets
confirm that AnyPcc sets a new state-of-the-art in point cloud compression. Our
code and datasets will be released to encourage reproducible research.

</details>


### [44] [AccuQuant: Simulating Multiple Denoising Steps for Quantizing Diffusion Models](https://arxiv.org/abs/2510.20348)
*Seunghoon Lee,Jeongwoo Choi,Byunggwan Son,Jaehyeon Moon,Jeimin Jeon,Bumsub Ham*

Main category: cs.CV

TL;DR: AccuQuant is a post-training quantization method for diffusion models that minimizes error accumulation over denoising steps, offering efficient implementation and improved performance.


<details>
  <summary>Details</summary>
Motivation: Quantization errors accumulate over denoising steps in diffusion models, degrading performance. AccuQuant aims to address this by minimizing discrepancies between full-precision and quantized models across multiple steps.

Method: AccuQuant simulates multiple denoising steps explicitly for quantization, focusing on accumulated errors. It includes an efficient implementation reducing memory complexity from O(n) to O(1).

Result: AccuQuant demonstrates efficacy and efficiency across various tasks and diffusion models on standard benchmarks.

Conclusion: AccuQuant effectively reduces quantization error accumulation in diffusion models, providing a practical solution with improved memory efficiency.

Abstract: We present in this paper a novel post-training quantization (PTQ) method,
dubbed AccuQuant, for diffusion models. We show analytically and empirically
that quantization errors for diffusion models are accumulated over denoising
steps in a sampling process. To alleviate the error accumulation problem,
AccuQuant minimizes the discrepancies between outputs of a full-precision
diffusion model and its quantized version within a couple of denoising steps.
That is, it simulates multiple denoising steps of a diffusion sampling process
explicitly for quantization, accounting the accumulated errors over multiple
denoising steps, which is in contrast to previous approaches to imitating a
training process of diffusion models, namely, minimizing the discrepancies
independently for each step. We also present an efficient implementation
technique for AccuQuant, together with a novel objective, which reduces a
memory complexity significantly from $\mathcal{O}(n)$ to $\mathcal{O}(1)$,
where $n$ is the number of denoising steps. We demonstrate the efficacy and
efficiency of AccuQuant across various tasks and diffusion models on standard
benchmarks.

</details>


### [45] [Positional Encoding Field](https://arxiv.org/abs/2510.20385)
*Yunpeng Bai,Haoxiang Li,Qixing Huang*

Main category: cs.CV

TL;DR: Diffusion Transformers (DiTs) use positional encodings (PEs) for visual generation, showing surprising independence of patch tokens. Introducing PE-Field improves 3D reasoning and control.


<details>
  <summary>Details</summary>
Motivation: Revisiting DiTs' spatial coherence led to discovering patch tokens' independence from PEs, inspiring PE-Field for better 3D modeling.

Method: PE-Field extends PEs to a 3D field with depth-aware and hierarchical encodings, enhancing DiTs' spatial reasoning.

Result: PE-Field-augmented DiTs achieve state-of-the-art in novel view synthesis and spatial image editing.

Conclusion: PE-Field improves DiTs by enabling direct 3D geometry modeling and finer spatial control.

Abstract: Diffusion Transformers (DiTs) have emerged as the dominant architecture for
visual generation, powering state-of-the-art image and video models. By
representing images as patch tokens with positional encodings (PEs), DiTs
combine Transformer scalability with spatial and temporal inductive biases. In
this work, we revisit how DiTs organize visual content and discover that patch
tokens exhibit a surprising degree of independence: even when PEs are
perturbed, DiTs still produce globally coherent outputs, indicating that
spatial coherence is primarily governed by PEs. Motivated by this finding, we
introduce the Positional Encoding Field (PE-Field), which extends positional
encodings from the 2D plane to a structured 3D field. PE-Field incorporates
depth-aware encodings for volumetric reasoning and hierarchical encodings for
fine-grained sub-patch control, enabling DiTs to model geometry directly in 3D
space. Our PE-Field-augmented DiT achieves state-of-the-art performance on
single-image novel view synthesis and generalizes to controllable spatial image
editing.

</details>


### [46] [Mitigating Cross-modal Representation Bias for Multicultural Image-to-Recipe Retrieval](https://arxiv.org/abs/2510.20393)
*Qing Wang,Chong-Wah Ngo,Yu Cao,Ee-Peng Lim*

Main category: cs.CV

TL;DR: The paper proposes a causal approach to address biases in image-to-recipe retrieval by predicting overlooked culinary elements and integrating them into cross-modal representation learning.


<details>
  <summary>Details</summary>
Motivation: Existing methods assume food images capture all recipe details, but they overlook non-visual nuances like ingredients and cooking methods, leading to biased representations.

Method: A novel causal approach predicts missing culinary elements from images and injects them into cross-modal representation learning.

Result: The method effectively uncovers subtle ingredients and cooking actions, improving retrieval performance on monolingual and multilingual datasets.

Conclusion: The causal approach mitigates biases in representation learning, enhancing recipe retrieval accuracy by incorporating overlooked culinary details.

Abstract: Existing approaches for image-to-recipe retrieval have the implicit
assumption that a food image can fully capture the details textually documented
in its recipe. However, a food image only reflects the visual outcome of a
cooked dish and not the underlying cooking process. Consequently, learning
cross-modal representations to bridge the modality gap between images and
recipes tends to ignore subtle, recipe-specific details that are not visually
apparent but are crucial for recipe retrieval. Specifically, the
representations are biased to capture the dominant visual elements, resulting
in difficulty in ranking similar recipes with subtle differences in use of
ingredients and cooking methods. The bias in representation learning is
expected to be more severe when the training data is mixed of images and
recipes sourced from different cuisines. This paper proposes a novel causal
approach that predicts the culinary elements potentially overlooked in images,
while explicitly injecting these elements into cross-modal representation
learning to mitigate biases. Experiments are conducted on the standard
monolingual Recipe1M dataset and a newly curated multilingual multicultural
cuisine dataset. The results indicate that the proposed causal representation
learning is capable of uncovering subtle ingredients and cooking actions and
achieves impressive retrieval performance on both monolingual and multilingual
multicultural datasets.

</details>


### [47] [Dynamic Weight Adjustment for Knowledge Distillation: Leveraging Vision Transformer for High-Accuracy Lung Cancer Detection and Real-Time Deployment](https://arxiv.org/abs/2510.20438)
*Saif Ur Rehman Khan,Muhammad Nabeel Asim,Sebastian Vollmer,Andreas Dengel*

Main category: cs.CV

TL;DR: The paper introduces FuzzyDistillViT-MobileNet, a dynamic fuzzy logic-driven knowledge distillation method for lung cancer classification, improving uncertainty handling and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To address uncertainty and complexity in lung cancer diagnosis by dynamically adjusting distillation weights using fuzzy logic, enhancing model focus on high-confidence regions.

Method: Uses ViT-B32 as instructor and MobileNet as student models, with dynamic wait adjustment, pixel-level image fusion techniques, and GA for model selection. Evaluated on LC25000 and IQOTH/NCCD datasets.

Result: Achieves 99.16% accuracy on LC25000 histopathological images and 99.54% on IQOTH/NCCD CT-scan images, demonstrating robustness.

Conclusion: The proposed model effectively handles uncertainty, improves computational efficiency, and achieves high accuracy across diverse lung cancer imaging domains.

Abstract: This paper presents the FuzzyDistillViT-MobileNet model, a novel approach for
lung cancer (LC) classification, leveraging dynamic fuzzy logic-driven
knowledge distillation (KD) to address uncertainty and complexity in disease
diagnosis. Unlike traditional models that rely on static KD with fixed weights,
our method dynamically adjusts the distillation weight using fuzzy logic,
enabling the student model to focus on high-confidence regions while reducing
attention to ambiguous areas. This dynamic adjustment improves the model
ability to handle varying uncertainty levels across different regions of LC
images. We employ the Vision Transformer (ViT-B32) as the instructor model,
which effectively transfers knowledge to the student model, MobileNet,
enhancing the student generalization capabilities. The training process is
further optimized using a dynamic wait adjustment mechanism that adapts the
training procedure for improved convergence and performance. To enhance image
quality, we introduce pixel-level image fusion improvement techniques such as
Gamma correction and Histogram Equalization. The processed images (Pix1 and
Pix2) are fused using a wavelet-based fusion method to improve image resolution
and feature preservation. This fusion method uses the wavedec2 function to
standardize images to a 224x224 resolution, decompose them into multi-scale
frequency components, and recursively average coefficients at each level for
better feature representation. To address computational efficiency, Genetic
Algorithm (GA) is used to select the most suitable pre-trained student model
from a pool of 12 candidates, balancing model performance with computational
cost. The model is evaluated on two datasets, including LC25000
histopathological images (99.16% accuracy) and IQOTH/NCCD CT-scan images
(99.54% accuracy), demonstrating robustness across different imaging domains.

</details>


### [48] [Conan: Progressive Learning to Reason Like a Detective over Multi-Scale Visual Evidence](https://arxiv.org/abs/2510.20470)
*Kun Ouyang,Yuanxin Liu,Linli Yao,Yishuo Cai,Hao Zhou,Jie Zhou,Fandong Meng,Xu Sun*

Main category: cs.CV

TL;DR: Conan is a framework for evidence-grounded multi-step video reasoning that outperforms baselines by 10% in accuracy, using a dataset and RL-based training.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with ungrounded conclusions or inaccurate evidence localization in video reasoning.

Method: Conan uses contextual/evidence frames, cross-frame clues, and adaptive decisions. It leverages Conan-91K dataset and AIR RLVR training.

Result: Conan surpasses Qwen2.5-VL-7B-Instruct by 10% accuracy on benchmarks and generalizes well.

Conclusion: Conan achieves state-of-the-art performance and scalability in video reasoning tasks.

Abstract: Video reasoning, which requires multi-step deduction across frames, remains a
major challenge for multimodal large language models (MLLMs). While
reinforcement learning (RL)-based methods enhance reasoning capabilities, they
often rely on text-only chains that yield ungrounded or hallucinated
conclusions. Conversely, frame-retrieval approaches introduce visual grounding
but still struggle with inaccurate evidence localization. To address these
challenges, we present Conan, a framework for evidence-grounded multi-step
video reasoning. Conan identifies contextual and evidence frames, reasons over
cross-frame clues, and adaptively decides when to conclude or explore further.
To achieve this, we (1) construct Conan-91K, a large-scale dataset of
automatically generated reasoning traces that includes frame identification,
evidence reasoning, and action decision, and (2) design a multi-stage
progressive cold-start strategy combined with an
Identification-Reasoning-Action (AIR) RLVR training framework to jointly
enhance multi-step visual reasoning. Extensive experiments on six multi-step
reasoning benchmarks demonstrate that Conan surpasses the baseline
Qwen2.5-VL-7B-Instruct by an average of over 10% in accuracy, achieving
state-of-the-art performance. Furthermore, Conan generalizes effectively to
long-video understanding tasks, validating its strong scalability and
robustness.

</details>


### [49] [Reliable and Reproducible Demographic Inference for Fairness in Face Analysis](https://arxiv.org/abs/2510.20482)
*Alexandre Fournier-Montgieux,Hervé Le Borgne,Adrian Popescu,Bertrand Luvison*

Main category: cs.CV

TL;DR: The paper proposes a modular transfer learning approach for demographic attribute inference (DAI) to improve fairness evaluation in face analysis systems (FAS). It introduces a robustness metric and outperforms baselines, especially for ethnicity inference.


<details>
  <summary>Details</summary>
Motivation: Fairness auditing relies on reliable DAI, but current methods depend on predefined demographic segmentation, limiting their validity.

Method: A modular transfer learning pipeline replaces end-to-end training, combining pretrained face recognition encoders with non-linear classifiers.

Result: The method outperforms baselines, particularly for challenging attributes like ethnicity, and introduces a robustness metric for consistency.

Conclusion: The work provides a reliable foundation for DAI in fairness auditing, promoting transparency with released datasets, code, and models.

Abstract: Fairness evaluation in face analysis systems (FAS) typically depends on
automatic demographic attribute inference (DAI), which itself relies on
predefined demographic segmentation. However, the validity of fairness auditing
hinges on the reliability of the DAI process. We begin by providing a
theoretical motivation for this dependency, showing that improved DAI
reliability leads to less biased and lower-variance estimates of FAS fairness.
To address this, we propose a fully reproducible DAI pipeline that replaces
conventional end-to-end training with a modular transfer learning approach. Our
design integrates pretrained face recognition encoders with non-linear
classification heads. We audit this pipeline across three dimensions: accuracy,
fairness, and a newly introduced notion of robustness, defined via
intra-identity consistency. The proposed robustness metric is applicable to any
demographic segmentation scheme. We benchmark the pipeline on gender and
ethnicity inference across multiple datasets and training setups. Our results
show that the proposed method outperforms strong baselines, particularly on
ethnicity, which is the more challenging attribute. To promote transparency and
reproducibility, we will publicly release the training dataset metadata, full
codebase, pretrained models, and evaluation toolkit. This work contributes a
reliable foundation for demographic inference in fairness auditing.

</details>


### [50] [EchoDistill: Bidirectional Concept Distillation for One-Step Diffusion Personalization](https://arxiv.org/abs/2510.20512)
*Yixiong Yang,Tao Wu,Senmao Li,Shiqi Yang,Yaxing Wang,Joost van de Weijer,Kai Wang*

Main category: cs.CV

TL;DR: EchoDistill is a bidirectional concept distillation framework enabling one-step diffusion personalization (1-SDP) by jointly training teacher (multi-step) and student (one-step) models, enhancing personalization and generative quality.


<details>
  <summary>Details</summary>
Motivation: Personalizing one-step text-to-image diffusion models is challenging due to limited capacity for capturing new concept distributions.

Method: Proposes EchoDistill, training teacher and student models simultaneously with shared text encoder, adversarial and alignment losses, and bidirectional echoing refinement.

Result: Significantly outperforms existing personalization methods in 1-SDP setup, improving both student and teacher models.

Conclusion: EchoDistill establishes a novel paradigm for rapid and effective personalization in T2I diffusion models.

Abstract: Recent advances in accelerating text-to-image (T2I) diffusion models have
enabled the synthesis of high-fidelity images even in a single step. However,
personalizing these models to incorporate novel concepts remains a challenge
due to the limited capacity of one-step models to capture new concept
distributions effectively. We propose a bidirectional concept distillation
framework, EchoDistill, to enable one-step diffusion personalization (1-SDP).
Our approach involves an end-to-end training process where a multi-step
diffusion model (teacher) and a one-step diffusion model (student) are trained
simultaneously. The concept is first distilled from the teacher model to the
student, and then echoed back from the student to the teacher. During the
EchoDistill, we share the text encoder between the two models to ensure
consistent semantic understanding. Following this, the student model is
optimized with adversarial losses to align with the real image distribution and
with alignment losses to maintain consistency with the teacher's output.
Furthermore, we introduce the bidirectional echoing refinement strategy,
wherein the student model leverages its faster generation capability to
feedback to the teacher model. This bidirectional concept distillation
mechanism not only enhances the student ability to personalize novel concepts
but also improves the generative quality of the teacher model. Our experiments
demonstrate that this collaborative framework significantly outperforms
existing personalization methods over the 1-SDP setup, establishing a novel
paradigm for rapid and effective personalization in T2I diffusion models.

</details>


### [51] [Metis-HOME: Hybrid Optimized Mixture-of-Experts for Multimodal Reasoning](https://arxiv.org/abs/2510.20519)
*Xiaohan Lan,Fanfan Liu,Haibo Qiu,Siqi Yang,Delian Ruan,Peng Shi,Lin Ma*

Main category: cs.CV

TL;DR: Metis-HOME introduces a Hybrid Optimized Mixture-of-Experts framework to enhance multimodal reasoning efficiency and generalization by dividing tasks between specialized and generalist branches.


<details>
  <summary>Details</summary>
Motivation: Address inefficiencies and generalization limitations in current multimodal reasoning models by optimizing task-specific and general reasoning capabilities.

Method: Proposes Metis-HOME, a hybrid framework with a thinking branch for complex reasoning and a non-thinking branch for rapid inference, using a dynamic router for query allocation.

Result: Improves both complex reasoning and general capabilities, reversing performance trade-offs seen in specialized models.

Conclusion: Metis-HOME provides a versatile solution for balancing reasoning and generalization in multimodal large language models.

Abstract: Inspired by recent advancements in LLM reasoning, the field of multimodal
reasoning has seen remarkable progress, achieving significant performance gains
on intricate tasks such as mathematical problem-solving. Despite this progress,
current multimodal large reasoning models exhibit two key limitations. They
tend to employ computationally expensive reasoning even for simple queries,
leading to inefficiency. Furthermore, this focus on specialized reasoning often
impairs their broader, more general understanding capabilities. In this paper,
we propose Metis-HOME: a Hybrid Optimized Mixture-of-Experts framework designed
to address this trade-off. Metis-HOME enables a ''Hybrid Thinking'' paradigm by
structuring the original dense model into two distinct expert branches: a
thinking branch tailored for complex, multi-step reasoning, and a non-thinking
branch optimized for rapid, direct inference on tasks like general VQA and OCR.
A lightweight, trainable router dynamically allocates queries to the most
suitable expert. We instantiate Metis-HOME by adapting the Qwen2.5-VL-7B into
an MoE architecture. Comprehensive evaluations reveal that our approach not
only substantially enhances complex reasoning abilities but also improves the
model's general capabilities, reversing the degradation trend observed in other
reasoning-specialized models. Our work establishes a new paradigm for building
powerful and versatile MLLMs, effectively resolving the prevalent
reasoning-vs-generalization dilemma.

</details>


### [52] [Fake-in-Facext: Towards Fine-Grained Explainable DeepFake Analysis](https://arxiv.org/abs/2510.20531)
*Lixiong Qin,Yang Zhang,Mei Wang,Jiani Hu,Weihong Deng,Weiran Xu*

Main category: cs.CV

TL;DR: The paper introduces FiFa, a framework improving Explainable DeepFake Analysis by enhancing fine-grained awareness through better data annotation and a novel unified model.


<details>
  <summary>Details</summary>
Motivation: Existing Explainable DeepFake Analysis lacks fine-grained awareness, unreliable annotations, and insufficient grounding in visual context.

Method: Proposes FiFa-Annotator for reliable fine-grained data annotation and FiFa-MLLM, a unified model supporting multimodal inputs/outputs for fine-grained analysis.

Result: FiFa-MLLM outperforms baselines on the AGE task and achieves SOTA performance on XDFA datasets.

Conclusion: FiFa enhances Explainable DeepFake Analysis with fine-grained awareness and reliable annotations, supported by open-source code/data.

Abstract: The advancement of Multimodal Large Language Models (MLLMs) has bridged the
gap between vision and language tasks, enabling the implementation of
Explainable DeepFake Analysis (XDFA). However, current methods suffer from a
lack of fine-grained awareness: the description of artifacts in data annotation
is unreliable and coarse-grained, and the models fail to support the output of
connections between textual forgery explanations and the visual evidence of
artifacts, as well as the input of queries for arbitrary facial regions. As a
result, their responses are not sufficiently grounded in Face Visual Context
(Facext). To address this limitation, we propose the Fake-in-Facext (FiFa)
framework, with contributions focusing on data annotation and model
construction. We first define a Facial Image Concept Tree (FICT) to divide
facial images into fine-grained regional concepts, thereby obtaining a more
reliable data annotation pipeline, FiFa-Annotator, for forgery explanation.
Based on this dedicated data annotation, we introduce a novel
Artifact-Grounding Explanation (AGE) task, which generates textual forgery
explanations interleaved with segmentation masks of manipulated artifacts. We
propose a unified multi-task learning architecture, FiFa-MLLM, to
simultaneously support abundant multimodal inputs and outputs for fine-grained
Explainable DeepFake Analysis. With multiple auxiliary supervision tasks,
FiFa-MLLM can outperform strong baselines on the AGE task and achieve SOTA
performance on existing XDFA datasets. The code and data will be made
open-source at https://github.com/lxq1000/Fake-in-Facext.

</details>


### [53] [Blur2seq: Blind Deblurring and Camera Trajectory Estimation from a Single Camera Motion-blurred Image](https://arxiv.org/abs/2510.20539)
*Guillermo Carbajal,Andrés Almansa,Pablo Musé*

Main category: cs.CV

TL;DR: The paper introduces a deep learning framework that jointly estimates sharp images and camera motion trajectories from blurry images, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Motion blur from camera shake under large or rotational movements is a significant challenge in image restoration. The authors aim to address this by jointly estimating the sharp image and camera motion trajectory.

Method: The method uses a Projective Motion Blur Model (PMBM) with a differentiable blur creation module. A neural network predicts a 3D rotation trajectory, guiding a model-based restoration network trained end-to-end. Post-inference optimization refines results using a reblur loss.

Result: The framework outperforms existing methods, especially for severe or spatially variant blur, and provides interpretability by revealing camera motion.

Conclusion: The proposed modular architecture is effective for deblurring, offering both high performance and interpretability, with code and models made publicly available.

Abstract: Motion blur caused by camera shake, particularly under large or rotational
movements, remains a major challenge in image restoration. We propose a deep
learning framework that jointly estimates the latent sharp image and the
underlying camera motion trajectory from a single blurry image. Our method
leverages the Projective Motion Blur Model (PMBM), implemented efficiently
using a differentiable blur creation module compatible with modern networks. A
neural network predicts a full 3D rotation trajectory, which guides a
model-based restoration network trained end-to-end. This modular architecture
provides interpretability by revealing the camera motion that produced the
blur. Moreover, this trajectory enables the reconstruction of the sequence of
sharp images that generated the observed blurry image. To further refine
results, we optimize the trajectory post-inference via a reblur loss, improving
consistency between the blurry input and the restored output. Extensive
experiments show that our method achieves state-of-the-art performance on both
synthetic and real datasets, particularly in cases with severe or spatially
variant blur, where end-to-end deblurring networks struggle.
  Code and trained models are available at
https://github.com/GuillermoCarbajal/Blur2Seq/

</details>


### [54] [Deep Learning-Powered Visual SLAM Aimed at Assisting Visually Impaired Navigation](https://arxiv.org/abs/2510.20549)
*Marziyeh Bamdad,Hans-Peter Hutter,Alireza Darvishy*

Main category: cs.CV

TL;DR: SELM-SLAM3, a deep learning-enhanced SLAM framework, improves robustness in low-texture, motion-blur, and challenging lighting conditions, outperforming ORB-SLAM3 and other RGB-D SLAM systems.


<details>
  <summary>Details</summary>
Motivation: Robust SLAM operation under difficult conditions (e.g., low-texture, motion-blur) is crucial for applications like assistive navigation for the visually impaired.

Method: Integrates SuperPoint and LightGlue for enhanced feature extraction and matching.

Result: Outperforms ORB-SLAM3 by 87.84% and other RGB-D SLAM systems by 36.77%.

Conclusion: SELM-SLAM3 offers reliable performance in challenging scenarios, advancing navigation aids for the visually impaired.

Abstract: Despite advancements in SLAM technologies, robust operation under challenging
conditions such as low-texture, motion-blur, or challenging lighting remains an
open challenge. Such conditions are common in applications such as assistive
navigation for the visually impaired. These challenges undermine localization
accuracy and tracking stability, reducing navigation reliability and safety. To
overcome these limitations, we present SELM-SLAM3, a deep learning-enhanced
visual SLAM framework that integrates SuperPoint and LightGlue for robust
feature extraction and matching. We evaluated our framework using TUM RGB-D,
ICL-NUIM, and TartanAir datasets, which feature diverse and challenging
scenarios. SELM-SLAM3 outperforms conventional ORB-SLAM3 by an average of
87.84% and exceeds state-of-the-art RGB-D SLAM systems by 36.77%. Our framework
demonstrates enhanced performance under challenging conditions, such as
low-texture scenes and fast motion, providing a reliable platform for
developing navigation aids for the visually impaired.

</details>


### [55] [From Cheap to Pro: A Learning-based Adaptive Camera Parameter Network for Professional-Style Imaging](https://arxiv.org/abs/2510.20550)
*Fuchen Li,Yansong Du,Wenbo Cheng,Xiaoxia Zhou,Sen Yin*

Main category: cs.CV

TL;DR: ACamera-Net is a lightweight, real-time network for optimizing camera exposure and white balance from RAW inputs, improving image quality under varied lighting conditions.


<details>
  <summary>Details</summary>
Motivation: Consumer cameras struggle with image quality under complex lighting, leading to issues like underexposure and color inconsistency, which affect downstream vision tasks.

Method: ACamera-Net uses two modules: ACamera-Exposure for ISO adjustment and ACamera-Color for color temperature and gain, both trained on annotated real-world data.

Result: The model enhances image quality and stabilizes perception outputs, outperforming traditional auto modes and baselines without extra modules.

Conclusion: ACamera-Net effectively addresses camera parameter optimization for better image quality in diverse lighting, suitable for edge devices.

Abstract: Consumer-grade camera systems often struggle to maintain stable image quality
under complex illumination conditions such as low light, high dynamic range,
and backlighting, as well as spatial color temperature variation. These issues
lead to underexposure, color casts, and tonal inconsistency, which degrade the
performance of downstream vision tasks. To address this, we propose
ACamera-Net, a lightweight and scene-adaptive camera parameter adjustment
network that directly predicts optimal exposure and white balance from RAW
inputs. The framework consists of two modules: ACamera-Exposure, which
estimates ISO to alleviate underexposure and contrast loss, and ACamera-Color,
which predicts correlated color temperature and gain factors for improved color
consistency. Optimized for real-time inference on edge devices, ACamera-Net can
be seamlessly integrated into imaging pipelines. Trained on diverse real-world
data with annotated references, the model generalizes well across lighting
conditions. Extensive experiments demonstrate that ACamera-Net consistently
enhances image quality and stabilizes perception outputs, outperforming
conventional auto modes and lightweight baselines without relying on additional
image enhancement modules.

</details>


### [56] [From Far and Near: Perceptual Evaluation of Crowd Representations Across Levels of Detail](https://arxiv.org/abs/2510.20558)
*Xiaohan Sun,Carol O'Sullivan*

Main category: cs.CV

TL;DR: Study on user perception of visual quality in crowd character representations (geometric meshes, impostors, NeRFs, 3D Gaussians) across LoD and viewing distances, revealing trade-offs between fidelity and performance.


<details>
  <summary>Details</summary>
Motivation: To understand how users perceive visual quality in crowd representations at varying LoD and distances, guiding perceptually optimized rendering strategies.

Method: Qualitative and quantitative analysis of geometric meshes, image-based impostors, NeRFs, and 3D Gaussians.

Result: Identified trade-offs between visual fidelity and computational performance for each representation.

Conclusion: Insights provided can optimize LoD strategies for crowd rendering based on perceptual quality.

Abstract: In this paper, we investigate how users perceive the visual quality of crowd
character representations at different levels of detail (LoD) and viewing
distances. Each representation: geometric meshes, image-based impostors, Neural
Radiance Fields (NeRFs), and 3D Gaussians, exhibits distinct trade-offs between
visual fidelity and computational performance. Our qualitative and quantitative
results provide insights to guide the design of perceptually optimized LoD
strategies for crowd rendering.

</details>


### [57] [EmbodiedBrain: Expanding Performance Boundaries of Task Planning for Embodied Intelligence](https://arxiv.org/abs/2510.20578)
*Ding Zou,Feifan Wang,Mengyu Ge,Siyuan Fan,Zongbing Zhang,Wei Chen,Lingfeng Wang,Zhongyou Hu,Wenrui Yan,Zhengwei Gao,Hao Wang,Weizhao Jin,Yu Zhang,Hainan Zhao,Mingliang Zhang,Xianxian Xi,Yaru Zhang,Wenyuan Li,Zhengguang Gao,Yurui Zhu*

Main category: cs.CV

TL;DR: EmbodiedBrain proposes a vision-language foundation model addressing limitations in current LLMs/MLLMs for embodied AI, integrating advanced training methods and evaluation benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current LLMs/MLLMs for embodied tasks lack alignment with agent needs, face trade-offs between latency and performance, and rely on unrealistic metrics.

Method: EmbodiedBrain uses agent-aligned data, SFT with Step-GRPO for long-horizon tasks, and a GRM-based reward system.

Result: The model outperforms benchmarks in general, planning, and simulation tasks, setting a new SOTA.

Conclusion: EmbodiedBrain advances embodied AI by releasing open-source data, models, and evaluation tools for future research.

Abstract: The realization of Artificial General Intelligence (AGI) necessitates
Embodied AI agents capable of robust spatial perception, effective task
planning, and adaptive execution in physical environments. However, current
large language models (LLMs) and multimodal LLMs (MLLMs) for embodied tasks
suffer from key limitations, including a significant gap between model design
and agent requirements, an unavoidable trade-off between real-time latency and
performance, and the use of unauthentic, offline evaluation metrics. To address
these challenges, we propose EmbodiedBrain, a novel vision-language foundation
model available in both 7B and 32B parameter sizes. Our framework features an
agent-aligned data structure and employs a powerful training methodology that
integrates large-scale Supervised Fine-Tuning (SFT) with Step-Augumented Group
Relative Policy Optimization (Step-GRPO), which boosts long-horizon task
success by integrating preceding steps as Guided Precursors. Furthermore, we
incorporate a comprehensive reward system, including a Generative Reward Model
(GRM) accelerated at the infrastructure level, to improve training efficiency.
For enable thorough validation, we establish a three-part evaluation system
encompassing General, Planning, and End-to-End Simulation Benchmarks,
highlighted by the proposal and open-sourcing of a novel, challenging
simulation environment. Experimental results demonstrate that EmbodiedBrain
achieves superior performance across all metrics, establishing a new
state-of-the-art for embodied foundation models. Towards paving the way for the
next generation of generalist embodied agents, we open-source all of our data,
model weight, and evaluating methods, which are available at
https://zterobot.github.io/EmbodiedBrain.github.io.

</details>


### [58] [Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence](https://arxiv.org/abs/2510.20579)
*Jiahao Meng,Xiangtai Li,Haochen Wang,Yue Tan,Tao Zhang,Lingdong Kong,Yunhai Tong,Anran Wang,Zhiyang Teng,Yujing Wang,Zhuochen Wang*

Main category: cs.CV

TL;DR: Open-o3 Video integrates spatio-temporal evidence into video reasoning, using curated datasets and reinforcement learning to achieve state-of-the-art performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing video reasoning models lack explicit spatio-temporal evidence, making it challenging to track and localize key evidence dynamically.

Method: The framework uses curated datasets (STGR-CoT-30k and STGR-RL-36k) and a cold-start reinforcement learning strategy with specialized rewards for accuracy, temporal alignment, and spatial precision.

Result: Open-o3 Video achieves SOTA performance, improving mAM by 14.4% and mLGM by 24.2% on benchmarks like V-STAR, VideoMME, and others.

Conclusion: The model not only enhances accuracy but also provides reliable reasoning traces for test-time scaling and verification.

Abstract: Most video reasoning models only generate textual reasoning traces without
indicating when and where key evidence appears. Recent models such as OpenAI-o3
have sparked wide interest in evidence-centered reasoning for images, yet
extending this ability to videos is more challenging, as it requires joint
temporal tracking and spatial localization across dynamic scenes. We introduce
Open-o3 Video, a non-agent framework that integrates explicit spatio-temporal
evidence into video reasoning, and carefully collect training data and design
training strategies to address the aforementioned challenges. The model
highlights key timestamps, objects, and bounding boxes alongside its answers,
allowing reasoning to be grounded in concrete visual observations. To enable
this functionality, we first curate and build two high-quality datasets,
STGR-CoT-30k for SFT and STGR-RL-36k for RL, with carefully constructed
temporal and spatial annotations, since most existing datasets offer either
temporal spans for videos or spatial boxes on images, lacking unified
spatio-temporal supervision and reasoning traces. Then, we adopt a cold-start
reinforcement learning strategy with multiple specially designed rewards that
jointly encourage answer accuracy, temporal alignment, and spatial precision.
On V-STAR benchmark, Open-o3 Video achieves state-of-the-art performance,
raising mAM by 14.4% and mLGM by 24.2% on the Qwen2.5-VL baseline. Consistent
improvements are also observed on a broad range of video understanding
benchmarks, including VideoMME, WorldSense, VideoMMMU, and TVGBench. Beyond
accuracy, the reasoning traces produced by Open-o3 Video also provide valuable
signals for test-time scaling, enabling confidence-aware verification and
improving answer reliability.

</details>


### [59] [GenColorBench: A Color Evaluation Benchmark for Text-to-Image Generation Models](https://arxiv.org/abs/2510.20586)
*Muhammad Atif Butt,Alexandra Gomez-Villa,Tao Wu,Javier Vazquez-Corral,Joost Van De Weijer,Kai Wang*

Main category: cs.CV

TL;DR: GenColorBench is introduced as the first comprehensive benchmark for evaluating text-to-image models' color precision, addressing gaps in existing benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image models lack fine-grained color controllability, impacting applications requiring precise color matching (e.g., branding). Existing benchmarks overlook systematic color precision evaluation.

Method: Proposes GenColorBench, grounded in color systems (ISCC-NBS, CSS3/X11), with 44K prompts covering 400+ colors. Includes perceptual and automated assessments.

Result: Evaluations reveal performance variations in popular models, identifying strengths and failure modes in color generation.

Conclusion: GenColorBench provides a tool to improve precise color generation in text-to-image models and will be publicly released.

Abstract: Recent years have seen impressive advances in text-to-image generation, with
image generative or unified models producing high-quality images from text. Yet
these models still struggle with fine-grained color controllability, often
failing to accurately match colors specified in text prompts. While existing
benchmarks evaluate compositional reasoning and prompt adherence, none
systematically assess color precision. Color is fundamental to human visual
perception and communication, critical for applications from art to design
workflows requiring brand consistency. However, current benchmarks either
neglect color or rely on coarse assessments, missing key capabilities such as
interpreting RGB values or aligning with human expectations. To this end, we
propose GenColorBench, the first comprehensive benchmark for text-to-image
color generation, grounded in color systems like ISCC-NBS and CSS3/X11,
including numerical colors which are absent elsewhere. With 44K color-focused
prompts covering 400+ colors, it reveals models' true capabilities via
perceptual and automated assessments. Evaluations of popular text-to-image
models using GenColorBench show performance variations, highlighting which
color conventions models understand best and identifying failure modes. Our
GenColorBench assessments will guide improvements in precise color generation.
The benchmark will be made public upon acceptance.

</details>


### [60] [Unsupervised Domain Adaptation via Similarity-based Prototypes for Cross-Modality Segmentation](https://arxiv.org/abs/2510.20596)
*Ziyu Ye,Chen Ju,Chaofan Ma,Xiaoyun Zhang*

Main category: cs.CV

TL;DR: A novel framework for cross-modality segmentation uses similarity-based prototypes to reduce domain gap and improve performance.


<details>
  <summary>Details</summary>
Motivation: Address performance degradation of deep learning models on unseen data due to domain shift, avoiding costly annotations.

Method: Learns class-wise prototypes in an embedding space with similarity constraints, uses dictionaries for prototype storage, and enables contrastive learning.

Result: Extensive experiments show superior performance compared to state-of-the-art methods.

Conclusion: The proposed framework effectively reduces the domain gap and enhances segmentation accuracy.

Abstract: Deep learning models have achieved great success on various vision
challenges, but a well-trained model would face drastic performance degradation
when applied to unseen data. Since the model is sensitive to domain shift,
unsupervised domain adaptation attempts to reduce the domain gap and avoid
costly annotation of unseen domains. This paper proposes a novel framework for
cross-modality segmentation via similarity-based prototypes. In specific, we
learn class-wise prototypes within an embedding space, then introduce a
similarity constraint to make these prototypes representative for each semantic
class while separable from different classes. Moreover, we use dictionaries to
store prototypes extracted from different images, which prevents the
class-missing problem and enables the contrastive learning of prototypes, and
further improves performance. Extensive experiments show that our method
achieves better results than other state-of-the-art methods.

</details>


### [61] [OnlineSplatter: Pose-Free Online 3D Reconstruction for Free-Moving Objects](https://arxiv.org/abs/2510.20605)
*Mark He Huang,Lin Geng Foo,Christian Theobalt,Ying Sun,De Wen Soh*

Main category: cs.CV

TL;DR: OnlineSplatter is an online feed-forward framework for reconstructing free-moving objects from monocular video without camera pose or depth priors, using high-quality 3D Gaussians.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in reconstructing free-moving objects from monocular video without reliable pose or depth cues and under arbitrary motion.

Method: The framework anchors reconstruction on the first frame and refines it progressively via a dense Gaussian primitive field. A dual-key memory module combines latent and explicit keys to fuse features effectively.

Result: OnlineSplatter outperforms state-of-the-art pose-free reconstruction baselines, improving with more observations while maintaining efficiency.

Conclusion: The approach enables robust, high-quality reconstruction of free-moving objects with constant computational cost.

Abstract: Free-moving object reconstruction from monocular video remains challenging,
particularly without reliable pose or depth cues and under arbitrary object
motion. We introduce OnlineSplatter, a novel online feed-forward framework
generating high-quality, object-centric 3D Gaussians directly from RGB frames
without requiring camera pose, depth priors, or bundle optimization. Our
approach anchors reconstruction using the first frame and progressively refines
the object representation through a dense Gaussian primitive field, maintaining
constant computational cost regardless of video sequence length. Our core
contribution is a dual-key memory module combining latent appearance-geometry
keys with explicit directional keys, robustly fusing current frame features
with temporally aggregated object states. This design enables effective
handling of free-moving objects via spatial-guided memory readout and an
efficient sparsification mechanism, ensuring comprehensive yet compact object
coverage. Evaluations on real-world datasets demonstrate that OnlineSplatter
significantly outperforms state-of-the-art pose-free reconstruction baselines,
consistently improving with more observations while maintaining constant memory
and runtime.

</details>


### [62] [SeViCES: Unifying Semantic-Visual Evidence Consensus for Long Video Understanding](https://arxiv.org/abs/2510.20622)
*Yuan Sheng,Yanbin Hao,Chenxu Li,Shuo Wang,Xiangnan He*

Main category: cs.CV

TL;DR: SeViCES improves long video understanding by selecting informative frames using semantic-visual consensus, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Long videos are complex and computationally intensive for current Video-LLMs, requiring better frame selection methods.

Method: SeViCES uses semantic-visual consensus for frame selection (SVCFS) and refines answers (ACR) to resolve inconsistencies.

Result: SeViCES outperforms state-of-the-art methods in accuracy and robustness.

Conclusion: Consensus-driven evidence selection enhances Video-LLMs' performance in long video understanding.

Abstract: Long video understanding remains challenging due to its complex, diverse, and
temporally scattered content. Although video large language models (Video-LLMs)
can process videos lasting tens of minutes, applying them to truly long
sequences is computationally prohibitive and often leads to unfocused or
inconsistent reasoning. A promising solution is to select only the most
informative frames, yet existing approaches typically ignore temporal
dependencies or rely on unimodal evidence, limiting their ability to provide
complete and query-relevant context. We propose a Semantic-Visual Consensus
Evidence Selection (SeViCES) framework for effective and reliable long video
understanding. SeViCES is training-free and model-agnostic, and introduces two
key components. The Semantic-Visual Consensus Frame Selection (SVCFS) module
selects frames through (1) a temporal-aware semantic branch that leverages LLM
reasoning over captions, and (2) a cluster-guided visual branch that aligns
embeddings with semantic scores via mutual information. The Answer Consensus
Refinement (ACR) module further resolves inconsistencies between semantic- and
visual-based predictions by fusing evidence and constraining the answer space.
Extensive experiments on long video understanding benchmarks show that SeViCES
consistently outperforms state-of-the-art methods in both accuracy and
robustness, demonstrating the importance of consensus-driven evidence selection
for Video-LLMs.

</details>


### [63] [Deep Learning in Dental Image Analysis: A Systematic Review of Datasets, Methodologies, and Emerging Challenges](https://arxiv.org/abs/2510.20634)
*Zhenhuan Zhou,Jingbo Zhu,Yuchen Zhang,Xiaohang Guan,Peng Wang,Tao Li*

Main category: cs.CV

TL;DR: The paper reviews AI-based automated dental image analysis (DIA) using deep learning (DL), covering datasets, models, challenges, and future directions.


<details>
  <summary>Details</summary>
Motivation: Improve dental diagnosis and treatment planning by addressing challenges like low contrast, artifacts, and clinician subjectivity through AI-driven automation.

Method: Systematic review of 260 studies (49 on datasets, 211 on DL algorithms), analyzing DL techniques, network architectures, and performance metrics.

Result: Comprehensive summary of DL applications in DIA, including dataset characteristics, model categorizations, and evaluation metrics.

Conclusion: Highlights current challenges and future directions, providing a valuable reference for researchers in AI-driven dental imaging.

Abstract: Efficient analysis and processing of dental images are crucial for dentists
to achieve accurate diagnosis and optimal treatment planning. However, dental
imaging inherently poses several challenges, such as low contrast, metallic
artifacts, and variations in projection angles. Combined with the subjectivity
arising from differences in clinicians' expertise, manual interpretation often
proves time-consuming and prone to inconsistency. Artificial intelligence
(AI)-based automated dental image analysis (DIA) offers a promising solution to
these issues and has become an integral part of computer-aided dental diagnosis
and treatment. Among various AI technologies, deep learning (DL) stands out as
the most widely applied and influential approach due to its superior feature
extraction and representation capabilities. To comprehensively summarize recent
progress in this field, we focus on the two fundamental aspects of DL
research-datasets and models. In this paper, we systematically review 260
studies on DL applications in DIA, including 49 papers on publicly available
dental datasets and 211 papers on DL-based algorithms. We first introduce the
basic concepts of dental imaging and summarize the characteristics and
acquisition methods of existing datasets. Then, we present the foundational
techniques of DL and categorize relevant models and algorithms according to
different DIA tasks, analyzing their network architectures, optimization
strategies, training methods, and performance. Furthermore, we summarize
commonly used training and evaluation metrics in the DIA domain. Finally, we
discuss the current challenges of existing research and outline potential
future directions. We hope that this work provides a valuable and systematic
reference for researchers in this field. All supplementary materials and
detailed comparison tables will be made publicly available on GitHub.

</details>


### [64] [UltraHR-100K: Enhancing UHR Image Synthesis with A Large-Scale High-Quality Dataset](https://arxiv.org/abs/2510.20661)
*Chen Zhao,En Ci,Yunzhe Xu,Tiehan Fan,Shanyan Guan,Yanhao Ge,Jian Yang,Ying Tai*

Main category: cs.CV

TL;DR: The paper addresses challenges in UHR T2I generation by introducing a high-quality dataset (UltraHR-100K) and proposing a frequency-aware post-training method (DOTS and SWFR) to enhance fine-detail synthesis.


<details>
  <summary>Details</summary>
Motivation: To overcome the lack of large-scale high-quality UHR T2I datasets and improve fine-grained detail synthesis in UHR scenarios.

Method: Introduces UltraHR-100K dataset and proposes a frequency-aware post-training method with Detail-Oriented Timestep Sampling (DOTS) and Soft-Weighting Frequency Regularization (SWFR).

Result: Significant improvement in fine-grained detail quality and overall fidelity of UHR image generation, validated on UltraHR-eval4K benchmarks.

Conclusion: The proposed dataset and training strategies effectively address key challenges in UHR T2I generation, leading to better detail preservation and image quality.

Abstract: Ultra-high-resolution (UHR) text-to-image (T2I) generation has seen notable
progress. However, two key challenges remain : 1) the absence of a large-scale
high-quality UHR T2I dataset, and (2) the neglect of tailored training
strategies for fine-grained detail synthesis in UHR scenarios. To tackle the
first challenge, we introduce \textbf{UltraHR-100K}, a high-quality dataset of
100K UHR images with rich captions, offering diverse content and strong visual
fidelity. Each image exceeds 3K resolution and is rigorously curated based on
detail richness, content complexity, and aesthetic quality. To tackle the
second challenge, we propose a frequency-aware post-training method that
enhances fine-detail generation in T2I diffusion models. Specifically, we
design (i) \textit{Detail-Oriented Timestep Sampling (DOTS)} to focus learning
on detail-critical denoising steps, and (ii) \textit{Soft-Weighting Frequency
Regularization (SWFR)}, which leverages Discrete Fourier Transform (DFT) to
softly constrain frequency components, encouraging high-frequency detail
preservation. Extensive experiments on our proposed UltraHR-eval4K benchmarks
demonstrate that our approach significantly improves the fine-grained detail
quality and overall fidelity of UHR image generation. The code is available at
\href{https://github.com/NJU-PCALab/UltraHR-100k}{here}.

</details>


### [65] [HybridSOMSpikeNet: A Deep Model with Differentiable Soft Self-Organizing Maps and Spiking Dynamics for Waste Classification](https://arxiv.org/abs/2510.20669)
*Debojyoti Ghosh,Adrijit Goswami*

Main category: cs.CV

TL;DR: HybridSOMSpikeNet, a hybrid deep learning framework, achieves 97.39% accuracy in waste classification, enhancing recycling efficiency and sustainability.


<details>
  <summary>Details</summary>
Motivation: Accurate waste classification is crucial for sustainable waste management, reducing landfill accumulation and greenhouse gas emissions.

Method: The study combines convolutional feature extraction (ResNet-152), differentiable self-organization (Soft-SOM), and spiking-inspired temporal processing for energy-efficient and intelligent classification.

Result: HybridSOMSpikeNet outperforms state-of-the-art models with 97.39% accuracy and supports real-world deployment.

Conclusion: The framework offers environmental benefits by improving recycling efficiency and aligning with UN sustainability goals (SDG 11 and SDG 12).

Abstract: Accurate waste classification is vital for achieving sustainable waste
management and reducing the environmental footprint of urbanization.
Misclassification of recyclable materials contributes to landfill accumulation,
inefficient recycling, and increased greenhouse gas emissions. To address these
issues, this study introduces HybridSOMSpikeNet, a hybrid deep learning
framework that integrates convolutional feature extraction, differentiable
self-organization, and spiking-inspired temporal processing to enable
intelligent and energy-efficient waste classification. The proposed model
employs a pre-trained ResNet-152 backbone to extract deep spatial
representations, followed by a Differentiable Soft Self-Organizing Map
(Soft-SOM) that enhances topological clustering and interpretability. A spiking
neural head accumulates temporal activations over discrete time steps,
improving robustness and generalization. Trained on a ten-class waste dataset,
HybridSOMSpikeNet achieved a test accuracy of 97.39%, outperforming several
state-of-the-art architectures while maintaining a lightweight computational
profile suitable for real-world deployment. Beyond its technical innovations,
the framework provides tangible environmental benefits. By enabling precise and
automated waste segregation, it supports higher recycling efficiency, reduces
contamination in recyclable streams, and minimizes the ecological and
operational costs of waste processing. The approach aligns with global
sustainability priorities, particularly the United Nations Sustainable
Development Goals (SDG 11 and SDG 12), by contributing to cleaner cities,
circular economy initiatives, and intelligent environmental management systems.

</details>


### [66] [Efficient Multi-bit Quantization Network Training via Weight Bias Correction and Bit-wise Coreset Sampling](https://arxiv.org/abs/2510.20673)
*Jinhee Kim,Jae Jun An,Kang Eun Jeon,Jong Hwan Ko*

Main category: cs.CV

TL;DR: The paper proposes techniques to reduce training overhead in multi-bit quantization networks by correcting weight bias and using bit-wise coreset sampling, achieving competitive accuracy with up to 7.88x faster training.


<details>
  <summary>Details</summary>
Motivation: Existing multi-bit quantization methods suffer from high training overhead due to repeated full-dataset updates for each bit-width and extra fine-tuning stages.

Method: Introduces weight bias correction and bit-wise coreset sampling to reduce training time while maintaining accuracy.

Result: Experiments on CIFAR-10/100, TinyImageNet, and ImageNet-1K show competitive or superior accuracy with up to 7.88x faster training.

Conclusion: The proposed methods effectively reduce training overhead without compromising model utility.

Abstract: Multi-bit quantization networks enable flexible deployment of deep neural
networks by supporting multiple precision levels within a single model.
However, existing approaches suffer from significant training overhead as
full-dataset updates are repeated for each supported bit-width, resulting in a
cost that scales linearly with the number of precisions. Additionally, extra
fine-tuning stages are often required to support additional or intermediate
precision options, further compounding the overall training burden. To address
this issue, we propose two techniques that greatly reduce the training overhead
without compromising model utility: (i) Weight bias correction enables shared
batch normalization and eliminates the need for fine-tuning by neutralizing
quantization-induced bias across bit-widths and aligning activation
distributions; and (ii) Bit-wise coreset sampling strategy allows each child
model to train on a compact, informative subset selected via gradient-based
importance scores by exploiting the implicit knowledge transfer phenomenon.
Experiments on CIFAR-10/100, TinyImageNet, and ImageNet-1K with both ResNet and
ViT architectures demonstrate that our method achieves competitive or superior
accuracy while reducing training time up to 7.88x. Our code is released at
https://github.com/a2jinhee/EMQNet_jk.

</details>


### [67] [Diagnosing Visual Reasoning: Challenges, Insights, and a Path Forward](https://arxiv.org/abs/2510.20696)
*Jing Bi,Guangyu Sun,Ali Vosoughi,Chen Chen,Chenliang Xu*

Main category: cs.CV

TL;DR: Summary: The paper diagnoses issues in MLLMs like visual hallucinations and textual bias, proposes an agent-based architecture with visual modules, and shows improved performance over baselines.


<details>
  <summary>Details</summary>
Motivation: Multimodal LLMs suffer from visual hallucinations and over-reliance on textual priors, prompting the need for better integration of visual reasoning.

Method: A three-stage evaluation framework diagnoses issues, leading to an agent-based architecture combining LLM reasoning with lightweight visual modules for iterative refinement.

Result: The proposed system achieves significant gains (+10.3 on MMMU, +6.0 on MathVista) and matches or surpasses larger models.

Conclusion: Future visual reasoning models should integrate specialized tools for visual content analysis; the framework will be released for further research.

Abstract: Multimodal large language models (MLLMs) that integrate visual and textual
reasoning leverage chain-of-thought (CoT) prompting to tackle complex visual
tasks, yet continue to exhibit visual hallucinations and an over-reliance on
textual priors. We present a systematic diagnosis of state-of-the-art
vision-language models using a three-stage evaluation framework, uncovering key
failure modes. To address these, we propose an agent-based architecture that
combines LLM reasoning with lightweight visual modules, enabling fine-grained
analysis and iterative refinement of reasoning chains. Our results highlight
future visual reasoning models should focus on integrating a broader set of
specialized tools for analyzing visual content. Our system achieves significant
gains (+10.3 on MMMU, +6.0 on MathVista over a 7B baseline), matching or
surpassing much larger models. We will release our framework and evaluation
suite to facilitate future research.

</details>


### [68] [Mixing Importance with Diversity: Joint Optimization for KV Cache Compression in Large Vision-Language Models](https://arxiv.org/abs/2510.20707)
*Xuyang Liu,Xiyan Gui,Yuchao Zhang,Linfeng Zhang*

Main category: cs.CV

TL;DR: MixKV introduces a novel method combining importance and diversity for KV cache compression in LVLMs, outperforming baselines across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing KV cache compression methods overlook modality-specific redundancy, limiting semantic coverage and scalability.

Method: MixKV balances importance and diversity, adapting to head-wise redundancy in LVLMs' KV caches.

Result: MixKV improves baseline methods by 5.1% on average and achieves up to 9.0% gains in specific tasks.

Conclusion: MixKV effectively addresses KV cache compression limitations, enhancing performance while maintaining efficiency.

Abstract: Recent large vision-language models (LVLMs) demonstrate remarkable
capabilities in processing extended multi-modal sequences, yet the resulting
key-value (KV) cache expansion creates a critical memory bottleneck that
fundamentally limits deployment scalability. While existing KV cache
compression methods focus on retaining high-importance KV pairs to minimize
storage, they often overlook the modality-specific semantic redundancy patterns
that emerge distinctively in multi-modal KV caches. In this work, we first
analyze how, beyond simple importance, the KV cache in LVLMs exhibits varying
levels of redundancy across attention heads. We show that relying solely on
importance can only cover a subset of the full KV cache information
distribution, leading to potential loss of semantic coverage. To address this,
we propose \texttt{MixKV}, a novel method that mixes importance with diversity
for optimized KV cache compression in LVLMs. \texttt{MixKV} adapts to head-wise
semantic redundancy, selectively balancing diversity and importance when
compressing KV pairs. Extensive experiments demonstrate that \texttt{MixKV}
consistently enhances existing methods across multiple LVLMs. Under extreme
compression (budget=64), \texttt{MixKV} improves baseline methods by an average
of \textbf{5.1\%} across five multi-modal understanding benchmarks and achieves
remarkable gains of \textbf{8.0\%} and \textbf{9.0\%} for SnapKV and AdaKV on
GUI grounding tasks, all while maintaining comparable inference efficiency.
Furthermore, \texttt{MixKV} extends seamlessly to LLMs with comparable
performance gains. Our code is available at
\href{https://github.com/xuyang-liu16/MixKV}{\textcolor{citeblue}{https://github.com/xuyang-liu16/MixKV}}.

</details>


### [69] [ALICE-LRI: A General Method for Lossless Range Image Generation for Spinning LiDAR Sensors without Calibration Metadata](https://arxiv.org/abs/2510.20708)
*Samuel Soutullo,Miguel Yermo,David L. Vilariño,Óscar G. Lorenzo,José C. Cabaleiro,Francisco F. Rivera*

Main category: cs.CV

TL;DR: ALICE-LRI introduces a sensor-agnostic method for lossless range image generation from LiDAR point clouds, eliminating information loss and enabling complete point cloud reconstruction.


<details>
  <summary>Details</summary>
Motivation: Conventional LiDAR projection methods cause irreversible geometric inconsistencies and information loss, limiting high-fidelity applications.

Method: ALICE-LRI reverse-engineers LiDAR sensor intrinsic geometry (laser beam configuration, angular distributions, per-beam corrections) without requiring manufacturer data, enabling lossless projection.

Result: Achieves zero point loss and geometric accuracy within sensor limits across KITTI and DurLAR datasets. Demonstrates real-time performance and downstream benefits like improved compression.

Conclusion: ALICE-LRI shifts from approximate to lossless LiDAR projections, enabling high-precision remote sensing applications.

Abstract: 3D LiDAR sensors are essential for autonomous navigation, environmental
monitoring, and precision mapping in remote sensing applications. To
efficiently process the massive point clouds generated by these sensors, LiDAR
data is often projected into 2D range images that organize points by their
angular positions and distances. While these range image representations enable
efficient processing, conventional projection methods suffer from fundamental
geometric inconsistencies that cause irreversible information loss,
compromising high-fidelity applications. We present ALICE-LRI (Automatic LiDAR
Intrinsic Calibration Estimation for Lossless Range Images), the first general,
sensor-agnostic method that achieves lossless range image generation from
spinning LiDAR point clouds without requiring manufacturer metadata or
calibration files. Our algorithm automatically reverse-engineers the intrinsic
geometry of any spinning LiDAR sensor by inferring critical parameters
including laser beam configuration, angular distributions, and per-beam
calibration corrections, enabling lossless projection and complete point cloud
reconstruction with zero point loss. Comprehensive evaluation across the
complete KITTI and DurLAR datasets demonstrates that ALICE-LRI achieves perfect
point preservation, with zero points lost across all point clouds. Geometric
accuracy is maintained well within sensor precision limits, establishing
geometric losslessness with real-time performance. We also present a
compression case study that validates substantial downstream benefits,
demonstrating significant quality improvements in practical applications. This
paradigm shift from approximate to lossless LiDAR projections opens new
possibilities for high-precision remote sensing applications requiring complete
geometric preservation.

</details>


### [70] [AutoScape: Geometry-Consistent Long-Horizon Scene Generation](https://arxiv.org/abs/2510.20726)
*Jiacheng Chen,Ziyu Jiang,Mingfu Liang,Bingbing Zhuang,Jong-Chyi Su,Sparsh Garg,Ying Wu,Manmohan Chandraker*

Main category: cs.CV

TL;DR: AutoScape is a framework for generating long-horizon driving scenes using RGB-D diffusion to create consistent keyframes, achieving better metrics than prior methods.


<details>
  <summary>Details</summary>
Motivation: The aim is to improve long-horizon driving scene generation by ensuring geometric consistency and realism.

Method: Uses RGB-D diffusion for keyframes, conditions on existing geometry, employs warp-consistent guidance, and interpolates with video diffusion.

Result: Produces 20+ second realistic driving videos, improving FID and FVD scores by 48.6% and 43.0%.

Conclusion: AutoScape effectively enhances long-horizon driving scene generation with superior consistency and realism.

Abstract: This paper proposes AutoScape, a long-horizon driving scene generation
framework. At its core is a novel RGB-D diffusion model that iteratively
generates sparse, geometrically consistent keyframes, serving as reliable
anchors for the scene's appearance and geometry. To maintain long-range
geometric consistency, the model 1) jointly handles image and depth in a shared
latent space, 2) explicitly conditions on the existing scene geometry (i.e.,
rendered point clouds) from previously generated keyframes, and 3) steers the
sampling process with a warp-consistent guidance. Given high-quality RGB-D
keyframes, a video diffusion model then interpolates between them to produce
dense and coherent video frames. AutoScape generates realistic and
geometrically consistent driving videos of over 20 seconds, improving the
long-horizon FID and FVD scores over the prior state-of-the-art by 48.6\% and
43.0\%, respectively.

</details>


### [71] [ACS-SegNet: An Attention-Based CNN-SegFormer Segmentation Network for Tissue Segmentation in Histopathology](https://arxiv.org/abs/2510.20754)
*Nima Torbati,Anastasia Meshcheryakova,Ramona Woitek,Diana Mechtcheriakova,Amirreza Mahbod*

Main category: cs.CV

TL;DR: The paper introduces a novel dual-encoder model combining CNNs and ViTs with attention-driven feature fusion to improve semantic segmentation in histopathological images, achieving superior performance on public datasets.


<details>
  <summary>Details</summary>
Motivation: Automated histopathological image analysis is crucial for disease diagnosis, and deep learning methods have shown promise. The study aims to enhance segmentation performance by integrating CNNs and ViTs.

Method: Proposes a dual-encoder model with attention-driven feature fusion of CNNs and ViTs for semantic segmentation in histological images.

Result: Achieved μIoU/μDice scores of 76.79%/86.87% on GCPS and 64.93%/76.60% on PUMA, outperforming benchmarks.

Conclusion: The proposed model demonstrates improved performance in semantic segmentation for histopathological images, with publicly available implementation.

Abstract: Automated histopathological image analysis plays a vital role in
computer-aided diagnosis of various diseases. Among developed algorithms, deep
learning-based approaches have demonstrated excellent performance in multiple
tasks, including semantic tissue segmentation in histological images. In this
study, we propose a novel approach based on attention-driven feature fusion of
convolutional neural networks (CNNs) and vision transformers (ViTs) within a
unified dual-encoder model to improve semantic segmentation performance.
Evaluation on two publicly available datasets showed that our model achieved
{\mu}IoU/{\mu}Dice scores of 76.79%/86.87% on the GCPS dataset and
64.93%/76.60% on the PUMA dataset, outperforming state-of-the-art and baseline
benchmarks. The implementation of our method is publicly available in a GitHub
repository: https://github.com/NimaTorbati/ACS-SegNet

</details>


### [72] [DyPE: Dynamic Position Extrapolation for Ultra High Resolution Diffusion](https://arxiv.org/abs/2510.20766)
*Noam Issachar,Guy Yariv,Sagie Benaim,Yossi Adi,Dani Lischinski,Raanan Fattal*

Main category: cs.CV

TL;DR: DyPE is a training-free method enabling diffusion transformers to generate ultra-high-resolution images beyond their training data, dynamically adjusting positional encoding for state-of-the-art fidelity.


<details>
  <summary>Details</summary>
Motivation: Training diffusion transformers at ultra-high resolutions is costly due to quadratic scaling of self-attention. DyPE addresses this without additional training or sampling costs.

Method: DyPE dynamically adjusts positional encoding during diffusion steps to match the frequency spectrum of the generative process, leveraging spectral progression.

Result: DyPE achieves state-of-the-art fidelity in ultra-high-resolution image generation, e.g., 16 million pixels, with consistent performance gains at higher resolutions.

Conclusion: DyPE offers a scalable, cost-effective solution for ultra-high-resolution image generation without retraining.

Abstract: Diffusion Transformer models can generate images with remarkable fidelity and
detail, yet training them at ultra-high resolutions remains extremely costly
due to the self-attention mechanism's quadratic scaling with the number of
image tokens. In this paper, we introduce Dynamic Position Extrapolation
(DyPE), a novel, training-free method that enables pre-trained diffusion
transformers to synthesize images at resolutions far beyond their training
data, with no additional sampling cost. DyPE takes advantage of the spectral
progression inherent to the diffusion process, where low-frequency structures
converge early, while high-frequencies take more steps to resolve.
Specifically, DyPE dynamically adjusts the model's positional encoding at each
diffusion step, matching their frequency spectrum with the current stage of the
generative process. This approach allows us to generate images at resolutions
that exceed the training resolution dramatically, e.g., 16 million pixels using
FLUX. On multiple benchmarks, DyPE consistently improves performance and
achieves state-of-the-art fidelity in ultra-high-resolution image generation,
with gains becoming even more pronounced at higher resolutions. Project page is
available at https://noamissachar.github.io/DyPE/.

</details>


### [73] [AlphaFlow: Understanding and Improving MeanFlow Models](https://arxiv.org/abs/2510.20771)
*Huijie Zhang,Aliaksandr Siarohin,Willi Menapace,Michael Vasilkovsky,Sergey Tulyakov,Qing Qu,Ivan Skorokhodov*

Main category: cs.CV

TL;DR: The paper introduces α-Flow, a unified framework addressing optimization conflicts in MeanFlow, improving convergence and performance in few-step generative modeling.


<details>
  <summary>Details</summary>
Motivation: MeanFlow's effectiveness lacks full understanding, and its optimization suffers from conflicting objectives (trajectory flow matching and consistency).

Method: α-Flow introduces a curriculum strategy to smoothly transition from trajectory flow matching to MeanFlow, resolving optimization conflicts.

Result: α-Flow outperforms MeanFlow in class-conditional ImageNet-1K tasks, achieving state-of-the-art FID scores (2.58 for 1-NFE, 2.15 for 2-NFE).

Conclusion: α-Flow effectively disentangles conflicting objectives in MeanFlow, leading to better convergence and performance in generative modeling.

Abstract: MeanFlow has recently emerged as a powerful framework for few-step generative
modeling trained from scratch, but its success is not yet fully understood. In
this work, we show that the MeanFlow objective naturally decomposes into two
parts: trajectory flow matching and trajectory consistency. Through gradient
analysis, we find that these terms are strongly negatively correlated, causing
optimization conflict and slow convergence. Motivated by these insights, we
introduce $\alpha$-Flow, a broad family of objectives that unifies trajectory
flow matching, Shortcut Model, and MeanFlow under one formulation. By adopting
a curriculum strategy that smoothly anneals from trajectory flow matching to
MeanFlow, $\alpha$-Flow disentangles the conflicting objectives, and achieves
better convergence. When trained from scratch on class-conditional ImageNet-1K
256x256 with vanilla DiT backbones, $\alpha$-Flow consistently outperforms
MeanFlow across scales and settings. Our largest $\alpha$-Flow-XL/2+ model
achieves new state-of-the-art results using vanilla DiT backbones, with FID
scores of 2.58 (1-NFE) and 2.15 (2-NFE).

</details>


### [74] [CUPID: Pose-Grounded Generative 3D Reconstruction from a Single Image](https://arxiv.org/abs/2510.20776)
*Binbin Huang,Haobin Duan,Yiqun Zhao,Zibo Zhao,Yi Ma,Shenghua Gao*

Main category: cs.CV

TL;DR: Cupid, a new 3D reconstruction method, infers camera pose, 3D shape, and texture from a single 2D image using conditional sampling and a two-stage flow matching pipeline.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve 3D reconstruction accuracy and robustness by unifying pose estimation and shape generation under a single generative framework.

Method: Cupid uses a learned distribution of 3D objects and a two-stage pipeline: coarse geometry generation followed by refinement with pose-aligned image features.

Result: Cupid outperforms leading methods with a 3 dB PSNR gain, 10% Chamfer Distance reduction, and matches monocular pose estimators in accuracy.

Conclusion: Cupid advances 3D reconstruction by integrating pose and shape estimation in a generative model, achieving superior visual fidelity and accuracy.

Abstract: This work proposes a new generation-based 3D reconstruction method, named
Cupid, that accurately infers the camera pose, 3D shape, and texture of an
object from a single 2D image. Cupid casts 3D reconstruction as a conditional
sampling process from a learned distribution of 3D objects, and it jointly
generates voxels and pixel-voxel correspondences, enabling robust pose and
shape estimation under a unified generative framework. By representing both
input camera poses and 3D shape as a distribution in a shared 3D latent space,
Cupid adopts a two-stage flow matching pipeline: (1) a coarse stage that
produces initial 3D geometry with associated 2D projections for pose recovery;
and (2) a refinement stage that integrates pose-aligned image features to
enhance structural fidelity and appearance details. Extensive experiments
demonstrate Cupid outperforms leading 3D reconstruction methods with an over 3
dB PSNR gain and an over 10% Chamfer Distance reduction, while matching
monocular estimators on pose accuracy and delivering superior visual fidelity
over baseline 3D generative models. For an immersive view of the 3D results
generated by Cupid, please visit cupid3d.github.io.

</details>


### [75] [Small Drafts, Big Verdict: Information-Intensive Visual Reasoning via Speculation](https://arxiv.org/abs/2510.20812)
*Yuhan Liu,Lianhui Qin,Shengjie Wang*

Main category: cs.CV

TL;DR: SV is a training-free framework combining lightweight draft experts with a strong verdict model to improve reasoning over dense, information-intensive images.


<details>
  <summary>Details</summary>
Motivation: Large VLMs struggle with precise localization and multi-hop reasoning in dense layouts.

Method: SV uses draft experts for diverse localization candidates and a verdict model for synthesis, with consensus expert selection for efficiency.

Result: SV improves performance on benchmarks like InfographicVQA and ChartQAPro, balancing accuracy and computational cost.

Conclusion: SV offers error correction and cost-efficiency without requiring training or proprietary models.

Abstract: Large Vision-Language Models (VLMs) have achieved remarkable progress in
multimodal understanding, yet they struggle when reasoning over
information-intensive images that densely interleave textual annotations with
fine-grained graphical elements. The main challenges lie in precisely
localizing critical cues in dense layouts and multi-hop reasoning to integrate
dispersed evidence. We propose Speculative Verdict (SV), a training-free
framework inspired by speculative decoding that combines multiple lightweight
draft experts with a large verdict model. In the draft stage, small VLMs act as
draft experts to generate reasoning paths that provide diverse localization
candidates; in the verdict stage, a strong VLM synthesizes these paths to
produce the final answer, minimizing computational cost while recovering
correct answers. To further improve efficiency and accuracy, SV introduces a
consensus expert selection mechanism that forwards only high-agreement
reasoning paths to the verdict. Empirically, SV achieves consistent gains on
challenging information-intensive and high-resolution visual question answering
benchmarks, including InfographicVQA, ChartMuseum, ChartQAPro, and HR-Bench 4K.
By synthesizing correct insights from multiple partially accurate reasoning
paths, SV achieves both error correction and cost-efficiency compared to large
proprietary models or training pipelines. Code is available at
https://github.com/Tinaliu0123/speculative-verdict

</details>


### [76] [Radar-Camera Fused Multi-Object Tracking: Online Calibration and Common Feature](https://arxiv.org/abs/2510.20794)
*Lei Cheng,Siyang Cao*

Main category: cs.CV

TL;DR: A MOT framework using radar-camera fusion improves tracking by leveraging online calibration and common features, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Existing MOT approaches often underutilize radar despite its depth accuracy. This work aims to integrate radar-camera data seamlessly.

Method: Develops a fusion framework using online calibration, common features, and category-consistency checks for precise object tracking.

Result: The framework simplifies sensor integration and enhances tracking accuracy, validated in real-world traffic scenarios.

Conclusion: The study advances MOT by effectively combining radar-camera data, offering a robust solution for autonomous tracking.

Abstract: This paper presents a Multi-Object Tracking (MOT) framework that fuses radar
and camera data to enhance tracking efficiency while minimizing manual
interventions. Contrary to many studies that underutilize radar and assign it a
supplementary role--despite its capability to provide accurate range/depth
information of targets in a world 3D coordinate system--our approach positions
radar in a crucial role. Meanwhile, this paper utilizes common features to
enable online calibration to autonomously associate detections from radar and
camera. The main contributions of this work include: (1) the development of a
radar-camera fusion MOT framework that exploits online radar-camera calibration
to simplify the integration of detection results from these two sensors, (2)
the utilization of common features between radar and camera data to accurately
derive real-world positions of detected objects, and (3) the adoption of
feature matching and category-consistency checking to surpass the limitations
of mere position matching in enhancing sensor association accuracy. To the best
of our knowledge, we are the first to investigate the integration of
radar-camera common features and their use in online calibration for achieving
MOT. The efficacy of our framework is demonstrated by its ability to streamline
the radar-camera mapping process and improve tracking precision, as evidenced
by real-world experiments conducted in both controlled environments and actual
traffic scenarios. Code is available at
https://github.com/radar-lab/Radar_Camera_MOT

</details>


### [77] [ARGenSeg: Image Segmentation with Autoregressive Image Generation Model](https://arxiv.org/abs/2510.20803)
*Xiaolong Wang,Lixiang Ru,Ziyuan Huang,Kaixiang Ji,Dandan Zheng,Jingdong Chen,Jun Zhou*

Main category: cs.CV

TL;DR: ARGenSeg introduces an autoregressive generation-based framework for image segmentation in MLLMs, leveraging visual tokens and parallel processing for improved performance and speed.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of existing methods that rely on discrete representations or task-specific decoders, hindering fine-grained visual detail capture.

Method: Propose ARGenSeg, a framework using MLLMs to output visual tokens, detokenized into images via VQ-VAE, and employ parallel next-scale-prediction for efficiency.

Result: Outperforms state-of-the-art methods on segmentation datasets with enhanced inference speed and maintains strong understanding.

Conclusion: ARGenSeg successfully unifies multimodal understanding and pixel-level perception, demonstrating superior performance and efficiency.

Abstract: We propose a novel AutoRegressive Generation-based paradigm for image
Segmentation (ARGenSeg), achieving multimodal understanding and pixel-level
perception within a unified framework. Prior works integrating image
segmentation into multimodal large language models (MLLMs) typically employ
either boundary points representation or dedicated segmentation heads. These
methods rely on discrete representations or semantic prompts fed into
task-specific decoders, which limits the ability of the MLLM to capture
fine-grained visual details. To address these challenges, we introduce a
segmentation framework for MLLM based on image generation, which naturally
produces dense masks for target objects. We leverage MLLM to output visual
tokens and detokenize them into images using an universal VQ-VAE, making the
segmentation fully dependent on the pixel-level understanding of the MLLM. To
reduce inference latency, we employ a next-scale-prediction strategy to
generate required visual tokens in parallel. Extensive experiments demonstrate
that our method surpasses prior state-of-the-art approaches on multiple
segmentation datasets with a remarkable boost in inference speed, while
maintaining strong understanding capabilities.

</details>


### [78] [Video Prediction of Dynamic Physical Simulations With Pixel-Space Spatiotemporal Transformers](https://arxiv.org/abs/2510.20807)
*Dean L Slack,G Thomas Hudson,Thomas Winterbottom,Noura Al Moubayed*

Main category: cs.CV

TL;DR: The paper explores a transformer-based model for autoregressive video prediction, focusing on causal modeling of physical simulations. It introduces a simple, effective approach that outperforms existing methods in predicting physical accuracy without complex training.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the limitations of current video-generative approaches, particularly in causal modeling of physical simulations over time. It seeks to improve spatiotemporal reasoning via object tracking metrics and unsupervised training.

Method: A pure transformer model is proposed for autoregressive video prediction, using continuous pixel-space representations. The method avoids complex training strategies or latent feature-learning, focusing on spatiotemporal self-attention layouts.

Result: The approach extends the time horizon for physically accurate predictions by up to 50% compared to latent-space methods while maintaining video quality. Interpretability experiments reveal network regions useful for estimating simulation parameters.

Conclusion: The work establishes a foundational platform for attention-based spatiotemporal video modeling, offering a simple, parameter-efficient, and interpretable solution.

Abstract: Inspired by the performance and scalability of autoregressive large language
models (LLMs), transformer-based models have seen recent success in the visual
domain. This study investigates a transformer adaptation for video prediction
with a simple end-to-end approach, comparing various spatiotemporal
self-attention layouts. Focusing on causal modeling of physical simulations
over time; a common shortcoming of existing video-generative approaches, we
attempt to isolate spatiotemporal reasoning via physical object tracking
metrics and unsupervised training on physical simulation datasets. We introduce
a simple yet effective pure transformer model for autoregressive video
prediction, utilizing continuous pixel-space representations for video
prediction. Without the need for complex training strategies or latent
feature-learning components, our approach significantly extends the time
horizon for physically accurate predictions by up to 50% when compared with
existing latent-space approaches, while maintaining comparable performance on
common video quality metrics. In addition, we conduct interpretability
experiments to identify network regions that encode information useful to
perform accurate estimations of PDE simulation parameters via probing models,
and find that this generalizes to the estimation of out-of-distribution
simulation parameters. This work serves as a platform for further
attention-based spatiotemporal modeling of videos via a simple, parameter
efficient, and interpretable approach.

</details>


### [79] [SpectraMorph: Structured Latent Learning for Self-Supervised Hyperspectral Super-Resolution](https://arxiv.org/abs/2510.20814)
*Ritik Shah,Marco F Duarte*

Main category: cs.CV

TL;DR: SpectraMorph is a physics-guided, self-supervised framework for hyperspectral super-resolution, using an unmixing bottleneck and linear mixing for interpretable, robust performance.


<details>
  <summary>Details</summary>
Motivation: Hyperspectral images suffer from low spatial resolution, while co-registered sensors provide high-resolution detail, motivating fusion methods like HSI-MSI. Existing deep learning lacks interpretability and struggles with few MSI bands.

Method: SpectraMorph enforces an unmixing bottleneck: extracting endmember signatures from HSI and predicting abundance maps from MSI via MLP. Spectra are reconstructed linearly, trained self-supervised via MSI sensor response.

Result: SpectraMorph outperforms unsupervised/self-supervised baselines and remains competitive with supervised ones, even with single-band MSI. It trains quickly (<1 min) and provides interpretable intermediates.

Conclusion: SpectraMorph offers a robust, interpretable, and efficient solution for hyperspectral super-resolution, addressing limitations of existing methods.

Abstract: Hyperspectral sensors capture dense spectra per pixel but suffer from low
spatial resolution, causing blurred boundaries and mixed-pixel effects.
Co-registered companion sensors such as multispectral, RGB, or panchromatic
cameras provide high-resolution spatial detail, motivating hyperspectral
super-resolution through the fusion of hyperspectral and multispectral images
(HSI-MSI). Existing deep learning based methods achieve strong performance but
rely on opaque regressors that lack interpretability and often fail when the
MSI has very few bands. We propose SpectraMorph, a physics-guided
self-supervised fusion framework with a structured latent space. Instead of
direct regression, SpectraMorph enforces an unmixing bottleneck: endmember
signatures are extracted from the low-resolution HSI, and a compact multilayer
perceptron predicts abundance-like maps from the MSI. Spectra are reconstructed
by linear mixing, with training performed in a self-supervised manner via the
MSI sensor's spectral response function. SpectraMorph produces interpretable
intermediates, trains in under a minute, and remains robust even with a
single-band (pan-chromatic) MSI. Experiments on synthetic and real-world
datasets show SpectraMorph consistently outperforming state-of-the-art
unsupervised/self-supervised baselines while remaining very competitive against
supervised baselines.

</details>


### [80] [Towards General Modality Translation with Contrastive and Predictive Latent Diffusion Bridge](https://arxiv.org/abs/2510.20819)
*Nimrod Berman,Omkar Joglekar,Eitan Kosman,Dotan Di Castro,Omri Azencot*

Main category: cs.CV

TL;DR: Diffusion models excel in single-modality tasks but struggle with Modality Translation (MT). The proposed Latent Denoising Diffusion Bridge Model (LDDBM) addresses this by learning a latent-space bridge between arbitrary modalities, supported by contrastive alignment and predictive losses.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models for MT rely on restrictive assumptions (shared dimensionality, Gaussian priors, modality-specific architectures), limiting their generality. LDDBM aims to overcome these limitations.

Method: LDDBM extends Denoising Diffusion Bridge Models to latent space, using a shared latent space for cross-modality translation. It employs contrastive alignment loss, domain-agnostic encoder-decoder architecture, and predictive loss for accurate translation.

Result: LDDBM performs strongly on diverse MT tasks (multi-view to 3D shape generation, image super-resolution, multi-view scene synthesis), validated by comprehensive experiments.

Conclusion: LDDBM establishes a new baseline for general modality translation, overcoming limitations of existing approaches.

Abstract: Recent advances in generative modeling have positioned diffusion models as
state-of-the-art tools for sampling from complex data distributions. While
these models have shown remarkable success across single-modality domains such
as images and audio, extending their capabilities to Modality Translation (MT),
translating information across different sensory modalities, remains an open
challenge. Existing approaches often rely on restrictive assumptions, including
shared dimensionality, Gaussian source priors, and modality-specific
architectures, which limit their generality and theoretical grounding. In this
work, we propose the Latent Denoising Diffusion Bridge Model (LDDBM), a
general-purpose framework for modality translation based on a latent-variable
extension of Denoising Diffusion Bridge Models. By operating in a shared latent
space, our method learns a bridge between arbitrary modalities without
requiring aligned dimensions. We introduce a contrastive alignment loss to
enforce semantic consistency between paired samples and design a
domain-agnostic encoder-decoder architecture tailored for noise prediction in
latent space. Additionally, we propose a predictive loss to guide training
toward accurate cross-domain translation and explore several training
strategies to improve stability. Our approach supports arbitrary modality pairs
and performs strongly on diverse MT tasks, including multi-view to 3D shape
generation, image super-resolution, and multi-view scene synthesis.
Comprehensive experiments and ablations validate the effectiveness of our
framework, establishing a new strong baseline in general modality translation.
For more information, see our project page:
https://sites.google.com/view/lddbm/home.

</details>


### [81] [LayerComposer: Interactive Personalized T2I via Spatially-Aware Layered Canvas](https://arxiv.org/abs/2510.20820)
*Guocheng Gordon Qian,Ruihang Zhang,Tsai-Shien Chen,Yusuf Dalva,Anujraaj Argo Goyal,Willi Menapace,Ivan Skorokhodov,Meng Dong,Arpit Sahni,Daniil Ostashev,Ju Hu,Sergey Tulyakov,Kuan-Chieh Jackson Wang*

Main category: cs.CV

TL;DR: LayerComposer is an interactive framework for personalized, multi-subject text-to-image generation, offering spatial control and identity preservation through layered canvas and locking mechanisms.


<details>
  <summary>Details</summary>
Motivation: Existing personalized generative models lack interactive spatial control and struggle with multiple subjects, prompting the development of LayerComposer.

Method: LayerComposer introduces a layered canvas for occlusion-free composition and a locking mechanism for flexible adaptation while preserving selected layers.

Result: Extensive experiments show LayerComposer outperforms state-of-the-art methods in spatial control and identity preservation.

Conclusion: LayerComposer effectively addresses limitations in multi-subject personalized image generation with its innovative layered approach and locking mechanism.

Abstract: Despite their impressive visual fidelity, existing personalized generative
models lack interactive control over spatial composition and scale poorly to
multiple subjects. To address these limitations, we present LayerComposer, an
interactive framework for personalized, multi-subject text-to-image generation.
Our approach introduces two main contributions: (1) a layered canvas, a novel
representation in which each subject is placed on a distinct layer, enabling
occlusion-free composition; and (2) a locking mechanism that preserves selected
layers with high fidelity while allowing the remaining layers to adapt flexibly
to the surrounding context. Similar to professional image-editing software, the
proposed layered canvas allows users to place, resize, or lock input subjects
through intuitive layer manipulation. Our versatile locking mechanism requires
no architectural changes, relying instead on inherent positional embeddings
combined with a new complementary data sampling strategy. Extensive experiments
demonstrate that LayerComposer achieves superior spatial control and identity
preservation compared to the state-of-the-art methods in multi-subject
personalized image generation.

</details>


### [82] [HoloCine: Holistic Generation of Cinematic Multi-Shot Long Video Narratives](https://arxiv.org/abs/2510.20822)
*Yihao Meng,Hao Ouyang,Yue Yu,Qiuyu Wang,Wen Wang,Ka Leong Cheng,Hanlin Wang,Yixuan Li,Cheng Chen,Yanhong Zeng,Yujun Shen,Huamin Qu*

Main category: cs.CV

TL;DR: HoloCine bridges the 'narrative gap' in text-to-video models by generating coherent multi-shot scenes with global consistency, using advanced attention mechanisms for precise control and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-video models struggle with creating cohesive, multi-shot narratives, which are crucial for storytelling. HoloCine aims to overcome this limitation.

Method: HoloCine employs Window Cross-Attention for localized text prompt control and Sparse Inter-Shot Self-Attention for efficient shot-to-shot consistency, enabling holistic scene generation.

Result: HoloCine achieves state-of-the-art narrative coherence and exhibits emergent abilities like persistent memory and intuitive cinematic grasp.

Conclusion: HoloCine represents a shift from clip synthesis to automated filmmaking, paving the way for end-to-end cinematic creation.

Abstract: State-of-the-art text-to-video models excel at generating isolated clips but
fall short of creating the coherent, multi-shot narratives, which are the
essence of storytelling. We bridge this "narrative gap" with HoloCine, a model
that generates entire scenes holistically to ensure global consistency from the
first shot to the last. Our architecture achieves precise directorial control
through a Window Cross-Attention mechanism that localizes text prompts to
specific shots, while a Sparse Inter-Shot Self-Attention pattern (dense within
shots but sparse between them) ensures the efficiency required for minute-scale
generation. Beyond setting a new state-of-the-art in narrative coherence,
HoloCine develops remarkable emergent abilities: a persistent memory for
characters and scenes, and an intuitive grasp of cinematic techniques. Our work
marks a pivotal shift from clip synthesis towards automated filmmaking, making
end-to-end cinematic creation a tangible future. Our code is available at:
https://holo-cine.github.io/.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [83] [DeBERTa-KC: A Transformer-Based Classifier for Knowledge Construction in Online Learning Discourse](https://arxiv.org/abs/2510.19858)
*Jindi Wang,Yidi Zhang,Zhaoxing Li*

Main category: cs.CL

TL;DR: DeBERTa-KC is a transformer-based model for classifying knowledge construction levels in online science discourse, achieving high performance and sensitivity.


<details>
  <summary>Details</summary>
Motivation: The study aims to automate the classification of knowledge construction levels in online science learning discourse to provide scalable, theory-informed tools for analyzing epistemic engagement.

Method: Extends DeBERTa-v3 with Focal Loss, Label Smoothing, and R-Drop regularization. Uses a balanced corpus of 20,000 annotated samples from YouTube science channels.

Result: Achieved a macro-F1 of 0.836 ± 0.008, outperforming baselines. Shows strong sensitivity to higher-order epistemic engagement.

Conclusion: Large language models can effectively analyze knowledge construction in informal digital learning environments, supporting automated discourse analysis.

Abstract: This study presents DeBERTa-KC, a transformer-based model for automatic
classification of knowledge construction (KC) levels in online science learning
discourse. Using comments collected from four popular YouTube science channels
(2022--2024), a balanced corpus of 20,000 manually annotated samples was
created across four KC categories: \textit{nonKC}, \textit{Share},
\textit{Explore}, and \textit{Negotiate}. The proposed model extends DeBERTa-v3
with Focal Loss, Label Smoothing, and R-Drop regularization to address class
imbalance and enhance generalization. A reproducible end-to-end pipeline was
implemented, encompassing data extraction, annotation, preprocessing, training,
and evaluation. Across 10-fold stratified cross-validation, DeBERTa-KC achieved
a macro-F1 of $0.836 \pm 0.008$, significantly out-performing both classical
and transformer baselines ($p<0.01$). Per-category results indicate strong
sensitivity to higher-order epistemic engagement, particularly in
\textit{Explore} and \textit{Negotiate} discourse. These findings demonstrate
that large language models can effectively capture nuanced indicators of
knowledge construction in informal digital learning environments, offering
scalable, theory-informed approaches to discourse analysis and the development
of automated tools for assessing epistemic engagement.

</details>


### [84] [An Evaluation of the Pedagogical Soundness and Usability of AI-Generated Lesson Plans Across Different Models and Prompt Frameworks in High-School Physics](https://arxiv.org/abs/2510.19866)
*Xincheng Liu*

Main category: cs.CL

TL;DR: The study evaluates AI-generated lesson plans from five language models using three prompt frameworks. DeepSeek produced the most readable plans, while Claude's were densest. RACE framework ensured highest factual accuracy and curriculum alignment. Lesson plans mostly lacked higher-order learning objectives. Optimal configuration combines readability-focused models with RACE framework and explicit checklists.


<details>
  <summary>Details</summary>
Motivation: To assess the pedagogical soundness and usability of AI-generated lesson plans, considering model choice and prompt frameworks for educational applications.

Method: Generated fifteen lesson plans on The Electromagnetic Spectrum using five models and three frameworks. Analyzed via readability, factual accuracy, curriculum alignment, and cognitive demand metrics.

Result: DeepSeek had best readability (FKGL=8.64); Claude was densest (FKGL=19.89). RACE framework ensured highest accuracy and curriculum alignment. Lesson plans lacked higher-order objectives.

Conclusion: Readability depends on model design, while instructional reliability relies on prompt framework. Combining readability-optimized models with RACE framework and explicit checklists yields best results.

Abstract: This study evaluates the pedagogical soundness and usability of AI-generated
lesson plans across five leading large language models: ChatGPT (GPT-5), Claude
Sonnet 4.5, Gemini 2.5 Flash, DeepSeek V3.2, and Grok 4. Beyond model choice,
three structured prompt frameworks were tested: TAG (Task, Audience, Goal),
RACE (Role, Audience, Context, Execution), and COSTAR (Context, Objective,
Style, Tone, Audience, Response Format).
  Fifteen lesson plans were generated for a single high-school physics topic,
The Electromagnetic Spectrum. The lesson plans were analyzed through four
automated computational metrics: (1) readability and linguistic complexity, (2)
factual accuracy and hallucination detection, (3) standards and curriculum
alignment, and (4) cognitive demand of learning objectives.
  Results indicate that model selection exerted the strongest influence on
linguistic accessibility, with DeepSeek producing the most readable teaching
plan (FKGL = 8.64) and Claude generating the densest language (FKGL = 19.89).
  The prompt framework structure most strongly affected the factual accuracy
and pedagogical completeness, with the RACE framework yielding the lowest
hallucination index and the highest incidental alignment with NGSS curriculum
standards. Across all models, the learning objectives in the fifteen lesson
plans clustered at the Remember and Understand tiers of Bloom's taxonomy. There
were limited higher-order verbs in the learning objectives extracted.
  Overall, the findings suggest that readability is significantly governed by
model design, while instructional reliability and curricular alignment depend
more on the prompt framework. The most effective configuration for lesson plans
identified in the results was to combine a readability-optimized model with the
RACE framework and an explicit checklist of physics concepts, curriculum
standards, and higher-order objectives.

</details>


### [85] [From Denoising to Refining: A Corrective Framework for Vision-Language Diffusion Model](https://arxiv.org/abs/2510.19871)
*Yatai Ji,Teng Wang,Yuying Ge,Zhiheng Liu,Sidi Yang,Ying Shan,Ping Luo*

Main category: cs.CL

TL;DR: The paper introduces ReDiff, a refining-enhanced diffusion framework that addresses error cascades in discrete diffusion models by teaching the model to self-correct through a two-stage training process.


<details>
  <summary>Details</summary>
Motivation: Discrete diffusion models suffer from train-inference discrepancy, causing catastrophic error cascades during parallel decoding. The authors aim to improve coherence and factual accuracy in generated content.

Method: ReDiff reframes generation as active refining, featuring a two-stage training process: foundational revision of synthetic errors and an online self-correction loop where the model learns to refine its own flawed drafts.

Result: ReDiff significantly improves coherence and factual accuracy, enabling stable and efficient parallel generation, outperforming traditional denoising methods.

Conclusion: The proposed approach effectively breaks the error cascade, making ReDiff a superior solution for vision-language tasks.

Abstract: Discrete diffusion models have emerged as a promising direction for
vision-language tasks, offering bidirectional context modeling and theoretical
parallelization. However, their practical application is severely hindered by a
train-inference discrepancy, which leads to catastrophic error cascades:
initial token errors during parallel decoding pollute the generation context,
triggering a chain reaction of compounding errors and leading to syntactic
errors and semantic hallucinations. To address this fundamental challenge, we
reframe the generation process from passive denoising to active refining. We
introduce ReDiff, a refining-enhanced diffusion framework that teaches the
model to identify and correct its own errors. Our approach features a two-stage
training process: first, we instill a foundational revision capability by
training the model to revise synthetic errors; second, we implement a novel
online self-correction loop where the model is explicitly trained to revise its
own flawed drafts by learning from an expert's corrections. This mistake-driven
learning endows the model with the crucial ability to revisit and refine its
already generated output, effectively breaking the error cascade. Extensive
experiments demonstrate that ReDiff significantly improves the coherence and
factual accuracy of generated content, enabling stable and efficient parallel
generation far superior to traditional denoising methods. Our codes and models
are available at https://rediff-hku.github.io/.

</details>


### [86] [Stream: Scaling up Mechanistic Interpretability to Long Context in LLMs via Sparse Attention](https://arxiv.org/abs/2510.19875)
*J Rosser,José Luis Redondo García,Gustavo Penha,Konstantina Palla,Hugues Bouchard*

Main category: cs.CL

TL;DR: Sparse Tracing introduces Stream, a near-linear time and linear space technique for analyzing long-context attention patterns in LLMs, reducing memory usage while preserving critical interactions.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for analyzing attention patterns in LLMs scale poorly with context length, requiring impractical memory resources.

Method: Stream uses dynamic sparse attention and hierarchical pruning to estimate attention masks efficiently, retaining only top-k key blocks per query.

Result: Stream prunes 97-99% of token interactions while preserving next-token behavior and critical retrieval paths, making interpretation feasible on consumer GPUs.

Conclusion: Sparse Tracing democratizes interpretability for long-context LLMs by offering a practical, efficient tool for attention analysis.

Abstract: As Large Language Models (LLMs) scale to million-token contexts, traditional
Mechanistic Interpretability techniques for analyzing attention scale
quadratically with context length, demanding terabytes of memory beyond 100,000
tokens. We introduce Sparse Tracing, a novel technique that leverages dynamic
sparse attention to efficiently analyze long context attention patterns. We
present Stream, a compilable hierarchical pruning algorithm that estimates
per-head sparse attention masks in near-linear time $O(T \log T)$ and linear
space $O(T)$, enabling one-pass interpretability at scale. Stream performs a
binary-search-style refinement to retain only the top-$k$ key blocks per query
while preserving the model's next-token behavior. We apply Stream to long
chain-of-thought reasoning traces and identify thought anchors while pruning
97-99\% of token interactions. On the RULER benchmark, Stream preserves
critical retrieval paths while discarding 90-96\% of interactions and exposes
layer-wise routes from the needle to output. Our method offers a practical
drop-in tool for analyzing attention patterns and tracing information flow
without terabytes of caches. By making long context interpretability feasible
on consumer GPUs, Sparse Tracing helps democratize chain-of-thought monitoring.
Code is available at https://anonymous.4open.science/r/stream-03B8/.

</details>


### [87] [Automated HIV Screening on Dutch EHR with Large Language Models](https://arxiv.org/abs/2510.19879)
*Lang Zhou,Amrish Jhingoer,Yinghao Luo,Klaske Vliegenthart--Jongbloed,Carlijn Jordans,Ben Werkhoven,Tom Seinen,Erik van Mulligen,Casper Rokx,Yunlei Li*

Main category: cs.CL

TL;DR: The paper proposes a pipeline using a Large Language Model (LLM) to analyze unstructured EHR text for HIV screening, achieving high accuracy and low false negatives.


<details>
  <summary>Details</summary>
Motivation: Early HIV diagnosis is crucial for reducing transmission. Existing methods focus on structured data, neglecting potentially valuable unstructured clinical notes.

Method: A novel pipeline employing an LLM to analyze unstructured EHR text for HIV testing eligibility.

Result: High accuracy and low false negative rate demonstrated on clinical data from Erasmus University Medical Center Rotterdam.

Conclusion: The pipeline effectively leverages unstructured EHR text for improved HIV screening.

Abstract: Efficient screening and early diagnosis of HIV are critical for reducing
onward transmission. Although large scale laboratory testing is not feasible,
the widespread adoption of Electronic Health Records (EHRs) offers new
opportunities to address this challenge. Existing research primarily focuses on
applying machine learning methods to structured data, such as patient
demographics, for improving HIV diagnosis. However, these approaches often
overlook unstructured text data such as clinical notes, which potentially
contain valuable information relevant to HIV risk. In this study, we propose a
novel pipeline that leverages a Large Language Model (LLM) to analyze
unstructured EHR text and determine a patient's eligibility for further HIV
testing. Experimental results on clinical data from Erasmus University Medical
Center Rotterdam demonstrate that our pipeline achieved high accuracy while
maintaining a low false negative rate.

</details>


### [88] [An Expert-grounded benchmark of General Purpose LLMs in LCA](https://arxiv.org/abs/2510.19886)
*Artur Donaldson,Bharathan Balaji,Cajetan Oriekezie,Manish Kumar,Laure Patouillard*

Main category: cs.CL

TL;DR: This study benchmarks 11 LLMs in LCA tasks, revealing significant risks like inaccuracies and hallucinations but also benefits in explanation quality and task efficiency.


<details>
  <summary>Details</summary>
Motivation: To assess the reliability and usability of LLMs in LCA due to the lack of standardized frameworks and clear ground truth.

Method: Evaluated 11 LLMs across 22 LCA tasks, reviewed by 17 experts for accuracy, explanation quality, robustness, verifiability, and instruction adherence.

Result: 37% of responses had inaccuracies; hallucination rates varied (up to 40%). Open-weight models performed comparably to closed-weight models in accuracy and explanation quality.

Conclusion: LLMs pose risks if used naively but offer benefits in explanation quality and task efficiency. Grounding mechanisms are crucial for reliable use.

Abstract: Purpose: Artificial intelligence (AI), and in particular large language
models (LLMs), are increasingly being explored as tools to support life cycle
assessment (LCA). While demonstrations exist across environmental and social
domains, systematic evidence on their reliability, robustness, and usability
remains limited. This study provides the first expert-grounded benchmark of
LLMs in LCA, addressing the absence of standardized evaluation frameworks in a
field where no clear ground truth or consensus protocols exist.
  Methods: We evaluated eleven general-purpose LLMs, spanning both commercial
and open-source families, across 22 LCA-related tasks. Seventeen experienced
practitioners reviewed model outputs against criteria directly relevant to LCA
practice, including scientific accuracy, explanation quality, robustness,
verifiability, and adherence to instructions. We collected 168 expert reviews.
  Results: Experts judged 37% of responses to contain inaccurate or misleading
information. Ratings of accuracy and quality of explanation were generally
rated average or good on many models even smaller models, and format adherence
was generally rated favourably. Hallucination rates varied significantly, with
some models producing hallucinated citations at rates of up to 40%. There was
no clear-cut distinction between ratings on open-weight versus closed-weight
LLMs, with open-weight models outperforming or competing on par with
closed-weight models on criteria such as accuracy and quality of explanation.
  Conclusion: These findings highlight the risks of applying LLMs na\"ively in
LCA, such as when LLMs are treated as free-form oracles, while also showing
benefits especially around quality of explanation and alleviating labour
intensiveness of simple tasks. The use of general-purpose LLMs without
grounding mechanisms presents ...

</details>


### [89] [Can They Dixit? Yes they Can! Dixit as a Playground for Multimodal Language Model Capabilities](https://arxiv.org/abs/2510.19892)
*Nishant Balepur,Dang Nguyen,Dayeon Ki*

Main category: cs.CL

TL;DR: The paper proposes game-based evaluations, specifically using Dixit, to holistically assess multi-modal large language models (MLMs) by overcoming limitations of static benchmarks and subjective comparisons.


<details>
  <summary>Details</summary>
Motivation: Current MLM evaluations rely on static benchmarks or subjective pairwise comparisons, which are limited, expensive, and prone to exploitation.

Method: The authors introduce game-based evaluations, exemplified by Dixit, to assess MLMs holistically and objectively using competitive, multi-ability games with fixed rules.

Result: Quantitative experiments with five MLMs show Dixit win-rate rankings align perfectly with popular benchmarks, while human-MLM comparisons reveal strategy differences and reasoning gaps.

Conclusion: Game-based evaluations like Dixit offer a robust, engaging, and objective framework for assessing MLMs, highlighting areas for improvement.

Abstract: Multi-modal large language models (MLMs) are often assessed on static,
individual benchmarks -- which cannot jointly assess MLM capabilities in a
single task -- or rely on human or model pairwise comparisons -- which is
highly subjective, expensive, and allows models to exploit superficial
shortcuts (e.g., verbosity) to inflate their win-rates. To overcome these
issues, we propose game-based evaluations to holistically assess MLM
capabilities. Games require multiple abilities for players to win, are
inherently competitive, and are governed by fix, objective rules, and makes
evaluation more engaging, providing a robust framework to address the
aforementioned challenges. We manifest this evaluation specifically through
Dixit, a fantasy card game where players must generate captions for a card that
trick some, but not all players, into selecting the played card. Our
quantitative experiments with five MLMs show Dixit win-rate rankings are
perfectly correlated with those on popular MLM benchmarks, while games between
human and MLM players in Dixit reveal several differences between agent
strategies and areas of improvement for MLM reasoning.

</details>


### [90] [Large Language Model enabled Mathematical Modeling](https://arxiv.org/abs/2510.19895)
*Guoyun Zhang*

Main category: cs.CL

TL;DR: The paper explores using DeepSeek-R1 LLM to bridge the gap in optimization modeling by reducing reliance on domain expertise, addressing challenges like hallucinations and high costs.


<details>
  <summary>Details</summary>
Motivation: Traditional optimization methods require domain expertise for problem formulation, and existing LLMs face limitations like high costs and hallucinations.

Method: The study evaluates DeepSeek-R1 on OR benchmarks (NL4OPT, IndustryOR, EasyLP, ComplexOR) using techniques like LLM-as-a-Judge and Few-shot Learning.

Result: DeepSeek-R1 shows promise in improving formulation accuracy and reducing hallucinations in optimization modeling.

Conclusion: DeepSeek-R1 offers a cost-effective solution for optimization modeling, though further exploration is needed for real-world OR applications.

Abstract: The integration of Large Language Models (LLMs) with optimization modeling
offers a promising avenue for advancing decision-making in operations research
(OR). Traditional optimization methods,such as linear programming, mixed
integer programming, and simulation depend heavily on domain expertise to
translate real-world problems into solvable mathematical models. While solvers
like Gurobi and COPT are powerful, expert input remains essential for defining
objectives, constraints, and variables. This research investigates the
potential of LLMs, specifically the DeepSeek-R1 model, to bridge this
formulation gap using natural language understanding and code generation.
Although prior models like GPT-4, Claude, and Bard have shown strong
performance in NLP and reasoning tasks, their high token costs and tendency
toward hallucinations limit real-world applicability in supply chain contexts.
In contrast, DeepSeek-R1, a cost-efficient and high-performing model trained
with reinforcement learning, presents a viable alternative. Despite its success
in benchmarks such as LiveCodeBench and Math-500, its effectiveness in applied
OR scenarios remains under explored. This study systematically evaluates
DeepSeek-R1 across four key OR benchmarks: NL4OPT, IndustryOR, EasyLP, and
ComplexOR. Our methodology includes baseline assessments, the development of a
hallucination taxonomy, and the application of mitigation strategies like
LLM-as-a-Judge, Few-shot Learning (FSL), Tool Calling, and a Multi-agent
Framework. These techniques aim to reduce hallucinations, enhance formulation
accuracy, and better align model outputs with user intent.

</details>


### [91] [Learning from Supervision with Semantic and Episodic Memory: A Reflective Approach to Agent Adaptation](https://arxiv.org/abs/2510.19897)
*Jackson Hassell,Dan Zhang,Hannah Kim,Tom Mitchell,Estevam Hruschka*

Main category: cs.CL

TL;DR: The paper explores how large language model (LLM) agents can learn classification tasks without parameter updates, using a memory-augmented framework that improves accuracy by leveraging critiques alongside labeled data.


<details>
  <summary>Details</summary>
Motivation: Traditional methods like fine-tuning are costly and inflexible. The study aims to develop a more adaptive and interpretable approach by utilizing LLM-generated critiques and memory strategies.

Method: A memory-augmented framework combines episodic memory (storing instance-level critiques) and semantic memory (distilling critiques into task-level guidance).

Result: Incorporating critiques boosts accuracy by up to 24.8% over retrieval-based baselines. Differences between OpenAI and open-source models in handling data types are noted.

Conclusion: Memory-driven learning enhances LLM agents' adaptability and interpretability, with the suggestibility metric offering insights into model behaviors.

Abstract: We investigate how agents built on pretrained large language models can learn
target classification functions from labeled examples without parameter
updates. While conventional approaches like fine-tuning are often costly,
inflexible, and opaque, we propose a memory-augmented framework that leverages
both labeled data and LLM-generated critiques. Our framework uses episodic
memory to store instance-level critiques-capturing specific past
experiences-and semantic memory to distill these into reusable, task-level
guidance. Across a diverse set of tasks, incorporating critiques yields up to a
24.8 percent accuracy improvement over retrieval-based (RAG-style) baselines
that rely only on labels. Through extensive empirical evaluation, we uncover
distinct behavioral differences between OpenAI and opensource models,
particularly in how they handle fact-oriented versus preference-based data. To
interpret how models respond to different representations of supervision
encoded in memory, we introduce a novel metric, suggestibility. This helps
explain observed behaviors and illuminates how model characteristics and memory
strategies jointly shape learning dynamics. Our findings highlight the promise
of memory-driven, reflective learning for building more adaptive and
interpretable LLM agents.

</details>


### [92] [LyriCAR: A Difficulty-Aware Curriculum Reinforcement Learning Framework For Controllable Lyric Translation](https://arxiv.org/abs/2510.19967)
*Le Ren,Xiangjian Zeng,Qingqiang Wu,Ruoxuan Liang*

Main category: cs.CL

TL;DR: LyriCAR is a novel unsupervised framework for lyric translation that uses adaptive curriculum learning to improve translation quality and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing lyric translation methods rely on rigid rules and sentence-level models, limiting their ability to handle paragraph-level coherence and musical constraints.

Method: LyriCAR employs a difficulty-aware curriculum designer and adaptive strategy to allocate training resources effectively, introducing increasingly complex challenges.

Result: LyriCAR outperforms baselines in EN-ZH lyric translation, achieving state-of-the-art results and reducing training steps by nearly 40%.

Conclusion: The adaptive curriculum strategy enhances translation quality and efficiency, making LyriCAR a robust solution for lyric translation.

Abstract: Lyric translation is a challenging task that requires balancing multiple
musical constraints. Existing methods often rely on hand-crafted rules and
sentence-level modeling, which restrict their ability to internalize
musical-linguistic patterns and to generalize effectively at the paragraph
level, where cross-line coherence and global rhyme are crucial. In this work,
we propose LyriCAR, a novel framework for controllable lyric translation that
operates in a fully unsupervised manner. LyriCAR introduces a difficulty-aware
curriculum designer and an adaptive curriculum strategy, ensuring efficient
allocation of training resources, accelerating convergence, and improving
overall translation quality by guiding the model with increasingly complex
challenges. Extensive experiments on the EN-ZH lyric translation task show that
LyriCAR achieves state-of-the-art results across both standard translation
metrics and multi-dimensional reward scores, surpassing strong baselines.
Notably, the adaptive curriculum strategy reduces training steps by nearly 40%
while maintaining superior performance. Code, data and model can be accessed at
https://github.com/rle27/LyriCAR.

</details>


### [93] [LLM-Augmented Symbolic NLU System for More Reliable Continuous Causal Statement Interpretation](https://arxiv.org/abs/2510.19988)
*Xin Lian,Kenneth D. Forbus*

Main category: cs.CL

TL;DR: A hybrid NLU approach combining LLMs' broad coverage with symbolic NLU's structured representations outperforms symbolic-only methods in extracting and interpreting quantities and causal laws from texts.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of LLMs (e.g., hallucination, inconsistency) and symbolic NLU systems (e.g., limited coverage, maintenance challenges) by integrating their strengths.

Method: Combine LLMs (for rephrasing, simplification, and broad coverage) with symbolic NLU (for structured representations and reasoning). Evaluate on extracting quantities and causal laws from science texts.

Result: The hybrid method performs significantly better than symbolic-only pipelines.

Conclusion: Integrating LLMs and symbolic NLU leverages their respective strengths, improving accuracy and coverage in NLU tasks.

Abstract: Despite the broad applicability of large language models (LLMs), their
reliance on probabilistic inference makes them vulnerable to errors such as
hallucination in generated facts and inconsistent output structure in natural
language understanding (NLU) tasks. By contrast, symbolic NLU systems provide
interpretable understanding grounded in curated lexicons, semantic resources,
and syntactic & semantic interpretation rules. They produce relational
representations that can be used for accurate reasoning and planning, as well
as incremental debuggable learning. However, symbolic NLU systems tend to be
more limited in coverage than LLMs and require scarce knowledge representation
and linguistics skills to extend and maintain. This paper explores a hybrid
approach that integrates the broad-coverage language processing of LLMs with
the symbolic NLU capabilities of producing structured relational
representations to hopefully get the best of both approaches. We use LLMs for
rephrasing and text simplification, to provide broad coverage, and as a source
of information to fill in knowledge gaps more automatically. We use symbolic
NLU to produce representations that can be used for reasoning and for
incremental learning. We evaluate this approach on the task of extracting and
interpreting quantities and causal laws from commonsense science texts, along
with symbolic- and LLM-only pipelines. Our results suggest that our hybrid
method works significantly better than the symbolic-only pipeline.

</details>


### [94] [A Fundamental Algorithm for Dependency Parsing (With Corrections)](https://arxiv.org/abs/2510.19996)
*Michael A. Covington*

Main category: cs.CL

TL;DR: A novel algorithm for dependency parsing mimics human brain processing by attaching words incrementally with comparable efficiency to phrase-structure parsers.


<details>
  <summary>Details</summary>
Motivation: To develop a parsing algorithm that emulates human brain processing by incrementally attaching words into dependency trees.

Method: The algorithm parses sentences word-by-word, attaching each word as soon as possible, with a worst-case complexity of O(n^3) similar to phrase-structure parsing.

Result: The algorithm achieves efficient parsing for human language, where the worst-case complexity is rarely encountered for small n.

Conclusion: The proposed dependency parsing algorithm offers a biologically plausible and computationally efficient alternative to traditional phrase-structure parsing.

Abstract: This paper presents a fundamental algorithm for parsing natural language
sentences into dependency trees. Unlike phrase-structure (constituency)
parsers, this algorithm operates one word at a time, attaching each word as
soon as it can be attached, corresponding to properties claimed for the parser
in the human brain. Like phrase-structure parsing, its worst-case complexity is
$O(n^3)$, but in human language, the worst case occurs only for small $n$.

</details>


### [95] [Beyond MedQA: Towards Real-world Clinical Decision Making in the Era of LLMs](https://arxiv.org/abs/2510.20001)
*Yunpeng Xiao,Carl Yang,Mark Mai,Xiao Hu,Kai Shu*

Main category: cs.CL

TL;DR: The paper proposes a unifying paradigm for evaluating LLMs in clinical decision-making by categorizing tasks into Clinical Backgrounds and Questions, aiming to better reflect real-world complexity and improve evaluation methods.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of LLMs for clinical use rely on simplified datasets like MedQA, which inadequately represent real-world clinical decision-making.

Method: The authors introduce a paradigm that organizes clinical tasks along two dimensions (Clinical Backgrounds and Questions) and review methods (training-time and test-time techniques) to address these tasks.

Result: The paradigm helps clarify assumptions, standardize comparisons, and guide LLM development for clinically meaningful applications, extending evaluation beyond accuracy to efficiency and explainability.

Conclusion: The proposed framework improves the assessment and development of LLMs for clinical use by aligning evaluations more closely with real-world clinical challenges.

Abstract: Large language models (LLMs) show promise for clinical use. They are often
evaluated using datasets such as MedQA. However, Many medical datasets, such as
MedQA, rely on simplified Question-Answering (Q\A) that underrepresents
real-world clinical decision-making. Based on this, we propose a unifying
paradigm that characterizes clinical decision-making tasks along two
dimensions: Clinical Backgrounds and Clinical Questions. As the background and
questions approach the real clinical environment, the difficulty increases. We
summarize the settings of existing datasets and benchmarks along two
dimensions. Then we review methods to address clinical decision-making,
including training-time and test-time techniques, and summarize when they help.
Next, we extend evaluation beyond accuracy to include efficiency,
explainability. Finally, we highlight open challenges. Our paradigm clarifies
assumptions, standardizes comparisons, and guides the development of clinically
meaningful LLMs.

</details>


### [96] [Forging GEMs: Advancing Greek NLP through Quality-Based Corpus Curation and Specialized Pre-training](https://arxiv.org/abs/2510.20002)
*Alexandra Apostolopoulou,Konstantinos Kanaris,Athanasios Koursaris,Dimitris Tsakalidis,George Domalis,Ioannis E. Livieris*

Main category: cs.CL

TL;DR: The paper introduces Greek Embedding Models (GEMs) for Modern Greek addressing limitations in NLP for morphologically rich languages, focusing on legal domains.


<details>
  <summary>Details</summary>
Motivation: Advance NLP for Greek, overcoming fragmented research, limited architectural diversity, and insufficient context-length models, especially in specialized legal domains.

Method: Construct large-scale Greek corpora with quality-driven curation, pre-train diverse architectures (ELECTRA, ConvBERT, ModernBERT), and develop bilingual Greek-English models.

Result: GEM-RoBERTa and GEM-ConvBERT outperform baselines in downstream tasks, demonstrating the effectiveness of the approach.

Conclusion: The proposed models set a new standard for Greek NLP, particularly in legal applications, by leveraging quality data and modern architectures.

Abstract: The advancement of natural language processing for morphologically rich,
moderately-resourced languages like Modern Greek is often hindered by a
fragmented research landscape, a lack of architectural diversity and reliance
on limited context-length models. This is particularly true in specialized,
high-value domains such as law, where existing models are frequently confined
to early transformer architectures with a restrictive 512-token window,
insufficient for analyzing long legal documents. To address these challenges,
this paper presents Greek Embedding Models, a new family of transformer models
for Greek language built upon a foundation of extensive, quality-driven data
curation. We detail the construction of several large-scale Greek corpora,
emphasizing a rigorous, quality-based filtering and preprocessing methodology
to create high-value training datasets from both general-domain and specialized
legal sources. On this carefully curated foundation, we pre-train and
systematically evaluate a diverse suite of modern architectures, which has not
previously applied to Greek language, such as ELECTRA, ConvBERT and ModernBERT.
Furthermore, we propose the first bilingual Greek-English Embedding Models
tailored for the legal domain. The extensive experiments on downstream tasks
demonstrate that the new class of models establish the effectiveness of the
proposed approach, highlighting that the GEM-RoBERTa and GEM-ConvBERT models
significantly outperform existing baselines.

</details>


### [97] [Improving Transfer Learning for Sequence Labeling Tasks by Adapting Pre-trained Neural Language Models](https://arxiv.org/abs/2510.20033)
*David Dukić*

Main category: cs.CL

TL;DR: Improves transfer learning for sequence labeling by adapting pre-trained models with multi-task learning, architectural modifications, and supervised in-context fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Enhance performance of pre-trained neural language models in sequence labeling tasks through targeted transfer learning.

Method: Introduces a multi-task model with additional signals, modifies autoregressive model architectures for bidirectional flow, and applies supervised in-context fine-tuning.

Result: Demonstrates improved performance in sequence labeling tasks using the proposed adaptations.

Conclusion: Targeted transfer learning paradigms optimize pre-trained models for sequence labeling tasks.

Abstract: This doctoral thesis improves the transfer learning for sequence labeling
tasks by adapting pre-trained neural language models. The proposed improvements
in transfer learning involve introducing a multi-task model that incorporates
an additional signal, a method based on architectural modifications in
autoregressive large language models, and a sequence labeling framework for
autoregressive large language models utilizing supervised in-context
fine-tuning combined with response-oriented adaptation strategies. The first
improvement is given in the context of domain transfer for the event trigger
detection task. The domain transfer of the event trigger detection task can be
improved by incorporating an additional signal obtained from a
domain-independent text processing system into a multi-task model. The second
improvement involves modifying the model's architecture. For that purpose, a
method is proposed to enable bidirectional information flow across layers of
autoregressive large language models. The third improvement utilizes
autoregressive large language models as text generators through a generative
supervised in-context fine-tuning framework. The proposed model, method, and
framework demonstrate that pre-trained neural language models achieve their
best performance on sequence labeling tasks when adapted through targeted
transfer learning paradigms.

</details>


### [98] [ToolScope: Enhancing LLM Agent Tool Use through Tool Merging and Context-Aware Filtering](https://arxiv.org/abs/2510.20036)
*Marianne Menglin Liu,Daniel Garcia,Fjona Parllaku,Vikas Upadhyay,Syed Fahad Allam Shah,Dan Roth*

Main category: cs.CL

TL;DR: ToolScope enhances LLM agents by reducing tool ambiguity and optimizing tool selection within context limits, improving accuracy by up to 38.6%.


<details>
  <summary>Details</summary>
Motivation: Redundant tools and context limits hinder LLM agents' performance; ToolScope aims to resolve these issues.

Method: ToolScopeMerger merges redundant tools, while ToolScopeRetriever selects relevant tools to fit context limits.

Result: ToolScope improves tool selection accuracy by 8.38% to 38.6% across benchmarks.

Conclusion: ToolScope effectively enhances LLM tool use by addressing redundancy and context constraints.

Abstract: Large language model (LLM) agents rely on external tools to solve complex
tasks, but real-world toolsets often contain redundant tools with overlapping
names and descriptions, introducing ambiguity and reducing selection accuracy.
LLMs also face strict input context limits, preventing efficient consideration
of large toolsets. To address these challenges, we propose ToolScope, which
includes: (1) ToolScopeMerger with Auto-Correction to automatically audit and
fix tool merges, reducing redundancy, and (2) ToolScopeRetriever to rank and
select only the most relevant tools for each query, compressing toolsets to fit
within context limits without sacrificing accuracy. Evaluations on three
state-of-the-art LLMs and three open-source tool-use benchmarks show gains of
8.38% to 38.6% in tool selection accuracy, demonstrating ToolScope's
effectiveness in enhancing LLM tool use.

</details>


### [99] [From Facts to Folklore: Evaluating Large Language Models on Bengali Cultural Knowledge](https://arxiv.org/abs/2510.20043)
*Nafis Chowdhury,Moinul Haque,Anika Ahmed,Nazia Tasnim,Md. Istiak Hossain Shihab,Sajjadur Rahman,Farig Sadeque*

Main category: cs.CL

TL;DR: The paper highlights gaps in multilingual LLMs' ability to capture cultural nuances, particularly in low-resource languages like Bengali, and introduces the BLanCK dataset to address this. Context improves performance.


<details>
  <summary>Details</summary>
Motivation: To address the lack of cultural evaluation in LLMs for low-resource languages like Bengali by creating a dedicated dataset (BLanCK).

Method: Developed the BLanCK dataset covering folk traditions, culinary arts, and dialects; evaluated multilingual LLMs' performance on cultural tasks with and without context.

Result: LLMs performed poorly on cultural tasks but improved significantly with context, highlighting the need for context-aware models and culturally curated data.

Conclusion: The study underscores the importance of context-aware architectures and culturally rich datasets for improving LLMs' performance in low-resource cultural tasks.

Abstract: Recent progress in NLP research has demonstrated remarkable capabilities of
large language models (LLMs) across a wide range of tasks. While recent
multilingual benchmarks have advanced cultural evaluation for LLMs, critical
gaps remain in capturing the nuances of low-resource cultures. Our work
addresses these limitations through a Bengali Language Cultural Knowledge
(BLanCK) dataset including folk traditions, culinary arts, and regional
dialects. Our investigation of several multilingual language models shows that
while these models perform well in non-cultural categories, they struggle
significantly with cultural knowledge and performance improves substantially
across all models when context is provided, emphasizing context-aware
architectures and culturally curated training data.

</details>


### [100] [Enhancing Reasoning Skills in Small Persian Medical Language Models Can Outperform Large-Scale Data Training](https://arxiv.org/abs/2510.20059)
*Mehrdad Ghassabi,Sadra Hakim,Hamidreza Baradaran Kashani,Pedram Rostami*

Main category: cs.CL

TL;DR: The study enhances reasoning in Persian language models using RLAIF and DPO, achieving better performance with less data.


<details>
  <summary>Details</summary>
Motivation: Improving reasoning skills in small language models for specialized tasks like medical QA in underrepresented languages.

Method: Used RLAIF and DPO to train models with translated medical QA data, generating correct and incorrect reasoning pairs.

Result: The trained model outperformed its predecessor despite using a smaller dataset (4.5M vs. 57M tokens).

Conclusion: Reasoning-focused training efficiently improves domain-specific models with limited data.

Abstract: Enhancing reasoning capabilities in small language models is critical for
specialized applications such as medical question answering, particularly in
underrepresented languages like Persian. In this study, we employ Reinforcement
Learning with AI Feedback (RLAIF) and Direct preference optimization (DPO) to
improve the reasoning skills of a general-purpose Persian language model. To
achieve this, we translated a multiple-choice medical question-answering
dataset into Persian and used RLAIF to generate rejected-preferred answer
pairs, which are essential for DPO training. By prompting both teacher and
student models to produce Chain-of-Thought (CoT) reasoning responses, we
compiled a dataset containing correct and incorrect reasoning trajectories.
This dataset, comprising 2 million tokens in preferred answers and 2.5 million
tokens in rejected ones, was used to train a baseline model, significantly
enhancing its medical reasoning capabilities in Persian. Remarkably, the
resulting model outperformed its predecessor, gaokerena-V, which was trained on
approximately 57 million tokens, despite leveraging a much smaller dataset.
These results highlight the efficiency and effectiveness of reasoning-focused
training approaches in developing domain-specific language models with limited
data availability.

</details>


### [101] [CreativityPrism: A Holistic Benchmark for Large Language Model Creativity](https://arxiv.org/abs/2510.20091)
*Zhaoyi Joey Hou,Bowei Alvin Zhang,Yining Lu,Bhiman Kumar Baghel,Anneliese Brei,Ximing Lu,Meng Jiang,Faeze Brahman,Snigdha Chaturvedi,Haw-Shiuan Chang,Daniel Khashabi,Xiang Lorraine Li*

Main category: cs.CL

TL;DR: The paper proposes CreativityPrism, a framework to evaluate LLM creativity across quality, novelty, and diversity dimensions using diverse tasks, domains, and metrics. It reveals gaps between proprietary and open-source models and shows weak generalization of creativity performance across tasks/dimensions.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a holistic framework for evaluating LLM creativity, which is currently fragmented due to varying definitions and measurements.

Method: CreativityPrism decomposes creativity into quality, novelty, and diversity, evaluating 17 LLMs across nine tasks, three domains, and twenty metrics.

Result: Proprietary models outperform open-source ones; performance correlates strongly within domains but weakly across domains. Diversity and quality metrics are strongly correlated, while novelty is weakly correlated with others.

Conclusion: Creativity performance does not generalize across tasks/dimensions, highlighting the need for holistic evaluation frameworks like CreativityPrism.

Abstract: Creativity is often seen as a hallmark of human intelligence. While large
language models (LLMs) are increasingly perceived as producing creative text,
there is still no holistic framework to evaluate their creativity across
diverse scenarios. Existing evaluation methods remain fragmented, with dramatic
variation across domains and tasks, largely due to differing definitions and
measurements of creativity. Inspired by the hypothesis that creativity is not
one fixed idea, we propose CreativityPrism, an evaluation analysis framework
that decomposes creativity into three dimensions: quality, novelty, and
diversity. CreativityPrism incorporates nine tasks, three domains, i.e.,
divergent thinking, creative writing, and logical reasoning, and twenty
evaluation metrics, which measure each dimension in task-specific, unique ways.
We evaluate 17 state-of-the-art (SoTA) proprietary and open-sourced LLMs on
CreativityPrism and analyze the performance correlations among different
metrics and task domains. Our results reveal a notable gap between proprietary
and open-source models. Overall, model performance tends to be highly
correlated across tasks within the same domain and less so across different
domains. Among evaluation dimensions, diversity and quality metrics show strong
correlations - models that perform well on one often excel on the other -
whereas novelty exhibits much weaker correlation with either. These findings
support our hypothesis that strong performance in one creativity task or
dimension does not necessarily generalize to others, underscoring the need for
a holistic evaluation of LLM creativity.

</details>


### [102] [Leveraging the Power of Large Language Models in Entity Linking via Adaptive Routing and Targeted Reasoning](https://arxiv.org/abs/2510.20098)
*Yajie Li,Albert Galimov,Mitra Datta Ganapaneni,Pujitha Thejaswi,De Meng,Priyanshu Kumar,Saloni Potdar*

Main category: cs.CL

TL;DR: ARTER introduces a structured pipeline combining low-cost entity linking and selective LLM reasoning, outperforming traditional methods while improving efficiency.


<details>
  <summary>Details</summary>
Motivation: Reduce reliance on expensive LLM-based reasoning in Entity Linking by strategically handling easy and hard cases separately.

Method: ARTER uses candidate generation, context-based scoring, adaptive routing, and selective reasoning to categorize mentions and optimize resource usage.

Result: Outperforms ReFinED by up to +4.47% on benchmarks, matching LLM-heavy pipelines while using half the tokens.

Conclusion: ARTER balances performance and efficiency in Entity Linking by adaptive routing and selective LLM reasoning.

Abstract: Entity Linking (EL) has traditionally relied on large annotated datasets and
extensive model fine-tuning. While recent few-shot methods leverage large
language models (LLMs) through prompting to reduce training requirements, they
often suffer from inefficiencies due to expensive LLM-based reasoning. ARTER
(Adaptive Routing and Targeted Entity Reasoning) presents a structured pipeline
that achieves high performance without deep fine-tuning by strategically
combining candidate generation, context-based scoring, adaptive routing, and
selective reasoning. ARTER computes a small set of complementary signals(both
embedding and LLM-based) over the retrieved candidates to categorize contextual
mentions into easy and hard cases. The cases are then handled by a
low-computational entity linker (e.g. ReFinED) and more expensive targeted
LLM-based reasoning respectively. On standard benchmarks, ARTER outperforms
ReFinED by up to +4.47%, with an average gain of +2.53% on 5 out of 6 datasets,
and performs comparably to pipelines using LLM-based reasoning for all
mentions, while being as twice as efficient in terms of the number of LLM
tokens.

</details>


### [103] [BoundRL: Efficient Structured Text Segmentation through Reinforced Boundary Generation](https://arxiv.org/abs/2510.20151)
*Haoyuan Li,Zhengyuan Shen,Sullam Jeoung,Yueyan Chen,Jiayu Li,Qi Zhu,Shuai Wang,Vassilis Ioannidis,Huzefa Rangwala*

Main category: cs.CL

TL;DR: BoundRL is a novel approach for segmenting structured texts efficiently by predicting segment-start tokens and reconstructing content, reducing costs and improving accuracy with reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Complex structured texts with diverse elements like tables and code snippets require advanced segmentation methods beyond traditional sentence or paragraph-level approaches.

Method: BoundRL jointly performs token-level segmentation and label prediction, using reinforcement learning (RLVR) with rewards optimizing fidelity and alignment, and mitigates entropy collapse via intermediate candidate perturbations.

Result: BoundRL enables small models (1.7B parameters) to outperform larger models in few-shot prompting, with RLVR improving performance over fine-tuning and intermediate candidates enhancing generalization.

Conclusion: BoundRL effectively addresses segmentation challenges in structured texts, offering cost efficiency and superior performance with smaller models.

Abstract: As structured texts become increasingly complex across diverse domains --
from technical reports to generative AI prompts -- the need for text
segmentation into semantically meaningful components becomes critical. Such
texts often contain elements beyond plain language, including tables, code
snippets, and placeholders, which conventional sentence- or paragraph-level
segmentation methods cannot handle effectively. To address this challenge, we
propose BoundRL, a novel and efficient approach that jointly performs
token-level text segmentation and label prediction for long structured texts.
Instead of generating complete contents for each segment, it generates only a
sequence of starting tokens and reconstructs the complete contents by locating
these tokens within the original texts, thereby reducing inference costs by
orders of magnitude and minimizing hallucination. To adapt the model for the
output format, BoundRL~performs reinforcement learning with verifiable rewards
(RLVR) with a specifically designed reward that jointly optimizes document
reconstruction fidelity and semantic alignment. To mitigate entropy collapse,
it further constructs intermediate candidates by systematically perturbing a
fraction of generated sequences of segments to create stepping stones toward
higher-quality solutions. To demonstrate BoundRL's effectiveness on
particularly challenging structured texts, we focus evaluation on complex
prompts used for LLM applications. Experiments show that BoundRL enables small
language models (1.7B parameters) to outperform few-shot prompting of much
larger models. Moreover, RLVR with our designed reward yields significant
improvements over supervised fine-tuning, and incorporating intermediate
candidates further improves both performance and generalization.

</details>


### [104] [Are Stereotypes Leading LLMs' Zero-Shot Stance Detection ?](https://arxiv.org/abs/2510.20154)
*Anthony Dubreuil,Antoine Gourru,Christine Largeron,Amine Trabelsi*

Main category: cs.CL

TL;DR: LLMs show bias in stance detection tasks, linking certain dialects or text complexity to incorrect views.


<details>
  <summary>Details</summary>
Motivation: Address overlooked bias in stance detection by LLMs, focusing on dialects and text complexity.

Method: Automatically annotate datasets with dialect/text complexity attributes to analyze LLM bias.

Result: LLMs exhibit stereotypes, like associating low text complexity with pro-marijuana views.

Conclusion: LLMs display significant bias in stance detection, influenced by dialects and readability.

Abstract: Large Language Models inherit stereotypes from their pretraining data,
leading to biased behavior toward certain social groups in many Natural
Language Processing tasks, such as hateful speech detection or sentiment
analysis. Surprisingly, the evaluation of this kind of bias in stance detection
methods has been largely overlooked by the community. Stance Detection involves
labeling a statement as being against, in favor, or neutral towards a specific
target and is among the most sensitive NLP tasks, as it often relates to
political leanings. In this paper, we focus on the bias of Large Language
Models when performing stance detection in a zero-shot setting. We
automatically annotate posts in pre-existing stance detection datasets with two
attributes: dialect or vernacular of a specific group and text
complexity/readability, to investigate whether these attributes influence the
model's stance detection decisions. Our results show that LLMs exhibit
significant stereotypes in stance detection tasks, such as incorrectly
associating pro-marijuana views with low text complexity and African American
dialect with opposition to Donald Trump.

</details>


### [105] [DeepWideSearch: Benchmarking Depth and Width in Agentic Information Seeking](https://arxiv.org/abs/2510.20168)
*Tian Lan,Bin Zhu,Qianghuai Jia,Junyang Ren,Haijun Li,Longyue Wang,Zhao Xu,Weihua Luo,Kaifu Zhang*

Main category: cs.CL

TL;DR: DeepWideSearch is a new benchmark evaluating agents' ability to integrate deep reasoning and wide-scale information retrieval, revealing significant challenges with current models.


<details>
  <summary>Details</summary>
Motivation: Addressing the gap in search agents' ability to simultaneously perform deep reasoning and wide-scale information collection for real-world applications.

Method: Two methods convert established datasets into 220 questions across 15 domains, testing agents on multi-hop retrieval and large-scale data processing.

Result: State-of-the-art agents achieve only a 2.39% success rate, uncovering limitations like lack of reflection and insufficient retrieval.

Conclusion: DeepWideSearch highlights the need for improved agent architectures and is released to spur research.

Abstract: Current search agents fundamentally lack the ability to simultaneously
perform \textit{deep} reasoning over multi-hop retrieval and
\textit{wide}-scale information collection-a critical deficiency for real-world
applications like comprehensive market analysis and business development. To
bridge this gap, we introduce DeepWideSearch, the first benchmark explicitly
designed to evaluate agents to integrate depth and width in information
seeking. In DeepWideSearch, agents must process a large volume of data, each
requiring deep reasoning over multi-hop retrieval paths. Specifically, we
propose two methods to converse established datasets, resulting in a curated
collection of 220 questions spanning 15 diverse domains. Extensive experiments
demonstrate that even state-of-the-art agents achieve only 2.39% average
success rate on DeepWideSearch, highlighting the substantial challenge of
integrating depth and width search in information-seeking tasks. Furthermore,
our error analysis reveals four failure modes: lack of reflection, overreliance
on internal knowledge, insufficient retrieval, and context overflow-exposing
key limitations in current agent architectures. We publicly release
DeepWideSearch to catalyze future research on more capable and robust
information-seeking agents.

</details>


### [106] [Mixture-of-Minds: Multi-Agent Reinforcement Learning for Table Understanding](https://arxiv.org/abs/2510.20176)
*Yuhang Zhou,Mingrui Zhang,Ke Li,Mingyi Wang,Qiao Liu,Qifei wang,Jiayi Liu,Fei Liu,Serena Li,Weiwi Li,Mingze Gao,Abhishek Kumar,Xiangjun Fan,Zhuokai Zhao,Lizhu Zhang*

Main category: cs.CL

TL;DR: The paper proposes Mixture-of-Minds, a multi-agent framework for table reasoning, integrating planning, coding, and answering roles to address limitations in current LLM and tool-based methods.


<details>
  <summary>Details</summary>
Motivation: Current LLM approaches struggle with arithmetic errors and hallucination, while tool-based methods lack semantic understanding. The paper aims to combine robust reasoning with reliable table processing.

Method: Mixture-of-Minds decomposes table reasoning into three specialized roles (planning, coding, answering) and uses MCTS rollouts and RL for self-improvement.

Result: The framework achieves 62.13% on TableBench, outperforming OpenAI-o4-mini-high.

Conclusion: Combining multi-agent workflows with RL effectively advances table understanding.

Abstract: Understanding and reasoning over tables is a critical capability for many
real-world applications. Large language models (LLMs) have shown promise on
this task, but current approaches remain limited. Fine-tuning based methods
strengthen language reasoning; yet they are prone to arithmetic errors and
hallucination. In contrast, tool-based methods enable precise table
manipulation but rely on rigid schemas and lack semantic understanding. These
complementary drawbacks highlight the need for approaches that integrate robust
reasoning with reliable table processing. In this work, we propose
Mixture-of-Minds, a multi-agent framework that decomposes table reasoning into
three specialized roles: planning, coding, and answering. This design enables
each agent to focus on a specific aspect of the task while leveraging code
execution for precise table manipulation. Building on this workflow, we
introduce a self-improvement training framework that employs Monte Carlo Tree
Search (MCTS) rollouts to generate pseudo-gold trajectories and optimize agents
with reinforcement learning (RL). Extensive experiments show that
Mixture-of-Minds delivers substantial gains, reaching 62.13% on TableBench and
surpassing OpenAI-o4-mini-high. These results demonstrate the promise of
combining structured multi-agent workflows with RL to advance table
understanding.

</details>


### [107] [Stuck in the Matrix: Probing Spatial Reasoning in Large Language Models](https://arxiv.org/abs/2510.20198)
*Maggie Bai,Ava Kim Cohen,Eleanor Koss,Charlie Lichtenbaum*

Main category: cs.CL

TL;DR: LLMs show moderate spatial reasoning skills on small-scale tasks but struggle significantly as complexity increases, revealing gaps in their spatial representations.


<details>
  <summary>Details</summary>
Motivation: To assess and understand the spatial reasoning capabilities of LLMs using textual inputs, focusing on their limitations and performance degradation with increasing task complexity.

Method: Tested LLMs on five spatial reasoning tasks (quadrant identification, geometric transformations, distance evaluation, word searches, tile sliding) with varying grid dimensions to measure scalability.

Result: Performance declines sharply with increased complexity (average accuracy loss of 42.7%, up to 84%), indicating weak spatial reasoning abilities in LLMs.

Conclusion: LLMs lack robust spatial representations, highlighting a gap between linguistic and spatial reasoning. This calls for future benchmarks integrating language and geometry.

Abstract: This paper explores the spatial reasoning capability of large language models
(LLMs) over textual input through a suite of five tasks aimed at probing their
spatial understanding and computational abilities. The models were tested on
both fundamental spatial reasoning and multi-step problem-solving within
structured grid-based environments using tasks such as quadrant identification,
geometric transformations, distance evaluation, word searches, and tile
sliding. Each task was scaled in complexity through increasing grid dimensions,
requiring models to extend beyond simple pattern recognition into abstract
spatial reasoning. Our results reveal that while LLMs demonstrate moderate
success in all tasks with small complexity and size, performance drops off
rapidly as scale increases, with an average loss in accuracy of 42.7%, and
reaching as high as 84%. Every test that began with over 50% accuracy showed a
loss of at least 48%, illustrating the consistent nature of the deterioration.
Furthermore, their struggles with scaling complexity hint at a lack of robust
spatial representations in their underlying architectures. This paper
underscores the gap between linguistic and spatial reasoning in LLMs, offering
insights into their current limitations, and laying the groundwork for future
integrative benchmarks at the intersection of language and geometry.

</details>


### [108] [Decoding-Free Sampling Strategies for LLM Marginalization](https://arxiv.org/abs/2510.20208)
*David Pohl,Marco Cognetta,Junyoung Lee,Naoaki Okazaki*

Main category: cs.CL

TL;DR: The paper explores decoding-free sampling strategies to efficiently approximate tokenization marginalization in language models, reducing runtime costs while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Subword tokenization in language models leads to multiple possible tokenizations for the same text, but current evaluation methods only consider one. Marginalization over all tokenizations offers a better alternative but is computationally expensive.

Method: The study proposes decoding-free sampling strategies that avoid costly generation steps, relying instead on cheap, model-agnostic sampling methods.

Result: Decoding-free strategies provide accurate marginal estimates significantly faster than traditional sampling, validated across open models.

Conclusion: Decoding-free sampling is a viable, efficient approach for marginalization in language model evaluation, as demonstrated in downstream tasks.

Abstract: Modern language models operate on subword-tokenized text in order to make a
trade-off between model size, inference speed, and vocabulary coverage. A side
effect of this is that, during inference, models are evaluated by measuring the
probability of only the specific tokenization produced as the output, despite
there being many possible ways to represent the same text with a subword
vocabulary. Recent studies have argued instead for evaluating LLMs by
marginalization - the probability mass of all tokenizations of a given text.
  Marginalization is difficult due to the number of possible tokenizations of a
text, so often approximate marginalization is done via sampling. However, a
downside of sampling is that an expensive generation step must be performed by
the LLM for each sample, which limits the number of samples that can be
acquired given a runtime budget, and therefore also the accuracy of the
approximation. Since computing the probability of a sequence given the
tokenization is relatively cheap compared to actually generating it, we
investigate sampling strategies that are decoding-free - they require no
generation from the LLM, instead relying entirely on extremely cheap sampling
strategies that are model and tokenizer agnostic.
  We investigate the approximation quality and speed of decoding-free sampling
strategies for a number of open models to find that they provide sufficiently
accurate marginal estimates at a small fraction of the runtime cost and
demonstrate its use on a set of downstream inference tasks.

</details>


### [109] [Tri-Modal Severity Fused Diagnosis across Depression and Post-traumatic Stress Disorders](https://arxiv.org/abs/2510.20239)
*Filippo Cenacchi,Deborah Richards,Longbing Cao*

Main category: cs.CL

TL;DR: The paper introduces a tri-modal framework combining text, audio, and facial signals to assess depression and PTSD severity, outperforming unimodal methods and improving robustness.


<details>
  <summary>Details</summary>
Motivation: Depression and PTSD often co-occur, complicating binary, disorder-specific automated assessments. Clinically useful diagnosis requires severity-aware, cross-disorder estimates with explanations.

Method: A tri-modal framework fuses interview text (transformer embeddings), audio (log Mel statistics), and facial signals (action units, gaze, etc.) using a calibrated late fusion classifier.

Result: The fused model matches unimodal baselines in accuracy and F1, improves robustness under noisy/missing data, and reduces regression error for PTSD. Text is key for depression; audio/facial cues are critical for PTSD.

Conclusion: The approach provides reproducible, severity-aware clinical decision support, aligning with linguistic and behavioral markers for affective disorders.

Abstract: Depression and post traumatic stress disorder (PTSD) often co-occur with
connected symptoms, complicating automated assessment, which is often binary
and disorder specific. Clinically useful diagnosis needs severity aware cross
disorder estimates and decision support explanations. Our unified tri modal
affective severity framework synchronizes and fuses interview text with
sentence level transformer embeddings, audio with log Mel statistics with
deltas, and facial signals with action units, gaze, head and pose descriptors
to output graded severities for diagnosing both depression (PHQ-8; 5 classes)
and PTSD (3 classes). Standardized features are fused via a calibrated late
fusion classifier, yielding per disorder probabilities and feature-level
attributions. This severity aware tri-modal affective fusion approach is demoed
on multi disorder concurrent depression and PTSD assessment. Stratified cross
validation on DAIC derived corpora outperforms unimodal/ablation baselines. The
fused model matches the strongest unimodal baseline on accuracy and weighted
F1, while improving decision curve utility and robustness under noisy or
missing modalities. For PTSD specifically, fusion reduces regression error and
improves class concordance. Errors cluster between adjacent severities; extreme
classes are identified reliably. Ablations show text contributes most to
depression severity, audio and facial cues are critical for PTSD, whereas
attributions align with linguistic and behavioral markers. Our approach offers
reproducible evaluation and clinician in the loop support for affective
clinical decision making.

</details>


### [110] [Context-level Language Modeling by Learning Predictive Context Embeddings](https://arxiv.org/abs/2510.20280)
*Beiya Dai,Yuliang Liu,Daozheng Xue,Qipeng Guo,Kai Chen,Xinbing Wang*

Main category: cs.CL

TL;DR: ContextLM improves language model pretraining by adding next-context prediction alongside traditional next-token prediction, enhancing semantic understanding and long-range coherence without extra computational cost.


<details>
  <summary>Details</summary>
Motivation: Next-token prediction limits models' ability to capture higher-level semantics and long-range context, prompting the need for ContextLM.

Method: ContextLM augments pretraining with next-context prediction, using error signals from future token chunks while maintaining compatibility with standard autoregressive evaluation.

Result: Experiments show improved perplexity and downstream task performance in GPT2 and Pythia models up to 1.5B parameters.

Conclusion: Next-context prediction offers a scalable, efficient way to strengthen language models, improving coherence and attention allocation.

Abstract: Next-token prediction (NTP) is the cornerstone of modern large language
models (LLMs) pretraining, driving their unprecedented capabilities in text
generation, reasoning, and instruction following. However, the token-level
prediction limits the model's capacity to capture higher-level semantic
structures and long-range contextual relationships. To overcome this
limitation, we introduce \textbf{ContextLM}, a framework that augments standard
pretraining with an inherent \textbf{next-context prediction} objective. This
mechanism trains the model to learn predictive representations of multi-token
contexts, leveraging error signals derived from future token chunks. Crucially,
ContextLM achieves this enhancement while remaining fully compatible with the
standard autoregressive, token-by-token evaluation paradigm (e.g., perplexity).
Extensive experiments on the GPT2 and Pythia model families, scaled up to
$1.5$B parameters, show that ContextLM delivers consistent improvements in both
perplexity and downstream task performance. Our analysis indicates that
next-context prediction provides a scalable and efficient pathway to stronger
language modeling, yielding better long-range coherence and more effective
attention allocation with minimal computational overhead.

</details>


### [111] [Citation Failure: Definition, Analysis and Efficient Mitigation](https://arxiv.org/abs/2510.20303)
*Jan Buchmann,Iryna Gurevych*

Main category: cs.CL

TL;DR: The paper addresses citation failure in LLM-based RAG systems, distinguishing it from response failure. It introduces CITECONTROL to study failure modes and CITENTION to improve citations.


<details>
  <summary>Details</summary>
Motivation: Citation failure occurs when LLMs provide helpful responses but fail to cite complete evidence, unlike response failure where the response itself is flawed. The study aims to understand and mitigate this issue.

Method: The paper follows a two-step approach: (1) Analyzing when citation failure occurs using CITECONTROL, a benchmark varying response-evidence relations, and (2) Proposing CITENTION, a framework combining generative, attention-based, and retrieval-based methods to improve citations.

Result: Experiments show citation failures increase with relational complexity. CITENTION demonstrates significant improvements in citation quality on CITECONTROL and transfer settings.

Conclusion: Disentangling citation failure from response failure is crucial. CITECONTROL helps identify failure modes, while CITENTION effectively mitigates them, enhancing LLM citation reliability.

Abstract: Citations from LLM-based RAG systems are supposed to simplify response
verification. However, this does not hold for citation failure, when a model
generates a helpful response, but fails to cite complete evidence. In contrast
to previous work, we propose to disentangle this from response failure, where
the response itself is flawed, and citing complete evidence is impossible. To
address citation failure, this work follows a two-step approach: (1) We study
when citation failure occurs and (2) how it can be mitigated. For step 1, we
extend prior work by investigating how the relation between response and
evidence affects citation quality. We introduce CITECONTROL, a benchmark that
systematically varies this relation to analyze failure modes. Experiments show
that failures increase with relational complexity and suggest that combining
citation methods could improve performance, motivating step 2. To improve LLM
citation efficiently, we propose CITENTION, a framework integrating generative,
attention-based, and retrieval-based methods. Results demonstrate substantial
citation improvements on CITECONTROL and in transfer settings. We make our data
and code publicly available.

</details>


### [112] [Exploring Generative Process Reward Modeling for Semi-Structured Data: A Case Study of Table Question Answering](https://arxiv.org/abs/2510.20304)
*Lei Tang,Wei Zhou,Mohsen Mesgar*

Main category: cs.CL

TL;DR: Process reward models (PRMs) are effective for complex reasoning tasks like mathematics but face challenges in semi-structured tasks like table question answering (TQA) due to irrelevant information and weak step dependencies.


<details>
  <summary>Details</summary>
Motivation: To explore the applicability of PRMs in semi-structured data tasks like TQA, addressing challenges such as domain-specific reasoning and step dependencies.

Method: Evaluated state-of-the-art generative PRMs on TQA, combining textual and code verification for solution selection.

Result: PRMs with combined verification aid selection but generalize poorly to out-of-domain data; weak correlation between step-level verification and answer accuracy.

Conclusion: Current PRMs have limitations for TQA; insights are provided for building more robust, process-aware verifiers.

Abstract: Process reward models (PRMs) improve complex reasoning in large language
models (LLMs) by grading candidate solutions step-by-step and selecting answers
via aggregated step scores. While effective in domains such as mathematics,
their applicability to tasks involving semi-structured data, like table
question answering (TQA) remains unexplored. TQA poses unique challenges for
PRMs, including abundant irrelevant information, loosely connected reasoning
steps, and domain-specific reasoning. This work presents the first systematic
study of PRMs for TQA. We evaluate state-of-the-art generative PRMs on TQA from
both answer and step perspectives. Results show that PRMs that combine textual
and code verification can aid solution selection but struggle to generalize to
out-of-domain data. Analysis reveals a weak correlation between performance in
step-level verification and answer accuracy, possibly stemming from weak step
dependencies and loose causal links. Our findings highlight limitations of
current PRMs on TQA and offer valuable insights for building more robust,
process-aware verifiers.

</details>


### [113] [Teaching Language Models to Reason with Tools](https://arxiv.org/abs/2510.20342)
*Chengpeng Li,Zhengyang Tang,Ziniu Li,Mingfeng Xue,Keqin Bao,Tian Ding,Ruoyu Sun,Benyou Wang,Xiang Wang,Junyang Lin,Dayiheng Liu*

Main category: cs.CL

TL;DR: CoRT is a post-training framework that enhances large reasoning models' ability to integrate computational tools like Code Interpreters, improving accuracy and efficiency in mathematical reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Large reasoning models struggle with complex mathematical operations due to conflicts between probabilistic reasoning and deterministic tools like Code Interpreters. CoRT aims to resolve this by optimizing LRM-CI interaction.

Method: CoRT introduces Hint-Engineering, a data synthesis strategy that injects hints into reasoning paths, and refines CI integration using rejection sampling and reinforcement learning.

Result: CoRT improves model performance by 4-8% across five datasets and reduces token usage by 30-50% compared to baselines.

Conclusion: CoRT successfully enhances the efficiency and accuracy of large reasoning models in mathematical tasks by optimizing their interaction with computational tools.

Abstract: Large reasoning models (LRMs) like OpenAI-o1 have shown impressive
capabilities in natural language reasoning. However, these models frequently
demonstrate inefficiencies or inaccuracies when tackling complex mathematical
operations. While integrating computational tools such as Code Interpreters
(CIs) offers a promising solution, it introduces a critical challenge: a
conflict between the model's internal, probabilistic reasoning and the
external, deterministic knowledge provided by the CI, which often leads models
to unproductive deliberation. To overcome this, we introduce CoRT
(Code-Optimized Reasoning Training), a post-training framework designed to
teach LRMs to effectively utilize CIs. We propose \emph{Hint-Engineering}, a
new data synthesis strategy that strategically injects diverse hints at optimal
points within reasoning paths. This approach generates high-quality,
code-integrated reasoning data specifically tailored to optimize LRM-CI
interaction. Using this method, we have synthesized 30 high-quality samples to
post-train models ranging from 1.5B to 32B parameters through supervised
fine-tuning. CoRT further refines the multi-round interleaving of external CI
usage and internal thinking by employing rejection sampling and reinforcement
learning. Our experimental evaluations demonstrate CoRT's effectiveness,
yielding absolute improvements of 4\% and 8\% on DeepSeek-R1-Distill-Qwen-32B
and DeepSeek-R1-Distill-Qwen-1.5B, respectively, across five challenging
mathematical reasoning datasets. Moreover, CoRT significantly enhances
efficiency, reducing token usage by approximately 30\% for the 32B model and
50\% for the 1.5B model compared to pure natural language reasoning baselines.
The models and code are available at: https://github.com/ChengpengLi1003/CoRT.

</details>


### [114] [Evaluating Latent Knowledge of Public Tabular Datasets in Large Language Models](https://arxiv.org/abs/2510.20351)
*Matteo Silvestri,Flavio Giorgi,Fabrizio Silvestri,Gabriele Tolomei*

Main category: cs.CL

TL;DR: The paper investigates whether LLMs' performance on tabular benchmarks is influenced by memorization rather than genuine reasoning, revealing that semantic cues in datasets significantly affect outcomes.


<details>
  <summary>Details</summary>
Motivation: To address the overlooked issue of dataset contamination in evaluating LLMs' reasoning abilities on structured data.

Method: Conducts controlled probing experiments on tabular benchmarks like Adult Income and Titanic, manipulating semantic cues.

Result: LLMs show contamination effects only for datasets with strong semantic cues; performance drops to random levels when cues are removed.

Conclusion: LLMs' apparent reasoning on tabular data may stem from memorization, necessitating evaluation protocols to distinguish memorization from genuine reasoning.

Abstract: Large Language Models (LLMs) are increasingly evaluated on their ability to
reason over structured data, yet such assessments often overlook a crucial
confound: dataset contamination. In this work, we investigate whether LLMs
exhibit prior knowledge of widely used tabular benchmarks such as Adult Income,
Titanic, and others. Through a series of controlled probing experiments, we
reveal that contamination effects emerge exclusively for datasets containing
strong semantic cues-for instance, meaningful column names or interpretable
value categories. In contrast, when such cues are removed or randomized,
performance sharply declines to near-random levels. These findings suggest that
LLMs' apparent competence on tabular reasoning tasks may, in part, reflect
memorization of publicly available datasets rather than genuine generalization.
We discuss implications for evaluation protocols and propose strategies to
disentangle semantic leakage from authentic reasoning ability in future LLM
assessments.

</details>


### [115] [FreeChunker: A Cross-Granularity Chunking Framework](https://arxiv.org/abs/2510.20356)
*Wenxuan Zhang,Yuan-Hao Jiang,Yonghe Wu*

Main category: cs.CL

TL;DR: FreeChunker introduces a flexible sentence-level chunking framework for RAG systems, improving adaptability and efficiency compared to fixed-granularity methods.


<details>
  <summary>Details</summary>
Motivation: Traditional chunking methods lack adaptability due to static boundaries, limiting effectiveness for diverse queries.

Method: FreeChunker treats sentences as atomic units, enabling flexible retrieval via arbitrary combinations, reducing computational overhead.

Result: Superior retrieval performance and computational efficiency demonstrated on LongBench V2.

Conclusion: FreeChunker's paradigm shift enhances adaptability and efficiency in RAG systems.

Abstract: Chunking strategies significantly impact the effectiveness of
Retrieval-Augmented Generation (RAG) systems. Existing methods operate within
fixed-granularity paradigms that rely on static boundary identification,
limiting their adaptability to diverse query requirements. This paper presents
FreeChunker, a Cross-Granularity Encoding Framework that fundamentally
transforms the traditional chunking paradigm: the framework treats sentences as
atomic units and shifts from static chunk segmentation to flexible retrieval
supporting arbitrary sentence combinations. This paradigm shift not only
significantly reduces the computational overhead required for semantic boundary
detection but also enhances adaptability to complex queries. Experimental
evaluation on LongBench V2 demonstrates that FreeChunker achieves superior
retrieval performance compared to traditional chunking methods, while
significantly outperforming existing approaches in computational efficiency.

</details>


### [116] [Dialogue Is Not Enough to Make a Communicative BabyLM (But Neither Is Developmentally Inspired Reinforcement Learning)](https://arxiv.org/abs/2510.20358)
*Francesca Padovani,Bastian Bunzeck,Manar Ali,Omar Momen,Arianna Bisazza,Hendrik Buschmeier,Sina Zarrieß*

Main category: cs.CL

TL;DR: The study explores if dialogue-only pre-training creates effective small language models, finding success in dialogue tasks despite underwhelming standard benchmarks.


<details>
  <summary>Details</summary>
Motivation: To determine if pre-training solely on dialogue data can produce functionally and formally suitable small language models.

Method: Pre-trained llamalogue model fine-tuned with various strategies, including PPO and DPO, to enhance communicative text generation.

Result: Models performed poorly on standard benchmarks but excelled in dialogue continuation prediction; DPO fine-tuning improved performance further.

Conclusion: Dialogue-exclusive pre-training shows promise for specialized tasks, with DPO fine-tuning as a beneficial strategy.

Abstract: We investigate whether pre-training exclusively on dialogue data results in
formally and functionally apt small language models. Based on this pre-trained
llamalogue model, we employ a variety of fine-tuning strategies to enforce
"more communicative" text generations by our models. Although our models
underperform on most standard BabyLM benchmarks, they excel at dialogue
continuation prediction in a minimal pair setting. While PPO fine-tuning has
mixed to adversarial effects on our models, DPO fine-tuning further improves
their performance on our custom dialogue benchmark.

</details>


### [117] [The Impact of Negated Text on Hallucination with Large Language Models](https://arxiv.org/abs/2510.20375)
*Jaehyung Seo,Hyeonseok Moon,Heuiseok Lim*

Main category: cs.CL

TL;DR: The paper explores how negation affects hallucination detection in LLMs, revealing challenges in detecting hallucinations in negated text.


<details>
  <summary>Details</summary>
Motivation: Recent studies overlook the impact of negated text on LLM hallucination, prompting this research.

Method: LLMs are tested on their ability to detect hallucinations in negated contexts using the NegHalu dataset.

Result: LLMs struggle with hallucination detection in negated text, often providing unreliable judgments.

Conclusion: Negation complicates hallucination detection in LLMs, highlighting unresolved challenges in this area.

Abstract: Recent studies on hallucination in large language models (LLMs) have been
actively progressing in natural language processing. However, the impact of
negated text on hallucination with LLMs remains largely unexplored. In this
paper, we set three important yet unanswered research questions and aim to
address them. To derive the answers, we investigate whether LLMs can recognize
contextual shifts caused by negation and still reliably distinguish
hallucinations comparable to affirmative cases. We also design the NegHalu
dataset by reconstructing existing hallucination detection datasets with
negated expressions. Our experiments demonstrate that LLMs struggle to detect
hallucinations in negated text effectively, often producing logically
inconsistent or unfaithful judgments. Moreover, we trace the internal state of
LLMs as they process negated inputs at the token level and reveal the
challenges of mitigating their unintended effects.

</details>


### [118] [VLSP 2025 MLQA-TSR Challenge: Vietnamese Multimodal Legal Question Answering on Traffic Sign Regulation](https://arxiv.org/abs/2510.20381)
*Son T. Luu,Trung Vo,Hiep Nguyen,Khanh Quoc Tran,Kiet Van Nguyen,Vu Tran,Ngan Luu-Thuy Nguyen,Le-Minh Nguyen*

Main category: cs.CL

TL;DR: The paper introduces VLSP 2025 MLQA-TSR, a shared task focusing on multimodal legal question answering for traffic sign regulation in Vietnam, with two subtasks achieving notable results.


<details>
  <summary>Details</summary>
Motivation: To advance research in Vietnamese multimodal legal text processing and provide a benchmark dataset for intelligent systems in multimodal legal domains, specifically traffic sign regulation.

Method: The shared task consists of two subtasks: multimodal legal retrieval and multimodal question answering.

Result: The best results reported are an F2 score of 64.55% for legal retrieval and an accuracy of 86.30% for question answering.

Conclusion: VLSP 2025 MLQA-TSR successfully establishes a benchmark for multimodal legal processing in Vietnam, demonstrating promising performance in both subtasks.

Abstract: This paper presents the VLSP 2025 MLQA-TSR - the multimodal legal question
answering on traffic sign regulation shared task at VLSP 2025. VLSP 2025
MLQA-TSR comprises two subtasks: multimodal legal retrieval and multimodal
question answering. The goal is to advance research on Vietnamese multimodal
legal text processing and to provide a benchmark dataset for building and
evaluating intelligent systems in multimodal legal domains, with a focus on
traffic sign regulation in Vietnam. The best-reported results on VLSP 2025
MLQA-TSR are an F2 score of 64.55% for multimodal legal retrieval and an
accuracy of 86.30% for multimodal question answering.

</details>


### [119] [NeoDictaBERT: Pushing the Frontier of BERT models for Hebrew](https://arxiv.org/abs/2510.20386)
*Shaltiel Shmidman,Avi Shmidman,Moshe Koppel*

Main category: cs.CL

TL;DR: The paper introduces NeoDictaBERT and NeoDictaBERT-bilingual, BERT-style models optimized for Hebrew texts, outperforming existing models on Hebrew benchmarks and excelling in retrieval tasks.


<details>
  <summary>Details</summary>
Motivation: To address the outdated architectural choices of BERT models and bridge the gap with newer transformer-based models like Llama3 and Qwen3, focusing on Hebrew NLP.

Method: The models are trained using the NeoBERT architecture, with a dedicated focus on Hebrew texts, and evaluated across various benchmarks.

Result: NeoDictaBERT and NeoDictaBERT-bilingual outperform existing models on Hebrew benchmarks, with the bilingual version showing strong performance in retrieval tasks.

Conclusion: The released models provide a strong foundation for Hebrew NLP tasks and aim to advance research and development in this domain.

Abstract: Since their initial release, BERT models have demonstrated exceptional
performance on a variety of tasks, despite their relatively small size
(BERT-base has ~100M parameters). Nevertheless, the architectural choices used
in these models are outdated compared to newer transformer-based models such as
Llama3 and Qwen3. In recent months, several architectures have been proposed to
close this gap. ModernBERT and NeoBERT both show strong improvements on English
benchmarks and significantly extend the supported context window. Following
their successes, we introduce NeoDictaBERT and NeoDictaBERT-bilingual:
BERT-style models trained using the same architecture as NeoBERT, with a
dedicated focus on Hebrew texts. These models outperform existing ones on
almost all Hebrew benchmarks and provide a strong foundation for downstream
tasks. Notably, the NeoDictaBERT-bilingual model shows strong results on
retrieval tasks, outperforming other multilingual models of similar size. In
this paper, we describe the training process and report results across various
benchmarks. We release the models to the community as part of our goal to
advance research and development in Hebrew NLP.

</details>


### [120] [Teacher Demonstrations in a BabyLM's Zone of Proximal Development for Contingent Multi-Turn Interaction](https://arxiv.org/abs/2510.20411)
*Suchir Salhan,Hongyi Gu,Donya Rooein,Diana Galvan-Sosa,Gabrielle Gaudeau,Andrew Caines,Zheng Yuan,Paula Buttery*

Main category: cs.CL

TL;DR: ContingentChat improves dialogue quality in BabyLM through targeted post-training, enhancing grammaticality and cohesion, but contingency remains challenging.


<details>
  <summary>Details</summary>
Motivation: The paper aims to enhance multi-turn contingency in dialogues between children and caregivers using a BabyLM, addressing grammaticality and cohesion.

Method: Introduces ContingentChat, a teacher-student framework, and uses a novel alignment dataset for post-training BabyLM on 100M words. Tests adaptive teacher decoding strategies.

Result: BabyLM generates more grammatical and cohesive responses, but adaptive strategies yield limited gains. ContingentChat highlights the challenge of achieving contingency.

Conclusion: Targeted post-training improves dialogue quality, but contingency remains a difficult goal for BabyLMs.

Abstract: Multi-turn dialogues between a child and a caregiver are characterized by a
property called contingency - that is, prompt, direct, and meaningful exchanges
between interlocutors. We introduce ContingentChat, a teacher-student framework
that benchmarks and improves multi-turn contingency in a BabyLM trained on 100M
words. Using a novel alignment dataset for post-training, BabyLM generates
responses that are more grammatical and cohesive. Experiments with adaptive
teacher decoding strategies show limited additional gains. ContingentChat
demonstrates the benefits of targeted post-training for dialogue quality and
indicates that contingency remains a challenging goal for BabyLMs.

</details>


### [121] [LM-mixup: Text Data Augmentation via Language Model based Mixup](https://arxiv.org/abs/2510.20449)
*Zhijie Deng,Zhouan Shen,Ling Li,Yao Zhou,Zhaowei Zhu,Yanji He,Wei Wang,Jiaheng Wei*

Main category: cs.CL

TL;DR: The paper introduces Instruction Distillation to transform low-quality data into high-quality instruction-output pairs, creating the MIXTURE dataset and LM-Mixup method, which outperforms full-dataset training and rivals top data selection methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of scarce high-quality instruction-following data and the underutilization of abundant low-quality data, the paper aims to enhance LLM alignment.

Method: Proposes Instruction Distillation, constructs MIXTURE dataset (144K samples), and develops LM-Mixup—combining supervised fine-tuning and reinforcement learning with GRPO for optimization.

Result: LM-Mixup's distilled data (3% of the dataset) surpasses full-dataset training and competes with state-of-the-art methods on multiple benchmarks.

Conclusion: Low-quality data, when properly distilled and augmented using LM-Mixup, significantly boosts LLM efficiency and performance.

Abstract: Instruction tuning is crucial for aligning Large Language Models (LLMs), yet
the quality of instruction-following data varies significantly. While
high-quality data is paramount, it is often scarce; conversely, abundant
low-quality data is frequently discarded, leading to substantial information
loss. Existing data augmentation methods struggle to augment this low-quality
data effectively, and the evaluation of such techniques remains poorly defined.
To address this, we formally define the task of Instruction Distillation:
distilling multiple low-quality and redundant inputs into high-quality and
coherent instruction-output pairs. Specifically, we introduce a comprehensive
data construction pipeline to create MIXTURE, a 144K-sample dataset pairing
low-quality or semantically redundant imperfect instruction clusters with their
high-quality distillations. We then introduce LM-Mixup, by first performing
supervised fine-tuning on MIXTURE and then optimizing it with reinforcement
learning. This process uses three complementary reward signals: quality,
semantic alignment, and format compliance, via Group Relative Policy
Optimization (GRPO). We demonstrate that LM-Mixup effectively augments
imperfect datasets: fine-tuning LLMs on its distilled data, which accounts for
only about 3% of the entire dataset, not only surpasses full-dataset training
but also competes with state-of-the-art high-quality data selection methods
across multiple benchmarks. Our work establishes that low-quality data is a
valuable resource when properly distilled and augmented with LM-Mixup,
significantly enhancing the efficiency and performance of instruction-tuned
LLMs.

</details>


### [122] [Systematic Evaluation of Uncertainty Estimation Methods in Large Language Models](https://arxiv.org/abs/2510.20460)
*Christian Hobelsberger,Theresa Winner,Andreas Nawroth,Oliver Mitevski,Anna-Carolina Haensch*

Main category: cs.CL

TL;DR: The paper evaluates four confidence estimation methods for LLM outputs (VCE, MSP, Sample Consistency, CoCoA), finding CoCoA the most reliable for improving calibration and discrimination of correct answers.


<details>
  <summary>Details</summary>
Motivation: To address the uncertainty and correctness variability in LLM outputs by quantifying model confidence for practical reliability.

Method: Systematic evaluation of four confidence estimation approaches (VCE, MSP, Sample Consistency, CoCoA) on four question-answering tasks using an open-source LLM.

Result: CoCoA outperforms others, enhancing calibration and discrimination of correct answers. Each method captures different confidence facets.

Conclusion: CoCoA is recommended for LLM reliability; trade-offs of each method are discussed.

Abstract: Large language models (LLMs) produce outputs with varying levels of
uncertainty, and, just as often, varying levels of correctness; making their
practical reliability far from guaranteed. To quantify this uncertainty, we
systematically evaluate four approaches for confidence estimation in LLM
outputs: VCE, MSP, Sample Consistency, and CoCoA (Vashurin et al., 2025). For
the evaluation of the approaches, we conduct experiments on four
question-answering tasks using a state-of-the-art open-source LLM. Our results
show that each uncertainty metric captures a different facet of model
confidence and that the hybrid CoCoA approach yields the best reliability
overall, improving both calibration and discrimination of correct answers. We
discuss the trade-offs of each method and provide recommendations for selecting
uncertainty measures in LLM applications.

</details>


### [123] [Mask and You Shall Receive: Optimizing Masked Language Modeling For Pretraining BabyLMs](https://arxiv.org/abs/2510.20475)
*Lukas Edman,Alexander Fraser*

Main category: cs.CL

TL;DR: Improved Masked Language Modeling (MLM) with adaptive token masking and sub-token embeddings boosts performance on GLUE tasks and beats the baseline in the BabyLM Challenge's strict-small track.


<details>
  <summary>Details</summary>
Motivation: To enhance the performance of language models by improving Masked Language Modeling (MLM) and incorporating sub-token embeddings.

Method: Adapted MLM by adjusting token masking probabilities based on prediction difficulty and added sub-token embeddings for better morphological generalization.

Result: Substantial performance increase on (Super)GLUE tasks and surpassing the baseline in the strict-small track.

Conclusion: The proposed improvements to MLM and sub-token embeddings effectively enhance model performance.

Abstract: We describe our strategy for the 2025 edition of the BabyLM Challenge. Our
main contribution is that of an improved form of Masked Language Modeling
(MLM), which adapts the probabilities of the tokens masked according to the
model's ability to predict them. The results show a substantial increase in
performance on (Super)GLUE tasks over the standard MLM. We also incorporate
sub-token embeddings, finding that this increases the model's morphological
generalization capabilities. Our submission beats the baseline in the
strict-small track.

</details>


### [124] [RECALL: REpresentation-aligned Catastrophic-forgetting ALLeviation via Hierarchical Model Merging](https://arxiv.org/abs/2510.20479)
*Bowen Wang,Haiyuan Wan,Liwen Shi,Chen Yang,Peng He,Yue Ma,Haochen Han,Wenhao Li,Tiao Tan,Yongjian Li,Fangming Liu,Yifan Gong,Sheng Zhang*

Main category: cs.CL

TL;DR: RECALL is a representation-aware model merging framework for continual learning in LLMs, leveraging internal representations to fuse knowledge across models without historical data.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of continual learning in LLMs without accessing historical data, aiming to prevent catastrophic forgetting while integrating multi-domain knowledge.

Method: RECALL computes layer-wise similarities from hidden representations over clustered samples, enabling adaptive, hierarchical parameter fusion to align knowledge.

Result: RECALL outperforms baselines in knowledge retention and generalization across five NLP tasks, showing resistance to catastrophic forgetting.

Conclusion: RECALL provides a scalable, data-free solution for evolving LLMs, preserving domain-general features while adapting task-specific ones.

Abstract: We unveil that internal representations in large language models (LLMs) serve
as reliable proxies of learned knowledge, and propose RECALL, a novel
representation-aware model merging framework for continual learning without
access to historical data. RECALL computes inter-model similarity from
layer-wise hidden representations over clustered typical samples, and performs
adaptive, hierarchical parameter fusion to align knowledge across models. This
design enables the preservation of domain-general features in shallow layers
while allowing task-specific adaptation in deeper layers. Unlike prior methods
that require task labels or incur performance trade-offs, RECALL achieves
seamless multi-domain integration and strong resistance to catastrophic
forgetting. Extensive experiments across five NLP tasks and multiple continual
learning scenarios show that RECALL outperforms baselines in both knowledge
retention and generalization, providing a scalable and data-free solution for
evolving LLMs.

</details>


### [125] [Steering Evaluation-Aware Language Models To Act Like They Are Deployed](https://arxiv.org/abs/2510.20487)
*Tim Tian Hua,Andrew Qin,Samuel Marks,Neel Nanda*

Main category: cs.CL

TL;DR: Adding a steering vector to LLM activations can suppress evaluation-awareness, improving safety evaluation reliability by making the model act as if deployed.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) may adjust behavior during evaluations, compromising reliability. This paper aims to address this by suppressing evaluation-awareness.

Method: A two-step training process: (1) continued pretraining on factual descriptions mimicking natural emergence of evaluation-awareness, (2) expert iteration to use Python type hints in evaluations. Activation steering is then applied to suppress awareness.

Result: Steering suppresses evaluation-awareness, making the model act like it's deployed even with evaluation cues. The steering vector was effective when derived from the original model.

Conclusion: Activation steering can enhance the reliability of safety evaluations by ensuring models behave as if deployed, addressing evaluation-awareness issues.

Abstract: Large language models (LLMs) can sometimes detect when they are being
evaluated and adjust their behavior to appear more aligned, compromising the
reliability of safety evaluations. In this paper, we show that adding a
steering vector to an LLM's activations can suppress evaluation-awareness and
make the model act like it is deployed during evaluation. To study our steering
technique, we train an LLM to exhibit evaluation-aware behavior using a
two-step training process designed to mimic how this behavior could emerge
naturally. First, we perform continued pretraining on documents with factual
descriptions of the model (1) using Python type hints during evaluation but not
during deployment and (2) recognizing that the presence of a certain evaluation
cue always means that it is being tested. Then, we train the model with expert
iteration to use Python type hints in evaluation settings. The resulting model
is evaluation-aware: it writes type hints in evaluation contexts more than
deployment contexts. However, this gap can only be observed by removing the
evaluation cue. We find that activation steering can suppress evaluation
awareness and make the model act like it is deployed even when the cue is
present. Importantly, we constructed our steering vector using the original
model before our additional training. Our results suggest that AI evaluators
could improve the reliability of safety evaluations by steering models to act
like they are deployed.

</details>


### [126] [Robust Preference Alignment via Directional Neighborhood Consensus](https://arxiv.org/abs/2510.20498)
*Ruochen Mao,Yuling Shi,Xiaodong Gu,Jiaheng Wei*

Main category: cs.CL

TL;DR: A training-free method called Robust Preference Selection (RPS) improves alignment of large language models with diverse human preferences by sampling responses from a local neighborhood and selecting the best match.


<details>
  <summary>Details</summary>
Motivation: Current LLMs struggle with nuanced or underrepresented user preferences due to training data biases, leading to unpredictable performance gaps.

Method: RPS leverages directional neighborhood consensus to sample multiple responses from related preferences and selects the optimal one without retraining.

Result: RPS outperforms baselines, achieving win rates up to 69% on underrepresented preferences across three alignment paradigms.

Conclusion: RPS provides a practical, theoretically sound solution to enhance model reliability for diverse preferences without costly retraining.

Abstract: Aligning large language models with human preferences is critical for
creating reliable and controllable AI systems. A human preference can be
visualized as a high-dimensional vector where different directions represent
trade-offs between desired attributes (e.g., helpfulness vs. verbosity). Yet,
because the training data often reflects dominant, average preferences, LLMs
tend to perform well on common requests but fall short in specific, individual
needs. This mismatch creates a preference coverage gap. Existing methods often
address this through costly retraining, which may not be generalized to the
full spectrum of diverse preferences. This brittleness means that when a user's
request reflects a nuanced preference deviating from the training data's
central tendency, model performance can degrade unpredictably. To address this
challenge, we introduce Robust Preference Selection (RPS), a post-hoc,
training-free method by leveraging directional neighborhood consensus. Instead
of forcing a model to generate a response from a single, highly specific
preference, RPS samples multiple responses from a local neighborhood of related
preferences to create a superior candidate pool. It then selects the response
that best aligns with the user's original intent. We provide a theoretical
framework showing our neighborhood generation strategy is provably superior to
a strong baseline that also samples multiple candidates. Comprehensive
experiments across three distinct alignment paradigms (DPA, DPO, and SFT)
demonstrate that RPS consistently improves robustness against this baseline,
achieving win rates of up to 69% on challenging preferences from
under-represented regions of the space without any model retraining. Our work
presents a practical, theoretically-grounded solution for enhancing the
reliability of preference-aligned models.

</details>


### [127] [Hierarchical Sequence Iteration for Heterogeneous Question Answering](https://arxiv.org/abs/2510.20505)
*Ruiyi Yang,Hao Xue,Imran Razzak,Hakim Hacid,Flora D. Salim*

Main category: cs.CL

TL;DR: The paper introduces Hierarchical Sequence (HSEQ) Iteration, a framework improving retrieval-augmented generation (RAG) by unifying document, table, and KG processing into structured sequences and guided iterations for accurate, efficient QA.


<details>
  <summary>Details</summary>
Motivation: Current RAG struggles with multi-step questions and diverse evidence sources, balancing accuracy against latency and resource budgets. HSEQ aims to address these limitations.

Method: HSEQ linearizes diverse data into reversible hierarchical sequences with tags, uses structure-aware iteration (guided by Head and Iteration Agents) for evidence collection, and synthesizes answers with optional refinement.

Result: Experiments on HotpotQA, HybridQA/TAT-QA, and MetaQA show HSEQ outperforms RAG baselines in EM/F1 scores efficiently, offering format-agnostic unification, guided iteration, and reliable QA.

Conclusion: HSEQ provides a robust, efficient solution for heterogeneous QA, unifying diverse data formats and improving accuracy while reducing unnecessary resource usage.

Abstract: Retrieval-augmented generation (RAG) remains brittle on multi-step questions
and heterogeneous evidence sources, trading accuracy against latency and
token/tool budgets. This paper introducesHierarchical Sequence (HSEQ) Iteration
for Heterogeneous Question Answering, a unified framework that (i) linearize
documents, tables, and knowledge graphs into a reversible hierarchical sequence
with lightweight structural tags, and (ii) perform structure-aware iteration to
collect just-enough evidence before answer synthesis. A Head Agent provides
guidance that leads retrieval, while an Iteration Agent selects and expands
HSeq via structure-respecting actions (e.g., parent/child hops, table
row/column neighbors, KG relations); Finally the head agent composes
canonicalized evidence to genearte the final answer, with an optional
refinement loop to resolve detected contradictions. Experiments on HotpotQA
(text), HybridQA/TAT-QA (table+text), and MetaQA (KG) show consistent EM/F1
gains over strong single-pass, multi-hop, and agentic RAG baselines with high
efficiency. Besides, HSEQ exhibits three key advantages: (1) a format-agnostic
unification that enables a single policy to operate across text, tables, and
KGs without per-dataset specialization; (2) guided, budget-aware iteration that
reduces unnecessary hops, tool calls, and tokens while preserving accuracy; and
(3) evidence canonicalization for reliable QA, improving answers consistency
and auditability.

</details>


### [128] [Assessing the Political Fairness of Multilingual LLMs: A Case Study based on a 21-way Multiparallel EuroParl Dataset](https://arxiv.org/abs/2510.20508)
*Paul Lerner,François Yvon*

Main category: cs.CL

TL;DR: The paper introduces a new multilingual dataset to assess political biases in LLMs by comparing translation quality of speeches in the European Parliament, revealing systematic disparities favoring majority parties.


<details>
  <summary>Details</summary>
Motivation: To reframe political bias assessment in LLMs by analyzing fairness in multilingual translation, focusing on disparities in translation quality among political parties.

Method: Systematically compare translation quality of speeches from the European Parliament using a new 21-way multiparallel dataset (EuroParl) with political affiliations.

Result: Majority parties (left, center, right) are translated better than outsider parties, highlighting systematic biases.

Conclusion: The study provides a novel approach to evaluating political biases in LLMs and underscores disparities in translation fairness.

Abstract: The political biases of Large Language Models (LLMs) are usually assessed by
simulating their answers to English surveys. In this work, we propose an
alternative framing of political biases, relying on principles of fairness in
multilingual translation. We systematically compare the translation quality of
speeches in the European Parliament (EP), observing systematic differences with
majority parties from left, center, and right being better translated than
outsider parties. This study is made possible by a new, 21-way multiparallel
version of EuroParl, the parliamentary proceedings of the EP, which includes
the political affiliations of each speaker. The dataset consists of 1.5M
sentences for a total of 40M words and 249M characters. It covers three years,
1000+ speakers, 7 countries, 12 EU parties, 25 EU committees, and hundreds of
national parties.

</details>


### [129] [ARC-Encoder: learning compressed text representations for large language models](https://arxiv.org/abs/2510.20535)
*Hippolyte Pilchen,Edouard Grave,Patrick Pérez*

Main category: cs.CL

TL;DR: ARC-Encoder is a flexible and efficient context compression technique for LLMs, achieving state-of-the-art performance without fine-tuning or modifying the target model.


<details>
  <summary>Details</summary>
Motivation: To reduce inference costs caused by longer contexts in LLMs without compromising their general abilities.

Method: An encoder compresses context into continuous representations, replacing token embeddings in decoder LLMs. Training strategies and architecture choices are systematically studied.

Result: Achieves SOTA performance on benchmarks, improves computational efficiency, and generalizes across multiple decoders.

Conclusion: ARC-Encoder provides a portable and efficient solution for seamless context compression across LLMs.

Abstract: Recent techniques such as retrieval-augmented generation or chain-of-thought
reasoning have led to longer contexts and increased inference costs. Context
compression techniques can reduce these costs, but the most effective
approaches require fine-tuning the target model or even modifying its
architecture. This can degrade its general abilities when not used for this
specific purpose. Here we explore an alternative approach: an encoder that
compresses the context into continuous representations which replace token
embeddings in decoder LLMs. First, we perform a systematic study of training
strategies and architecture choices for the encoder. Our findings led to the
design of an Adaptable text Representations Compressor, named ARC-Encoder,
which outputs $x$-times fewer continuous representations (typically
$x\!\in\!\{4,8\}$) than text tokens. We evaluate ARC-Encoder across a variety
of LLM usage scenarios, ranging from in-context learning to context window
extension, on both instruct and base decoders. Results show that ARC-Encoder
achieves state-of-the-art performance on several benchmarks while improving
computational efficiency at inference. Finally, we demonstrate that our models
can be adapted to multiple decoders simultaneously, allowing a single encoder
to generalize across different decoder LLMs. This makes ARC-Encoder a flexible
and efficient solution for portable encoders that work seamlessly with multiple
LLMs. We release a training code at https://github.com/kyutai-labs/ARC-Encoder
, fine-tuning dataset and pretrained models are available at
https://huggingface.co/collections/kyutai/arc-encoders-68ee18787301407d60a57047 .

</details>


### [130] [The Dog the Cat Chased Stumped the Model: Measuring When Language Models Abandon Structure for Shortcuts](https://arxiv.org/abs/2510.20543)
*Sangmitra Madhusudan,Kaige Chen,Ali Emami*

Main category: cs.CL

TL;DR: CenterBench is introduced to distinguish structural understanding from semantic pattern matching in language models by testing comprehension of center-embedded sentences. Results show models rely more on semantic associations as complexity increases.


<details>
  <summary>Details</summary>
Motivation: To address the lack of methods distinguishing structural understanding from semantic pattern matching in language models.

Method: CenterBench dataset with 9,720 comprehension questions on center-embedded sentences, including syntactically identical but semantically implausible counterparts. Six models were tested.

Result: Models show widening performance gaps (up to 26.8 percentage points) between plausible and implausible sentences as complexity increases, indicating a shift to semantic associations.

Conclusion: CenterBench identifies when models abandon structural analysis for semantic shortcuts, highlighting limitations in current reasoning models.

Abstract: When language models correctly parse "The cat that the dog chased meowed,"
are they analyzing syntax or simply familiar with dogs chasing cats? Despite
extensive benchmarking, we lack methods to distinguish structural understanding
from semantic pattern matching. We introduce CenterBench, a dataset of 9,720
comprehension questions on center-embedded sentences (like "The cat [that the
dog chased] meowed") where relative clauses nest recursively, creating
processing demands from simple to deeply nested structures. Each sentence has a
syntactically identical but semantically implausible counterpart (e.g., mailmen
prescribe medicine, doctors deliver mail) and six comprehension questions
testing surface understanding, syntactic dependencies, and causal reasoning.
Testing six models reveals that performance gaps between plausible and
implausible sentences widen systematically with complexity, with models showing
median gaps up to 26.8 percentage points, quantifying when they abandon
structural analysis for semantic associations. Notably, semantic plausibility
harms performance on questions about resulting actions, where following causal
relationships matters more than semantic coherence. Reasoning models improve
accuracy but their traces show semantic shortcuts, overthinking, and answer
refusal. Unlike models whose plausibility advantage systematically widens with
complexity, humans shows variable semantic effects. CenterBench provides the
first framework to identify when models shift from structural analysis to
pattern matching.

</details>


### [131] [GlobalRAG: Enhancing Global Reasoning in Multi-hop Question Answering via Reinforcement Learning](https://arxiv.org/abs/2510.20548)
*Jinchang Luo,Mingquan Cheng,Fan Wan,Ni Li,Xiaoling Xia,Shuangshuang Tian,Tingcheng Bian,Haiwei Wang,Haohuan Fu,Yan Tao*

Main category: cs.CL

TL;DR: GlobalRAG, a reinforcement learning framework, enhances multi-hop QA by addressing global planning and execution issues, improving performance with minimal training data.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning has promise for retrieval-augmented generation (RAG), but multi-hop QA suffers from global planning absence and unfaithful execution.

Method: GlobalRAG decomposes questions into subgoals, coordinates retrieval and reasoning, and iteratively refines evidence using Planning Quality Reward and SubGoal Completion Reward.

Result: GlobalRAG outperforms baselines using only 8k training data (42% less), achieving 14.2% average improvements in EM and F1.

Conclusion: GlobalRAG effectively improves multi-hop QA by integrating global reasoning and reliable execution, demonstrating robust performance.

Abstract: Reinforcement learning has recently shown promise in improving
retrieval-augmented generation (RAG). Despite these advances, its effectiveness
in multi-hop question answering (QA) remains limited by two fundamental
limitations: (i) global planning absence to structure multi-step reasoning, and
(ii) unfaithful execution, which hinders effective query formulation and
consistent use of retrieved evidence. We propose GlobalRAG, a reinforcement
learning framework designed to enhance global reasoning in multi-hop QA.
GlobalRAG decomposes questions into subgoals, coordinates retrieval with
reasoning, and refines evidence iteratively. To guide this process, we
introduce Planning Quality Reward and SubGoal Completion Reward, which
encourage coherent planning and reliable subgoal execution. In addition, a
progressive weight annealing strategy balances process-oriented and
outcome-based objectives. Extensive experiments on both in-domain and
out-of-domain benchmarks demonstrate that GlobalRAG significantly outperforms
strong baselines while using only 8k training data (42% of the training data
used by strong baselines), achieving average improvements of 14.2% in both EM
and F1.

</details>


### [132] [Beyond Retrieval-Ranking: A Multi-Agent Cognitive Decision Framework for E-Commerce Search](https://arxiv.org/abs/2510.20567)
*Zhouwei Zhai,Mengxiang Chen,Haoyun Xia,Jin Li,Renquan Zhou,Min Yang*

Main category: cs.CL

TL;DR: The paper proposes MACDF, a multi-agent cognitive decision framework to improve e-commerce search by shifting from passive retrieval to proactive decision support, addressing limitations like semantic gaps and high decision costs.


<details>
  <summary>Details</summary>
Motivation: The retrieval-ranking paradigm in e-commerce misaligns with users' multi-stage cognitive decision processes, leading to semantic gaps, high decision costs, and lack of shopping guidance.

Method: The authors introduce MACDF, a Multi-Agent Cognitive Decision Framework, focusing on proactive decision support rather than passive retrieval.

Result: Offline evaluations show MACDF improves recommendation accuracy and user satisfaction, especially for complex queries. Online A/B testing on JD's platform confirms its effectiveness.

Conclusion: MACDF demonstrates transformative potential in redefining e-commerce search through multi-agent cognitive systems.

Abstract: The retrieval-ranking paradigm has long dominated e-commerce search, but its
reliance on query-item matching fundamentally misaligns with multi-stage
cognitive decision processes of platform users. This misalignment introduces
critical limitations: semantic gaps in complex queries, high decision costs due
to cross-platform information foraging, and the absence of professional
shopping guidance. To address these issues, we propose a Multi-Agent Cognitive
Decision Framework (MACDF), which shifts the paradigm from passive retrieval to
proactive decision support. Extensive offline evaluations demonstrate MACDF's
significant improvements in recommendation accuracy and user satisfaction,
particularly for complex queries involving negation, multi-constraint, or
reasoning demands. Online A/B testing on JD search platform confirms its
practical efficacy. This work highlights the transformative potential of
multi-agent cognitive systems in redefining e-commerce search.

</details>


### [133] [Can ChatGPT Code Communication Data Fairly?: Empirical Evidence from Multiple Collaborative Tasks](https://arxiv.org/abs/2510.20584)
*Jiangang Hao,Wenju Cui,Patrick Kyllonen,Emily Kerzabi*

Main category: cs.CL

TL;DR: ChatGPT-based coding for communication data shows no significant bias across gender and racial groups, supporting its use in large-scale collaboration assessments.


<details>
  <summary>Details</summary>
Motivation: To investigate potential bias in ChatGPT-based automated coding of communication data across demographic groups.

Method: Used ChatGPT to code communication data from negotiation, problem-solving, and decision-making tasks, comparing results across gender and racial groups.

Result: ChatGPT's coding showed no significant bias against gender or racial groups.

Conclusion: Supports adopting ChatGPT for large-scale communication and collaboration assessments due to its unbiased performance.

Abstract: Assessing communication and collaboration at scale depends on a labor
intensive task of coding communication data into categories according to
different frameworks. Prior research has established that ChatGPT can be
directly instructed with coding rubrics to code the communication data and
achieves accuracy comparable to human raters. However, whether the coding from
ChatGPT or similar AI technology exhibits bias against different demographic
groups, such as gender and race, remains unclear. To fill this gap, this paper
investigates ChatGPT-based automated coding of communication data using a
typical coding framework for collaborative problem solving, examining
differences across gender and racial groups. The analysis draws on data from
three types of collaborative tasks: negotiation, problem solving, and decision
making. Our results show that ChatGPT-based coding exhibits no significant bias
across gender and racial groups, paving the road for its adoption in
large-scale assessment of collaboration and communication.

</details>


### [134] [BUSTED at AraGenEval Shared Task: A Comparative Study of Transformer-Based Models for Arabic AI-Generated Text Detection](https://arxiv.org/abs/2510.20610)
*Ali Zain,Sareem Farooqui,Muhammad Rafi*

Main category: cs.CL

TL;DR: Submission to Arabic AI-generated text detection task, comparing AraELECTRA, CAMeLBERT, and XLM-RoBERTa. XLM-RoBERTa outperformed specialized Arabic models with an F1 score of 0.7701.


<details>
  <summary>Details</summary>
Motivation: To evaluate the effectiveness of pre-trained transformer models in detecting AI-generated Arabic text.

Method: Fine-tuned AraELECTRA, CAMeLBERT, and XLM-RoBERTa on a binary classification dataset.

Result: XLM-RoBERTa, a multilingual model, achieved the highest performance (F1 score: 0.7701), surpassing specialized Arabic models.

Conclusion: Multilingual models exhibit strong generalization capabilities for AI-generated text detection, though the task remains complex.

Abstract: This paper details our submission to the Ara- GenEval Shared Task on Arabic
AI-generated text detection, where our team, BUSTED, se- cured 5th place. We
investigated the effec- tiveness of three pre-trained transformer mod- els:
AraELECTRA, CAMeLBERT, and XLM- RoBERTa. Our approach involved fine-tuning each
model on the provided dataset for a binary classification task. Our findings
revealed a sur- prising result: the multilingual XLM-RoBERTa model achieved the
highest performance with an F1 score of 0.7701, outperforming the spe- cialized
Arabic models. This work underscores the complexities of AI-generated text
detection and highlights the strong generalization capa- bilities of
multilingual models.

</details>


### [135] [Why Did Apple Fall To The Ground: Evaluating Curiosity In Large Language Model](https://arxiv.org/abs/2510.20635)
*Haoyu Wang,Sihang Jiang,Yuyan Chen,Yitong Wang,Yanghua Xiao*

Main category: cs.CL

TL;DR: This paper evaluates LLMs' curiosity using the 5DCR framework, finding they seek knowledge more than humans but act conservatively in uncertainty. Curiosity enhances their reasoning and learning.


<details>
  <summary>Details</summary>
Motivation: To assess whether LLMs possess curiosity-driven learning akin to humans, inspired by the human curiosity assessment questionnaire 5DCR.

Method: Designed an evaluation framework based on 5DCR dimensions (e.g., Information Seeking, Thrill Seeking) to measure LLMs' curiosity.

Result: LLMs show stronger knowledge-seeking than humans but conservative choices in uncertainty. Curiosity improves reasoning and active learning.

Conclusion: LLMs can exhibit human-like curiosity, supporting future research into their learning and innovation capabilities.

Abstract: Curiosity serves as a pivotal conduit for human beings to discover and learn
new knowledge. Recent advancements of large language models (LLMs) in natural
language processing have sparked discussions regarding whether these models
possess capability of curiosity-driven learning akin to humans. In this paper,
starting from the human curiosity assessment questionnaire Five-Dimensional
Curiosity scale Revised (5DCR), we design a comprehensive evaluation framework
that covers dimensions such as Information Seeking, Thrill Seeking, and Social
Curiosity to assess the extent of curiosity exhibited by LLMs. The results
demonstrate that LLMs exhibit a stronger thirst for knowledge than humans but
still tend to make conservative choices when faced with uncertain environments.
We further investigated the relationship between curiosity and thinking of
LLMs, confirming that curious behaviors can enhance the model's reasoning and
active learning abilities. These findings suggest that LLMs have the potential
to exhibit curiosity similar to that of humans, providing experimental support
for the future development of learning capabilities and innovative research in
LLMs.

</details>


### [136] [\textsc{CantoNLU}: A benchmark for Cantonese natural language understanding](https://arxiv.org/abs/2510.20670)
*Junghyun Min,York Hay Ng,Sophia Chan,Helena Shunhua Zhao,En-Shiun Annie Lee*

Main category: cs.CL

TL;DR: The paper introduces CantoNLU, a benchmark for Cantonese NLU, covering seven tasks. It evaluates various models, finding Cantonese-adapted models perform best overall, with monolingual models excelling in syntactic tasks.


<details>
  <summary>Details</summary>
Motivation: To address the lack of evaluation frameworks for Cantonese due to policy and diglossia.

Method: Introduces CantoNLU, a benchmark with seven NLU tasks, and evaluates performance of Mandarin, Cantonese-adapted, and monolingual Cantonese models.

Result: Cantonese-adapted models perform best overall; monolingual models excel in syntactic tasks. Mandarin models remain competitive in some cases.

Conclusion: Direct transfer from Mandarin may suffice when Cantonese data is scarce. The benchmark, datasets, code, and model weights are released for future research.

Abstract: Cantonese, although spoken by millions, remains under-resourced due to policy
and diglossia. To address this scarcity of evaluation frameworks for Cantonese,
we introduce \textsc{\textbf{CantoNLU}}, a benchmark for Cantonese natural
language understanding (NLU). This novel benchmark spans seven tasks covering
syntax and semantics, including word sense disambiguation, linguistic
acceptability judgment, language detection, natural language inference,
sentiment analysis, part-of-speech tagging, and dependency parsing. In addition
to the benchmark, we provide model baseline performance across a set of models:
a Mandarin model without Cantonese training, two Cantonese-adapted models
obtained by continual pre-training a Mandarin model on Cantonese text, and a
monolingual Cantonese model trained from scratch. Results show that
Cantonese-adapted models perform best overall, while monolingual models perform
better on syntactic tasks. Mandarin models remain competitive in certain
settings, indicating that direct transfer may be sufficient when Cantonese
domain data is scarce. We release all datasets, code, and model weights to
facilitate future research in Cantonese NLP.

</details>


### [137] [Neural Diversity Regularizes Hallucinations in Small Models](https://arxiv.org/abs/2510.20690)
*Kushal Chakrabarti,Nirmal Balachundhar*

Main category: cs.CL

TL;DR: Neural diversity (decorrelated parallel representations) reduces hallucination in language models without sacrificing accuracy, validated by ND-LoRA achieving up to 25.6% reduction in hallucinations.


<details>
  <summary>Details</summary>
Motivation: Addressing persistent hallucinations in language models despite increased parameters, compute, and data, by exploring neural diversity as a solution.

Method: Propose ND-LoRA, combining parallel LoRA adapters with Barlow Twins regularization, and analyze its impact on hallucination rates and accuracy.

Result: ND-LoRA reduces hallucinations by up to 25.6% (avg. 14.6%) without degrading general accuracy, with neural diversity identified as the key factor.

Conclusion: Neural diversity is a viable third scaling axis (alongside parameters and data) to enhance language model reliability at fixed budgets.

Abstract: Language models continue to hallucinate despite increases in parameters,
compute, and data. We propose neural diversity -- decorrelated parallel
representations -- as a principled mechanism that reduces hallucination rates
at fixed parameter and data budgets. Inspired by portfolio theory, where
uncorrelated assets reduce risk by $\sqrt{P}$, we prove hallucination
probability is bounded by representational correlation: $P(H) \leq
f(\sigma^2((1-\rho(P))/P + \rho(P)), \mu^2)$, which predicts that language
models need an optimal amount of neurodiversity. To validate this, we introduce
ND-LoRA (Neural Diversity Low-Rank Adaptation), combining parallel LoRA
adapters with Barlow Twins regularization, and demonstrate that ND-LoRA reduces
hallucinations by up to 25.6% (and 14.6% on average) without degrading general
accuracy. Ablations show LoRA adapters and regularization act synergistically,
causal interventions prove neurodiversity as the mediating factor and
correlational analyses indicate scale: a 0.1% neural correlation increase is
associated with a 3.8% hallucination increase. Finally, task-dependent
optimality emerges: different tasks require different amounts of optimal
neurodiversity. Together, our results highlight neural diversity as a third
axis of scaling -- orthogonal to parameters and data -- to improve the
reliability of language models at fixed budgets.

</details>


### [138] [Structure-Conditional Minimum Bayes Risk Decoding](https://arxiv.org/abs/2510.20700)
*Bryan Eikema,Anna Rutkiewicz,Mario Giulianelli*

Main category: cs.CL

TL;DR: MBR decoding is effective in constrained tasks like translation but struggles in open-ended tasks due to sub-optimal response selection. The paper introduces adaptations to improve sensitivity to structural variability, showing significant improvements in generation quality.


<details>
  <summary>Details</summary>
Motivation: MBR decoding faces challenges in open-ended tasks like dialogue, where standard utility functions may select sub-optimal responses lacking structural awareness.

Method: Three lightweight adaptations to the utility function are introduced to enhance structural sensitivity. A dataset with latent structures (dialogue act, emotion, response structure) is curated, and new evaluation metrics are proposed.

Result: Proposed adaptations improve structural optimality, with up to 13.7 percentage points win rate improvement on benchmarks like AlpacaEval and MT-Bench.

Conclusion: Enhancing MBR's sensitivity to structural variability significantly boosts performance in open-ended tasks, validating the utility of the proposed adaptations.

Abstract: Minimum Bayes Risk (MBR) decoding has seen renewed interest as an alternative
to traditional generation strategies. While MBR has proven effective in machine
translation, where the variability of a language model's outcome space is
naturally constrained, it may face challenges in more open-ended tasks such as
dialogue or instruction-following. We hypothesise that in such settings,
applying MBR with standard similarity-based utility functions may result in
selecting responses that are broadly representative of the model's
distribution, yet sub-optimal with respect to any particular grouping of
generations that share an underlying latent structure. In this work, we
introduce three lightweight adaptations to the utility function, designed to
make MBR more sensitive to structural variability in the outcome space. To test
our hypothesis, we curate a dataset capturing three representative types of
latent structure: dialogue act, emotion, and response structure (e.g., a
sentence, a paragraph, or a list). We further propose two metrics to evaluate
the structural optimality of MBR. Our analysis demonstrates that common
similarity-based utility functions fall short by these metrics. In contrast,
our proposed adaptations considerably improve structural optimality. Finally,
we evaluate our approaches on real-world instruction-following benchmarks,
AlpacaEval and MT-Bench, and show that increased structural sensitivity
improves generation quality by up to 13.7 percentage points in win rate.

</details>


### [139] [User Perceptions of Privacy and Helpfulness in LLM Responses to Privacy-Sensitive Scenarios](https://arxiv.org/abs/2510.20721)
*Xiaoyuan Wu,Roshni Kaushik,Wenkai Li,Lujo Bauer,Koichi Onoue*

Main category: cs.CL

TL;DR: This paper evaluates how users perceive the privacy-preservation quality and helpfulness of LLM responses in privacy-sensitive scenarios, revealing low agreement among users and poor correlation between proxy LLMs and user evaluations.


<details>
  <summary>Details</summary>
Motivation: The study aims to address gaps in prior work, which relied on proxy LLMs for privacy evaluations and overlooked real user perceptions of privacy and helpfulness.

Method: A user study with 94 participants was conducted using 90 scenarios from PrivacyLens to assess LLM responses.

Result: Users showed low agreement on privacy and helpfulness evaluations, while proxy LLMs exhibited high agreement but low correlation with user perceptions.

Conclusion: Privacy and helpfulness evaluations are user-specific, highlighting the need for user-centered studies and better alignment between proxy LLMs and user perceptions.

Abstract: Large language models (LLMs) have seen rapid adoption for tasks such as
drafting emails, summarizing meetings, and answering health questions. In such
uses, users may need to share private information (e.g., health records,
contact details). To evaluate LLMs' ability to identify and redact such private
information, prior work developed benchmarks (e.g., ConfAIde, PrivacyLens) with
real-life scenarios. Using these benchmarks, researchers have found that LLMs
sometimes fail to keep secrets private when responding to complex tasks (e.g.,
leaking employee salaries in meeting summaries). However, these evaluations
rely on LLMs (proxy LLMs) to gauge compliance with privacy norms, overlooking
real users' perceptions. Moreover, prior work primarily focused on the
privacy-preservation quality of responses, without investigating nuanced
differences in helpfulness. To understand how users perceive the
privacy-preservation quality and helpfulness of LLM responses to
privacy-sensitive scenarios, we conducted a user study with 94 participants
using 90 scenarios from PrivacyLens. We found that, when evaluating identical
responses to the same scenario, users showed low agreement with each other on
the privacy-preservation quality and helpfulness of the LLM response. Further,
we found high agreement among five proxy LLMs, while each individual LLM had
low correlation with users' evaluations. These results indicate that the
privacy and helpfulness of LLM responses are often specific to individuals, and
proxy LLMs are poor estimates of how real users would perceive these responses
in privacy-sensitive scenarios. Our results suggest the need to conduct
user-centered studies on measuring LLMs' ability to help users while preserving
privacy. Additionally, future research could investigate ways to improve the
alignment between proxy LLMs and users for better estimation of users'
perceived privacy and utility.

</details>


### [140] [Automated Extraction of Fluoropyrimidine Treatment and Treatment-Related Toxicities from Clinical Notes Using Natural Language Processing](https://arxiv.org/abs/2510.20727)
*Xizhi Wu,Madeline S. Kreider,Philip E. Empey,Chenyu Li,Yanshan Wang*

Main category: cs.CL

TL;DR: This paper evaluates NLP methods to extract fluoropyrimidine treatment and toxicity data from clinical notes, finding LLM-based approaches most effective.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of extracting embedded toxicity and treatment data from clinical notes for fluoropyrimidine therapies, which cause significant side effects.

Method: Built a gold-standard dataset of 236 notes, annotated by experts, and tested rule-based, ML, DL, and LLM-based NLP methods (train-test split 80:20).

Result: LLM-based error-analysis prompting achieved perfect F1 scores (1.000), outperforming other methods like SVM (F1=0.937) and BERT (F1=0.873-0.886).

Conclusion: LLM-based NLP is highly effective for extracting treatment and toxicity data, showing strong potential for oncology research and pharmacovigilance.

Abstract: Objective: Fluoropyrimidines are widely prescribed for colorectal and breast
cancers, but are associated with toxicities such as hand-foot syndrome and
cardiotoxicity. Since toxicity documentation is often embedded in clinical
notes, we aimed to develop and evaluate natural language processing (NLP)
methods to extract treatment and toxicity information.
  Materials and Methods: We constructed a gold-standard dataset of 236 clinical
notes from 204,165 adult oncology patients. Domain experts annotated categories
related to treatment regimens and toxicities. We developed rule-based, machine
learning-based (Random Forest, Support Vector Machine [SVM], Logistic
Regression [LR]), deep learning-based (BERT, ClinicalBERT), and large language
models (LLM)-based NLP approaches (zero-shot and error-analysis prompting).
Models used an 80:20 train-test split.
  Results: Sufficient data existed to train and evaluate 5 annotated
categories. Error-analysis prompting achieved optimal precision, recall, and F1
scores (F1=1.000) for treatment and toxicities extraction, whereas zero-shot
prompting reached F1=1.000 for treatment and F1=0.876 for toxicities
extraction.LR and SVM ranked second for toxicities (F1=0.937). Deep learning
underperformed, with BERT (F1=0.873 treatment; F1= 0.839 toxicities) and
ClinicalBERT (F1=0.873 treatment; F1 = 0.886 toxicities). Rule-based methods
served as our baseline with F1 scores of 0.857 in treatment and 0.858 in
toxicities.
  Discussion: LMM-based approaches outperformed all others, followed by machine
learning methods. Machine and deep learning approaches were limited by small
training data and showed limited generalizability, particularly for rare
categories.
  Conclusion: LLM-based NLP most effectively extracted fluoropyrimidine
treatment and toxicity information from clinical notes, and has strong
potential to support oncology research and pharmacovigilance.

</details>


### [141] [Are Large Reasoning Models Good Translation Evaluators? Analysis and Performance Boost](https://arxiv.org/abs/2510.20780)
*Runzhe Zhan,Zhihong Huang,Xinyi Yang,Lidia S. Chao,Min Yang,Derek F. Wong*

Main category: cs.CL

TL;DR: The paper explores using large reasoning models (LRMs) as evaluators for machine translation (MT) quality, identifying challenges and proposing calibration via synthetic human-like thinking trajectories to improve performance.


<details>
  <summary>Details</summary>
Motivation: The potential of LRMs as evaluators for MT quality is underexplored, and the paper aims to systematically analyze their effectiveness and address key challenges.

Method: The study proposes calibrating LRMs by training them on synthetic, human-like thinking trajectories to address issues like overthinking and scoring mechanism flaws.

Result: Experiments on WMT24 Metrics benchmarks show a significant reduction in thinking budgets (~35x) and improved evaluation performance (e.g., +8.7 correlation points for R1-Distill-Qwen-7B).

Conclusion: Efficiently calibrated LRMs hold promise for advancing fine-grained automatic MT evaluation.

Abstract: Recent advancements in large reasoning models (LRMs) have introduced an
intermediate "thinking" process prior to generating final answers, improving
their reasoning capabilities on complex downstream tasks. However, the
potential of LRMs as evaluators for machine translation (MT) quality remains
underexplored. We provides the first systematic analysis of LRM-as-a-judge in
MT evaluation. We identify key challenges, revealing LRMs require tailored
evaluation materials, tend to "overthink" simpler instances and have issues
with scoring mechanisms leading to overestimation. To address these, we propose
to calibrate LRM thinking by training them on synthetic, human-like thinking
trajectories. Our experiments on WMT24 Metrics benchmarks demonstrate that this
approach largely reduces thinking budgets by ~35x while concurrently improving
evaluation performance across different LRM scales from 7B to 32B (e.g.,
R1-Distill-Qwen-7B achieves a +8.7 correlation point improvement). These
findings highlight the potential of efficiently calibrated LRMs to advance
fine-grained automatic MT evaluation.

</details>


### [142] [A Use-Case Specific Dataset for Measuring Dimensions of Responsible Performance in LLM-generated Text](https://arxiv.org/abs/2510.20782)
*Alicia Sagae,Chia-Jung Lee,Sandeep Avula,Brandon Dang,Vanessa Murdock*

Main category: cs.CL

TL;DR: The paper highlights limitations in current LLM evaluation methods, introduces a dataset for assessing Responsible AI dimensions like fairness, and proposes a method to identify gaps in LLM performance.


<details>
  <summary>Details</summary>
Motivation: Existing LLM evaluations often overlook application-specific fairness concerns, necessitating a targeted approach.

Method: A dataset was created using real-world product descriptions, parameterized by fairness attributes and gendered adjectives.

Result: The dataset helps identify gaps in quality, veracity, safety, and fairness of LLMs.

Conclusion: The work contributes a practical resource and evaluation framework for Responsible AI research.

Abstract: Current methods for evaluating large language models (LLMs) typically focus
on high-level tasks such as text generation, without targeting a particular AI
application. This approach is not sufficient for evaluating LLMs for
Responsible AI dimensions like fairness, since protected attributes that are
highly relevant in one application may be less relevant in another. In this
work, we construct a dataset that is driven by a real-world application
(generate a plain-text product description, given a list of product features),
parameterized by fairness attributes intersected with gendered adjectives and
product categories, yielding a rich set of labeled prompts. We show how to use
the data to identify quality, veracity, safety, and fairness gaps in LLMs,
contributing a proposal for LLM evaluation paired with a concrete resource for
the research community.

</details>


### [143] [Alleviating Forgetfulness of Linear Attention by Hybrid Sparse Attention and Contextualized Learnable Token Eviction](https://arxiv.org/abs/2510.20787)
*Mutian He,Philip N. Garner*

Main category: cs.CL

TL;DR: Hybrid models combining linear attention with sparse and sliding-window attention mitigate forgetfulness in retrieval tasks while maintaining efficiency.


<details>
  <summary>Details</summary>
Motivation: Linear-attention models are efficient but suffer from forgetfulness in retrieval-intensive tasks, prompting exploration of hybrid solutions.

Method: Proposes hybrid models with token mixers (sparse attention, sliding-window) and learnable token eviction to retain critical KV-pairs efficiently.

Result: Empirical evaluations show the hybrid models are effective for retrieval-intensive tasks.

Conclusion: The proposed hybrid approaches address forgetfulness in linear-attention models while preserving computational efficiency.

Abstract: Linear-attention models that compress the entire input sequence into a
fixed-size recurrent state offer an efficient alternative to Transformers, but
their finite memory induces forgetfulness that harms retrieval-intensive tasks.
To mitigate the issue, we explore a series of hybrid models that restore direct
access to past tokens. We interleave token mixers with intermediate time and
space complexity between linear and full attention, including sparse attention
with token eviction, and the query-aware native sparse attention. Particularly,
we propose a novel learnable token eviction approach. Combined with
sliding-window attention, an end-to-end trainable lightweight CNN aggregates
information from both past and future adjacent tokens to adaptively retain a
limited set of critical KV-pairs per head, maintaining linear attention's
constant time and space complexity. Efficient Triton kernels for the sparse
attention mechanisms are provided. Empirical evaluations on retrieval-intensive
benchmarks support the effectiveness of our approaches.

</details>


### [144] [Simple Context Compression: Mean-Pooling and Multi-Ratio Training](https://arxiv.org/abs/2510.20797)
*Yair Feldman,Yoav Artzi*

Main category: cs.CL

TL;DR: A simple mean-pooling approach outperforms existing methods for soft context compression in RAG with LLMs, showing strong performance across datasets and model scales.


<details>
  <summary>Details</summary>
Motivation: To reduce computational costs of long contexts in RAG with LLMs by improving soft context compression methods.

Method: Develop a lightweight mean-pooling approach and train it for multiple compression ratios, comparing it with compression-tokens architecture.

Result: Mean-pooling outperforms compression-tokens, with minimal performance drop when handling multiple compression ratios.

Conclusion: Mean-pooling is effective, but trade-offs in compression methods remain complex.

Abstract: A common strategy to reduce the computational costs of using long contexts in
retrieval-augmented generation (RAG) with large language models (LLMs) is soft
context compression, where the input sequence is transformed into a shorter
continuous representation. We develop a lightweight and simple mean-pooling
approach that consistently outperforms the widely used compression-tokens
architecture, and study training the same compressor to output multiple
compression ratios. We conduct extensive experiments across in-domain and
out-of-domain QA datasets, as well as across model families, scales, and
compression ratios. Overall, our simple mean-pooling approach achieves the
strongest performance, with a relatively small drop when training for multiple
compression ratios. More broadly though, across architectures and training
regimes the trade-offs are more nuanced, illustrating the complex landscape of
compression methods.

</details>


### [145] [On the Detectability of LLM-Generated Text: What Exactly Is LLM-Generated Text?](https://arxiv.org/abs/2510.20810)
*Mingmeng Geng,Thierry Poibeau*

Main category: cs.CL

TL;DR: The paper discusses challenges in defining and detecting LLM-generated text due to diverse usage and human edits, highlighting limitations in current benchmarks and detectors.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of a precise definition for LLM-generated text and the growing difficulty in detecting it due to diverse usage scenarios and human involvement.

Method: The paper critiques existing benchmarks and evaluation approaches for LLM-generated text detection, noting their inadequacy in real-world applications.

Result: Results indicate that current detectors are useful only under specific conditions and their outputs should be interpreted cautiously, not as definitive proof.

Conclusion: The conclusion emphasizes the need for clearer definitions and improved benchmarks, while advising that detector results should be treated as references rather than decisive indicators.

Abstract: With the widespread use of large language models (LLMs), many researchers
have turned their attention to detecting text generated by them. However, there
is no consistent or precise definition of their target, namely "LLM-generated
text". Differences in usage scenarios and the diversity of LLMs further
increase the difficulty of detection. What is commonly regarded as the
detecting target usually represents only a subset of the text that LLMs can
potentially produce. Human edits to LLM outputs, together with the subtle
influences that LLMs exert on their users, are blurring the line between
LLM-generated and human-written text. Existing benchmarks and evaluation
approaches do not adequately address the various conditions in real-world
detector applications. Hence, the numerical results of detectors are often
misunderstood, and their significance is diminishing. Therefore, detectors
remain useful under specific conditions, but their results should be
interpreted only as references rather than decisive indicators.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [146] [An Integrated Approach to Neural Architecture Search for Deep Q-Networks](https://arxiv.org/abs/2510.19872)
*Iman Rahmani,Saman Yazdannik,Morteza Tayefi,Jafar Roshanian*

Main category: cs.LG

TL;DR: NAS-DQN, an adaptive architecture optimization agent, outperforms fixed designs in DRL by dynamically reconfiguring networks based on performance feedback, achieving better efficiency and stability.


<details>
  <summary>Details</summary>
Motivation: Traditional DRL agents' performance is limited by static neural architectures, requiring costly hyperparameter searches. This work explores if adaptive architecture optimization can surpass static designs.

Method: Introduces NAS-DQN, integrating a neural architecture search controller into DRL training for dynamic reconfiguration. Evaluated against fixed architectures and random search on continuous control tasks.

Result: NAS-DQN shows superior final performance, sample efficiency, and policy stability with minimal overhead, outperforming random search and poor fixed designs.

Conclusion: Architecture adaptation is essential for optimal sample efficiency in online DRL, suggesting dynamic integration as part of the learning process.

Abstract: The performance of deep reinforcement learning agents is fundamentally
constrained by their neural network architecture, a choice traditionally made
through expensive hyperparameter searches and then fixed throughout training.
This work investigates whether online, adaptive architecture optimization can
escape this constraint and outperform static designs. We introduce NAS-DQN, an
agent that integrates a learned neural architecture search controller directly
into the DRL training loop, enabling dynamic network reconfiguration based on
cumulative performance feedback. We evaluate NAS-DQN against three
fixed-architecture baselines and a random search control on a continuous
control task, conducting experiments over multiple random seeds. Our results
demonstrate that NAS-DQN achieves superior final performance, sample
efficiency, and policy stability while incurring negligible computational
overhead. Critically, the learned search strategy substantially outperforms
both undirected random architecture exploration and poorly-chosen fixed
designs, indicating that intelligent, performance-guided search is the key
mechanism driving success. These findings establish that architecture
adaptation is not merely beneficial but necessary for optimal sample efficiency
in online deep reinforcement learning, and suggest that the design of RL agents
need not be a static offline choice but can instead be seamlessly integrated as
a dynamic component of the learning process itself.

</details>


### [147] [From Large to Small: Transferring CUDA Optimization Expertise via Reasoning Graph](https://arxiv.org/abs/2510.19873)
*Junfeng Gong,Zhiyi Wei,Junying Chen,Cheng Liu,Huawei Li*

Main category: cs.LG

TL;DR: ReGraphT is a retrieval-augmented framework that enhances small language models (SLMs) to perform CUDA code generation at LLM-level efficiency, addressing privacy and computational concerns.


<details>
  <summary>Details</summary>
Motivation: Challenges with using LLMs (privacy risks, computational costs) and limitations of SLMs in complex CUDA generation motivate the development of ReGraphT.

Method: ReGraphT uses a structured reasoning graph for CUDA optimizations, models state transitions, and employs Monte Carlo Graph Search (MCGS) for exploration.

Result: ReGraphT achieves a 2.33X average speedup on benchmarks and enables SLMs to match LLM performance without privacy or computational drawbacks.

Conclusion: ReGraphT bridges the gap between SLMs and LLMs in CUDA generation, offering a lightweight, privacy-friendly solution.

Abstract: Despite significant evolution of CUDA programming and domain-specific
libraries, effectively utilizing GPUs with massively parallel engines remains
difficult. Large language models (LLMs) show strong potential in generating
optimized CUDA code from sequential code. However, using LLMs in practice faces
two major challenges: cloud-based APIs pose risks of code leakage, and local
deployment is often computationally expensive and inefficient. These drawbacks
have spurred interest in small language models (SLMs), which are more
lightweight and privacy-friendly. Encouragingly, recent studies show that SLMs
can achieve performance comparable to LLMs on specific tasks. While SLMs can
match LLMs on domain-specific tasks, their limited reasoning abilities lead to
suboptimal performance in complex CUDA generation according to our experiments.
To bridge this gap, we propose ReGraphT, a training-free, retrieval-augmented
generation framework that transfers LLM-level reasoning to smaller models.
ReGraphT organizes CUDA optimization trajectories into a structured reasoning
graph, modeling the combined CUDA optimizations as state transitions, and
leverages Monte Carlo Graph Search (MCGS) for efficient exploration. We also
present a CUDA-specific benchmark with difficulty tiers defined by reasoning
complexity to evaluate models more comprehensively. Experiments show that
ReGraphT outperforms HPC-specific fine-tuned models and other
retrieval-augmented approaches, achieving an average 2.33X speedup on CUDAEval
and ParEval. When paired with DeepSeek-Coder-V2-Lite-Instruct and
Qwen2.5-Coder-7B-Instruct, ReGraphT enables SLMs to approach LLM-level
performance without the associated privacy risks or excessive computing
overhead.

</details>


### [148] [Balancing Specialization and Centralization: A Multi-Agent Reinforcement Learning Benchmark for Sequential Industrial Control](https://arxiv.org/abs/2510.20408)
*Tom Maus,Asma Atamna,Tobias Glasmachers*

Main category: cs.LG

TL;DR: The paper introduces an industry-inspired benchmark combining sorting and pressing tasks to evaluate modular vs. monolithic RL architectures, finding that action masking improves performance and reduces the gap between approaches.


<details>
  <summary>Details</summary>
Motivation: Industrial adoption of RL is limited by challenges like reward design and action space management. The study aims to provide a practical benchmark for multi-agent RL in industrial automation.

Method: The study evaluates modular (specialized agents) and monolithic (single agent) architectures with and without action masking using a sequential recycling scenario.

Result: Without action masking, modular architectures perform better. With action masking, both improve significantly, narrowing the performance gap.

Conclusion: Action space constraints are crucial, and specialization advantages diminish as action complexity is reduced. The benchmark aids in exploring robust RL solutions for industrial automation.

Abstract: Autonomous control of multi-stage industrial processes requires both local
specialization and global coordination. Reinforcement learning (RL) offers a
promising approach, but its industrial adoption remains limited due to
challenges such as reward design, modularity, and action space management. Many
academic benchmarks differ markedly from industrial control problems, limiting
their transferability to real-world applications. This study introduces an
enhanced industry-inspired benchmark environment that combines tasks from two
existing benchmarks, SortingEnv and ContainerGym, into a sequential recycling
scenario with sorting and pressing operations. We evaluate two control
strategies: a modular architecture with specialized agents and a monolithic
agent governing the full system, while also analyzing the impact of action
masking. Our experiments show that without action masking, agents struggle to
learn effective policies, with the modular architecture performing better. When
action masking is applied, both architectures improve substantially, and the
performance gap narrows considerably. These results highlight the decisive role
of action space constraints and suggest that the advantages of specialization
diminish as action complexity is reduced. The proposed benchmark thus provides
a valuable testbed for exploring practical and robust multi-agent RL solutions
in industrial automation, while contributing to the ongoing debate on
centralization versus specialization.

</details>


### [149] [From Optimization to Prediction: Transformer-Based Path-Flow Estimation to the Traffic Assignment Problem](https://arxiv.org/abs/2510.19889)
*Mostafa Ameli,Van Anh Le,Sulthana Shams,Alexander Skabardonis*

Main category: cs.LG

TL;DR: The paper introduces a Transformer-based deep learning model to predict equilibrium path flows in traffic networks, offering faster computation and better accuracy than traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional traffic assignment methods are computationally expensive for large networks. A more efficient and flexible solution is needed.

Method: Uses a Transformer-based deep neural network to predict path-level traffic flows, capturing correlations between OD pairs.

Result: The model is significantly faster than conventional optimization, reduces computational costs, and improves prediction accuracy.

Conclusion: The proposed approach enables efficient traffic flow analysis and supports rapid 'what-if' scenarios for transportation planning.

Abstract: The traffic assignment problem is essential for traffic flow analysis,
traditionally solved using mathematical programs under the Equilibrium
principle. These methods become computationally prohibitive for large-scale
networks due to non-linear growth in complexity with the number of OD pairs.
This study introduces a novel data-driven approach using deep neural networks,
specifically leveraging the Transformer architecture, to predict equilibrium
path flows directly. By focusing on path-level traffic distribution, the
proposed model captures intricate correlations between OD pairs, offering a
more detailed and flexible analysis compared to traditional link-level
approaches. The Transformer-based model drastically reduces computation time,
while adapting to changes in demand and network structure without the need for
recalculation. Numerical experiments are conducted on the Manhattan-like
synthetic network, the Sioux Falls network, and the Eastern-Massachusetts
network. The results demonstrate that the proposed model is orders of magnitude
faster than conventional optimization. It efficiently estimates path-level
traffic flows in multi-class networks, reducing computational costs and
improving prediction accuracy by capturing detailed trip and flow information.
The model also adapts flexibly to varying demand and network conditions,
supporting traffic management and enabling rapid `what-if' analyses for
enhanced transportation planning and policy-making.

</details>


### [150] [Thought Communication in Multiagent Collaboration](https://arxiv.org/abs/2510.20733)
*Yujia Zheng,Zhuokai Zhao,Zijian Li,Yaqi Xie,Mingze Gao,Lizhu Zhang,Kun Zhang*

Main category: cs.LG

TL;DR: The paper introduces 'thought communication,' a paradigm enabling direct mind-to-mind interaction among agents, overcoming limitations of natural language. It formalizes latent thoughts extraction, proves identifiability, and demonstrates collaborative advantages in experiments.


<details>
  <summary>Details</summary>
Motivation: Natural language's limitations hinder collective intelligence. Machines could surpass these constraints, but current systems still rely on language. This work seeks to unlock deeper agent collaboration through direct thought exchange.

Method: The authors formalize latent thoughts as a general latent variable model, proving identifiability without auxiliary data. They develop a framework to extract and assign latent thoughts to agents based on sharing patterns.

Result: Experiments on synthetic and real-world benchmarks validate the theory, showing collaborative benefits of thought communication over traditional language-based methods.

Conclusion: The work highlights the potential of leveraging latent thoughts for collective intelligence, addressing challenges unsolvable via surface-level observation alone.

Abstract: Natural language has long enabled human cooperation, but its lossy,
ambiguous, and indirect nature limits the potential of collective intelligence.
While machines are not subject to these constraints, most LLM-based multi-agent
systems still rely solely on natural language, exchanging tokens or their
embeddings. To go beyond language, we introduce a new paradigm, thought
communication, which enables agents to interact directly mind-to-mind, akin to
telepathy. To uncover these latent thoughts in a principled way, we formalize
the process as a general latent variable model, where agent states are
generated by an unknown function of underlying thoughts. We prove that, in a
nonparametric setting without auxiliary information, both shared and private
latent thoughts between any pair of agents can be identified. Moreover, the
global structure of thought sharing, including which agents share which
thoughts and how these relationships are structured, can also be recovered with
theoretical guarantees. Guided by the established theory, we develop a
framework that extracts latent thoughts from all agents prior to communication
and assigns each agent the relevant thoughts, along with their sharing
patterns. This paradigm naturally extends beyond LLMs to all modalities, as
most observational data arise from hidden generative processes. Experiments on
both synthetic and real-world benchmarks validate the theory and demonstrate
the collaborative advantages of thought communication. We hope this work
illuminates the potential of leveraging the hidden world, as many challenges
remain unsolvable through surface-level observation alone, regardless of
compute or data scale.

</details>


### [151] [FairGRPO: Fair Reinforcement Learning for Equitable Clinical Reasoning](https://arxiv.org/abs/2510.19893)
*Shiqi Dai,Wei Dai,Jiaee Cheong,Paul Pu Liang*

Main category: cs.LG

TL;DR: FairGRPO introduces hierarchical reinforcement learning to reduce bias in medical AI by adaptive weighting and unsupervised clustering, improving fairness and performance across diverse datasets.


<details>
  <summary>Details</summary>
Motivation: Address performance disparities in medical AI systems across demographic groups to prevent harm to underrepresented populations.

Method: FairGRPO uses hierarchical reinforcement learning with adaptive importance weighting and unsupervised clustering for missing demographic labels.

Result: Reduces predictive parity by 27.2% and improves F1 score by 12.49% across 7 clinical datasets.

Conclusion: FairGRPO enhances fairness and performance, leading to the release of FairMedGemma-4B, a fairness-aware clinical model.

Abstract: Medical artificial intelligence systems have achieved remarkable diagnostic
capabilities, yet they consistently exhibit performance disparities across
demographic groups, causing real-world harm to underrepresented populations.
While recent multimodal reasoning foundation models have advanced clinical
diagnosis through integrated analysis of diverse medical data, reasoning
trainings via reinforcement learning inherit and often amplify biases present
in training datasets dominated by majority populations. We introduce
Fairness-aware Group Relative Policy Optimization (FairGRPO), a hierarchical
reinforcement learning approach that promotes equitable learning across
heterogeneous clinical populations. FairGRPO employs adaptive importance
weighting of advantages based on representation, task difficulty, and data
source. To address the common issue of missing demographic labels in the
clinical domain, we further employ unsupervised clustering, which automatically
discovers latent demographic groups when labels are unavailable. Through
comprehensive experiments across 7 clinical diagnostic datasets spanning 5
clinical modalities across X-ray, CT scan, dermoscropy, mammography and
ultrasound, we demonstrate that FairGRPO reduces predictive parity by 27.2%
against all vanilla and bias mitigated RL baselines, while improving F1 score
by 12.49%. Furthermore, training dynamics analysis reveals that FairGRPO
progressively improves fairness throughout optimization, while baseline RL
methods exhibit deteriorating fairness as training progresses. Based on
FairGRPO, we release FairMedGemma-4B, a fairness-aware clinical VLLM that
achieves state-of-the-art performance while demonstrating significantly reduced
disparities across demographic groups.

</details>


### [152] [FINDER: Feature Inference on Noisy Datasets using Eigenspace Residuals](https://arxiv.org/abs/2510.19917)
*Trajan Murphy,Akshunna S. Dogra,Hanfeng Gu,Caleb Meredith,Mark Kon,Julio Enrique Castrillion-Candas*

Main category: cs.LG

TL;DR: FINDER is a framework for classification in noisy datasets, using stochastic features and KLE to improve accuracy in challenging domains like Alzheimer's disease and deforestation detection.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of noisy datasets (low signal-to-noise ratios, small sample sizes, etc.) in classification tasks with both theoretical and practical implications.

Method: FINDER integrates stochastic analysis into feature learning and inference, mapping datasets to Hilbert spaces and using KLE to decompose stochastic features into irreducible components for classification via eigen-decomposition.

Result: FINDER achieves state-of-the-art results in Alzheimer's Disease stage classification and remote sensing detection of deforestation.

Conclusion: FINDER outperforms existing methods in specific noisy-data scenarios but has limitations and identified failure modes.

Abstract: ''Noisy'' datasets (regimes with low signal to noise ratios, small sample
sizes, faulty data collection, etc) remain a key research frontier for
classification methods with both theoretical and practical implications. We
introduce FINDER, a rigorous framework for analyzing generic classification
problems, with tailored algorithms for noisy datasets. FINDER incorporates
fundamental stochastic analysis ideas into the feature learning and inference
stages to optimally account for the randomness inherent to all empirical
datasets. We construct ''stochastic features'' by first viewing empirical
datasets as realizations from an underlying random field (without assumptions
on its exact distribution) and then mapping them to appropriate Hilbert spaces.
The Kosambi-Karhunen-Lo\'eve expansion (KLE) breaks these stochastic features
into computable irreducible components, which allow classification over noisy
datasets via an eigen-decomposition: data from different classes resides in
distinct regions, identified by analyzing the spectrum of the associated
operators. We validate FINDER on several challenging, data-deficient scientific
domains, producing state of the art breakthroughs in: (i) Alzheimer's Disease
stage classification, (ii) Remote sensing detection of deforestation. We end
with a discussion on when FINDER is expected to outperform existing methods,
its failure modes, and other limitations.

</details>


### [153] [Beyond the Ideal: Analyzing the Inexact Muon Update](https://arxiv.org/abs/2510.19933)
*Egor Shulgin,Sultan AlRashed,Francesco Orabona,Peter Richtárik*

Main category: cs.LG

TL;DR: The paper analyzes Muon optimizer's practical efficiency, focusing on inexact orthogonalization updates and their impact on performance.


<details>
  <summary>Details</summary>
Motivation: Address the theory-practice gap in Muon optimizer by analyzing its inexact updates instead of idealized assumptions.

Method: Develop analysis within Linear Minimization Oracle (LMO)-based framework, introducing an additive error model.

Result: Bounds quantify performance degradation due to inexactness, revealing coupling between step size, momentum, and precision.

Conclusion: Approximation precision is critical and must be co-tuned with learning schedule.

Abstract: The Muon optimizer has rapidly emerged as a powerful, geometry-aware
alternative to AdamW, demonstrating strong performance in large-scale training
of neural networks. However, a critical theory-practice disconnect exists:
Muon's efficiency relies on fast, approximate orthogonalization, yet all prior
theoretical work analyzes an idealized, computationally intractable version
assuming exact SVD-based updates. This work moves beyond the ideal by providing
the first analysis of the inexact orthogonalized update at Muon's core. We
develop our analysis within the general framework of Linear Minimization Oracle
(LMO)-based optimization, introducing a realistic additive error model to
capture the inexactness of practical approximation schemes. Our analysis yields
explicit bounds that quantify performance degradation as a function of the LMO
inexactness/error. We reveal a fundamental coupling between this inexactness
and the optimal step size and momentum: lower oracle precision requires a
smaller step size but larger momentum parameter. These findings elevate the
approximation procedure (e.g., the number of Newton-Schulz steps) from an
implementation detail to a critical parameter that must be co-tuned with the
learning schedule. NanoGPT experiments directly confirm the predicted coupling,
with optimal learning rates clearly shifting as approximation precision
changes.

</details>


### [154] [Mitigating Privacy-Utility Trade-off in Decentralized Federated Learning via $f$-Differential Privacy](https://arxiv.org/abs/2510.19934)
*Xiang Li,Buxin Su,Chendi Wang,Qi Long,Weijie J. Su*

Main category: cs.LG

TL;DR: The paper introduces two new methods for privacy accounting in decentralized Federated Learning (FL) under differential privacy (DP), addressing challenges like sparse communication and local updates.


<details>
  <summary>Details</summary>
Motivation: Accurate privacy budget quantification in decentralized FL is difficult due to complex components like decentralized communication and local updates. The paper aims to tackle this.

Method: Proposes Pairwise Network $f$-DP (PN-$f$-DP) for privacy leakage quantification between user pairs and Secret-based $f$-Local DP (Sec-$f$-LDP) for structured noise injection.

Result: Experiments show tighter $(ε,δ)$ bounds and improved utility compared to Rényi DP-based methods.

Conclusion: The $f$-DP framework proves effective for decentralized privacy accounting, offering better privacy-utility trade-offs.

Abstract: Differentially private (DP) decentralized Federated Learning (FL) allows
local users to collaborate without sharing their data with a central server.
However, accurately quantifying the privacy budget of private FL algorithms is
challenging due to the co-existence of complex algorithmic components such as
decentralized communication and local updates. This paper addresses privacy
accounting for two decentralized FL algorithms within the $f$-differential
privacy ($f$-DP) framework. We develop two new $f$-DP-based accounting methods
tailored to decentralized settings: Pairwise Network $f$-DP (PN-$f$-DP), which
quantifies privacy leakage between user pairs under random-walk communication,
and Secret-based $f$-Local DP (Sec-$f$-LDP), which supports structured noise
injection via shared secrets. By combining tools from $f$-DP theory and Markov
chain concentration, our accounting framework captures privacy amplification
arising from sparse communication, local iterations, and correlated noise.
Experiments on synthetic and real datasets demonstrate that our methods yield
consistently tighter $(\epsilon,\delta)$ bounds and improved utility compared
to R\'enyi DP-based approaches, illustrating the benefits of $f$-DP in
decentralized privacy accounting.

</details>


### [155] [Are Greedy Task Orderings Better Than Random in Continual Linear Regression?](https://arxiv.org/abs/2510.19941)
*Matan Tsipory,Ran Levinstein,Itay Evron,Mark Kong,Deanna Needell,Daniel Soudry*

Main category: cs.LG

TL;DR: The paper explores task orderings in continual learning for linear regression, focusing on greedy orderings that maximize dissimilarity. It shows faster convergence for greedy orderings compared to random ones and provides analytical insights into their performance under different conditions.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand the impact of task orderings in continual learning, specifically focusing on greedy orderings that maximize dissimilarity between tasks, which has not been thoroughly explored previously.

Method: The authors formalize greedy task orderings using tools from the Kaczmarz method literature. They provide geometric and algebraic intuitions and conduct empirical tests on linear regression and CIFAR-100 classification tasks. Analytical proofs are also provided for high-rank and general rank settings.

Result: Greedy orderings converge faster than random ones empirically. Analytically, greedy orderings perform comparably to random ones in high-rank settings but show a repetition-dependent separation in general rank settings, with single-pass greedy orderings potentially failing catastrophically.

Conclusion: The study highlights nuanced differences between greedy and random task orderings, showing that greedy orderings can outperform random ones but require careful consideration of repetition to avoid failure in certain scenarios.

Abstract: We analyze task orderings in continual learning for linear regression,
assuming joint realizability of training data. We focus on orderings that
greedily maximize dissimilarity between consecutive tasks, a concept briefly
explored in prior work but still surrounded by open questions. Using tools from
the Kaczmarz method literature, we formalize such orderings and develop
geometric and algebraic intuitions around them. Empirically, we demonstrate
that greedy orderings converge faster than random ones in terms of the average
loss across tasks, both for linear regression with random data and for linear
probing on CIFAR-100 classification tasks. Analytically, in a high-rank
regression setting, we prove a loss bound for greedy orderings analogous to
that of random ones. However, under general rank, we establish a
repetition-dependent separation. Specifically, while prior work showed that for
random orderings, with or without replacement, the average loss after $k$
iterations is bounded by $\mathcal{O}(1/\sqrt{k})$, we prove that single-pass
greedy orderings may fail catastrophically, whereas those allowing repetition
converge at rate $\mathcal{O}(1/\sqrt[3]{k})$. Overall, we reveal nuances
within and between greedy and random orderings.

</details>


### [156] [Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty Sets](https://arxiv.org/abs/2510.19950)
*Shaocong Ma,Heng Huang*

Main category: cs.LG

TL;DR: The paper proposes elliptic uncertainty sets to address the directional nature of market impact in RL for financial trading, improving robustness and performance.


<details>
  <summary>Details</summary>
Motivation: The mismatch between training RL agents on historical data (where actions don't influence prices) and live markets (where they do) degrades performance. Traditional robust RL methods fail to capture directional market impact.

Method: Develops elliptic uncertainty sets and provides implicit/explicit closed-form solutions for worst-case uncertainty, enabling efficient robust policy evaluation.

Result: Experiments show superior Sharpe ratio and robustness under increasing trade volumes compared to traditional methods.

Conclusion: The approach offers a scalable and faithful solution for RL in financial markets by addressing directional market impact.

Abstract: In financial applications, reinforcement learning (RL) agents are commonly
trained on historical data, where their actions do not influence prices.
However, during deployment, these agents trade in live markets where their own
transactions can shift asset prices, a phenomenon known as market impact. This
mismatch between training and deployment environments can significantly degrade
performance. Traditional robust RL approaches address this model
misspecification by optimizing the worst-case performance over a set of
uncertainties, but typically rely on symmetric structures that fail to capture
the directional nature of market impact. To address this issue, we develop a
novel class of elliptic uncertainty sets. We establish both implicit and
explicit closed-form solutions for the worst-case uncertainty under these sets,
enabling efficient and tractable robust policy evaluation. Experiments on
single-asset and multi-asset trading tasks demonstrate that our method achieves
superior Sharpe ratio and remains robust under increasing trade volumes,
offering a more faithful and scalable approach to RL in financial markets.

</details>


### [157] [On the Optimal Construction of Unbiased Gradient Estimators for Zeroth-Order Optimization](https://arxiv.org/abs/2510.19953)
*Shaocong Ma,Heng Huang*

Main category: cs.LG

TL;DR: The paper introduces a new family of unbiased gradient estimators for zeroth-order optimization (ZOO), addressing bias issues in existing methods. The estimators use function evaluations, reformulate directional derivatives, and sample from specific distributions to maintain low variance. Theoretical analysis confirms optimal complexity for smooth non-convex objectives, and experiments show superior performance.


<details>
  <summary>Details</summary>
Motivation: Existing ZOO methods face limitations due to biased gradient estimators unless perturbation stepsizes vanish. The paper aims to overcome this bias issue while maintaining favorable variance.

Method: The authors propose unbiased gradient estimators based on function evaluations, reformulating directional derivatives as telescoping series and sampling from carefully designed distributions. They analyze theoretical properties and derive optimal scaling distributions and perturbation stepsizes.

Result: The proposed estimators eliminate bias and maintain favorable variance. Theoretical analysis shows optimal complexity for smooth non-convex objectives, and experiments demonstrate superior accuracy and convergence compared to standard methods.

Conclusion: The novel family of unbiased gradient estimators improves ZOO by addressing bias issues, advancing theoretical understanding, and outperforming existing methods in practical applications.

Abstract: Zeroth-order optimization (ZOO) is an important framework for stochastic
optimization when gradients are unavailable or expensive to compute. A
potential limitation of existing ZOO methods is the bias inherent in most
gradient estimators unless the perturbation stepsize vanishes. In this paper,
we overcome this biasedness issue by proposing a novel family of unbiased
gradient estimators based solely on function evaluations. By reformulating
directional derivatives as a telescoping series and sampling from carefully
designed distributions, we construct estimators that eliminate bias while
maintaining favorable variance. We analyze their theoretical properties, derive
optimal scaling distributions and perturbation stepsizes of four specific
constructions, and prove that SGD using the proposed estimators achieves
optimal complexity for smooth non-convex objectives. Experiments on synthetic
tasks and language model fine-tuning confirm the superior accuracy and
convergence of our approach compared to standard methods.

</details>


### [158] [Revisiting Zeroth-Order Optimization: Minimum-Variance Two-Point Estimators and Directionally Aligned Perturbations](https://arxiv.org/abs/2510.19975)
*Shaocong Ma,Heng Huang*

Main category: cs.LG

TL;DR: The paper introduces directionally aligned perturbations (DAPs) for zeroth-order gradient estimation, showing they minimize asymptotic variance and improve accuracy compared to fixed-length perturbations.


<details>
  <summary>Details</summary>
Motivation: Existing research focuses on fixed-length perturbations, overlooking the benefits of directionally aligned perturbations, which can enhance gradient estimation accuracy.

Method: The study formulates a constrained functional optimization problem over perturbation distributions, proposing DAPs that align with the true gradient. Theoretical and empirical analyses are conducted, including convergence analysis for stochastic gradient descent.

Result: DAPs reduce asymptotic variance and outperform traditional methods in synthetic and practical tasks under certain conditions.

Conclusion: Directionally aligned perturbations offer significant advantages over fixed-length ones, improving gradient estimation accuracy and performance in specific scenarios.

Abstract: In this paper, we explore the two-point zeroth-order gradient estimator and
identify the distribution of random perturbations that minimizes the
estimator's asymptotic variance as the perturbation stepsize tends to zero. We
formulate it as a constrained functional optimization problem over the space of
perturbation distributions. Our findings reveal that such desired perturbations
can align directionally with the true gradient, instead of maintaining a fixed
length. While existing research has largely focused on fixed-length
perturbations, the potential advantages of directional alignment have been
overlooked. To address this gap, we delve into the theoretical and empirical
properties of the directionally aligned perturbation (DAP) scheme, which
adaptively offers higher accuracy along critical directions. Additionally, we
provide a convergence analysis for stochastic gradient descent using
$\delta$-unbiased random perturbations, extending existing complexity bounds to
a wider range of perturbations. Through empirical evaluations on both synthetic
problems and practical tasks, we demonstrate that DAPs outperform traditional
methods under specific conditions.

</details>


### [159] [Towards Strong Certified Defense with Universal Asymmetric Randomization](https://arxiv.org/abs/2510.19977)
*Hanbin Hong,Ashish Kundu,Ali Payani,Binghui Wang,Yuan Hong*

Main category: cs.LG

TL;DR: UCAN introduces anisotropic noise to enhance randomized smoothing, improving certified adversarial robustness significantly over isotropic methods.


<details>
  <summary>Details</summary>
Motivation: Current methods use isotropic noise, ignoring input heterogeneity, limiting robustness certification effectiveness.

Method: UCAN transforms isotropic noise to anisotropic, offering tailored defense with theoretical guarantees and versatile noise distributions.

Result: Empirical results show up to 182.6% improvement in certified accuracy on MNIST, CIFAR10, and ImageNet.

Conclusion: UCAN effectively addresses limitations of isotropic noise, advancing certified robustness with anisotropic techniques.

Abstract: Randomized smoothing has become essential for achieving certified adversarial
robustness in machine learning models. However, current methods primarily use
isotropic noise distributions that are uniform across all data dimensions, such
as image pixels, limiting the effectiveness of robustness certification by
ignoring the heterogeneity of inputs and data dimensions. To address this
limitation, we propose UCAN: a novel technique that \underline{U}niversally
\underline{C}ertifies adversarial robustness with \underline{A}nisotropic
\underline{N}oise. UCAN is designed to enhance any existing randomized
smoothing method, transforming it from symmetric (isotropic) to asymmetric
(anisotropic) noise distributions, thereby offering a more tailored defense
against adversarial attacks. Our theoretical framework is versatile, supporting
a wide array of noise distributions for certified robustness in different
$\ell_p$-norms and applicable to any arbitrary classifier by guaranteeing the
classifier's prediction over perturbed inputs with provable robustness bounds
through tailored noise injection. Additionally, we develop a novel framework
equipped with three exemplary noise parameter generators (NPGs) to optimally
fine-tune the anisotropic noise parameters for different data dimensions,
allowing for pursuing different levels of robustness enhancements in
practice.Empirical evaluations underscore the significant leap in UCAN's
performance over existing state-of-the-art methods, demonstrating up to
$182.6\%$ improvement in certified accuracy at large certified radii on MNIST,
CIFAR10, and ImageNet datasets.\footnote{Code is anonymously available at
\href{https://github.com/youbin2014/UCAN/}{https://github.com/youbin2014/UCAN/}}

</details>


### [160] [Abstain Mask Retain Core: Time Series Prediction by Adaptive Masking Loss with Representation Consistency](https://arxiv.org/abs/2510.19980)
*Renzhao Liang,Sizhe Xu,Chenggang Xie,Jingru Chen,Feiyang Ren,Shu Yang,Takahiro Yabe*

Main category: cs.LG

TL;DR: Truncating historical data can improve prediction accuracy by reducing redundant features. The proposed AMRC method dynamically masks discriminative segments and enforces representation consistency, enhancing model performance.


<details>
  <summary>Details</summary>
Motivation: Address limitations of deep learning models in time series forecasting, where redundant features compromise signal extraction.

Method: Introduces AMRC with dynamic masking loss and representation consistency constraint to guide training and stabilize mappings.

Result: AMRC suppresses redundant features and significantly improves forecasting accuracy.

Conclusion: Challenges conventional assumptions and offers theoretical insights for robust forecasting models.

Abstract: Time series forecasting plays a pivotal role in critical domains such as
energy management and financial markets. Although deep learning-based
approaches (e.g., MLP, RNN, Transformer) have achieved remarkable progress, the
prevailing "long-sequence information gain hypothesis" exhibits inherent
limitations. Through systematic experimentation, this study reveals a
counterintuitive phenomenon: appropriately truncating historical data can
paradoxically enhance prediction accuracy, indicating that existing models
learn substantial redundant features (e.g., noise or irrelevant fluctuations)
during training, thereby compromising effective signal extraction. Building
upon information bottleneck theory, we propose an innovative solution termed
Adaptive Masking Loss with Representation Consistency (AMRC), which features
two core components: 1) Dynamic masking loss, which adaptively identified
highly discriminative temporal segments to guide gradient descent during model
training; 2) Representation consistency constraint, which stabilized the
mapping relationships among inputs, labels, and predictions. Experimental
results demonstrate that AMRC effectively suppresses redundant feature learning
while significantly improving model performance. This work not only challenges
conventional assumptions in temporal modeling but also provides novel
theoretical insights and methodological breakthroughs for developing efficient
and robust forecasting models.

</details>


### [161] [Machine Learning-Based Localization Accuracy of RFID Sensor Networks via RSSI Decision Trees and CAD Modeling for Defense Applications](https://arxiv.org/abs/2510.20019)
*Curtis Lee Shull,Merrick Green*

Main category: cs.LG

TL;DR: A study using RFID tracking with RSSI data and Decision Tree classification for defense asset storage showed moderate accuracy but struggled with rare class detection.


<details>
  <summary>Details</summary>
Motivation: To address security vulnerabilities in RFID tracking for defense assets by improving location inference and anomaly detection.

Method: Supervised learning with realistic RSSI data and Decision Tree classification in a CAD-modeled floor plan, focusing on classifying 12 lab zones.

Result: 34.2% overall accuracy; F1-scores >0.40 for some zones, but rare classes like LabZoneC were often misclassified.

Conclusion: RSSI-based decision trees can enable zone-level anomaly detection, but performance in low-coverage zones could be improved with better antenna placement or sensor fusion.

Abstract: Radio Frequency Identification (RFID) tracking may be a viable solution for
defense assets that must be stored in accordance with security guidelines.
However, poor sensor specificity (vulnerabilities include long range detection,
spoofing, and counterfeiting) can lead to erroneous detection and operational
security events. We present a supervised learning simulation with realistic
Received Signal Strength Indicator (RSSI) data and Decision Tree classification
in a Computer Assisted Design (CAD)-modeled floor plan that encapsulates some
of the challenges encountered in defense storage. In this work, we focused on
classifying 12 lab zones (LabZoneA-L) to perform location inference. The raw
dataset had approximately 980,000 reads. Class frequencies were imbalanced, and
class weights were calculated to account for class imbalance in this
multi-class setting. The model, trained on stratified subsamples to 5,000
balanced observations, yielded an overall accuracy of 34.2% and F1-scores
greater than 0.40 for multiple zones (Zones F, G, H, etc.). However, rare
classes (most notably LabZoneC) were often misclassified, even with the use of
class weights. An adjacency-aware confusion matrix was calculated to allow
better interpretation of physically adjacent zones. These results suggest that
RSSI-based decision trees can be applied in realistic simulations to enable
zone-level anomaly detection or misplacement monitoring for defense supply
logistics. Reliable classification performance in low-coverage and low-signal
zones could be improved with better antenna placement or additional sensors and
sensor fusion with other modalities.

</details>


### [162] [SALT: Step-level Advantage Assignment for Long-horizon Agents via Trajectory Graph](https://arxiv.org/abs/2510.20022)
*Jiazheng Li,Yawei Wang,David Yan,Yijun Tian,Zhichao Xu,Huan Song,Panpan Xu,Lin Lee Cheong*

Main category: cs.LG

TL;DR: SALT is a lightweight framework addressing the challenge of finer-grained advantage assignment in group-based RL algorithms for LLMs, improving performance on complex tasks.


<details>
  <summary>Details</summary>
Motivation: Current RL methods for LLMs rely on sparse, outcome-based rewards, leading to training instability and suboptimal policies due to entangled beneficial and detrimental actions.

Method: SALT constructs a graph from trajectories to quantify step quality and assign advantages, integrating seamlessly with existing group-based RL algorithms.

Result: Extensive experiments show SALT consistently improves performance on benchmarks like WebShop, ALFWorld, and AppWorld across various model sizes.

Conclusion: SALT offers a plug-and-play solution for finer-grained advantage assignment, enhancing RL-based training for LLMs without significant computational overhead.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities,
enabling language agents to excel at single-turn tasks. However, their
application to complex, multi-step, and long-horizon tasks remains challenging.
While reinforcement learning (RL) offers a promising avenue for addressing
these challenges, mainstream approaches typically rely solely on sparse,
outcome-based rewards, a limitation that becomes especially problematic for
group-based RL algorithms lacking critic models, such as Group Relative Policy
Optimization (GRPO). In such methods, uniformly rewarding or penalizing all
actions within a trajectory can lead to training instability and suboptimal
policies, because beneficial and detrimental actions are often entangled across
multi-step interactions. To address this challenge, we propose SALT, a novel
and lightweight framework that provides a finer-grained advantage assignment,
derived solely from outcome rewards. We achieve this by constructing a graph
from trajectories of the same prompt, which allows us to quantify the quality
of each step and assign advantages accordingly. Crucially, SALT is designed as
a plug-and-play module that seamlessly integrates with existing group-based RL
algorithms, requiring no modifications to the rollout procedure and introducing
negligible computational overhead. Extensive experiments on the WebShop,
ALFWorld, and AppWorld benchmarks with various model sizes demonstrate that
SALT consistently improves performance. We also conduct a thorough analysis to
validate the design choices behind SALT and offer actionable insights.

</details>


### [163] [Speculative Sampling for Parametric Temporal Point Processes](https://arxiv.org/abs/2510.20031)
*Marin Biloš,Anderson Schneider,Yuriy Nevmyvaka*

Main category: cs.LG

TL;DR: Proposes a parallel sampling algorithm for temporal point processes using rejection sampling, enabling efficient generation without model changes.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of sequential sampling in temporal point processes (TPPs) by enabling parallel generation.

Method: Introduces a rejection sampling-based algorithm for exact parallel sampling of future values from existing TPP models.

Result: The method achieves empirical speedups on real-world datasets while maintaining theoretical guarantees.

Conclusion: Bridges the gap between expressive modeling and efficient parallel generation in TPP applications.

Abstract: Temporal point processes are powerful generative models for event sequences
that capture complex dependencies in time-series data. They are commonly
specified using autoregressive models that learn the distribution of the next
event from the previous events. This makes sampling inherently sequential,
limiting efficiency. In this paper, we propose a novel algorithm based on
rejection sampling that enables exact sampling of multiple future values from
existing TPP models, in parallel, and without requiring any architectural
changes or retraining. Besides theoretical guarantees, our method demonstrates
empirical speedups on real-world datasets, bridging the gap between expressive
modeling and efficient parallel generation for large-scale TPP applications.

</details>


### [164] [Learning Personalized Ad Impact via Contextual Reinforcement Learning under Delayed Rewards](https://arxiv.org/abs/2510.20055)
*Yuwei Cheng,Zifeng Zhao,Haifeng Xu*

Main category: cs.LG

TL;DR: The paper proposes a CMDP-based model for online ad bidding, addressing delayed rewards, cumulative impacts, and customer heterogeneity, and introduces a two-stage estimator and RL algorithm for near-optimal regret bounds.


<details>
  <summary>Details</summary>
Motivation: Existing bidding strategies often fail to jointly address delayed/long-term effects, cumulative ad impacts, and customer heterogeneity in ad auctions.

Method: The authors model ad bidding as a CMDP with delayed Poisson rewards, propose a two-stage maximum likelihood estimator for efficient estimation, and design an RL algorithm for personalized bidding.

Result: The approach achieves a near-optimal regret bound of O~(dH²√T) and is validated through simulations.

Conclusion: The proposed CMDP model and RL algorithm effectively address critical factors in ad bidding, offering theoretical and practical improvements.

Abstract: Online advertising platforms use automated auctions to connect advertisers
with potential customers, requiring effective bidding strategies to maximize
profits. Accurate ad impact estimation requires considering three key factors:
delayed and long-term effects, cumulative ad impacts such as reinforcement or
fatigue, and customer heterogeneity. However, these effects are often not
jointly addressed in previous studies. To capture these factors, we model ad
bidding as a Contextual Markov Decision Process (CMDP) with delayed Poisson
rewards. For efficient estimation, we propose a two-stage maximum likelihood
estimator combined with data-splitting strategies, ensuring controlled
estimation error based on the first-stage estimator's (in)accuracy. Building on
this, we design a reinforcement learning algorithm to derive efficient
personalized bidding strategies. This approach achieves a near-optimal regret
bound of $\tilde{O}{(dH^2\sqrt{T})}$, where $d$ is the contextual dimension,
$H$ is the number of rounds, and $T$ is the number of customers. Our
theoretical findings are validated by simulation experiments.

</details>


### [165] [Not-a-Bandit: Provably No-Regret Drafter Selection in Speculative Decoding for LLMs](https://arxiv.org/abs/2510.20064)
*Hongyi Liu,Jiaji Huang,Zhen Jia,Youngsuk Park,Yu-Xiang Wang*

Main category: cs.LG

TL;DR: An algorithm improves draft model selection in speculative decoding, outperforming existing methods like EAGLE3 and BanditSpec across diverse domains.


<details>
  <summary>Details</summary>
Motivation: To enhance large language model inference efficiency by optimizing online draft model selection without additional queries.

Method: Develops an algorithm competing with the best draft model for each query, evaluating all models efficiently and reducing computation overhead.

Result: Outperforms EAGLE3 and BanditSpec, especially in domains requiring long reasoning chains.

Conclusion: The proposed method significantly improves speculative decoding efficiency and applicability.

Abstract: Speculative decoding is widely used in accelerating large language model
(LLM) inference. In this work, we focus on the online draft model selection
problem in speculative decoding. We design an algorithm that provably competes
with the best draft model in hindsight for each query in terms of either the
token acceptance probability or expected acceptance length. In particular, we
show that we can accurately evaluate all draft models, instead of only the
chosen model without incurring additional queries to the target model, which
allows us to improve exponentially over the existing bandit-based approach as
the number of draft models increases. Our approach is generically applicable
with any speculative decoding methods (single draft, multi-drafts and
draft-trees). Moreover, we design system-efficient versions of online learners
and demonstrate that the overhead in computation and latency can be
substantially reduced. We conduct extensive experiments on open-source LLMs and
diverse datasets, demonstrating that our methods substantially outperform the
state-of-the-art EAGLE3 and the BanditSpec baseline in a variety of domains
where specialized domain-expert drafters are available, especially when long
reasoning chains are required.

</details>


### [166] [A Multi-Layer Machine Learning and Econometric Pipeline for Forecasting Market Risk: Evidence from Cryptoasset Liquidity Spillovers](https://arxiv.org/abs/2510.20066)
*Yimeng Qiu,Feihuang Fang*

Main category: cs.LG

TL;DR: The paper explores how liquidity and volatility proxies of cryptoassets forecast market-wide risk using a multi-layered statistical framework, machine learning, and empirical analysis from 2021-2025.


<details>
  <summary>Details</summary>
Motivation: To understand spillovers from cryptoasset liquidity and volatility proxies in forecasting market-wide risk.

Method: Integrates three statistical layers (interactions, principal-component relations, volatility-factor projections), vector autoregression, HAR-X models, and a leakage-safe machine learning protocol.

Result: Statistically significant Granger-causal relationships and moderate out-of-sample predictive accuracy are documented.

Conclusion: The findings highlight the predictive relevance of liquidity and volatility proxies in cryptoasset markets.

Abstract: We study whether liquidity and volatility proxies of a core set of
cryptoassets generate spillovers that forecast market-wide risk. Our empirical
framework integrates three statistical layers: (A) interactions between core
liquidity and returns, (B) principal-component relations linking liquidity and
returns, and (C) volatility-factor projections that capture cross-sectional
volatility crowding. The analysis is complemented by vector autoregression
impulse responses and forecast error variance decompositions (see Granger 1969;
Sims 1980), heterogeneous autoregressive models with exogenous regressors
(HAR-X, Corsi 2009), and a leakage-safe machine learning protocol using
temporal splits, early stopping, validation-only thresholding, and SHAP-based
interpretation. Using daily data from 2021 to 2025 (1462 observations across 74
assets), we document statistically significant Granger-causal relationships
across layers and moderate out-of-sample predictive accuracy. We report the
most informative figures, including the pipeline overview, Layer A heatmap,
Layer C robustness analysis, vector autoregression variance decompositions, and
the test-set precision-recall curve. Full data and figure outputs are provided
in the artifact repository.

</details>


### [167] [Coupled Transformer Autoencoder for Disentangling Multi-Region Neural Latent Dynamics](https://arxiv.org/abs/2510.20068)
*Ram Dyuthi Sristi,Sowmya Manojna Narasimha,Jingya Huang,Alice Despatin,Simon Musall,Vikash Gilja,Gal Mishne*

Main category: cs.LG

TL;DR: CTAE is a transformer-based model for analyzing neural dynamics across brain regions, separating shared and region-specific signals while capturing non-linear, non-stationary behavior.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to capture temporal dependencies or separate shared and private neural signals, limiting understanding of multi-region brain activity.

Method: CTAE uses transformer encoders/decoders to model long-range dynamics and partitions latent space into orthogonal shared and private subspaces.

Result: CTAE outperforms existing methods in decoding behavioral variables from multi-region electrophysiology datasets.

Conclusion: CTAE provides a unified framework for analyzing complex neural dynamics across brain regions, improving signal separation and behavioral decoding.

Abstract: Simultaneous recordings from thousands of neurons across multiple brain areas
reveal rich mixtures of activity that are shared between regions and dynamics
that are unique to each region. Existing alignment or multi-view methods
neglect temporal structure, whereas dynamical latent variable models capture
temporal dependencies but are usually restricted to a single area, assume
linear read-outs, or conflate shared and private signals. We introduce the
Coupled Transformer Autoencoder (CTAE) - a sequence model that addresses both
(i) non-stationary, non-linear dynamics and (ii) separation of shared versus
region-specific structure in a single framework. CTAE employs transformer
encoders and decoders to capture long-range neural dynamics and explicitly
partitions each region's latent space into orthogonal shared and private
subspaces. We demonstrate the effectiveness of CTAE on two high-density
electrophysiology datasets with simultaneous recordings from multiple regions,
one from motor cortical areas and the other from sensory areas. CTAE extracts
meaningful representations that better decode behavioral variables compared to
existing approaches.

</details>


### [168] [ShapeX: Shapelet-Driven Post Hoc Explanations for Time Series Classification Models](https://arxiv.org/abs/2510.20084)
*Bosong Huang,Ming Jin,Yuxuan Liang,Johan Barthelemy,Debo Cheng,Qingsong Wen,Chenghao Liu,Shirui Pan*

Main category: cs.LG

TL;DR: ShapeX is a framework for explaining time series classification by focusing on key shapelets, improving accuracy and causal relationships in explanations.


<details>
  <summary>Details</summary>
Motivation: Transparency and trust in high-stake applications like healthcare and finance necessitate better explanation methods for time series models, which current methods lack by ignoring shapelets.

Method: ShapeX segments time series into shapelet-driven segments and uses Shapley values to assess their importance, incorporating the Shapelet Describe-and-Detect (SDD) framework to identify crucial shapelets.

Result: ShapeX outperforms existing methods in identifying relevant subsequences, enhancing precision and causal fidelity in explanations, as shown in synthetic and real-world datasets.

Conclusion: ShapeX bridges the gap in time series explanations by leveraging shapelets, offering more precise and causally meaningful insights.

Abstract: Explaining time series classification models is crucial, particularly in
high-stakes applications such as healthcare and finance, where transparency and
trust play a critical role. Although numerous time series classification
methods have identified key subsequences, known as shapelets, as core features
for achieving state-of-the-art performance and validating their pivotal role in
classification outcomes, existing post-hoc time series explanation (PHTSE)
methods primarily focus on timestep-level feature attribution. These
explanation methods overlook the fundamental prior that classification outcomes
are predominantly driven by key shapelets. To bridge this gap, we present
ShapeX, an innovative framework that segments time series into meaningful
shapelet-driven segments and employs Shapley values to assess their saliency.
At the core of ShapeX lies the Shapelet Describe-and-Detect (SDD) framework,
which effectively learns a diverse set of shapelets essential for
classification. We further demonstrate that ShapeX produces explanations which
reveal causal relationships instead of just correlations, owing to the
atomicity properties of shapelets. Experimental results on both synthetic and
real-world datasets demonstrate that ShapeX outperforms existing methods in
identifying the most relevant subsequences, enhancing both the precision and
causal fidelity of time series explanations.

</details>


### [169] [Hierarchical Dual-Head Model for Suicide Risk Assessment via MentalRoBERTa](https://arxiv.org/abs/2510.20085)
*Chang Yang,Ziyi Wang,Wangfeng Tan,Zhiting Tan,Changrui Ji,Zhiming Zhou*

Main category: cs.LG

TL;DR: A hierarchical dual-head neural network using MentalRoBERTa is proposed for classifying suicide risk into four levels, addressing challenges like class imbalance and temporal complexity with a combined loss function and efficient training.


<details>
  <summary>Details</summary>
Motivation: Social media platforms are vital for detecting suicide risk, but automated systems face issues like class imbalance and temporal complexity in posting patterns.

Method: The model uses a dual-head approach (CORAL and standard classification) with a shared Transformer encoder, time interval embeddings, and a combined loss function (CORAL, Cross-Entropy, Focal Loss).

Result: The model is evaluated using 5-fold cross-validation with macro F1 score as the primary metric.

Conclusion: The proposed method effectively addresses challenges in suicide risk classification by leveraging ordinal and categorical distinctions.

Abstract: Social media platforms have become important sources for identifying suicide
risk, but automated detection systems face multiple challenges including severe
class imbalance, temporal complexity in posting patterns, and the dual nature
of risk levels as both ordinal and categorical. This paper proposes a
hierarchical dual-head neural network based on MentalRoBERTa for suicide risk
classification into four levels: indicator, ideation, behavior, and attempt.
The model employs two complementary prediction heads operating on a shared
sequence representation: a CORAL (Consistent Rank Logits) head that preserves
ordinal relationships between risk levels, and a standard classification head
that enables flexible categorical distinctions. A 3-layer Transformer encoder
with 8-head multi-head attention models temporal dependencies across post
sequences, while explicit time interval embeddings capture posting behavior
dynamics. The model is trained with a combined loss function (0.5 CORAL + 0.3
Cross-Entropy + 0.2 Focal Loss) that simultaneously addresses ordinal structure
preservation, overconfidence reduction, and class imbalance. To improve
computational efficiency, we freeze the first 6 layers (50%) of MentalRoBERTa
and employ mixed-precision training. The model is evaluated using 5-fold
stratified cross-validation with macro F1 score as the primary metric.

</details>


### [170] [Competition is the key: A Game Theoretic Causal Discovery Approach](https://arxiv.org/abs/2510.20106)
*Amartya Roy,Souvik Chakraborty*

Main category: cs.LG

TL;DR: The paper introduces a game-theoretic reinforcement learning framework for causal discovery, providing provable guarantees and scalability while improving empirical performance.


<details>
  <summary>Details</summary>
Motivation: Current causal discovery methods either lack finite-sample guarantees or fail to scale. The goal is to bridge this gap.

Method: A DDQN agent competes against strong baselines (GES or GraN-DAG), warm-starting from their solutions, ensuring guarantees like never performing worse and accelerating convergence.

Result: The method outperforms baselines on synthetic and real-world benchmarks, scales to large graphs, and matches theoretical predictions.

Conclusion: This work establishes a new class of RL-based causal discovery algorithms that are consistent, sample-efficient, and scalable, unifying empirical performance with theory.

Abstract: Causal discovery remains a central challenge in machine learning, yet
existing methods face a fundamental gap: algorithms like GES and GraN-DAG
achieve strong empirical performance but lack finite-sample guarantees, while
theoretically principled approaches fail to scale. We close this gap by
introducing a game-theoretic reinforcement learning framework for causal
discovery, where a DDQN agent directly competes against a strong baseline (GES
or GraN-DAG), always warm-starting from the opponent's solution. This design
yields three provable guarantees: the learned graph is never worse than the
opponent, warm-starting strictly accelerates convergence, and most importantly,
with high probability the algorithm selects the true best candidate graph. To
the best of our knowledge, our result makes a first-of-its-kind progress in
explaining such finite-sample guarantees in causal discovery: on synthetic SEMs
(30 nodes), the observed error probability decays with n, tightly matching
theory. On real-world benchmarks including Sachs, Asia, Alarm, Child, Hepar2,
Dream, and Andes, our method consistently improves upon GES and GraN-DAG while
remaining theoretically safe. Remarkably, it scales to large graphs such as
Hepar2 (70 nodes), Dream (100 nodes), and Andes (220 nodes). Together, these
results establish a new class of RL-based causal discovery algorithms that are
simultaneously provably consistent, sample-efficient, and practically scalable,
marking a decisive step toward unifying empirical performance with rigorous
finite-sample theory.

</details>


### [171] [On pattern classification with weighted dimensions](https://arxiv.org/abs/2510.20107)
*Ayatullah Faruk Mollah*

Main category: cs.LG

TL;DR: The paper analyzes distance measures in pattern classification, introduces a novel weighting scheme for dimensions, incorporates it into KNN, and tests it on synthetic and real datasets, showing improved accuracy.


<details>
  <summary>Details</summary>
Motivation: To address issues with Euclidean distance in pattern classification and improve KNN performance by weighting dimensions effectively.

Method: Analyzed distance norms and dimension weights, developed a new weighting scheme, integrated it into KNN, and tested on diverse datasets.

Result: The model outperformed traditional KNN, with a 10% accuracy gain in gene expression datasets, due to better neighbor selection via weighted Minkowski distance.

Conclusion: The proposed weighted KNN classifier generalizes traditional KNN, enhancing accuracy, especially in high-dimensional, small-sample datasets.

Abstract: Studies on various facets of pattern classification is often imperative while
working with multi-dimensional samples pertaining to diverse application
scenarios. In this notion, weighted dimension-based distance measure has been
one of the vital considerations in pattern analysis as it reflects the degree
of similarity between samples. Though it is often presumed to be settled with
the pervasive use of Euclidean distance, plethora of issues often surface. In
this paper, we present (a) a detail analysis on the impact of distance measure
norms and weights of dimensions along with visualization, (b) a novel weighting
scheme for each dimension, (c) incorporation of this dimensional weighting
schema into a KNN classifier, and (d) pattern classification on a variety of
synthetic as well as realistic datasets with the developed model. It has
performed well across diverse experiments in comparison to the traditional KNN
under the same experimental setups. Specifically, for gene expression datasets,
it yields significant and consistent gain in classification accuracy (around
10%) in all cross-validation experiments with different values of k. As such
datasets contain limited number of samples of high dimensions, meaningful
selection of nearest neighbours is desirable, and this requirement is
reasonably met by regulating the shape and size of the region enclosing the k
number of reference samples with the developed weighting schema and appropriate
norm. It, therefore, stands as an important generalization of KNN classifier
powered by weighted Minkowski distance with the present weighting schema.

</details>


### [172] [Why Prototypes Collapse: Diagnosing and Preventing Partial Collapse in Prototypical Self-Supervised Learning](https://arxiv.org/abs/2510.20108)
*Gabriel Y. Arteaga,Marius Aasan,Rwiddhi Chakraborty,Martine Hjelkrem-Tan,Thalles Silva,Michael Kampffmeyer,Adín Ramírez Rivera*

Main category: cs.LG

TL;DR: The paper addresses prototype collapse in self-supervised learning by decoupling prototype and encoder training, using a Gaussian mixture model for prototypes, which improves diversity and performance.


<details>
  <summary>Details</summary>
Motivation: Prototype collapse undermines the diversity and informativeness of targets in self-supervised learning, leading to redundant representations. Existing fixes like over-parameterization are ineffective.

Method: Introduces a decoupled training strategy where prototypes are modeled as a Gaussian mixture updated via online EM, separate from encoder optimization.

Result: Eliminates prototype collapse without regularization, yielding diverse prototypes and better downstream performance.

Conclusion: Decoupling prototype and encoder training resolves prototype collapse effectively, enhancing representation diversity and task performance.

Abstract: Prototypical self-supervised learning methods consistently suffer from
partial prototype collapse, where multiple prototypes converge to nearly
identical representations. This undermines their central purpose -- providing
diverse and informative targets to guide encoders toward rich representations
-- and has led practitioners to over-parameterize prototype sets or add ad-hoc
regularizers, which mitigate symptoms rather than address the root cause. We
empirically trace the collapse to the joint optimization of encoders and
prototypes, which encourages a type of shortcut learning: early in training
prototypes drift toward redundant representations that minimize loss without
necessarily enhancing representation diversity. To break the joint
optimization, we introduce a fully decoupled training strategy that learns
prototypes and encoders under separate objectives. Concretely, we model
prototypes as a Gaussian mixture updated with an online EM-style procedure,
independent of the encoder's loss. This simple yet principled decoupling
eliminates prototype collapse without explicit regularization and yields
consistently diverse prototypes and stronger downstream performance.

</details>


### [173] [There is No "apple" in Timeseries: Rethinking TSFM through the Lens of Invariance](https://arxiv.org/abs/2510.20119)
*Arian Prabowo,Flora D. Salim*

Main category: cs.LG

TL;DR: TSFMs often underperform due to improper adaptation of NLP/CV pipelines, necessitating datasets designed with temporal invariance principles for better generalization.


<details>
  <summary>Details</summary>
Motivation: Address the gap where TSFMs lag behind simpler models by shifting from opportunistic data aggregation to principled dataset design.

Method: Propose designing datasets systematically to cover temporal invariances, ensuring representational completeness.

Result: Highlights the need for an ontology of timeseries invariances based on first principles.

Conclusion: Principled dataset design is crucial for TSFMs to achieve generalization and emergent behavior.

Abstract: Timeseries foundation models (TSFMs) have multiplied, yet lightweight
supervised baselines and even classical models often match them. We argue this
gap stems from the naive importation of NLP or CV pipelines. In language and
vision, large web-scale corpora densely capture human concepts i.e. there are
countless images and text of apples. In contrast, timeseries data is built to
complement the image and text modalities. There are no timeseries dataset that
contains the concept apple. As a result, the scrape-everything-online paradigm
fails for TS. We posit that progress demands a shift from opportunistic
aggregation to principled design: constructing datasets that systematically
span the space of invariance that preserve temporal semantics. To this end, we
suggest that the ontology of timeseries invariances should be built based on
first principles. Only by ensuring representational completeness through
invariance coverage can TSFMs achieve the aligned structure necessary for
generalisation, reasoning, and truly emergent behaviour.

</details>


### [174] [Understanding Mechanistic Role of Structural and Functional Connectivity in Tau Propagation Through Multi-Layer Modeling](https://arxiv.org/abs/2510.20148)
*Tingting Dan,Xinwei Huang,Jiaqi Ding,Yinggang Zheng,Guorong Wu*

Main category: cs.LG

TL;DR: The study explores how structural (SC) and functional connectivity (FC) influence tau propagation in Alzheimer's disease (AD), revealing regional asymmetries and shifts over disease stages, linked to AD-associated genes and risk factors.


<details>
  <summary>Details</summary>
Motivation: The research aims to clarify the unclear interaction between SC and FC in driving tau spread across brain networks in AD.

Method: A multi-layer graph diffusion model was applied to analyze longitudinal neuroimaging data, examining SC-FC interactions and tau propagation.

Result: FC drives tau spread in subcortical, insula, frontal, and temporal areas, while SC dominates in occipital, parietal, and limbic regions. Dominance shifts from FC early to SC later in disease. Patterns align with AD-associated genes like CHUK and TMEM106B.

Conclusion: SC and FC asymmetrically influence tau propagation, with shifts over AD progression, and regional patterns correlate with genetic and biological risk factors.

Abstract: Emerging neuroimaging evidence shows that pathological tau proteins build up
along specific brain networks, suggesting that large-scale network architecture
plays a key role in the progression of Alzheimer's disease (AD). However, how
structural connectivity (SC) and functional connectivity (FC) interact to
influence tau propagation remains unclear. Leveraging an unprecedented volume
of longitudinal neuroimaging data, we examine SC-FC interactions through a
multi-layer graph diffusion model. Beyond showing that connectome architecture
constrains tau spread, our model reveals a regionally asymmetric contribution
of SC and FC. Specifically, FC predominantly drives tau spread in subcortical
areas, the insula, frontal and temporal cortices, whereas SC plays a larger
role in occipital, parietal, and limbic regions. The relative dominance of SC
versus FC shifts over the course of disease, with FC generally prevailing in
early AD and SC becoming primary in later stages. Spatial patterns of SC- and
FC-dominant regions strongly align with the regional expression of
AD-associated genes involved in inflammation, apoptosis, and lysosomal
function, including CHUK (IKK-alpha), TMEM106B, MCL1, NOTCH1, and TH. In
parallel, other non-modifiable risk factors (e.g., APOE genotype, sex) and
biological mechanisms (e.g., amyloid deposition) selectively reshape tau
propagation by shifting dominant routes between anatomical and functional
pathways in a region-specific manner. Findings are validated in an independent
AD cohort.

</details>


### [175] [ADP-VRSGP: Decentralized Learning with Adaptive Differential Privacy via Variance-Reduced Stochastic Gradient Push](https://arxiv.org/abs/2510.20157)
*Xiaoming Wu,Teng Liu,Xin Wang,Ming Yang,Jiguo Yu*

Main category: cs.LG

TL;DR: Proposes ADP-VRSGP, a decentralized learning method with adaptive differential privacy, dynamically adjusting noise variance and learning rate to enhance performance and training speed while ensuring privacy.


<details>
  <summary>Details</summary>
Motivation: Existing decentralized learning methods with fixed-variance noise degrade model performance and training efficiency; ADP-VRSGP addresses these issues.

Method: ADP-VRSGP uses stepwise-decaying schedules for noise variance and learning rate, progressive gradient fusion, and decentralized push-sum/aggregation techniques for time-varying topologies.

Result: Theoretical analysis shows robust convergence with improved stability and speed. Experiments confirm superiority over baselines in privacy-preserving decentralized learning.

Conclusion: ADP-VRSGP effectively balances privacy and performance in decentralized learning, offering adaptable solutions for time-varying topologies.

Abstract: Differential privacy is widely employed in decentralized learning to
safeguard sensitive data by introducing noise into model updates. However,
existing approaches that use fixed-variance noise often degrade model
performance and reduce training efficiency. To address these limitations, we
propose a novel approach called decentralized learning with adaptive
differential privacy via variance-reduced stochastic gradient push (ADP-VRSGP).
This method dynamically adjusts both the noise variance and the learning rate
using a stepwise-decaying schedule, which accelerates training and enhances
final model performance while providing node-level personalized privacy
guarantees. To counteract the slowed convergence caused by large-variance noise
in early iterations, we introduce a progressive gradient fusion strategy that
leverages historical gradients. Furthermore, ADP-VRSGP incorporates
decentralized push-sum and aggregation techniques, making it particularly
suitable for time-varying communication topologies. Through rigorous
theoretical analysis, we demonstrate that ADP-VRSGP achieves robust convergence
with an appropriate learning rate, significantly improving training stability
and speed. Experimental results validate that our method outperforms existing
baselines across multiple scenarios, highlighting its efficacy in addressing
the challenges of privacy-preserving decentralized learning.

</details>


### [176] [Empowering Targeted Neighborhood Search via Hyper Tour for Large-Scale TSP](https://arxiv.org/abs/2510.20169)
*Tongkai Lu,Shuai Ma,Chongyang Tao*

Main category: cs.LG

TL;DR: Hyper Tour Guided Neighborhood Search (HyperNS) efficiently solves large-scale TSPs by clustering and guiding search with hyper tours, outperforming neural methods.


<details>
  <summary>Details</summary>
Motivation: Existing neural-based TSP solutions struggle with scalability, memory constraints, and global guidance for large instances.

Method: HyperNS clusters TSP instances using sparse heatmaps, abstracts them as supernodes, and generates hyper tours to guide optimization.

Result: HyperNS outperforms neural methods, especially for large-scale TSPs, reducing the gap to optimal solutions.

Conclusion: HyperNS offers an effective, scalable solution for large TSP instances by combining clustering and guided search.

Abstract: Traveling Salesman Problem (TSP) is a classic NP-hard problem that has
garnered significant attention from both academia and industry. While
neural-based methods have shown promise for solving TSPs, they still face
challenges in scaling to larger instances, particularly in memory constraints
associated with global heatmaps, edge weights, or access matrices, as well as
in generating high-quality initial solutions and insufficient global guidance
for efficiently navigating vast search spaces. To address these challenges, we
propose a Hyper Tour Guided Neighborhood Search (HyperNS) method for
large-scale TSP instances. Inspired by the ``clustering first, route second"
strategy, our approach initially divides the TSP instance into clusters using a
sparse heatmap graph and abstracts them as supernodes, followed by the
generation of a hyper tour to guide both the initialization and optimization
processes. This method reduces the search space by focusing on edges relevant
to the hyper tour, leading to more efficient and effective optimization.
Experimental results on both synthetic and real-world datasets demonstrate that
our approach outperforms existing neural-based methods, particularly in
handling larger-scale instances, offering a significant reduction in the gap to
the optimal solution.

</details>


### [177] [Every Question Has Its Own Value: Reinforcement Learning with Explicit Human Values](https://arxiv.org/abs/2510.20187)
*Dian Yu,Yulai Zhao,Kishan Panaganti,Linfeng Song,Haitao Mi,Dong Yu*

Main category: cs.LG

TL;DR: RLEV aligns LLM optimization with human values by integrating explicit human value signals into rewards, outperforming correctness-only baselines and improving value-weighted accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods like RLVR focus on objective correctness but ignore task significance. RLEV addresses this by incorporating human-defined value signals.

Method: RLEV extends RLVR by adding human value signals to the reward function, using exam-style data with explicit value labels.

Result: RLEV outperforms correctness-only baselines across algorithms and scales, learning value-sensitive termination policies and showing robustness to noisy signals.

Conclusion: Optimizing for explicit utility functions via RLEV is a practical way to align LLMs with human priorities.

Abstract: We propose Reinforcement Learning with Explicit Human Values (RLEV), a method
that aligns Large Language Model (LLM) optimization directly with quantifiable
human value signals. While Reinforcement Learning with Verifiable Rewards
(RLVR) effectively trains models in objective domains using binary correctness
rewards, it overlooks that not all tasks are equally significant. RLEV extends
this framework by incorporating human-defined value signals directly into the
reward function. Using exam-style data with explicit ground-truth value labels,
RLEV consistently outperforms correctness-only baselines across multiple RL
algorithms and model scales. Crucially, RLEV policies not only improve
value-weighted accuracy but also learn a value-sensitive termination policy:
concise for low-value prompts, thorough for high-value ones. We demonstrate
this behavior stems from value-weighted gradient amplification on
end-of-sequence tokens. Ablation studies confirm the gain is causally linked to
value alignment. RLEV remains robust under noisy value signals, such as
difficulty-based labels, demonstrating that optimizing for an explicit utility
function offers a practical path to aligning LLMs with human priorities.

</details>


### [178] [Risk-Averse Constrained Reinforcement Learning with Optimized Certainty Equivalents](https://arxiv.org/abs/2510.20199)
*Jane H. Lee,Baturay Saglam,Spyridon Pougkakiotis,Amin Karbasi,Dionysis Kalogerias*

Main category: cs.LG

TL;DR: A framework for risk-aware constrained reinforcement learning is proposed, addressing the neglect of rare but critical events in traditional RL by using optimized certainty equivalents (OCEs).


<details>
  <summary>Details</summary>
Motivation: Traditional RL frameworks often overlook risky or catastrophic events in reward distributions, making them inadequate for high-stakes applications where tail risks matter.

Method: The framework leverages optimized certainty equivalents (OCEs) to ensure per-stage robustness in rewards and time, compatible with standard RL solvers like PPO.

Result: The approach provides an exact equivalent to the original constrained problem under strong Lagrangian duality and demonstrates risk-aware properties in experiments.

Conclusion: The proposed framework effectively addresses tail risks in RL, offers convergence guarantees, and integrates seamlessly with existing RL solvers.

Abstract: Constrained optimization provides a common framework for dealing with
conflicting objectives in reinforcement learning (RL). In most of these
settings, the objectives (and constraints) are expressed though the expected
accumulated reward. However, this formulation neglects risky or even possibly
catastrophic events at the tails of the reward distribution, and is often
insufficient for high-stakes applications in which the risk involved in
outliers is critical. In this work, we propose a framework for risk-aware
constrained RL, which exhibits per-stage robustness properties jointly in
reward values and time using optimized certainty equivalents (OCEs). Our
framework ensures an exact equivalent to the original constrained problem
within a parameterized strong Lagrangian duality framework under appropriate
constraint qualifications, and yields a simple algorithmic recipe which can be
wrapped around standard RL solvers, such as PPO. Lastly, we establish the
convergence of the proposed algorithm under common assumptions, and verify the
risk-aware properties of our approach through several numerical experiments.

</details>


### [179] [Approximate Replicability in Learning](https://arxiv.org/abs/2510.20200)
*Max Hopkins,Russell Impagliazzo,Christopher Ye*

Main category: cs.LG

TL;DR: The paper explores relaxed notions of replicability (Pointwise, Approximate, Semi) in PAC learning, showing sample-optimal agnostic PAC learners for each case.


<details>
  <summary>Details</summary>
Motivation: Replicability is a strong stability notion but can be too restrictive; this work investigates feasible relaxed versions for PAC learning.

Method: Proposes three relaxed replicability definitions (Pointwise, Approximate, Semi) and analyzes their feasibility in PAC learning.

Result: For constant replicability parameters, sample-optimal agnostic PAC learners are achieved: Pointwise and Approximate require Θ(d/α²), Semi requires Θ(d²/α²) labeled samples.

Conclusion: Relaxed replicability notions enable feasible learning tasks where full replicability fails, with varying sample efficiency.

Abstract: Replicability, introduced by (Impagliazzo et al. STOC '22), is the notion
that algorithms should remain stable under a resampling of their inputs (given
access to shared randomness). While a strong and interesting notion of
stability, the cost of replicability can be prohibitive: there is no replicable
algorithm, for instance, for tasks as simple as threshold learning (Bun et al.
STOC '23). Given such strong impossibility results we ask: under what
approximate notions of replicability is learning possible?
  In this work, we propose three natural relaxations of replicability in the
context of PAC learning: (1) Pointwise: the learner must be consistent on any
fixed input, but not across all inputs simultaneously, (2) Approximate: the
learner must output hypotheses that classify most of the distribution
consistently, (3) Semi: the algorithm is fully replicable, but may additionally
use shared unlabeled samples. In all three cases, for constant replicability
parameters, we obtain sample-optimal agnostic PAC learners: (1) and (2) are
achievable for ``free" using $\Theta(d/\alpha^2)$ samples, while (3) requires
$\Theta(d^2/\alpha^2)$ labeled samples.

</details>


### [180] [Assessing the Feasibility of Early Cancer Detection Using Routine Laboratory Data: An Evaluation of Machine Learning Approaches on an Imbalanced Dataset](https://arxiv.org/abs/2510.20209)
*Shumin Li*

Main category: cs.LG

TL;DR: The study evaluates machine learning pipelines for cancer risk classification in dogs using routine lab data, finding moderate ranking ability but poor clinical performance due to non-specific biomarkers and class imbalance.


<details>
  <summary>Details</summary>
Motivation: Early cancer detection in dogs is challenging due to non-specific biomarkers and class imbalance in screening populations. The study aims to assess the feasibility of using routine lab data for cancer risk classification under real-world constraints.

Method: The research tested 126 analytical pipelines combining machine learning models, feature selection methods, and data balancing techniques, using patient-level data partitioning to prevent leakage. The optimal model was Logistic Regression with class weighting and recursive feature elimination.

Result: The best model showed moderate ranking ability (AUROC = 0.815) but poor clinical classification (F1-score = 0.25). Non-specific features like age and inflammation markers drove predictions, limiting its reliability for clinical use.

Conclusion: Routine lab data alone have a weak cancer signal confounded by aging and inflammation, necessitating multi-modal data integration for reliable computational veterinary oncology.

Abstract: The development of accessible screening tools for early cancer detection in
dogs represents a significant challenge in veterinary medicine. Routine
laboratory data offer a promising, low-cost source for such tools, but their
utility is hampered by the non-specificity of individual biomarkers and the
severe class imbalance inherent in screening populations. This study assesses
the feasibility of cancer risk classification using the Golden Retriever
Lifetime Study (GRLS) cohort under real-world constraints, including the
grouping of diverse cancer types and the inclusion of post-diagnosis samples. A
comprehensive benchmark evaluation was conducted, systematically comparing 126
analytical pipelines that comprised various machine learning models, feature
selection methods, and data balancing techniques. Data were partitioned at the
patient level to prevent leakage. The optimal model, a Logistic Regression
classifier with class weighting and recursive feature elimination, demonstrated
moderate ranking ability (AUROC = 0.815; 95% CI: 0.793-0.836) but poor clinical
classification performance (F1-score = 0.25, Positive Predictive Value = 0.15).
While a high Negative Predictive Value (0.98) was achieved, insufficient recall
(0.79) precludes its use as a reliable rule-out test. Interpretability analysis
with SHapley Additive exPlanations (SHAP) revealed that predictions were driven
by non-specific features like age and markers of inflammation and anemia. It is
concluded that while a statistically detectable cancer signal exists in routine
lab data, it is too weak and confounded for clinically reliable discrimination
from normal aging or other inflammatory conditions. This work establishes a
critical performance ceiling for this data modality in isolation and
underscores that meaningful progress in computational veterinary oncology will
require integration of multi-modal data sources.

</details>


### [181] [CO-PFL: Contribution-Oriented Personalized Federated Learning for Heterogeneous Networks](https://arxiv.org/abs/2510.20219)
*Ke Xing,Yanjie Dong,Xiaoyi Fan,Runhao Zeng,Victor C. M. Leung,M. Jamal Deen,Xiping Hu*

Main category: cs.LG

TL;DR: CO-PFL introduces a dynamic client contribution estimation method for personalized federated learning, improving accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Address data heterogeneity and suboptimal personalization in conventional federated learning by enhancing client update utility.

Method: Jointly assesses gradient direction discrepancies and prediction deviations for discriminative aggregation weights.

Result: Outperforms state-of-the-art methods on benchmark datasets in accuracy, robustness, scalability, and convergence.

Conclusion: CO-PFL effectively mitigates aggregation bias and enhances local performance through tailored submodels.

Abstract: Personalized federated learning (PFL) addresses a critical challenge of
collaboratively training customized models for clients with heterogeneous and
scarce local data. Conventional federated learning, which relies on a single
consensus model, proves inadequate under such data heterogeneity. Its standard
aggregation method of weighting client updates heuristically or by data volume,
operates under an equal-contribution assumption, failing to account for the
actual utility and reliability of each client's update. This often results in
suboptimal personalization and aggregation bias. To overcome these limitations,
we introduce Contribution-Oriented PFL (CO-PFL), a novel algorithm that
dynamically estimates each client's contribution for global aggregation. CO-PFL
performs a joint assessment by analyzing both gradient direction discrepancies
and prediction deviations, leveraging information from gradient and data
subspaces. This dual-subspace analysis provides a principled and discriminative
aggregation weight for each client, emphasizing high-quality updates.
Furthermore, to bolster personalization adaptability and optimization
stability, CO-PFL cohesively integrates a parameter-wise personalization
mechanism with mask-aware momentum optimization. Our approach effectively
mitigates aggregation bias, strengthens global coordination, and enhances local
performance by facilitating the construction of tailored submodels with stable
updates. Extensive experiments on four benchmark datasets (CIFAR10, CIFAR10C,
CINIC10, and Mini-ImageNet) confirm that CO-PFL consistently surpasses
state-of-the-art methods in in personalization accuracy, robustness,
scalability and convergence stability.

</details>


### [182] [QKCV Attention: Enhancing Time Series Forecasting with Static Categorical Embeddings for Both Lightweight and Pre-trained Foundation Models](https://arxiv.org/abs/2510.20222)
*Hao Wang,Baojun Ma*

Main category: cs.LG

TL;DR: The paper introduces QKCV attention, an extension of the QKV framework, incorporating static categorical embeddings to improve time series forecasting accuracy.


<details>
  <summary>Details</summary>
Motivation: To enhance forecasting accuracy by leveraging category-specific information in time series data.

Method: Extends QKV attention with static categorical embedding (C) and integrates it as a plug-in module in existing models.

Result: Improves forecasting accuracy across diverse datasets and enables efficient fine-tuning of univariate time series models.

Conclusion: QKCV is a versatile and efficient solution for incorporating category-specific information in time series forecasting.

Abstract: In real-world time series forecasting tasks, category information plays a
pivotal role in capturing inherent data patterns. This paper introduces QKCV
(Query-Key-Category-Value) attention, an extension of the traditional QKV
framework that incorporates a static categorical embedding C to emphasize
category-specific information. As a versatile plug-in module, QKCV enhances the
forecasting accuracy of attention-based models (e.g., Vanilla Transformer,
Informer, PatchTST, TFT) across diverse real-world datasets. Furthermore, QKCV
demonstrates remarkable adaptability in fine-tuning univariate time series
foundation model by solely updating the static embedding C while preserving
pretrained weights, thereby reducing computational overhead and achieving
superior fine-tuning performance.

</details>


### [183] [Federated Learning via Meta-Variational Dropout](https://arxiv.org/abs/2510.20225)
*Insu Jeon,Minui Hong,Junhyeog Yun,Gunhee Kim*

Main category: cs.LG

TL;DR: MetaVD introduces Bayesian meta-learning to improve Federated Learning by personalizing models for non-IID data, enhancing accuracy and reducing overfitting.


<details>
  <summary>Details</summary>
Motivation: Address challenges in FL like model overfitting and divergent local models due to limited, non-IID data.

Method: Uses meta-variational dropout (MetaVD) via a hypernetwork to predict client-specific dropout rates, enabling model personalization.

Result: Demonstrates superior classification accuracy and uncertainty calibration, especially for OOD clients, while reducing communication costs.

Conclusion: MetaVD effectively personalizes FL models for non-IID settings, improving performance and efficiency.

Abstract: Federated Learning (FL) aims to train a global inference model from remotely
distributed clients, gaining popularity due to its benefit of improving data
privacy. However, traditional FL often faces challenges in practical
applications, including model overfitting and divergent local models due to
limited and non-IID data among clients. To address these issues, we introduce a
novel Bayesian meta-learning approach called meta-variational dropout (MetaVD).
MetaVD learns to predict client-dependent dropout rates via a shared
hypernetwork, enabling effective model personalization of FL algorithms in
limited non-IID data settings. We also emphasize the posterior adaptation view
of meta-learning and the posterior aggregation view of Bayesian FL via the
conditional dropout posterior. We conducted extensive experiments on various
sparse and non-IID FL datasets. MetaVD demonstrated excellent classification
accuracy and uncertainty calibration performance, especially for
out-of-distribution (OOD) clients. MetaVD compresses the local model parameters
needed for each client, mitigating model overfitting and reducing communication
costs. Code is available at https://github.com/insujeon/MetaVD.

</details>


### [184] [Sparse Local Implicit Image Function for sub-km Weather Downscaling](https://arxiv.org/abs/2510.20228)
*Yago del Valle Inclan Redondo,Enrique Arriaga-Varela,Dmitry Lyamzin,Pablo Cervantes,Tiago Ramalho*

Main category: cs.LG

TL;DR: SpLIIF generates neural representations for weather variable downscaling, outperforming interpolation and CorrDiff by up to 50% for temperature and 10-20% for wind.


<details>
  <summary>Details</summary>
Motivation: To improve downscaling accuracy of weather variables like temperature and wind using neural representations.

Method: Trained a model using sparse weather station data and topography over Japan, comparing to interpolation baseline and CorrDiff.

Result: SpLIIF was up to 50% better for temperature and 10-20% better for wind downscaling.

Conclusion: SpLIIF significantly outperforms existing methods for weather variable downscaling.

Abstract: We introduce SpLIIF to generate implicit neural representations and enable
arbitrary downscaling of weather variables. We train a model from sparse
weather stations and topography over Japan and evaluate in- and
out-of-distribution accuracy predicting temperature and wind, comparing it to
both an interpolation baseline and CorrDiff. We find the model to be up to 50%
better than both CorrDiff and the baseline at downscaling temperature, and
around 10-20% better for wind.

</details>


### [185] [Multi-Objective Reinforcement Learning with Max-Min Criterion: A Game-Theoretic Approach](https://arxiv.org/abs/2510.20235)
*Woohyeon Byeon,Giseung Park,Jongseong Chae,Amir Leshem,Youngchul Sung*

Main category: cs.LG

TL;DR: A convergent framework for multi-objective reinforcement learning using a game-theoretic approach and mirror descent, ensuring global convergence and outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of multi-objective reinforcement learning by leveraging game theory and efficient algorithms.

Method: Reformulates the problem as a two-player zero-sum regularized game and introduces mirror descent with adaptive regularization.

Result: Provides theoretical guarantees (convergence, complexity) and experimental validation showing superior performance.

Conclusion: The proposed framework is both theoretically sound and practically effective for multi-objective reinforcement learning.

Abstract: In this paper, we propose a provably convergent and practical framework for
multi-objective reinforcement learning with max-min criterion. From a
game-theoretic perspective, we reformulate max-min multi-objective
reinforcement learning as a two-player zero-sum regularized continuous game and
introduce an efficient algorithm based on mirror descent. Our approach
simplifies the policy update while ensuring global last-iterate convergence. We
provide a comprehensive theoretical analysis on our algorithm, including
iteration complexity under both exact and approximate policy evaluations, as
well as sample complexity bounds. To further enhance performance, we modify the
proposed algorithm with adaptive regularization. Our experiments demonstrate
the convergence behavior of the proposed algorithm in tabular settings, and our
implementation for deep reinforcement learning significantly outperforms
previous baselines in many MORL environments.

</details>


### [186] [Layer-to-Layer Knowledge Mixing in Graph Neural Network for Chemical Property Prediction](https://arxiv.org/abs/2510.20236)
*Teng Jiek See,Daokun Zhang,Mario Boley,David K. Chalmers*

Main category: cs.LG

TL;DR: LKM, a novel self-knowledge distillation method, improves GNN accuracy for molecular property prediction without adding significant computational cost.


<details>
  <summary>Details</summary>
Motivation: There's a need for more accurate GNN models without increasing computational complexity.

Method: Developed Layer-to-Layer Knowledge Mixing (LKM) to minimize distance between GNN layer embeddings, enhancing multi-scale feature representation.

Result: LKM reduced prediction errors by up to 9.8% (QM9), 45.3% (MD17), and 22.9% (Chignolin).

Conclusion: LKM effectively boosts GNN accuracy for chemical property prediction with minimal computational overhead.

Abstract: Graph Neural Networks (GNNs) are the currently most effective methods for
predicting molecular properties but there remains a need for more accurate
models. GNN accuracy can be improved by increasing the model complexity but
this also increases the computational cost and memory requirement during
training and inference. In this study, we develop Layer-to-Layer Knowledge
Mixing (LKM), a novel self-knowledge distillation method that increases the
accuracy of state-of-the-art GNNs while adding negligible computational
complexity during training and inference. By minimizing the mean absolute
distance between pre-existing hidden embeddings of GNN layers, LKM efficiently
aggregates multi-hop and multi-scale information, enabling improved
representation of both local and global molecular features. We evaluated LKM
using three diverse GNN architectures (DimeNet++, MXMNet, and PAMNet) using
datasets of quantum chemical properties (QM9, MD17 and Chignolin). We found
that the LKM method effectively reduces the mean absolute error of quantum
chemical and biophysical property predictions by up to 9.8% (QM9), 45.3% (MD17
Energy), and 22.9% (Chignolin). This work demonstrates the potential of LKM to
significantly improve the accuracy of GNNs for chemical property prediction
without any substantial increase in training and inference cost.

</details>


### [187] [What Does It Take to Build a Performant Selective Classifier?](https://arxiv.org/abs/2510.20242)
*Stephan Rabanser,Nicolas Papernot*

Main category: cs.LG

TL;DR: The paper identifies and decomposes the selective-classification gap into five sources of error, showing that monotone calibration has limited impact. It emphasizes the need for reordering predictions and validates findings on synthetic and real-world datasets.


<details>
  <summary>Details</summary>
Motivation: To improve selective classifiers by addressing the gap between their performance and the ideal perfect-ordering oracle, providing actionable insights for practitioners.

Method: The authors decompose the selective-classification gap into five components (Bayes noise, approximation error, ranking error, statistical noise, and slack) and validate their analysis with controlled experiments on synthetic and real-world benchmarks.

Result: Key findings include: Bayes noise and model capacity contribute significantly to the gap; feature-aware calibrators improve score ordering; and data shift introduces additional slack requiring robust training.

Conclusion: The decomposition offers a quantitative error budget and design guidelines to build better selective classifiers closer to ideal oracle behavior.

Abstract: Selective classifiers improve model reliability by abstaining on inputs the
model deems uncertain. However, few practical approaches achieve the
gold-standard performance of a perfect-ordering oracle that accepts examples
exactly in order of correctness. Our work formalizes this shortfall as the
selective-classification gap and present the first finite-sample decomposition
of this gap to five distinct sources of looseness: Bayes noise, approximation
error, ranking error, statistical noise, and implementation- or shift-induced
slack. Crucially, our analysis reveals that monotone post-hoc calibration --
often believed to strengthen selective classifiers -- has limited impact on
closing this gap, since it rarely alters the model's underlying score ranking.
Bridging the gap therefore requires scoring mechanisms that can effectively
reorder predictions rather than merely rescale them. We validate our
decomposition on synthetic two-moons data and on real-world vision and language
benchmarks, isolating each error component through controlled experiments. Our
results confirm that (i) Bayes noise and limited model capacity can account for
substantial gaps, (ii) only richer, feature-aware calibrators meaningfully
improve score ordering, and (iii) data shift introduces a separate slack that
demands distributionally robust training. Together, our decomposition yields a
quantitative error budget as well as actionable design guidelines that
practitioners can use to build selective classifiers which approximate ideal
oracle behavior more closely.

</details>


### [188] [FedGPS: Statistical Rectification Against Data Heterogeneity in Federated Learning](https://arxiv.org/abs/2510.20250)
*Zhiqin Yang,Yonggang Zhang,Chenxin Li,Yiu-ming Cheung,Bo Han,Yixuan Yuan*

Main category: cs.LG

TL;DR: FedGPS addresses FL's data heterogeneity challenge by integrating statistical and gradient information, improving robustness across diverse scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack robustness in diverse heterogeneity scenarios, prompting FedGPS to enhance performance by leveraging global perspectives.

Method: FedGPS combines static statistical distribution updates with dynamic gradient adjustments from clients.

Result: FedGPS outperforms state-of-the-art methods in various heterogeneity scenarios.

Conclusion: FedGPS effectively mitigates data heterogeneity, offering improved robustness and performance.

Abstract: Federated Learning (FL) confronts a significant challenge known as data
heterogeneity, which impairs model performance and convergence. Existing
methods have made notable progress in addressing this issue. However, improving
performance in certain heterogeneity scenarios remains an overlooked question:
\textit{How robust are these methods to deploy under diverse heterogeneity
scenarios?} To answer this, we conduct comprehensive evaluations across varied
heterogeneity scenarios, showing that most existing methods exhibit limited
robustness. Meanwhile, insights from these experiments highlight that sharing
statistical information can mitigate heterogeneity by enabling clients to
update with a global perspective. Motivated by this, we propose \textbf{FedGPS}
(\textbf{Fed}erated \textbf{G}oal-\textbf{P}ath \textbf{S}ynergy), a novel
framework that seamlessly integrates statistical distribution and gradient
information from others. Specifically, FedGPS statically modifies each client's
learning objective to implicitly model the global data distribution using
surrogate information, while dynamically adjusting local update directions with
gradient information from other clients at each round. Extensive experiments
show that FedGPS outperforms state-of-the-art methods across diverse
heterogeneity scenarios, validating its effectiveness and robustness. The code
is available at: https://github.com/CUHK-AIM-Group/FedGPS.

</details>


### [189] [Optimistic Task Inference for Behavior Foundation Models](https://arxiv.org/abs/2510.20264)
*Thomas Rupf,Marco Bagatella,Marin Vlastelica,Andreas Krause*

Main category: cs.LG

TL;DR: OpTI-BFM is an optimistic decision criterion for Behavior Foundation Models (BFMs) that enables efficient task inference purely through environment interaction, reducing the need for extensive reward computation or labeling.


<details>
  <summary>Details</summary>
Motivation: BFMs require significant data for reward computation or labeling, which can be inefficient. OpTI-BFM aims to address this by inferring tasks directly through environment interaction.

Method: OpTI-BFM models uncertainty over reward functions and guides BFMs in data collection for task inference, connecting to upper-confidence algorithms for linear bandits.

Result: Empirical evaluations show OpTI-BFM helps BFMs identify and optimize unseen reward functions quickly and with minimal compute overhead.

Conclusion: OpTI-BFM offers an efficient alternative to traditional BFMs by reducing reliance on reward computation and enabling faster task inference.

Abstract: Behavior Foundation Models (BFMs) are capable of retrieving high-performing
policy for any reward function specified directly at test-time, commonly
referred to as zero-shot reinforcement learning (RL). While this is a very
efficient process in terms of compute, it can be less so in terms of data: as a
standard assumption, BFMs require computing rewards over a non-negligible
inference dataset, assuming either access to a functional form of rewards, or
significant labeling efforts. To alleviate these limitations, we tackle the
problem of task inference purely through interaction with the environment at
test-time. We propose OpTI-BFM, an optimistic decision criterion that directly
models uncertainty over reward functions and guides BFMs in data collection for
task inference. Formally, we provide a regret bound for well-trained BFMs
through a direct connection to upper-confidence algorithms for linear bandits.
Empirically, we evaluate OpTI-BFM on established zero-shot benchmarks, and
observe that it enables successor-features-based BFMs to identify and optimize
an unseen reward function in a handful of episodes with minimal compute
overhead. Code is available at https://github.com/ThomasRupf/opti-bfm.

</details>


### [190] [ImpossibleBench: Measuring LLMs' Propensity of Exploiting Test Cases](https://arxiv.org/abs/2510.20270)
*Ziqian Zhong,Aditi Raghunathan,Nicholas Carlini*

Main category: cs.LG

TL;DR: ImpossibleBench is a benchmark framework designed to measure and mitigate LLM agents' tendency to exploit shortcuts in tasks, revealing cheating behaviors and aiding in developing more reliable systems.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the risk of LLMs exploiting shortcuts (e.g., deleting failing tests) which undermines benchmark validity and real-world reliability, necessitating tools to quantify and mitigate such behaviors.

Method: The authors introduce ImpossibleBench, which creates 'impossible' task variants from existing benchmarks by conflicting specifications and unit tests, measuring cheating rates (pass rates implying shortcuts).

Result: ImpossibleBench reveals diverse cheating behaviors (e.g., test modification, operator overloading) and aids in context engineering and monitoring tool development for robust LLM systems.

Conclusion: ImpossibleBench is a practical framework for studying, mitigating, and monitoring LLM cheating behaviors, contributing to more reliable LLM deployments.

Abstract: The tendency to find and exploit "shortcuts" to complete tasks poses
significant risks for reliable assessment and deployment of large language
models (LLMs). For example, an LLM agent with access to unit tests may delete
failing tests rather than fix the underlying bug. Such behavior undermines both
the validity of benchmark results and the reliability of real-world LLM coding
assistant deployments.
  To quantify, study, and mitigate such behavior, we introduce ImpossibleBench,
a benchmark framework that systematically measures LLM agents' propensity to
exploit test cases. ImpossibleBench creates "impossible" variants of tasks from
existing benchmarks like LiveCodeBench and SWE-bench by introducing direct
conflicts between the natural-language specification and the unit tests. We
measure an agent's "cheating rate" as its pass rate on these impossible tasks,
where any pass necessarily implies a specification-violating shortcut.
  As a practical framework, ImpossibleBench is not just an evaluation but a
versatile tool. We demonstrate its utility for: (1) studying model behaviors,
revealing more fine-grained details of cheating behaviors from simple test
modification to complex operator overloading; (2) context engineering, showing
how prompt, test access and feedback loop affect cheating rates; and (3)
developing monitoring tools, providing a testbed with verified deceptive
solutions. We hope ImpossibleBench serves as a useful framework for building
more robust and reliable LLM systems.
  Our implementation can be found at
https://github.com/safety-research/impossiblebench.

</details>


### [191] [Scalable GPU-Accelerated Euler Characteristic Curves: Optimization and Differentiable Learning for PyTorch](https://arxiv.org/abs/2510.20271)
*Udit Saxena*

Main category: cs.LG

TL;DR: Optimized GPU kernels for Euler Characteristic Curve computation achieve significant speedups and introduce a differentiable PyTorch layer for end-to-end learning.


<details>
  <summary>Details</summary>
Motivation: To make topological features computationally efficient and differentiable for deep learning applications.

Method: Develop optimized CUDA kernels for Ampere GPUs with 128B-coalesced access and hierarchical shared-memory accumulation, and introduce a differentiable PyTorch layer using sigmoid relaxation.

Result: Achieved 16-2000x speedups over prior GPU implementations on synthetic grids.

Conclusion: The work enables broader adoption of topological features in deep learning with practical computational efficiency and differentiability.

Abstract: Topological features capture global geometric structure in imaging data, but
practical adoption in deep learning requires both computational efficiency and
differentiability. We present optimized GPU kernels for the Euler
Characteristic Curve (ECC) computation achieving 16-2000\"O speedups over prior
GPU implementations on synthetic grids, and introduce a differentiable PyTorch
layer enabling end-to-end learning. Our CUDA kernels, optimized for Ampere GPUs
use 128B-coalesced access and hierarchical shared-memory accumulation. Our
PyTorch layer learns thresholds in a single direction via a Differentiable
Euler Characteristic Transform-style sigmoid relaxation. We discuss downstream
relevance, including applications highlighted by prior ECC work, and outline
batching/multi-GPU extensions to broaden adoption.

</details>


### [192] [Limits of PRM-Guided Tree Search for Mathematical Reasoning with LLMs](https://arxiv.org/abs/2510.20272)
*Tristan Cinquin,Geoff Pleiss,Agustinus Kristiadi*

Main category: cs.LG

TL;DR: Tree search guided by process reward models (PRMs) doesn't significantly improve mathematical reasoning over simpler methods like BoN, due to unreliable PRM scores and poor generalization.


<details>
  <summary>Details</summary>
Motivation: Current methods like BoN selection are linear and don't capture the branching nature of complex problem-solving, prompting exploration of tree search with PRMs.

Method: Proposed adaptive algorithm for PRM-guided tree search, tested on 23 problems using Qwen2.5-Math-7B-Instruct.

Result: No significant improvement over BoN; Monte Carlo and beam search outperform other methods; PRM reliability decreases with reasoning depth.

Conclusion: Tree search's reliance on unreliable PRM scores limits effectiveness; better reward modeling is needed for improvement.

Abstract: While chain-of-thought prompting with Best-of-N (BoN) selection has become
popular for mathematical reasoning in large language models (LLMs), its linear
structure fails to capture the branching and exploratory nature of complex
problem-solving. In this work, we propose an adaptive algorithm to maximize
process reward model (PRM) scores over the intractable action space, and
investigate whether PRM-guided tree search can improve mathematical reasoning
by exploring multiple partial solution paths. Across $23$ diverse mathematical
problems using Qwen2.5-Math-7B-Instruct with its associated PRM as a case
study, we find that: (1) PRM-guided tree search shows no statistically
significant improvements over BoN despite higher costs, (2) Monte Carlo tree
search and beam search outperform other PRM-guided tree search methods, (3)
PRMs poorly approximate state values and their reliability degrades with
reasoning depth, and (4) PRMs generalize poorly out of distribution. This
underperformance stems from tree search's greater reliance on unreliable PRM
scores, suggesting different reward modeling is necessary before tree search
can effectively enhance mathematical reasoning in LLMs.

</details>


### [193] [SynTSBench: Rethinking Temporal Pattern Learning in Deep Learning Models for Time Series](https://arxiv.org/abs/2510.20273)
*Qitai Tan,Yiyun Chen,Mo Li,Ruiwen Gu,Yilin Su,Xiao-Ping Zhang*

Main category: cs.LG

TL;DR: SynTSBench proposes a synthetic data-driven evaluation paradigm to systematically assess time series forecasting models, focusing on temporal feature decomposition, robustness analysis, and theoretical optimum benchmarking.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning models struggle with robust performance in real-world applications due to their black-box nature and limited evaluation frameworks.

Method: SynTSBench uses programmable feature configuration to evaluate models along three dimensions: temporal feature decomposition, robustness analysis, and theoretical optimum benchmarking.

Result: Experiments reveal that current deep learning models do not universally approach optimal baselines across all temporal features.

Conclusion: SynTSBench provides an interpretable evaluation system to better understand and compare time series forecasting models.

Abstract: Recent advances in deep learning have driven rapid progress in time series
forecasting, yet many state-of-the-art models continue to struggle with robust
performance in real-world applications, even when they achieve strong results
on standard benchmark datasets. This persistent gap can be attributed to the
black-box nature of deep learning architectures and the inherent limitations of
current evaluation frameworks, which frequently lack the capacity to provide
clear, quantitative insights into the specific strengths and weaknesses of
different models, thereby complicating the selection of appropriate models for
particular forecasting scenarios. To address these issues, we propose a
synthetic data-driven evaluation paradigm, SynTSBench, that systematically
assesses fundamental modeling capabilities of time series forecasting models
through programmable feature configuration. Our framework isolates confounding
factors and establishes an interpretable evaluation system with three core
analytical dimensions: (1) temporal feature decomposition and capability
mapping, which enables systematic evaluation of model capacities to learn
specific pattern types; (2) robustness analysis under data irregularities,
which quantifies noise tolerance thresholds and anomaly recovery capabilities;
and (3) theoretical optimum benchmarking, which establishes performance
boundaries for each pattern type-enabling direct comparison between model
predictions and mathematical optima. Our experiments show that current deep
learning models do not universally approach optimal baselines across all types
of temporal features.The code is available at
https://github.com/TanQitai/SynTSBench

</details>


### [194] [KCM: KAN-Based Collaboration Models Enhance Pretrained Large Models](https://arxiv.org/abs/2510.20278)
*Guangyu Dai,Siliang Tang,Yueting Zhuang*

Main category: cs.LG

TL;DR: The paper proposes a KAN-based Collaborative Model (KCM) to improve large-small model collaboration, addressing issues like accuracy degradation and catastrophic forgetting while reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: To enhance the collaboration between large and small models by reducing resource consumption and improving performance in specialized tasks.

Method: Introduces KCM, utilizing KAN (an alternative neural network architecture) for better interpretability and reduced catastrophic forgetting. Deployed in language, vision, and vision-language tasks.

Result: KCM reduces large model inference calls, maintains accuracy, lowers resource use, and improves long-tail data accuracy compared to MLP-based models.

Conclusion: KCM outperforms MCM across metrics, offering a more effective solution for large-small model collaboration.

Abstract: In recent years, Pretrained Large Models(PLMs) researchers proposed
large-small model collaboration frameworks, leveraged easily trainable small
models to assist large models, aim to(1) significantly reduce computational
resource consumption while maintaining comparable accuracy, and (2) enhance
large model performance in specialized domain tasks. However, this
collaborative paradigm suffers from issues such as significant accuracy
degradation, exacerbated catastrophic forgetting, and amplified hallucination
problems induced by small model knowledge. To address these challenges, we
propose a KAN-based Collaborative Model (KCM) as an improved approach to
large-small model collaboration. The KAN utilized in KCM represents an
alternative neural network architecture distinct from conventional MLPs.
Compared to MLPs, KAN offers superior visualizability and interpretability
while mitigating catastrophic forgetting. We deployed KCM in large-small model
collaborative systems across three scenarios: language, vision, and
vision-language cross-modal tasks. The experimental results demonstrate that,
compared with pure large model approaches, the large-small model collaboration
framework utilizing KCM as the collaborative model significantly reduces the
number of large model inference calls while maintaining near-identical task
accuracy, thereby substantially lowering computational resource consumption.
Concurrently, the KAN-based small collaborative model markedly mitigates
catastrophic forgetting, leading to significant accuracy improvements for
long-tail data. The results reveal that KCM demonstrates superior performance
across all metrics compared to MLP-based small collaborative models (MCM).

</details>


### [195] [ResearchGPT: Benchmarking and Training LLMs for End-to-End Computer Science Research Workflows](https://arxiv.org/abs/2510.20279)
*Penghao Wang,Yuhao Zhou,Mengxuan Wu,Ziheng Qin,Bangyuan Zhu,Shengbin Huang,Xuanlei Zhao,Panpan Zhang,Xiaojiang Peng,Yuzhang Shang,Jianfei Yang,Zheng Zhu,Tianlong Chen,Zhangyang Wang,Kai Wang*

Main category: cs.LG

TL;DR: The paper introduces ResearchGPT, aiming to create an AI collaborator for scientific research, and contributes CS-54k, a high-quality Q&A corpus in computer science, with subsets CS-4k for benchmarking and CS-50k for training. Experiments show domain-aligned training improves AI research assistance more than scale.


<details>
  <summary>Details</summary>
Motivation: The vision is to develop AI systems that can assist humans throughout the entire scientific research process, requiring benchmarks for end-to-end evaluation.

Method: The authors built CS-54k, a corpus of scientific Q&A pairs from CC-licensed papers using a scalable pipeline with retrieval-augmented generation and quality control. Subsets CS-4k and CS-50k were derived for benchmarking and training.

Result: CS-4k stratifies LLMs into capability tiers, while models trained on CS-50k outperform larger proprietary systems like GPT-4.1 and Gemini 2.5 Pro.

Conclusion: Domain-aligned training with high-quality data is more critical than pretraining scale for improving AI research assistants. CS-4k and CS-50k are released to support AI collaboration in CS research.

Abstract: As large language models (LLMs) advance, the ultimate vision for their role
in science is emerging: we could build an AI collaborator to effectively assist
human beings throughout the entire scientific research process. We refer to
this envisioned system as ResearchGPT. Given that scientific research
progresses through multiple interdependent phases, achieving this vision
requires rigorous benchmarks that evaluate the end-to-end workflow rather than
isolated sub-tasks. To this end, we contribute CS-54k, a high-quality corpus of
scientific Q&A pairs in computer science, built from 14k CC-licensed papers. It
is constructed through a scalable, paper-grounded pipeline that combines
retrieval-augmented generation (RAG) with multi-stage quality control to ensure
factual grounding. From this unified corpus, we derive two complementary
subsets: CS-4k, a carefully curated benchmark for evaluating AI's ability to
assist scientific research, and CS-50k, a large-scale training dataset.
Extensive experiments demonstrate that CS-4k stratifies state-of-the-art LLMs
into distinct capability tiers. Open models trained on CS-50k with supervised
training and reinforcement learning demonstrate substantial improvements. Even
7B-scale models, when properly trained, outperform many larger proprietary
systems, such as GPT-4.1, GPT-4o, and Gemini 2.5 Pro. This indicates that
making AI models better research assistants relies more on domain-aligned
training with high-quality data than on pretraining scale or general benchmark
performance. We release CS-4k and CS-50k in the hope of fostering AI systems as
reliable collaborators in CS research.

</details>


### [196] [Quantifying Distributional Invariance in Causal Subgraph for IRM-Free Graph Generalization](https://arxiv.org/abs/2510.20295)
*Yang Qiu,Yixiong Zou,Jun Wang,Wei Liu,Xiangyu Fu,Ruixuan Li*

Main category: cs.LG

TL;DR: A new IRM-free method identifies causal subgraphs using invariant distribution criteria, outperforming existing methods in graph generalization.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of out-of-distribution generalization in graph neural networks without relying on costly IRM frameworks.

Method: Develops a norm-guided invariant distribution objective to discover causal subgraphs based on their smaller distributional variations.

Result: The method consistently outperforms state-of-the-art techniques on benchmarks.

Conclusion: Proposed IRM-free approach effectively captures causal subgraphs, enhancing graph generalization.

Abstract: Out-of-distribution generalization under distributional shifts remains a
critical challenge for graph neural networks. Existing methods generally adopt
the Invariant Risk Minimization (IRM) framework, requiring costly environment
annotations or heuristically generated synthetic splits. To circumvent these
limitations, in this work, we aim to develop an IRM-free method for capturing
causal subgraphs. We first identify that causal subgraphs exhibit substantially
smaller distributional variations than non-causal components across diverse
environments, which we formalize as the Invariant Distribution Criterion and
theoretically prove in this paper. Building on this criterion, we
systematically uncover the quantitative relationship between distributional
shift and representation norm for identifying the causal subgraph, and
investigate its underlying mechanisms in depth. Finally, we propose an IRM-free
method by introducing a norm-guided invariant distribution objective for causal
subgraph discovery and prediction. Extensive experiments on two widely used
benchmarks demonstrate that our method consistently outperforms
state-of-the-art methods in graph generalization.

</details>


### [197] [DB-FGA-Net: Dual Backbone Frequency Gated Attention Network for Multi-Class Classification with Grad-CAM Interpretability](https://arxiv.org/abs/2510.20299)
*Saraf Anzum Shreya,MD. Abu Ismail Siddique,Sharaf Tasnim*

Main category: cs.LG

TL;DR: Proposes DB-FGA-Net, a double-backbone deep learning model with Frequency-Gated Attention Block, achieving high accuracy without data augmentation and offering interpretability via Grad-CAM.


<details>
  <summary>Details</summary>
Motivation: Early and precise brain tumor diagnosis is critical, but current deep learning methods rely on heavy augmentation, limiting clinical trust and generalization.

Method: Integrates VGG16 and Xception backbones with a Frequency-Gated Attention Block to capture local and global features, and uses Grad-CAM for transparency.

Result: Achieves 99.24% accuracy on 7K-DS dataset (4-class), outperforms baselines on 3K-DS dataset (95.77%), and includes a GUI for real-time use.

Conclusion: DB-FGA-Net demonstrates robustness, interpretability, and clinical usability, suggesting strong potential for reliable brain tumor diagnosis.

Abstract: Brain tumors are a challenging problem in neuro-oncology, where early and
precise diagnosis is important for successful treatment. Deep learning-based
brain tumor classification methods often rely on heavy data augmentation which
can limit generalization and trust in clinical applications. In this paper, we
propose a double-backbone network integrating VGG16 and Xception with a
Frequency-Gated Attention (FGA) Block to capture complementary local and global
features. Unlike previous studies, our model achieves state-of-the-art
performance without augmentation which demonstrates robustness to variably
sized and distributed datasets. For further transparency, Grad-CAM is
integrated to visualize the tumor regions based on which the model is giving
prediction, bridging the gap between model prediction and clinical
interpretability. The proposed framework achieves 99.24\% accuracy on the 7K-DS
dataset for the 4-class setting, along with 98.68\% and 99.85\% in the 3-class
and 2-class settings, respectively. On the independent 3K-DS dataset, the model
generalizes with 95.77\% accuracy, outperforming baseline and state-of-the-art
methods. To further support clinical usability, we developed a graphical user
interface (GUI) that provides real-time classification and Grad-CAM-based tumor
localization. These findings suggest that augmentation-free, interpretable, and
deployable deep learning models such as DB-FGA-Net hold strong potential for
reliable clinical translation in brain tumor diagnosis.

</details>


### [198] [InvDec: Inverted Decoder for Multivariate Time Series Forecasting with Separated Temporal and Variate Modeling](https://arxiv.org/abs/2510.20302)
*Yuhang Wang*

Main category: cs.LG

TL;DR: InvDec is a hybrid architecture combining temporal encoding and variate-level decoding for multivariate time series forecasting, outperforming benchmarks on high-dimensional datasets.


<details>
  <summary>Details</summary>
Motivation: Address limitations of channel-independent methods (ignoring variable correlations) and pure variate-attention approaches (sacrificing temporal encoding).

Method: Uses patch-based temporal encoder with inverted decoder (variate-wise self-attention), delayed variate embeddings, and adaptive residual fusion.

Result: Significant improvements on high-dimensional datasets (e.g., 20.9% MSE reduction on Electricity).

Conclusion: InvDec's effectiveness scales with dataset dimensionality, highlighting the importance of cross-variate modeling.

Abstract: Multivariate time series forecasting requires simultaneously modeling
temporal patterns and cross-variate dependencies. Channel-independent methods
such as PatchTST excel at temporal modeling but ignore variable correlations,
while pure variate-attention approaches such as iTransformer sacrifice temporal
encoding. We proposeInvDec (Inverted Decoder), a hybrid architecture that
achieves principled separation between temporal encoding and variate-level
decoding. InvDec combines a patch-based temporal encoder with an inverted
decoder operating on the variate dimension through variate-wise self-attention.
We introduce delayed variate embeddings that enrich variable-specific
representations only after temporal encoding, preserving temporal feature
integrity. An adaptive residual fusion mechanism dynamically balances temporal
and variate information across datasets of varying dimensions. Instantiating
InvDec with PatchTST yields InvDec-PatchTST. Extensive experiments on seven
benchmarks demonstrate significant gains on high-dimensional datasets: 20.9%
MSE reduction on Electricity (321 variables), 4.3% improvement on Weather, and
2.7% gain on Traffic compared to PatchTST, while maintaining competitive
performance on low-dimensional ETT datasets. Ablation studies validate each
component, and analysis reveals that InvDec's advantage grows with dataset
dimensionality, confirming that cross-variate modeling becomes critical as the
number of variables increases.

</details>


### [199] [LEGO: A Lightweight and Efficient Multiple-Attribute Unlearning Framework for Recommender Systems](https://arxiv.org/abs/2510.20327)
*Fengyuan Yu,Yuyuan Li,Xiaohua Feng,Junjie Fang,Tao Wang,Chaochao Chen*

Main category: cs.LG

TL;DR: LEGO is a framework for efficiently unlearning multiple sensitive attributes in recommender systems, addressing dynamic and simultaneous unlearning needs through embedding calibration and flexible combination.


<details>
  <summary>Details</summary>
Motivation: Existing single-attribute unlearning methods fail to handle multiple dynamic unlearning requests, necessitating a more adaptable solution.

Method: LEGO divides unlearning into two steps: Embedding Calibration removes attribute-specific information, and Flexible Combination merges embeddings to protect all attributes, framed as a mutual information minimization problem.

Result: Experiments on three datasets show LEGO's effectiveness and efficiency in multi-attribute unlearning across recommendation models.

Conclusion: LEGO successfully addresses the limitations of single-attribute unlearning, offering a lightweight, efficient solution for dynamic, multi-attribute scenarios.

Abstract: With the growing demand for safeguarding sensitive user information in
recommender systems, recommendation attribute unlearning is receiving
increasing attention. Existing studies predominantly focus on single-attribute
unlearning. However, privacy protection requirements in the real world often
involve multiple sensitive attributes and are dynamic. Existing
single-attribute unlearning methods cannot meet these real-world requirements
due to i) CH1: the inability to handle multiple unlearning requests
simultaneously, and ii) CH2: the lack of efficient adaptability to dynamic
unlearning needs. To address these challenges, we propose LEGO, a lightweight
and efficient multiple-attribute unlearning framework. Specifically, we divide
the multiple-attribute unlearning process into two steps: i) Embedding
Calibration removes information related to a specific attribute from user
embedding, and ii) Flexible Combination combines these embeddings into a single
embedding, protecting all sensitive attributes. We frame the unlearning process
as a mutual information minimization problem, providing LEGO a theoretical
guarantee of simultaneous unlearning, thereby addressing CH1. With the two-step
framework, where Embedding Calibration can be performed in parallel and
Flexible Combination is flexible and efficient, we address CH2. Extensive
experiments on three real-world datasets across three representative
recommendation models demonstrate the effectiveness and efficiency of our
proposed framework. Our code and appendix are available at
https://github.com/anonymifish/lego-rec-multiple-attribute-unlearning.

</details>


### [200] [Synthetic Data for Robust Runway Detection](https://arxiv.org/abs/2510.20349)
*Estelle Chigot,Dennis G. Wilson,Meriem Ghrib,Fabrice Jimenez,Thomas Oberlin*

Main category: cs.LG

TL;DR: The paper explores synthetic image generation to overcome the high costs of real-world data collection for training deep vision models, specifically for runway detection in autonomous landing systems.


<details>
  <summary>Details</summary>
Motivation: High costs and effort in data collection and labeling for critical applications like autonomous navigation necessitate synthetic data solutions to cover rare scenarios and diverse conditions.

Method: An image generation approach using a commercial flight simulator supplements few annotated real images, integrating synthetic and real data with controlled domain adaptation strategies.

Result: Standard object detection models achieve accurate predictions, demonstrating robustness in adverse conditions (e.g., nighttime) not covered by real data.

Conclusion: Customized domain adaptation enhances synthetic-to-real transitions, making synthetic data viable for critical applications like runway detection.

Abstract: Deep vision models are now mature enough to be integrated in industrial and
possibly critical applications such as autonomous navigation. Yet, data
collection and labeling to train such models requires too much efforts and
costs for a single company or product. This drawback is more significant in
critical applications, where training data must include all possible conditions
including rare scenarios. In this perspective, generating synthetic images is
an appealing solution, since it allows a cheap yet reliable covering of all the
conditions and environments, if the impact of the synthetic-to-real
distribution shift is mitigated. In this article, we consider the case of
runway detection that is a critical part in autonomous landing systems
developed by aircraft manufacturers. We propose an image generation approach
based on a commercial flight simulator that complements a few annotated real
images. By controlling the image generation and the integration of real and
synthetic data, we show that standard object detection models can achieve
accurate prediction. We also evaluate their robustness with respect to adverse
conditions, in our case nighttime images, that were not represented in the real
data, and show the interest of using a customized domain adaptation strategy.

</details>


### [201] [Ask a Strong LLM Judge when Your Reward Model is Uncertain](https://arxiv.org/abs/2510.20369)
*Zhenghao Xu,Qin Lu,Qingru Zhang,Liang Qiu,Ilgee Hong,Changlong Yu,Wenlin Yao,Yao Liu,Haoming Jiang,Lihong Li,Hyokun Yun,Tuo Zhao*

Main category: cs.LG

TL;DR: The paper introduces an uncertainty-based routing framework to combine a fast reward model (RM) with a costly but strong LLM judge, improving generalization and cost-efficiency in reinforcement learning with human feedback (RLHF).


<details>
  <summary>Details</summary>
Motivation: Classical RMs suffer from reward hacking and poor generalization, while LLM judges offer better generalization but are costly. The goal is to balance efficiency and performance in RLHF.

Method: An uncertainty-based routing framework is proposed, where uncertain preference classifications are routed to the LLM judge, and confident ones to the RM. This leverages the strengths of both models.

Result: Experiments show the routing strategy outperforms random judge calling at the same cost and improves downstream alignment in RLHF.

Conclusion: The uncertainty-based routing framework effectively combines fast RMs and strong LLM judges, enhancing RLHF performance without excessive costs.

Abstract: Reward model (RM) plays a pivotal role in reinforcement learning with human
feedback (RLHF) for aligning large language models (LLMs). However, classical
RMs trained on human preferences are vulnerable to reward hacking and
generalize poorly to out-of-distribution (OOD) inputs. By contrast, strong LLM
judges equipped with reasoning capabilities demonstrate superior
generalization, even without additional training, but incur significantly
higher inference costs, limiting their applicability in online RLHF. In this
work, we propose an uncertainty-based routing framework that efficiently
complements a fast RM with a strong but costly LLM judge. Our approach
formulates advantage estimation in policy gradient (PG) methods as pairwise
preference classification, enabling principled uncertainty quantification to
guide routing. Uncertain pairs are forwarded to the LLM judge, while confident
ones are evaluated by the RM. Experiments on RM benchmarks demonstrate that our
uncertainty-based routing strategy significantly outperforms random judge
calling at the same cost, and downstream alignment results showcase its
effectiveness in improving online RLHF.

</details>


### [202] [Hierarchical Time Series Forecasting with Robust Reconciliation](https://arxiv.org/abs/2510.20383)
*Shuhei Aikawa,Aru Suzuki,Kei Yoshitake,Kanata Teshigawara,Akira Iwabuchi,Ken Kobayashi,Kazuhide Nakata*

Main category: cs.LG

TL;DR: The paper proposes a robust optimization framework for hierarchical time-series forecasting to handle uncertainty in the covariance matrix, improving forecast coherence and performance.


<details>
  <summary>Details</summary>
Motivation: Existing hierarchical forecasting methods rely on estimated covariance matrices, which can degrade performance due to uncertainty in estimation.

Method: Introduces an uncertainty set for the covariance matrix and formulates a reconciliation problem minimizing worst-case expected squared error, solvable as a semidefinite optimization problem.

Result: Numerical experiments show the robust method outperforms existing techniques in forecast performance.

Conclusion: Integrating uncertainty into the reconciliation process enhances hierarchical forecasting robustness and accuracy.

Abstract: This paper focuses on forecasting hierarchical time-series data, where each
higher-level observation equals the sum of its corresponding lower-level time
series. In such contexts, the forecast values should be coherent, meaning that
the forecast value of each parent series exactly matches the sum of the
forecast values of its child series. Existing hierarchical forecasting methods
typically generate base forecasts independently for each series and then apply
a reconciliation procedure to adjust them so that the resulting forecast values
are coherent across the hierarchy. These methods generally derive an optimal
reconciliation, using a covariance matrix of the forecast error. In practice,
however, the true covariance matrix is unknown and has to be estimated from
finite samples in advance. This gap between the true and estimated covariance
matrix may degrade forecast performance. To address this issue, we propose a
robust optimization framework for hierarchical reconciliation that accounts for
uncertainty in the estimated covariance matrix. We first introduce an
uncertainty set for the estimated covariance matrix and formulate a
reconciliation problem that minimizes the worst-case expected squared error
over this uncertainty set. We show that our problem can be cast as a
semidefinite optimization problem. Numerical experiments demonstrate that the
proposed robust reconciliation method achieved better forecast performance than
existing hierarchical forecasting methods, which indicates the effectiveness of
integrating uncertainty into the reconciliation process.

</details>


### [203] [Relative-Based Scaling Law for Neural Language Models](https://arxiv.org/abs/2510.20387)
*Baoqing Yue,Jinyuan Zhou,Zixi Wei,Jingtao Zhan,Qingyao Ai,Yiqun Liu*

Main category: cs.LG

TL;DR: The paper introduces a new metric, Relative-Based Probability (RBP), to evaluate language model scaling laws beyond cross-entropy, focusing on the correct token's ranking. It proposes the Relative-Based Scaling Law and validates its robustness across datasets and models.


<details>
  <summary>Details</summary>
Motivation: Current scaling laws rely on cross-entropy, which ignores the relative ordering of tokens despite its importance in tasks like greedy sampling. The study aims to address this limitation.

Method: Develops the RBP metric to measure the correct token's ranking probability and establishes the Relative-Based Scaling Law. Tests the law extensively on four datasets and model families.

Result: Demonstrates the robustness and accuracy of the Relative-Based Scaling Law across diverse datasets and model scales.

Conclusion: The Relative-Based Scaling Law complements cross-entropy-based scaling laws, enhancing understanding of model performance and aiding practical and theoretical advancements.

Abstract: Scaling laws aim to accurately predict model performance across different
scales. Existing scaling-law studies almost exclusively rely on cross-entropy
as the evaluation metric. However, cross-entropy provides only a partial view
of performance: it measures the absolute probability assigned to the correct
token, but ignores the relative ordering between correct and incorrect tokens.
Yet, relative ordering is crucial for language models, such as in
greedy-sampling scenario. To address this limitation, we investigate scaling
from the perspective of relative ordering. We first propose the Relative-Based
Probability (RBP) metric, which quantifies the probability that the correct
token is ranked among the top predictions. Building on this metric, we
establish the Relative-Based Scaling Law, which characterizes how RBP improves
with increasing model size. Through extensive experiments on four datasets and
four model families spanning five orders of magnitude, we demonstrate the
robustness and accuracy of this law. Finally, we illustrate the broad
application of this law with two examples, namely providing a deeper
explanation of emergence phenomena and facilitating finding fundamental
theories of scaling laws. In summary, the Relative-Based Scaling Law
complements the cross-entropy perspective and contributes to a more complete
understanding of scaling large language models. Thus, it offers valuable
insights for both practical development and theoretical exploration.

</details>


### [204] [Why DPO is a Misspecified Estimator and How to Fix It](https://arxiv.org/abs/2510.20413)
*Aditya Gopalan,Sayak Ray Chowdhury,Debangshu Banerjee*

Main category: cs.LG

TL;DR: Direct Preference Optimization (DPO) simplifies alignment by using supervised learning instead of RLHF, but suffers from misspecification issues. AuxDPO, an improved version, addresses these by incorporating auxiliary variables, showing better performance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of DPO in aligning models with human preferences, particularly when the reward function is misspecified.

Method: The study analyzes DPO's misspecification issues and proposes AuxDPO, which adds auxiliary variables to the DPO loss function to align closer to RLHF solutions.

Result: AuxDPO outperforms DPO empirically in both bandit settings and LLM alignment tasks by mitigating misspecification.

Conclusion: AuxDPO provides a principled solution to DPO's limitations, offering better alignment performance through auxiliary variables.

Abstract: Direct alignment algorithms such as Direct Preference Optimization (DPO)
fine-tune models based on preference data, using only supervised learning
instead of two-stage reinforcement learning with human feedback (RLHF). We show
that DPO encodes a statistical estimation problem over reward functions induced
by a parametric policy class. When the true reward function that generates
preferences cannot be realized via the policy class, DPO becomes misspecified,
resulting in failure modes such as preference order reversal, worsening of
policy reward, and high sensitivity to the input preference data distribution.
On the other hand, we study the local behavior of two-stage RLHF for a
parametric class and relate it to a natural gradient step in policy space. Our
fine-grained geometric characterization allows us to propose AuxDPO, which
introduces additional auxiliary variables in the DPO loss function to help move
towards the RLHF solution in a principled manner and mitigate the
misspecification in DPO. We empirically demonstrate the superior performance of
AuxDPO on didactic bandit settings as well as LLM alignment tasks.

</details>


### [205] [Addressing Mark Imbalance in Integration-free Neural Marked Temporal Point Processes](https://arxiv.org/abs/2510.20414)
*Sishun Liu,Ke Deng,Xiuzhen Zhang,Yongli Ren,Yan Wang*

Main category: cs.LG

TL;DR: The paper addresses imbalanced mark distribution in MTPP models by proposing a thresholding method and a neural MTPP model to improve prediction accuracy for rare marks.


<details>
  <summary>Details</summary>
Motivation: Existing MTPP studies overlook the imbalance in event mark distributions, which reduces prediction accuracy, especially for rare marks.

Method: Proposes a thresholding method to tune mark probabilities based on prior probabilities and introduces a neural MTPP model for efficient time sampling and mark probability estimation.

Result: The solution outperforms baselines in predicting next event marks and times, validated on real-world datasets.

Conclusion: The proposed method effectively addresses mark imbalance and improves prediction performance while being computationally efficient.

Abstract: Marked Temporal Point Process (MTPP) has been well studied to model the event
distribution in marked event streams, which can be used to predict the mark and
arrival time of the next event. However, existing studies overlook that the
distribution of event marks is highly imbalanced in many real-world
applications, with some marks being frequent but others rare. The imbalance
poses a significant challenge to the performance of the next event prediction,
especially for events of rare marks. To address this issue, we propose a
thresholding method, which learns thresholds to tune the mark probability
normalized by the mark's prior probability to optimize mark prediction, rather
than predicting the mark directly based on the mark probability as in existing
studies. In conjunction with this method, we predict the mark first and then
the time. In particular, we develop a novel neural MTPP model to support
effective time sampling and estimation of mark probability without
computationally expensive numerical improper integration. Extensive experiments
on real-world datasets demonstrate the superior performance of our solution
against various baselines for the next event mark and time prediction. The code
is available at https://github.com/undes1red/IFNMTPP.

</details>


### [206] [An Empirical Study of Sample Selection Strategies for Large Language Model Repair](https://arxiv.org/abs/2510.20428)
*Xuran Li,Jingyi Wang*

Main category: cs.LG

TL;DR: The paper analyzes sample prioritization strategies for repairing toxic or biased outputs in large language models (LLMs), evaluating five methods and proposing SAPS, which balances detoxification, utility preservation, and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the issue of toxic or biased outputs in LLMs efficiently, focusing on post-hoc repair with selective use of repair data.

Method: Evaluates five data selection methods (random sampling, K-Center, GraNd, CCS, SAPS) for LLM repair, assessing effectiveness via toxicity reduction, perplexity, and composite metrics (RPS, OPS, RES).

Result: SAPS achieves the best repair balance, while random sampling works well for large models. High-overhead methods like CCS and GraNd offer limited benefits. Optimal data proportion varies by model scale and repair method.

Conclusion: Sample selection is a tunable component for efficient and scalable LLM repair, with SAPS providing superior outcomes and random sampling remaining viable for robust models.

Abstract: Large language models (LLMs) are increasingly deployed in real-world systems,
yet they can produce toxic or biased outputs that undermine safety and trust.
Post-hoc model repair provides a practical remedy, but the high cost of
parameter updates motivates selective use of repair data. Despite extensive
prior work on data selection for model training, it remains unclear which
sampling criteria are most effective and efficient when applied specifically to
behavioral repair of large generative models. Our study presents a systematic
analysis of sample prioritization strategies for LLM repair. We evaluate five
representative selection methods, including random sampling, K-Center,
gradient-norm-based selection(GraNd), stratified coverage (CCS), and a
Semantic-Aware Prioritized Sampling (SAPS) approach we proposed. Repair
effectiveness and trade-offs are assessed through toxicity reduction,
perplexity on WikiText-2 and LAMBADA, and three composite metrics: the Repair
Proximity Score (RPS), the Overall Performance Score (OPS), and the Repair
Efficiency Score (RES). Experimental results show that SAPS achieves the best
balance between detoxification, utility preservation, and efficiency,
delivering comparable or superior repair outcomes with substantially less data.
Random sampling remains effective for large or robust models, while
high-overhead methods such as CCS and GraNd provide limited benefit. The
optimal data proportion depends on model scale and repair method, indicating
that sample selection should be regarded as a tunable component of repair
pipelines. Overall, these findings establish selection-based repair as an
efficient and scalable paradigm for maintaining LLM reliability.

</details>


### [207] [Explainable Benchmarking through the Lense of Concept Learning](https://arxiv.org/abs/2510.20439)
*Quannian Zhang,Michael Röder,Nikit Srivastava,N'Dah Jean Kouagou,Axel-Cyrille Ngonga Ngomo*

Main category: cs.LG

TL;DR: The paper introduces explainable benchmarking to automate performance explanations for systems, specifically applied to knowledge-graph-based question answering, using PruneCEL.


<details>
  <summary>Details</summary>
Motivation: Current benchmarking lacks automated, unbiased explanations for system performance, hindering insights for development or use.

Method: Proposes explainable benchmarking with PruneCEL, a concept learning approach for large knowledge graphs.

Result: PruneCEL outperforms state-of-the-art by 0.55 F1 points; user study shows 80% accurate predictions from explanations.

Conclusion: Explainable benchmarking with PruneCEL effectively automates performance explanations, validated by outperforming benchmarks and user success.

Abstract: Evaluating competing systems in a comparable way, i.e., benchmarking them, is
an undeniable pillar of the scientific method. However, system performance is
often summarized via a small number of metrics. The analysis of the evaluation
details and the derivation of insights for further development or use remains a
tedious manual task with often biased results. Thus, this paper argues for a
new type of benchmarking, which is dubbed explainable benchmarking. The aim of
explainable benchmarking approaches is to automatically generate explanations
for the performance of systems in a benchmark. We provide a first instantiation
of this paradigm for knowledge-graph-based question answering systems. We
compute explanations by using a novel concept learning approach developed for
large knowledge graphs called PruneCEL. Our evaluation shows that PruneCEL
outperforms state-of-the-art concept learners on the task of explainable
benchmarking by up to 0.55 points F1 measure. A task-driven user study with 41
participants shows that in 80\% of the cases, the majority of participants can
accurately predict the behavior of a system based on our explanations. Our code
and data are available at https://github.com/dice-group/PruneCEL/tree/K-cap2025

</details>


### [208] [MolBridge: Atom-Level Joint Graph Refinement for Robust Drug-Drug Interaction Event Prediction](https://arxiv.org/abs/2510.20448)
*Xuan Lin,Aocheng Ding,Tengfei Ma,Hua Liang,Zhe Quan*

Main category: cs.LG

TL;DR: MolBridge is a novel atom-level joint graph refinement framework for predicting drug-drug interactions (DDIs), addressing limitations of existing methods by modeling fine-grained inter-drug relationships and preserving structural context.


<details>
  <summary>Details</summary>
Motivation: Existing DDI prediction methods fail to model atom-level cross-molecular interactions, limiting their effectiveness across diverse molecular complexities and DDI types.

Method: MolBridge constructs a joint graph integrating atomic structures of drug pairs and uses a structure consistency module to refine node features while preserving global structural context.

Result: MolBridge outperforms state-of-the-art baselines, achieving superior performance across diverse DDI types and scenarios, including long-tail and inductive cases.

Conclusion: MolBridge demonstrates the advantages of fine-grained graph refinement in improving DDI prediction accuracy, robustness, and interpretability, contributing to drug interaction network analysis.

Abstract: Drug combinations offer therapeutic benefits but also carry the risk of
adverse drug-drug interactions (DDIs), especially under complex molecular
structures. Accurate DDI event prediction requires capturing fine-grained
inter-drug relationships, which are critical for modeling metabolic mechanisms
such as enzyme-mediated competition. However, existing approaches typically
rely on isolated drug representations and fail to explicitly model atom-level
cross-molecular interactions, limiting their effectiveness across diverse
molecular complexities and DDI type distributions. To address these
limitations, we propose MolBridge, a novel atom-level joint graph refinement
framework for robust DDI event prediction. MolBridge constructs a joint graph
that integrates atomic structures of drug pairs, enabling direct modeling of
inter-drug associations. A central challenge in such joint graph settings is
the potential loss of information caused by over-smoothing when modeling
long-range atomic dependencies. To overcome this, we introduce a structure
consistency module that iteratively refines node features while preserving the
global structural context. This joint design allows MolBridge to effectively
learn both local and global interaction outperforms state-of-the-art baselines,
achieving superior performance across long-tail and inductive scenarios.
patterns, yielding robust representations across both frequent and rare DDI
types. Extensive experiments on two benchmark datasets show that MolBridge
consistently. These results demonstrate the advantages of fine-grained graph
refinement in improving the accuracy, robustness, and mechanistic
interpretability of DDI event prediction.This work contributes to Web Mining
and Content Analysis by developing graph-based methods for mining and analyzing
drug-drug interaction networks.

</details>


### [209] [Intransitive Player Dominance and Market Inefficiency in Tennis Forecasting: A Graph Neural Network Approach](https://arxiv.org/abs/2510.20454)
*Lawrence Clegg,John Cartlidge*

Main category: cs.LG

TL;DR: A graph neural network models intransitive relationships in tennis matches, outperforming bookmakers with 65.7% accuracy and yielding a 3.26% ROI.


<details>
  <summary>Details</summary>
Motivation: Intransitive player dominance is common in tennis but rarely incorporated in forecasting methods, creating a gap our approach aims to fill.

Method: Uses graph neural networks with temporal directed graphs (players as nodes, match outcomes as edges) to model intransitive relationships.

Result: Model achieves 65.7% accuracy and 0.215 Brier Score, yielding a 3.26% ROI, highlighting bookmakers' poor handling of intransitive matchups.

Conclusion: The graph-based approach captures relational dynamics better than bookmakers, exploiting market inefficiencies in intransitive matchups.

Abstract: Intransitive player dominance, where player A beats B, B beats C, but C beats
A, is common in competitive tennis. Yet, there are few known attempts to
incorporate it within forecasting methods. We address this problem with a graph
neural network approach that explicitly models these intransitive relationships
through temporal directed graphs, with players as nodes and their historical
match outcomes as directed edges. We find the bookmaker Pinnacle Sports poorly
handles matches with high intransitive complexity and posit that our
graph-based approach is uniquely positioned to capture relational dynamics in
these scenarios. When selectively betting on higher intransitivity matchups
with our model (65.7% accuracy, 0.215 Brier Score), we achieve significant
positive returns of 3.26% ROI with Kelly staking over 1903 bets, suggesting a
market inefficiency in handling intransitive matchups that our approach
successfully exploits.

</details>


### [210] [Transferable Black-Box One-Shot Forging of Watermarks via Image Preference Models](https://arxiv.org/abs/2510.20468)
*Tomáš Souček,Sylvestre-Alvise Rebuffi,Pierre Fernandez,Nikola Jovanović,Hady Elsahar,Valeriu Lacatusu,Tuan Tran,Alexandre Mourachko*

Main category: cs.LG

TL;DR: This paper explores watermark forging in post-hoc image watermarking, introducing a method to assess, remove, and forge watermarks without needing real watermarks or knowledge of the watermarking model.


<details>
  <summary>Details</summary>
Motivation: The rise of AI-generated content and legal pressures highlight the need for robust watermarking techniques, yet watermark forging remains underexplored.

Method: The authors train a preference model using procedurally generated images and optimize input images via backpropagation to remove or forge watermarks.

Result: The method successfully forges watermarks across various models, questioning current watermarking security.

Conclusion: The study exposes vulnerabilities in post-hoc image watermarking, suggesting a need for more secure approaches.

Abstract: Recent years have seen a surge in interest in digital content watermarking
techniques, driven by the proliferation of generative models and increased
legal pressure. With an ever-growing percentage of AI-generated content
available online, watermarking plays an increasingly important role in ensuring
content authenticity and attribution at scale. There have been many works
assessing the robustness of watermarking to removal attacks, yet, watermark
forging, the scenario when a watermark is stolen from genuine content and
applied to malicious content, remains underexplored. In this work, we
investigate watermark forging in the context of widely used post-hoc image
watermarking. Our contributions are as follows. First, we introduce a
preference model to assess whether an image is watermarked. The model is
trained using a ranking loss on purely procedurally generated images without
any need for real watermarks. Second, we demonstrate the model's capability to
remove and forge watermarks by optimizing the input image through
backpropagation. This technique requires only a single watermarked image and
works without knowledge of the watermarking model, making our attack much
simpler and more practical than attacks introduced in related work. Third, we
evaluate our proposed method on a variety of post-hoc image watermarking
models, demonstrating that our approach can effectively forge watermarks,
questioning the security of current watermarking approaches. Our code and
further resources are publicly available.

</details>


### [211] [Bi-CoG: Bi-Consistency-Guided Self-Training for Vision-Language Models](https://arxiv.org/abs/2510.20477)
*Rui Zhu,Song-Lin Lv,Zi-Kang Wang,Lan-Zhe Guo*

Main category: cs.LG

TL;DR: Bi-CoG is a novel semi-supervised fine-tuning method that addresses model bias and hyperparameter sensitivity by leveraging inter-model and intra-model consistency, outperforming existing methods across 14 datasets.


<details>
  <summary>Details</summary>
Motivation: Existing methods combining fine-tuning of pre-trained vision-language models with semi-supervised learning suffer from model bias and hyperparameter sensitivity due to reliance on prediction consistency or pre-defined confidence thresholds.

Method: The proposed Bi-CoG method assigns high-quality pseudo-labels by exploiting inter-model and intra-model consistency and uses an error-aware dynamic pseudo-label assignment strategy.

Result: Bi-CoG consistently and significantly improves performance across 14 datasets, as demonstrated by theoretical analysis and experiments.

Conclusion: Bi-CoG is a simple yet effective plug-and-play methodology that enhances semi-supervised fine-tuning by reducing bias and sensitivity, leading to superior performance.

Abstract: Exploiting unlabeled data through semi-supervised learning (SSL) or
leveraging pre-trained models via fine-tuning are two prevailing paradigms for
addressing label-scarce scenarios. Recently, growing attention has been given
to combining fine-tuning of pre-trained vision-language models (VLMs) with SSL,
forming the emerging paradigm of semi-supervised fine-tuning. However, existing
methods often suffer from model bias and hyperparameter sensitivity, due to
reliance on prediction consistency or pre-defined confidence thresholds. To
address these limitations, we propose a simple yet effective plug-and-play
methodology named
$\underline{\textbf{Bi-Co}}$nsistency-$\underline{\textbf{G}}$uided
Self-Training (Bi-CoG), which assigns high-quality and low-bias pseudo-labels,
by simultaneously exploiting inter-model and intra-model consistency, along
with an error-aware dynamic pseudo-label assignment strategy. Both theoretical
analysis and extensive experiments over 14 datasets demonstrate the
effectiveness of Bi-CoG, which consistently and significantly improves the
performance of existing methods.

</details>


### [212] [Hurdle-IMDL: An Imbalanced Learning Framework for Infrared Rainfall Retrieval](https://arxiv.org/abs/2510.20486)
*Fangjian Zhang,Xiaoyong Zhuge,Wenlan Wang,Haixia Xiao,Yuying Zhu,Siyang Cheng*

Main category: cs.LG

TL;DR: The paper proposes Hurdle-IMDL to address label imbalance in rainfall retrieval, improving heavy rain detection.


<details>
  <summary>Details</summary>
Motivation: Imbalanced label distribution in AI models favors common samples, degrading rare-event retrieval like heavy rainfall.

Method: Uses a hurdle model for zero inflation and IMDL for the long-tail imbalance, transforming learning into an unbiased inverse model.

Result: Outperforms conventional methods, reducing underestimation and enhancing heavy-to-extreme rain retrieval.

Conclusion: Hurdle-IMDL generalizes to environmental variables, improving rare-event detection.

Abstract: Artificial intelligence has advanced quantitative remote sensing, yet its
effectiveness is constrained by imbalanced label distribution. This imbalance
leads conventionally trained models to favor common samples, which in turn
degrades retrieval performance for rare ones. Rainfall retrieval exemplifies
this issue, with performance particularly compromised for heavy rain. This
study proposes Hurdle-Inversion Model Debiasing Learning (IMDL) framework.
Following a divide-and-conquer strategy, imbalance in the rain distribution is
decomposed into two components: zero inflation, defined by the predominance of
non-rain samples; and long tail, defined by the disproportionate abundance of
light-rain samples relative to heavy-rain samples. A hurdle model is adopted to
handle the zero inflation, while IMDL is proposed to address the long tail by
transforming the learning object into an unbiased ideal inverse model.
Comprehensive evaluation via statistical metrics and case studies investigating
rainy weather in eastern China confirms Hurdle-IMDL's superiority over
conventional, cost-sensitive, generative, and multi-task learning methods. Its
key advancements include effective mitigation of systematic underestimation and
a marked improvement in the retrieval of heavy-to-extreme rain. IMDL offers a
generalizable approach for addressing imbalance in distributions of
environmental variables, enabling enhanced retrieval of rare yet high-impact
events.

</details>


### [213] [SheafAlign: A Sheaf-theoretic Framework for Decentralized Multimodal Alignment](https://arxiv.org/abs/2510.20540)
*Abdulmomen Ghalkha,Zhuojun Tian,Chaouki Ben Issaid,Mehdi Bennis*

Main category: cs.LG

TL;DR: SheafAlign is a decentralized multimodal alignment framework using sheaf theory and contrastive learning, eliminating the need for mutual redundancy among modalities. It improves zero-shot generalization, cross-modal alignment, and robustness while reducing communication costs.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal alignment methods rely on mutual redundancy, which is unrealistic in distributed scenarios. SheafAlign addresses this limitation by modeling modality relations differently.

Method: SheafAlign replaces single-space alignment with multiple comparison spaces, using sheaf structures and decentralized contrastive learning objectives for training.

Result: Experiments show SheafAlign excels in zero-shot generalization, cross-modal alignment, and robustness to missing modalities, with 50% lower communication costs than baselines.

Conclusion: SheafAlign offers a scalable and efficient solution for decentralized multimodal alignment, preserving both shared and unique information across modalities.

Abstract: Conventional multimodal alignment methods assume mutual redundancy across all
modalities, an assumption that fails in real-world distributed scenarios. We
propose SheafAlign, a sheaf-theoretic framework for decentralized multimodal
alignment that replaces single-space alignment with multiple comparison spaces.
This approach models pairwise modality relations through sheaf structures and
leverages decentralized contrastive learning-based objectives for training.
SheafAlign overcomes the limitations of prior methods by not requiring mutual
redundancy among all modalities, preserving both shared and unique information.
Experiments on multimodal sensing datasets show superior zero-shot
generalization, cross-modal alignment, and robustness to missing modalities,
with 50\% lower communication cost than state-of-the-art baselines.

</details>


### [214] [A Unified Framework for Zero-Shot Reinforcement Learning](https://arxiv.org/abs/2510.20542)
*Jacopo Di Ventura,Jan Felix Kleuker,Aske Plaat,Thomas Moerland*

Main category: cs.LG

TL;DR: The paper introduces a unified framework for zero-shot RL, organizing existing approaches into two families and providing a common analytical lens for future research.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a common analytical framework in zero-shot RL, which hinders progress and comparison of methods.

Method: Proposes a unified framework with consistent notation and taxonomy, classifying algorithms into direct and compositional representations.

Result: Highlights shared principles, differences, and derives an extended bound for successor-feature methods.

Conclusion: The framework consolidates zero-shot RL research, offering a principled foundation and guiding future development of general agents.

Abstract: Zero-shot reinforcement learning (RL) has emerged as a setting for developing
general agents in an unsupervised manner, capable of solving downstream tasks
without additional training or planning at test-time. Unlike conventional RL,
which optimizes policies for a fixed reward, zero-shot RL requires agents to
encode representations rich enough to support immediate adaptation to any
objective, drawing parallels to vision and language foundation models. Despite
growing interest, the field lacks a common analytical lens.
  We present the first unified framework for zero-shot RL. Our formulation
introduces a consistent notation and taxonomy that organizes existing
approaches and allows direct comparison between them. Central to our framework
is the classification of algorithms into two families: direct representations,
which learn end-to-end mappings from rewards to policies, and compositional
representations, which decompose the representation leveraging the substructure
of the value function. Within this framework, we highlight shared principles
and key differences across methods, and we derive an extended bound for
successor-feature methods, offering a new perspective on their performance in
the zero-shot regime. By consolidating existing work under a common lens, our
framework provides a principled foundation for future research in zero-shot RL
and outlines a clear path toward developing more general agents.

</details>


### [215] [BadGraph: A Backdoor Attack Against Latent Diffusion Model for Text-Guided Graph Generation](https://arxiv.org/abs/2510.20792)
*Liang Ye,Shengqin Chen,Jiazhu Dai*

Main category: cs.LG

TL;DR: BadGraph introduces a backdoor attack method for text-guided graph generation, showing high success rates with minimal poisoning and negligible impact on clean inputs.


<details>
  <summary>Details</summary>
Motivation: Addresses unexplored security risks in conditional graph generation, specifically text-guided models, highlighting potential threats like drug discovery applications.

Method: BadGraph uses textual triggers to poison training data, implanting backdoors that activate attacker-specified subgraphs during inference when triggers are present.

Result: Achieves 50% attack success with <10% poisoning and 80% with 24%, with minimal performance drop on benign inputs. Backdoors are implanted during VAE and diffusion training.

Conclusion: The study exposes vulnerabilities in latent diffusion models for text-guided graph generation, emphasizing the need for defenses in critical applications.

Abstract: The rapid progress of graph generation has raised new security concerns,
particularly regarding backdoor vulnerabilities. While prior work has explored
backdoor attacks in image diffusion and unconditional graph generation,
conditional, especially text-guided graph generation remains largely
unexamined. This paper proposes BadGraph, a backdoor attack method targeting
latent diffusion models for text-guided graph generation. BadGraph leverages
textual triggers to poison training data, covertly implanting backdoors that
induce attacker-specified subgraphs during inference when triggers appear,
while preserving normal performance on clean inputs. Extensive experiments on
four benchmark datasets (PubChem, ChEBI-20, PCDes, MoMu) demonstrate the
effectiveness and stealth of the attack: less than 10% poisoning rate can
achieves 50% attack success rate, while 24% suffices for over 80% success rate,
with negligible performance degradation on benign samples. Ablation studies
further reveal that the backdoor is implanted during VAE and diffusion training
rather than pretraining. These findings reveal the security vulnerabilities in
latent diffusion models of text-guided graph generation, highlight the serious
risks in models' applications such as drug discovery and underscore the need
for robust defenses against the backdoor attack in such diffusion models.

</details>


### [216] [Structural Invariance Matters: Rethinking Graph Rewiring through Graph Metrics](https://arxiv.org/abs/2510.20556)
*Alexandre Benoit,Catherine Aitken,Yu He*

Main category: cs.LG

TL;DR: This paper systematically analyzes how graph rewiring affects structural metrics and downstream task performance in GNNs, revealing a pattern where successful methods preserve local structure while allowing global flexibility.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand which structural properties must be preserved during graph rewiring to balance performance gains and structural fidelity in GNNs and Graph Transformers.

Method: The authors analyze seven diverse rewiring strategies and correlate changes in local and global graph properties with node classification accuracy.

Result: Successful rewiring methods tend to preserve local structure while allowing flexibility in global connectivity.

Conclusion: The findings provide insights for designing effective rewiring strategies, bridging graph theory and practical GNN optimization.

Abstract: Graph rewiring has emerged as a key technique to alleviate over-squashing in
Graph Neural Networks (GNNs) and Graph Transformers by modifying the graph
topology to improve information flow. While effective, rewiring inherently
alters the graph's structure, raising the risk of distorting important
topology-dependent signals. Yet, despite the growing use of rewiring, little is
known about which structural properties must be preserved to ensure both
performance gains and structural fidelity. In this work, we provide the first
systematic analysis of how rewiring affects a range of graph structural
metrics, and how these changes relate to downstream task performance. We study
seven diverse rewiring strategies and correlate changes in local and global
graph properties with node classification accuracy. Our results reveal a
consistent pattern: successful rewiring methods tend to preserve local
structure while allowing for flexibility in global connectivity. These findings
offer new insights into the design of effective rewiring strategies, bridging
the gap between graph theory and practical GNN optimization.

</details>


### [217] [Compress to Impress: Efficient LLM Adaptation Using a Single Gradient Step on 100 Samples](https://arxiv.org/abs/2510.20800)
*Shiva Sreeram,Alaa Maalouf,Pratyusha Sharma,Daniela Rus*

Main category: cs.LG

TL;DR: The paper introduces an improved version of LASER, eliminating exhaustive searches by identifying key matrices, using singular value gradients, clustering subspaces, and reducing evaluation samples, resulting in faster LLM adaptation without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: LASER's impracticality due to exhaustive searches motivated the development of a faster, more efficient adaptation method for LLMs.

Method: The method involves selecting key matrices via singular value gradients, clustering subspaces for factorization, and evaluating on minimal samples (100) instead of full datasets.

Result: The approach reduces search time, decreases overfitting, and improves accuracy by up to 24.6 percentage points.

Conclusion: Combining these techniques provides a fast and robust adaptation algorithm for downstream tasks without fine-tuning.

Abstract: Recently, Sharma et al. suggested a method called Layer-SElective-Rank
reduction (LASER) which demonstrated that pruning high-order components of
carefully chosen LLM's weight matrices can boost downstream accuracy -- without
any gradient-based fine-tuning. Yet LASER's exhaustive, per-matrix search (each
requiring full-dataset forward passes) makes it impractical for rapid
deployment. We demonstrate that this overhead can be removed and find that: (i)
Only a small, carefully chosen subset of matrices needs to be inspected --
eliminating the layer-by-layer sweep, (ii) The gradient of each matrix's
singular values pinpoints which matrices merit reduction, (iii) Increasing the
factorization search space by allowing matrices rows to cluster around multiple
subspaces and then decomposing each cluster separately further reduces
overfitting on the original training data and further lifts accuracy by up to
24.6 percentage points, and finally, (iv) we discover that evaluating on just
100 samples rather than the full training data -- both for computing the
indicative gradients and for measuring the final accuracy -- suffices to
further reduce the search time; we explain that as adaptation to downstream
tasks is dominated by prompting style, not dataset size. As a result, we show
that combining these findings yields a fast and robust adaptation algorithm for
downstream tasks. Overall, with a single gradient step on 100 examples and a
quick scan of the top candidate layers and factorization techniques, we can
adapt LLMs to new datasets -- entirely without fine-tuning.

</details>


### [218] [Embedding the MLOps Lifecycle into OT Reference Models](https://arxiv.org/abs/2510.20590)
*Simon Schindler,Christoph Binder,Lukas Lürzer,Stefan Huber*

Main category: cs.LG

TL;DR: The paper explores challenges in integrating MLOps with Operational Technology (OT) systems, proposing a structured adaptation using existing OT reference models like RAMI 4.0 and ISA-95.


<details>
  <summary>Details</summary>
Motivation: To address the difficulties in applying MLOps practices within OT environments, which differ significantly from traditional IT settings.

Method: Analyzes obstacles and proposes embedding MLOps into OT reference models, evaluating RAMI 4.0 and ISA-95, and mapping MLOps components to RAMI 4.0 via a real-world use case.

Result: Demonstrates that standard MLOps practices require adaptation but can be successfully integrated into OT using reference models.

Conclusion: Structured adaptation of MLOps practices using OT reference models enables effective integration into OT environments.

Abstract: Machine Learning Operations (MLOps) practices are increas- ingly adopted in
industrial settings, yet their integration with Opera- tional Technology (OT)
systems presents significant challenges. This pa- per analyzes the fundamental
obstacles in combining MLOps with OT en- vironments and proposes a systematic
approach to embed MLOps prac- tices into established OT reference models. We
evaluate the suitability of the Reference Architectural Model for Industry 4.0
(RAMI 4.0) and the International Society of Automation Standard 95 (ISA-95) for
MLOps integration and present a detailed mapping of MLOps lifecycle compo-
nents to RAMI 4.0 exemplified by a real-world use case. Our findings
demonstrate that while standard MLOps practices cannot be directly transplanted
to OT environments, structured adaptation using existing reference models can
provide a pathway for successful integration.

</details>


### [219] [Generalizable Reasoning through Compositional Energy Minimization](https://arxiv.org/abs/2510.20607)
*Alexandru Oarga,Yilun Du*

Main category: cs.LG

TL;DR: The paper proposes a novel approach to improve generalization in reasoning tasks by learning energy landscapes for smaller subproblems and combining them for larger problems.



<details>
  <summary>Details</summary>
Motivation: Generalization beyond the training distribution is challenging in reasoning tasks, where end-to-end models often fail.


Method: Learns energy landscapes for tractable subproblems and combines them for larger problems, introducing Parallel Energy Minimization (PEM) for better sample quality.


Result: Outperforms state-of-the-art methods, showing improved generalization to complex problems.


Conclusion: The compositional approach with energy landscapes and PEM enhances reasoning generalization.


Abstract: Generalization is a key challenge in machine learning, specifically in
reasoning tasks, where models are expected to solve problems more complex than
those encountered during training. Existing approaches typically train
reasoning models in an end-to-end fashion, directly mapping input instances to
solutions. While this allows models to learn useful heuristics from data, it
often results in limited generalization beyond the training distribution. In
this work, we propose a novel approach to reasoning generalization by learning
energy landscapes over the solution spaces of smaller, more tractable
subproblems. At test time, we construct a global energy landscape for a given
problem by combining the energy functions of multiple subproblems. This
compositional approach enables the incorporation of additional constraints
during inference, allowing the construction of energy landscapes for problems
of increasing difficulty. To improve the sample quality from this newly
constructed energy landscape, we introduce Parallel Energy Minimization (PEM).
We evaluate our approach on a wide set of reasoning problems. Our method
outperforms existing state-of-the-art methods, demonstrating its ability to
generalize to larger and more complex problems. Project website can be found
at: https://alexoarga.github.io/compositional_reasoning/

</details>


### [220] [Convergence Analysis of SGD under Expected Smoothness](https://arxiv.org/abs/2510.20608)
*Yuta Kawamoto,Hideaki Iiduka*

Main category: cs.LG

TL;DR: This paper refines the Expected Smoothness (ES) condition for SGD, provides detailed convergence analysis, and proves O(1/K) rates with explicit residual errors under various step-size schedules.


<details>
  <summary>Details</summary>
Motivation: To address limitations in classical SGD analyses—either overly strong (bounded variance) or coarse (uniform noise)—the paper leverages the flexible ES condition for a more nuanced convergence analysis.

Method: The study refines ES with interpretations and sampling-dependent constants, derives bounds for the squared full gradient norm's expectation, and analyzes SGD convergence under ES with detailed proofs.

Result: The paper proves O(1/K) convergence rates with explicit residual errors for different step-size schedules, unifying and extending prior work.

Conclusion: The refined ES condition and detailed analysis provide a robust framework for understanding SGD's behavior in large-scale learning settings.

Abstract: Stochastic gradient descent (SGD) is the workhorse of large-scale learning,
yet classical analyses rely on assumptions that can be either too strong
(bounded variance) or too coarse (uniform noise). The expected smoothness (ES)
condition has emerged as a flexible alternative that ties the second moment of
stochastic gradients to the objective value and the full gradient. This paper
presents a self-contained convergence analysis of SGD under ES. We (i) refine
ES with interpretations and sampling-dependent constants; (ii) derive bounds of
the expectation of squared full gradient norm; and (iii) prove $O(1/K)$ rates
with explicit residual errors for various step-size schedules. All proofs are
given in full detail in the appendix. Our treatment unifies and extends recent
threads (Khaled and Richt\'arik, 2020; Umeda and Iiduka, 2025).

</details>


### [221] [Practical Code RAG at Scale: Task-Aware Retrieval Design Choices under Compute Budgets](https://arxiv.org/abs/2510.20609)
*Timur Galimzyanov,Olga Kolomyttseva,Egor Bogomolov*

Main category: cs.LG

TL;DR: The paper analyzes retrieval configurations for code-focused generation tasks, comparing chunking strategies, similarity scoring, and splitting granularity. It provides evidence-based recommendations for effective code-oriented RAG systems.


<details>
  <summary>Details</summary>
Motivation: To optimize retrieval design for code-focused tasks under realistic compute budgets, addressing efficiency and performance trade-offs.

Method: Systematic comparison of retrieval configurations across code completion and bug localization tasks using chunking strategies, similarity scoring, and splitting granularity.

Result: BM25 with word-level splitting is most effective for PL-PL tasks, dense encoders outperform sparse retrievers for NL-PL (with higher latency), optimal chunk size scales with context, and line-based chunking matches syntax-aware splitting.

Conclusion: The study provides practical recommendations for implementing efficient and effective code-oriented RAG systems based on task requirements and computational constraints.

Abstract: We study retrieval design for code-focused generation tasks under realistic
compute budgets. Using two complementary tasks from Long Code Arena -- code
completion and bug localization -- we systematically compare retrieval
configurations across various context window sizes along three axes: (i)
chunking strategy, (ii) similarity scoring, and (iii) splitting granularity.
(1) For PL-PL, sparse BM25 with word-level splitting is the most effective and
practical, significantly outperforming dense alternatives while being an order
of magnitude faster. (2) For NL-PL, proprietary dense encoders (Voyager-3
family) consistently beat sparse retrievers, however requiring 100x larger
latency. (3) Optimal chunk size scales with available context: 32-64 line
chunks work best at small budgets, and whole-file retrieval becomes competitive
at 16000 tokens. (4) Simple line-based chunking matches syntax-aware splitting
across budgets. (5) Retrieval latency varies by up to 200x across
configurations; BPE-based splitting is needlessly slow, and BM25 + word
splitting offers the best quality-latency trade-off. Thus, we provide
evidence-based recommendations for implementing effective code-oriented RAG
systems based on task requirements, model constraints, and computational
efficiency.

</details>


### [222] [PSO-XAI: A PSO-Enhanced Explainable AI Framework for Reliable Breast Cancer Detection](https://arxiv.org/abs/2510.20611)
*Mirza Raquib,Niloy Das,Farida Siddiqi Prity,Arafath Al Fahim,Saydul Akbar Murad,Mohammad Amzad Hossain,MD Jiabul Hoque,Mohammad Ali Moni*

Main category: cs.LG

TL;DR: The paper proposes a machine learning framework using Particle Swarm Optimization for feature selection to improve breast cancer diagnosis, achieving high accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Early and accurate breast cancer detection is crucial but limited by conventional methods' variability, cost, and misdiagnosis risks. Machine learning offers a promising solution.

Method: An integrated framework uses customized PSO for feature selection, evaluated on 29 models (classifiers, ensembles, neural networks, etc.) with cross-validation and explainable AI methods.

Result: The approach achieved 99.1% accuracy across metrics, reduced dimensionality, and provided transparent explanations, demonstrating robust performance.

Conclusion: Combining swarm intelligence with explainable ML enhances breast cancer diagnosis, making it trustworthy and clinically relevant.

Abstract: Breast cancer is considered the most critical and frequently diagnosed cancer
in women worldwide, leading to an increase in cancer-related mortality. Early
and accurate detection is crucial as it can help mitigate possible threats
while improving survival rates. In terms of prediction, conventional diagnostic
methods are often limited by variability, cost, and, most importantly, risk of
misdiagnosis. To address these challenges, machine learning (ML) has emerged as
a powerful tool for computer-aided diagnosis, with feature selection playing a
vital role in improving model performance and interpretability. This research
study proposes an integrated framework that incorporates customized Particle
Swarm Optimization (PSO) for feature selection. This framework has been
evaluated on a comprehensive set of 29 different models, spanning classical
classifiers, ensemble techniques, neural networks, probabilistic algorithms,
and instance-based algorithms. To ensure interpretability and clinical
relevance, the study uses cross-validation in conjunction with explainable AI
methods. Experimental evaluation showed that the proposed approach achieved a
superior score of 99.1\% across all performance metrics, including accuracy and
precision, while effectively reducing dimensionality and providing transparent,
model-agnostic explanations. The results highlight the potential of combining
swarm intelligence with explainable ML for robust, trustworthy, and clinically
meaningful breast cancer diagnosis.

</details>


### [223] [MS-BART: Unified Modeling of Mass Spectra and Molecules for Structure Elucidation](https://arxiv.org/abs/2510.20615)
*Yang Han,Pengyu Wang,Kai Yu,Xin Chen,Lu Chen*

Main category: cs.LG

TL;DR: MS-BART is a unified modeling framework for mass spectrometry data, enabling cross-modal learning through large-scale pretraining and achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of structure elucidation from MS data due to scarce annotated spectra by leveraging pretraining.

Method: MS-BART maps mass spectra and molecular structures into a shared token vocabulary and uses multi-task pretraining. Finetuning and chemical feedback improve robustness.

Result: Achieves SOTA performance in 5/12 metrics and is faster than competing methods.

Conclusion: MS-BART effectively bridges the gap between theoretical and experimental spectra, demonstrating robustness and efficiency.

Abstract: Mass spectrometry (MS) plays a critical role in molecular identification,
significantly advancing scientific discovery. However, structure elucidation
from MS data remains challenging due to the scarcity of annotated spectra.
While large-scale pretraining has proven effective in addressing data scarcity
in other domains, applying this paradigm to mass spectrometry is hindered by
the complexity and heterogeneity of raw spectral signals. To address this, we
propose MS-BART, a unified modeling framework that maps mass spectra and
molecular structures into a shared token vocabulary, enabling cross-modal
learning through large-scale pretraining on reliably computed
fingerprint-molecule datasets. Multi-task pretraining objectives further
enhance MS-BART's generalization by jointly optimizing denoising and
translation task. The pretrained model is subsequently transferred to
experimental spectra through finetuning on fingerprint predictions generated
with MIST, a pre-trained spectral inference model, thereby enhancing robustness
to real-world spectral variability. While finetuning alleviates the
distributional difference, MS-BART still suffers molecular hallucination and
requires further alignment. We therefore introduce a chemical feedback
mechanism that guides the model toward generating molecules closer to the
reference structure. Extensive evaluations demonstrate that MS-BART achieves
SOTA performance across 5/12 key metrics on MassSpecGym and NPLIB1 and is
faster by one order of magnitude than competing diffusion-based methods, while
comprehensive ablation studies systematically validate the model's
effectiveness and robustness.

</details>


### [224] [On Optimal Hyperparameters for Differentially Private Deep Transfer Learning](https://arxiv.org/abs/2510.20616)
*Aki Rehn,Linzh Zhao,Mikko A. Heikkilä,Antti Honkela*

Main category: cs.LG

TL;DR: The paper investigates the hyperparameters clipping bound (C) and batch size (B) in differentially private transfer learning, revealing mismatches between theory and practice, and suboptimal performance due to uniform hyperparameter settings.


<details>
  <summary>Details</summary>
Motivation: To address the gap between theoretical guidance and empirical outcomes in differentially private transfer learning, particularly regarding hyperparameter choices.

Method: Analyzes the impact of clipping bound (C) and batch size (B) under privacy constraints, with a focus on gradient distributions and cumulative DP noise.

Result: Larger C performs better empirically despite theoretical suggestions, and existing heuristics for B tuning fail; uniform hyperparameter settings lead to suboptimal performance.

Conclusion: Hyperparameter choices significantly affect performance in differentially private transfer learning, requiring task-specific adjustments and better theoretical alignment.

Abstract: Differentially private (DP) transfer learning, i.e., fine-tuning a pretrained
model on private data, is the current state-of-the-art approach for training
large models under privacy constraints. We focus on two key hyperparameters in
this setting: the clipping bound $C$ and batch size $B$. We show a clear
mismatch between the current theoretical understanding of how to choose an
optimal $C$ (stronger privacy requires smaller $C$) and empirical outcomes
(larger $C$ performs better under strong privacy), caused by changes in the
gradient distributions. Assuming a limited compute budget (fixed epochs), we
demonstrate that the existing heuristics for tuning $B$ do not work, while
cumulative DP noise better explains whether smaller or larger batches perform
better. We also highlight how the common practice of using a single $(C,B)$
setting across tasks can lead to suboptimal performance. We find that
performance drops especially when moving between loose and tight privacy and
between plentiful and limited compute, which we explain by analyzing clipping
as a form of gradient re-weighting and examining cumulative DP noise.

</details>


### [225] [H-SPLID: HSIC-based Saliency Preserving Latent Information Decomposition](https://arxiv.org/abs/2510.20627)
*Lukas Miklautz,Chengzhi Shi,Andrii Shkabrii,Theodoros Thirimachos Davarakis,Prudence Lam,Claudia Plant,Jennifer Dy,Stratis Ioannidis*

Main category: cs.LG

TL;DR: H-SPLID is a novel algorithm that decomposes salient and non-salient features into separate spaces, promoting low-dimensional, task-relevant features and linking robustness to latent representation compression.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve robustness and task-relevance in feature learning by explicitly separating salient and non-salient features.

Method: H-SPLID decomposes features into salient and non-salient spaces, leveraging dimensionality and HSIC to measure robustness and compression.

Result: Empirical results show reduced sensitivity to non-salient feature perturbations (e.g., image backgrounds), indicating improved robustness.

Conclusion: H-SPLID effectively links robustness to latent representation compression and enhances task-relevant feature learning.

Abstract: We introduce H-SPLID, a novel algorithm for learning salient feature
representations through the explicit decomposition of salient and non-salient
features into separate spaces. We show that H-SPLID promotes learning
low-dimensional, task-relevant features. We prove that the expected prediction
deviation under input perturbations is upper-bounded by the dimension of the
salient subspace and the Hilbert-Schmidt Independence Criterion (HSIC) between
inputs and representations. This establishes a link between robustness and
latent representation compression in terms of the dimensionality and
information preserved. Empirical evaluations on image classification tasks show
that models trained with H-SPLID primarily rely on salient input components, as
indicated by reduced sensitivity to perturbations affecting non-salient
features, such as image backgrounds. Our code is available at
https://github.com/neu-spiral/H-SPLID.

</details>


### [226] [Large Multimodal Models-Empowered Task-Oriented Autonomous Communications: Design Methodology and Implementation Challenges](https://arxiv.org/abs/2510.20637)
*Hyun Jong Yang,Hyunsoo Kim,Hyeonho Noh,Seungnyun Kim,Byonghyo Shim*

Main category: cs.LG

TL;DR: The paper explores the use of LLMs and LMMs in autonomous communications for 6G, highlighting multimodal sensing, adaptive strategies, and outperforming traditional DL methods.


<details>
  <summary>Details</summary>
Motivation: To leverage the transformative potential of LLMs and LMMs for autonomous communications among machines, vehicles, and humanoids in 6G networks.

Method: The framework integrates multimodal sensing, adaptive reconfiguration, and prompt/fine-tuning strategies, demonstrated through three case studies.

Result: LLM/LMM-aided systems outperform conventional DL methods, showing robustness under dynamic and heterogeneous conditions.

Conclusion: LLMs and LMMs are effective for autonomous communications, offering superior performance and adaptability in complex scenarios.

Abstract: Large language models (LLMs) and large multimodal models (LMMs) have achieved
unprecedented breakthrough, showcasing remarkable capabilities in natural
language understanding, generation, and complex reasoning. This transformative
potential has positioned them as key enablers for 6G autonomous communications
among machines, vehicles, and humanoids. In this article, we provide an
overview of task-oriented autonomous communications with LLMs/LMMs, focusing on
multimodal sensing integration, adaptive reconfiguration, and
prompt/fine-tuning strategies for wireless tasks. We demonstrate the framework
through three case studies: LMM-based traffic control, LLM-based robot
scheduling, and LMM-based environment-aware channel estimation. From
experimental results, we show that the proposed LLM/LMM-aided autonomous
systems significantly outperform conventional and discriminative deep learning
(DL) model-based techniques, maintaining robustness under dynamic objectives,
varying input parameters, and heterogeneous multimodal conditions where
conventional static optimization degrades.

</details>


### [227] [Attention Enhanced Entity Recommendation for Intelligent Monitoring in Cloud Systems](https://arxiv.org/abs/2510.20640)
*Fiza Hussain,Anson Bastos,Anjaly Parayil,Ayush Choure,Chetan Bansal,Rujia Wang,Saravan Rajmohan*

Main category: cs.LG

TL;DR: DiRecGNN is an attention-enhanced entity recommendation framework for cloud service monitoring, improving recommendation accuracy by capturing long-range dependencies and optimizing sparse data.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of recommending optimal attributes for cloud service monitoring, where traditional methods fail due to limited structural information and homophilic nature.

Method: The proposed DiRecGNN uses multi-head attention to focus on heterogeneous neighbors and random walks for long-range dependencies, with multi-faceted loss functions for optimization.

Result: DiRecGNN achieves a 43.1% increase in MRR and is rated 4.5 out of 5 by product teams for usefulness.

Conclusion: The framework effectively enhances entity recommendations for cloud service monitoring, demonstrating significant improvements over existing methods.

Abstract: In this paper, we present DiRecGNN, an attention-enhanced entity
recommendation framework for monitoring cloud services at Microsoft. We provide
insights on the usefulness of this feature as perceived by the cloud service
owners and lessons learned from deployment. Specifically, we introduce the
problem of recommending the optimal subset of attributes (dimensions) that
should be tracked by an automated watchdog (monitor) for cloud services. To
begin, we construct the monitor heterogeneous graph at production-scale. The
interaction dynamics of these entities are often characterized by limited
structural and engagement information, resulting in inferior performance of
state-of-the-art approaches. Moreover, traditional methods fail to capture the
dependencies between entities spanning a long range due to their homophilic
nature. Therefore, we propose an attention-enhanced entity ranking model
inspired by transformer architectures. Our model utilizes a multi-head
attention mechanism to focus on heterogeneous neighbors and their attributes,
and further attends to paths sampled using random walks to capture long-range
dependencies. We also employ multi-faceted loss functions to optimize for
relevant recommendations while respecting the inherent sparsity of the data.
Empirical evaluations demonstrate significant improvements over existing
methods, with our model achieving a 43.1% increase in MRR. Furthermore, product
teams who consumed these features perceive the feature as useful and rated it
4.5 out of 5.

</details>


### [228] [xTime: Extreme Event Prediction with Hierarchical Knowledge Distillation and Expert Fusion](https://arxiv.org/abs/2510.20651)
*Quan Li,Wenchao Yu,Suhang Wang,Minhua Lin,Lingwei Chen,Wei Cheng,Haifeng Chen*

Main category: cs.LG

TL;DR: The paper introduces xTime, a framework for improving extreme event forecasting in time series by leveraging knowledge distillation and a mixture of experts mechanism.


<details>
  <summary>Details</summary>
Motivation: Extreme events in time series (e.g., floods, medical episodes) have serious consequences but are often poorly predicted due to data imbalance and overlooked intermediate event information.

Method: xTime uses knowledge distillation to transfer insights from lower-rarity events and employs a mixture of experts (MoE) to dynamically combine outputs from models trained on different rarity levels.

Result: Experiments show xTime improves extreme event forecasting accuracy from 3% to 78% across multiple datasets.

Conclusion: The proposed framework significantly enhances extreme event prediction by addressing data imbalance and leveraging intermediate event information.

Abstract: Extreme events frequently occur in real-world time series and often carry
significant practical implications. In domains such as climate and healthcare,
these events, such as floods, heatwaves, or acute medical episodes, can lead to
serious consequences. Accurate forecasting of such events is therefore of
substantial importance. Most existing time series forecasting models are
optimized for overall performance within the prediction window, but often
struggle to accurately predict extreme events, such as high temperatures or
heart rate spikes. The main challenges are data imbalance and the neglect of
valuable information contained in intermediate events that precede extreme
events. In this paper, we propose xTime, a novel framework for extreme event
forecasting in time series. xTime leverages knowledge distillation to transfer
information from models trained on lower-rarity events, thereby improving
prediction performance on rarer ones. In addition, we introduce a mixture of
experts (MoE) mechanism that dynamically selects and fuses outputs from expert
models across different rarity levels, which further improves the forecasting
performance for extreme events. Experiments on multiple datasets show that
xTime achieves consistent improvements, with forecasting accuracy on extreme
events improving from 3% to 78%.

</details>


### [229] [Bayesian Jammer Localization with a Hybrid CNN and Path-Loss Mixture of Experts](https://arxiv.org/abs/2510.20666)
*Mariona Jaramillo-Civill,Luis González-Gudiño,Tales Imbiriba,Pau Closas*

Main category: cs.LG

TL;DR: A hybrid Bayesian framework combining physical path-loss models and CNNs improves GNSS jamming localization and RSS field reconstruction by leveraging urban building-height maps and Bayesian inference.


<details>
  <summary>Details</summary>
Motivation: GNSS signals are prone to jamming in urban areas due to multipath and shadowing, and existing data-driven methods fail to accurately reconstruct RSS fields.

Method: A hybrid Bayesian mixture-of-experts model fuses a path-loss expert (for physical consistency) and a CNN (using building-height maps) through log-linear pooling, with Bayesian inference for uncertainty estimation.

Result: The framework enhances localization accuracy and reduces uncertainty, especially near jammers and urban canyons, as validated by urban ray-tracing data.

Conclusion: The proposed hybrid approach outperforms previous methods by better capturing urban propagation effects and providing uncertainty estimates.

Abstract: Global Navigation Satellite System (GNSS) signals are vulnerable to jamming,
particularly in urban areas where multipath and shadowing distort received
power. Previous data-driven approaches achieved reasonable localization but
poorly reconstructed the received signal strength (RSS) field due to limited
spatial context. We propose a hybrid Bayesian mixture-of-experts framework that
fuses a physical path-loss (PL) model and a convolutional neural network (CNN)
through log-linear pooling. The PL expert ensures physical consistency, while
the CNN leverages building-height maps to capture urban propagation effects.
Bayesian inference with Laplace approximation provides posterior uncertainty
over both the jammer position and RSS field. Experiments on urban ray-tracing
data show that localization accuracy improves and uncertainty decreases with
more training points, while uncertainty concentrates near the jammer and along
urban canyons where propagation is most sensitive.

</details>


### [230] [From Masks to Worlds: A Hitchhiker's Guide to World Models](https://arxiv.org/abs/2510.20668)
*Jinbin Bai,Yu Lei,Hecong Wu,Yuchen Zhu,Shufan Li,Yi Xin,Xiangtai Li,Molei Tao,Aditya Grover,Ming-Hsuan Yang*

Main category: cs.LG

TL;DR: A focused guide on building world models, emphasizing unified architectures, interactive generative models, and memory systems for consistent worlds.


<details>
  <summary>Details</summary>
Motivation: To provide a clear roadmap for creating world models by concentrating on core components rather than cataloging existing work.

Method: Follows a structured path from masked models to unified architectures, interactive generative models, and memory-augmented systems.

Result: Identifies generative heart, interactive loop, and memory systems as key elements for successful world models.

Conclusion: This approach represents the most promising path toward developing true world models.

Abstract: This is not a typical survey of world models; it is a guide for those who
want to build worlds. We do not aim to catalog every paper that has ever
mentioned a ``world model". Instead, we follow one clear road: from early
masked models that unified representation learning across modalities, to
unified architectures that share a single paradigm, then to interactive
generative models that close the action-perception loop, and finally to
memory-augmented systems that sustain consistent worlds over time. We bypass
loosely related branches to focus on the core: the generative heart, the
interactive loop, and the memory system. We show that this is the most
promising path towards true world models.

</details>


### [231] [MEIcoder: Decoding Visual Stimuli from Neural Activity by Leveraging Most Exciting Inputs](https://arxiv.org/abs/2510.20762)
*Jan Sobotka,Luca Baroni,Ján Antolík*

Main category: cs.LG

TL;DR: MEIcoder, a biologically informed decoding method using MEIs, adversarial training, and SSIM loss, excels in reconstructing visual stimuli from sparse neural data, achieving high fidelity with minimal neurons and training samples.


<details>
  <summary>Details</summary>
Motivation: The scarcity of high-throughput neural data in primates/humans limits deep learning decoding techniques, prompting the need for biologically informed methods like MEIcoder.

Method: MEIcoder integrates neuron-specific most exciting inputs (MEIs), adversarial training, and SSIM loss to decode visual stimuli from single-cell V1 activity.

Result: MEIcoder outperforms existing methods, especially with small datasets (<1,000 samples), reconstructing images accurately from as few as 1,000-2,500 neurons.

Conclusion: MEIcoder validates reliable decoding in early visual systems and offers practical tools for neuroscience and neuroengineering applications.

Abstract: Decoding visual stimuli from neural population activity is crucial for
understanding the brain and for applications in brain-machine interfaces.
However, such biological data is often scarce, particularly in primates or
humans, where high-throughput recording techniques, such as two-photon imaging,
remain challenging or impossible to apply. This, in turn, poses a challenge for
deep learning decoding techniques. To overcome this, we introduce MEIcoder, a
biologically informed decoding method that leverages neuron-specific most
exciting inputs (MEIs), a structural similarity index measure loss, and
adversarial training. MEIcoder achieves state-of-the-art performance in
reconstructing visual stimuli from single-cell activity in primary visual
cortex (V1), especially excelling on small datasets with fewer recorded
neurons. Using ablation studies, we demonstrate that MEIs are the main drivers
of the performance, and in scaling experiments, we show that MEIcoder can
reconstruct high-fidelity natural-looking images from as few as 1,000-2,500
neurons and less than 1,000 training data points. We also propose a unified
benchmark with over 160,000 samples to foster future research. Our results
demonstrate the feasibility of reliable decoding in early visual system and
provide practical insights for neuroscience and neuroengineering applications.

</details>


### [232] [GRACE: GRaph-based Addiction Care prEdiction](https://arxiv.org/abs/2510.20671)
*Subham Kumar,Prakrithi Shivaprakash,Koustav Rudra,Lekhansh Shukla,Animesh Mukherjee*

Main category: cs.LG

TL;DR: Proposes GRACE, a graph neural network framework, to automate locus of care decisions for addiction patients, addressing class imbalance through feature engineering and unbiased meta-graph training.


<details>
  <summary>Details</summary>
Motivation: Addressing the unmet need for automated frameworks due to limited specialized treatment resources and severe class imbalances in addiction datasets.

Method: Introduces GRACE, a novel graph neural network framework, featuring extensive feature engineering and an unbiased meta-graph approach to handle class imbalance.

Result: Demonstrates an 11-35% improvement in F1 score for the minority class compared to baselines.

Conclusion: GRACE effectively automates locus of care decisions, offering a scalable solution with significant performance gains.

Abstract: Determining the appropriate locus of care for addiction patients is one of
the most critical clinical decisions that affects patient treatment outcomes
and effective use of resources. With a lack of sufficient specialized treatment
resources, such as inpatient beds or staff, there is an unmet need to develop
an automated framework for the same. Current decision-making approaches suffer
from severe class imbalances in addiction datasets. To address this limitation,
we propose a novel graph neural network (GRACE) framework that formalizes locus
of care prediction as a structured learning problem. Further, we perform
extensive feature engineering and propose a new approach of obtaining an
unbiased meta-graph to train a GNN to overcome the class imbalance problem.
Experimental results in real-world data show an improvement of 11-35% in terms
of the F1 score of the minority class over competitive baselines. The codes and
note embeddings are available at https://anonymous.4open.science/r/GRACE-F8E1/.

</details>


### [233] [A Scalable, Causal, and Energy Efficient Framework for Neural Decoding with Spiking Neural Networks](https://arxiv.org/abs/2510.20683)
*Georgios Mentzelopoulos,Ioannis Asmanis,Konrad P. Kording,Eva L. Dyer,Kostas Daniilidis,Flavia Vitale*

Main category: cs.LG

TL;DR: Spikachu is a scalable, causal, and energy-efficient neural decoding framework using SNNs, outperforming traditional models in real-time BCIs with lower energy consumption.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of current neural decoders in BCIs, which either lack generalization or struggle with real-time use due to high energy demands.

Method: Spikachu processes binned spikes into a latent space using SNNs, extracting features adapted to input timing for behavioral predictions.

Result: Evaluated on 43 hours of primate recordings, Spikachu outperforms causal baselines with 2.26-418.81x less energy and enables few-shot transfer.

Conclusion: Spikachu offers a competitive, energy-efficient solution for real-time neural decoding, scalable across sessions and tasks.

Abstract: Brain-computer interfaces (BCIs) promise to enable vital functions, such as
speech and prosthetic control, for individuals with neuromotor impairments.
Central to their success are neural decoders, models that map neural activity
to intended behavior. Current learning-based decoding approaches fall into two
classes: simple, causal models that lack generalization, or complex, non-causal
models that generalize and scale offline but struggle in real-time settings.
Both face a common challenge, their reliance on power-hungry artificial neural
network backbones, which makes integration into real-world, resource-limited
systems difficult. Spiking neural networks (SNNs) offer a promising
alternative. Because they operate causally these models are suitable for
real-time use, and their low energy demands make them ideal for
battery-constrained environments. To this end, we introduce Spikachu: a
scalable, causal, and energy-efficient neural decoding framework based on SNNs.
Our approach processes binned spikes directly by projecting them into a shared
latent space, where spiking modules, adapted to the timing of the input,
extract relevant features; these latent representations are then integrated and
decoded to generate behavioral predictions. We evaluate our approach on 113
recording sessions from 6 non-human primates, totaling 43 hours of recordings.
Our method outperforms causal baselines when trained on single sessions using
between 2.26 and 418.81 times less energy. Furthermore, we demonstrate that
scaling up training to multiple sessions and subjects improves performance and
enables few-shot transfer to unseen sessions, subjects, and tasks. Overall,
Spikachu introduces a scalable, online-compatible neural decoding framework
based on SNNs, whose performance is competitive relative to state-of-the-art
models while consuming orders of magnitude less energy.

</details>


### [234] [Separating the what and how of compositional computation to enable reuse and continual learning](https://arxiv.org/abs/2510.20709)
*Haozhe Shan,Sun Minni,Lea Duncker*

Main category: cs.LG

TL;DR: The paper proposes a two-system RNN model for continual learning and skill reuse, using a 'what' system for task inference and a 'how' system for implementation, demonstrating competitive performance and flexibility.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the lack of understanding about neural mechanisms enabling continual learning and flexible skill reuse, aiming to model these processes computationally.

Method: A two-system RNN approach: a 'what' system infers task computations via a probabilistic generative model, while a 'how' system implements these computations via low-rank RNN components. The model learns incrementally and avoids catastrophic forgetting.

Result: The framework shows competitive performance, enables continual learning without forgetting, and supports fast generalization to new tasks, demonstrating forward and backward transfer.

Conclusion: The two-system RNN model effectively addresses continual learning and compositional skill reuse, offering insights into neural mechanisms and computational efficiency.

Abstract: The ability to continually learn, retain and deploy skills to accomplish
goals is a key feature of intelligent and efficient behavior. However, the
neural mechanisms facilitating the continual learning and flexible
(re-)composition of skills remain elusive. Here, we study continual learning
and the compositional reuse of learned computations in recurrent neural network
(RNN) models using a novel two-system approach: one system that infers what
computation to perform, and one that implements how to perform it. We focus on
a set of compositional cognitive tasks commonly studied in neuroscience. To
construct the what system, we first show that a large family of tasks can be
systematically described by a probabilistic generative model, where
compositionality stems from a shared underlying vocabulary of discrete task
epochs. The shared epoch structure makes these tasks inherently compositional.
We first show that this compositionality can be systematically described by a
probabilistic generative model. Furthermore, We develop an unsupervised online
learning approach that can learn this model on a single-trial basis, building
its vocabulary incrementally as it is exposed to new tasks, and inferring the
latent epoch structure as a time-varying computational context within a trial.
We implement the how system as an RNN whose low-rank components are composed
according to the context inferred by the what system. Contextual inference
facilitates the creation, learning, and reuse of low-rank RNN components as new
tasks are introduced sequentially, enabling continual learning without
catastrophic forgetting. Using an example task set, we demonstrate the efficacy
and competitive performance of this two-system learning framework, its
potential for forward and backward transfer, as well as fast compositional
generalization to unseen tasks.

</details>


### [235] [Optimizing Clinical Fall Risk Prediction: A Data-Driven Integration of EHR Variables with the Johns Hopkins Fall Risk Assessment Tool](https://arxiv.org/abs/2510.20714)
*Fardin Ganjkhanloo,Emmett Springer,Erik H. Hoyer,Daniel L. Young,Kimia Ghobadi*

Main category: cs.LG

TL;DR: Summarize the paper


<details>
  <summary>Details</summary>
Motivation: Align fall risk prediction from JHFRAT with clinically meaningful measures

Method: Retrospective analysis of 54,209 admissions using CSO models

Result: CSO models improved predictive performance (AUC-ROC=0.91 vs. 0.86)

Conclusion: CSO provides a robust, interpretable method for enhancing fall risk prediction

Abstract: In this study we aim to better align fall risk prediction from the Johns
Hopkins Fall Risk Assessment Tool (JHFRAT) with additional clinically
meaningful measures via a data-driven modelling approach. We conducted a
retrospective analysis of 54,209 inpatient admissions from three Johns Hopkins
Health System hospitals between March 2022 and October 2023. A total of 20,208
admissions were included as high fall risk encounters, and 13,941 were included
as low fall risk encounters. To incorporate clinical knowledge and maintain
interpretability, we employed constrained score optimization (CSO) models on
JHFRAT assessment data and additional electronic health record (EHR) variables.
The model demonstrated significant improvements in predictive performance over
the current JHFRAT (CSO AUC-ROC=0.91, JHFRAT AUC-ROC=0.86). The constrained
score optimization models performed similarly with and without the EHR
variables. Although the benchmark black-box model (XGBoost), improves upon the
performance metrics of the knowledge-based constrained logistic regression
(AUC-ROC=0.94), the CSO demonstrates more robustness to variations in risk
labelling. This evidence-based approach provides a robust foundation for health
systems to systematically enhance inpatient fall prevention protocols and
patient safety using data-driven optimization techniques, contributing to
improved risk assessment and resource allocation in healthcare settings.

</details>


### [236] [Unsupervised Anomaly Prediction with N-BEATS and Graph Neural Network in Multi-variate Semiconductor Process Time Series](https://arxiv.org/abs/2510.20718)
*Daniel Sorensen,Bappaditya Dey,Minjin Hwang,Sandip Halder*

Main category: cs.LG

TL;DR: A novel anomaly prediction framework for semiconductor manufacturing uses forecasting models (N-BEATS and GNN) to detect deviations in time-series data, with GNN outperforming due to capturing inter-variable relationships efficiently.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges like high dimensionality, class imbalance, and complex interdependencies in semiconductor fabrication anomaly prediction.

Method: Two-stage framework: (a) train forecasting models on anomaly-free data, (b) compare forecasts with actual data to flag deviations as anomalies. Uses N-BEATS (univariate) and GNN (multivariate) models.

Result: Both models perform well up to 20 time points for forecasting and 50 for anomaly prediction. GNN excels with fewer parameters and lower computational cost.

Conclusion: GNN is promising for real-time anomaly forecasting in semiconductor manufacturing due to efficiency and performance.

Abstract: Semiconductor manufacturing is an extremely complex and precision-driven
process, characterized by thousands of interdependent parameters collected
across diverse tools and process steps. Multi-variate time-series analysis has
emerged as a critical field for real-time monitoring and fault detection in
such environments. However, anomaly prediction in semiconductor fabrication
presents several critical challenges, including high dimensionality of sensor
data and severe class imbalance due to the rarity of true faults. Furthermore,
the complex interdependencies between variables complicate both anomaly
prediction and root-cause-analysis. This paper proposes two novel approaches to
advance the field from anomaly detection to anomaly prediction, an essential
step toward enabling real-time process correction and proactive fault
prevention. The proposed anomaly prediction framework contains two main stages:
(a) training a forecasting model on a dataset assumed to contain no anomalies,
and (b) performing forecast on unseen time series data. The forecast is
compared with the forecast of the trained signal. Deviations beyond a
predefined threshold are flagged as anomalies. The two approaches differ in the
forecasting model employed. The first assumes independence between variables by
utilizing the N-BEATS model for univariate time series forecasting. The second
lifts this assumption by utilizing a Graph Neural Network (GNN) to capture
inter-variable relationships. Both models demonstrate strong forecasting
performance up to a horizon of 20 time points and maintain stable anomaly
prediction up to 50 time points. The GNN consistently outperforms the N-BEATS
model while requiring significantly fewer trainable parameters and lower
computational cost. These results position the GNN as promising solution for
online anomaly forecasting to be deployed in manufacturing environments.

</details>


### [237] [No-Regret Thompson Sampling for Finite-Horizon Markov Decision Processes with Gaussian Processes](https://arxiv.org/abs/2510.20725)
*Jasmine Bayrooti,Sattar Vakili,Amanda Prorok,Carl Henrik Ek*

Main category: cs.LG

TL;DR: Thompson sampling (TS) is analyzed in episodic RL with Gaussian priors, yielding a regret bound of $	ilde{O}(	ext{something})$.


<details>
  <summary>Details</summary>
Motivation: The theoretical foundations of TS in settings with complex temporal structure like RL are limited, prompting this study.

Method: TS is applied in episodic RL with joint Gaussian process priors over rewards and transitions, addressing non-Gaussian value functions and Bellman updates.

Result: A regret bound of $	ilde{O}(	ext{something})$ is proven, capturing the GP model's complexity.

Conclusion: This work enhances TS understanding in RL, showing how structural assumptions and model uncertainty affect performance.

Abstract: Thompson sampling (TS) is a powerful and widely used strategy for sequential
decision-making, with applications ranging from Bayesian optimization to
reinforcement learning (RL). Despite its success, the theoretical foundations
of TS remain limited, particularly in settings with complex temporal structure
such as RL. We address this gap by establishing no-regret guarantees for TS
using models with Gaussian marginal distributions. Specifically, we consider TS
in episodic RL with joint Gaussian process (GP) priors over rewards and
transitions. We prove a regret bound of
$\mathcal{\tilde{O}}(\sqrt{KH\Gamma(KH)})$ over $K$ episodes of horizon $H$,
where $\Gamma(\cdot)$ captures the complexity of the GP model. Our analysis
addresses several challenges, including the non-Gaussian nature of value
functions and the recursive structure of Bellman updates, and extends classical
tools such as the elliptical potential lemma to multi-output settings. This
work advances the understanding of TS in RL and highlights how structural
assumptions and model uncertainty shape its performance in finite-horizon
Markov Decision Processes.

</details>


### [238] [Amplifying Prominent Representations in Multimodal Learning via Variational Dirichlet Process](https://arxiv.org/abs/2510.20736)
*Tsai Hor Chan,Feng Wu,Yihang Chen,Guosheng Yin,Lequan Yu*

Main category: cs.LG

TL;DR: The paper proposes a DP-driven multimodal learning framework to balance intra-modal representation and cross-modal alignment, outperforming competitors on various datasets.


<details>
  <summary>Details</summary>
Motivation: Effective multimodal fusion is crucial in fields like healthcare and finance. The challenge lies in preserving feature expressiveness within modalities while learning cross-modal interactions without over-regularizing.

Method: The framework uses Dirichlet process (DP) mixture models to dynamically allocate feature contributions, leveraging DP's richer-gets-richer property for optimal balance between intra-modal and cross-modal learning.

Result: Experiments show superior performance over competitors, with ablation studies confirming DP's effectiveness in aligning modality distributions and robustness to hyperparameter changes.

Conclusion: The DP-driven framework successfully balances intra-modal and cross-modal learning, demonstrating its effectiveness and robustness in multimodal fusion.

Abstract: Developing effective multimodal fusion approaches has become increasingly
essential in many real-world scenarios, such as health care and finance. The
key challenge is how to preserve the feature expressiveness in each modality
while learning cross-modal interactions. Previous approaches primarily focus on
the cross-modal alignment, while over-emphasis on the alignment of marginal
distributions of modalities may impose excess regularization and obstruct
meaningful representations within each modality. The Dirichlet process (DP)
mixture model is a powerful Bayesian non-parametric method that can amplify the
most prominent features by its richer-gets-richer property, which allocates
increasing weights to them. Inspired by this unique characteristic of DP, we
propose a new DP-driven multimodal learning framework that automatically
achieves an optimal balance between prominent intra-modal representation
learning and cross-modal alignment. Specifically, we assume that each modality
follows a mixture of multivariate Gaussian distributions and further adopt DP
to calculate the mixture weights for all the components. This paradigm allows
DP to dynamically allocate the contributions of features and select the most
prominent ones, leveraging its richer-gets-richer property, thus facilitating
multimodal feature fusion. Extensive experiments on several multimodal datasets
demonstrate the superior performance of our model over other competitors.
Ablation analysis further validates the effectiveness of DP in aligning
modality distributions and its robustness to changes in key hyperparameters.
Code is anonymously available at https://github.com/HKU-MedAI/DPMM.git

</details>


### [239] [Out-of-distribution Tests Reveal Compositionality in Chess Transformers](https://arxiv.org/abs/2510.20783)
*Anna Mészáros,Patrik Reizinger,Ferenc Huszár*

Main category: cs.LG

TL;DR: Transformers show compositional generalization in chess, adhering to fundamental rules and performing well in out-of-distribution scenarios, though they lag behind symbolic AI in Chess960.


<details>
  <summary>Details</summary>
Motivation: To understand if Transformers truly grasp chess rules and generalize systematically beyond training data.

Method: Train a 270M parameter chess Transformer and test it on OOD scenarios, including Chess960.

Result: Transformers exhibit rule adherence and compositional generalization but are inferior to symbolic AI in Chess960.

Conclusion: Transformers demonstrate emergent understanding of chess rules but need improvement for complex variants.

Abstract: Chess is a canonical example of a task that requires rigorous reasoning and
long-term planning. Modern decision Transformers - trained similarly to LLMs -
are able to learn competent gameplay, but it is unclear to what extent they
truly capture the rules of chess. To investigate this, we train a 270M
parameter chess Transformer and test it on out-of-distribution scenarios,
designed to reveal failures of systematic generalization. Our analysis shows
that Transformers exhibit compositional generalization, as evidenced by strong
rule extrapolation: they adhere to fundamental syntactic rules of the game by
consistently choosing valid moves even in situations very different from the
training data. Moreover, they also generate high-quality moves for OOD puzzles.
In a more challenging test, we evaluate the models on variants including
Chess960 (Fischer Random Chess) - a variant of chess where starting positions
of pieces are randomized. We found that while the model exhibits basic strategy
adaptation, they are inferior to symbolic AI algorithms that perform explicit
search, but gap is smaller when playing against users on Lichess. Moreover, the
training dynamics revealed that the model initially learns to move only its own
pieces, suggesting an emergent compositional understanding of the game.

</details>


### [240] [KL-Regularized Reinforcement Learning is Designed to Mode Collapse](https://arxiv.org/abs/2510.20817)
*Anthony GX-Chen,Jatin Prakash,Jeff Guo,Rob Fergus,Rajesh Ranganath*

Main category: cs.LG

TL;DR: The paper challenges the belief that forward KL divergence ensures diverse sampling in reinforcement learning, showing that mode coverage depends on factors like regularization strength. It proposes a simple, scalable algorithm that improves diversity and quality in language models without external diversity signals.


<details>
  <summary>Details</summary>
Motivation: To address the misconception that forward KL divergence inherently ensures diverse sampling in reinforcement learning, and to develop a better approach for achieving diversity without external signals.

Method: The authors analyze the role of reverse/forward KL divergence and regularization strength empirically and mathematically. They propose a scalable algorithm that adjusts reward magnitudes minimally while ensuring diverse sampling modes.

Result: The new algorithm successfully improves solution quality and diversity in Large Language Models and Chemical Language Models, regardless of using forward or reverse KL divergence.

Conclusion: The choice of KL divergence type does not inherently determine mode coverage; factors like regularization strength are more critical. The proposed algorithm effectively optimizes for diversity without external signals.

Abstract: It is commonly believed that optimizing the reverse KL divergence results in
"mode seeking", while optimizing forward KL results in "mass covering", with
the latter being preferred if the goal is to sample from multiple diverse
modes. We show -- mathematically and empirically -- that this intuition does
not necessarily transfer well to doing reinforcement learning with
reverse/forward KL regularization (e.g. as commonly used with language models).
Instead, the choice of reverse/forward KL determines the family of optimal
target distributions, parameterized by the regularization coefficient. Mode
coverage depends primarily on other factors, such as regularization strength,
and relative scales between rewards and reference probabilities. Further, we
show commonly used settings such as low regularization strength and equal
verifiable rewards tend to specify unimodal target distributions, meaning the
optimization objective is, by construction, non-diverse. We leverage these
insights to construct a simple, scalable, and theoretically justified
algorithm. It makes minimal changes to reward magnitudes, yet optimizes for a
target distribution which puts high probability over all high-quality sampling
modes. In experiments, this simple modification works to post-train both Large
Language Models and Chemical Language Models to have higher solution quality
and diversity, without any external signals of diversity, and works with both
forward and reverse KL when using either naively fails.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [241] [A Quantum-Inspired Algorithm for Solving Sudoku Puzzles and the MaxCut Problem](https://arxiv.org/abs/2510.19835)
*Max B. Zhao,Fei Li*

Main category: cs.AI

TL;DR: A quantum-inspired algorithm using Matrix Product States (MPS) is proposed to solve QUBO problems, demonstrating effectiveness on Sudoku puzzles and MaxCut problems with scalability for industrial applications.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of solving Quadratic Unconstrained Binary Optimization (QUBO) problems efficiently, which are equivalent to finding ground states of Ising spin-glass Hamiltonians.

Method: The algorithm employs MPS to represent spin configurations and uses a driver Hamiltonian with a transverse magnetic field, updated via Density Matrix Renormalization Group (DMRG) to minimize energy iteratively.

Result: The algorithm reliably finds global minima in diverse QUBO instances, including Sudoku puzzles with over 200 spins and MaxCut problems with up to 251 nodes.

Conclusion: The quantum-inspired approach shows scalability, generalizability, and promise for industrial-scale QUBO applications.

Abstract: We propose and evaluate a quantum-inspired algorithm for solving Quadratic
Unconstrained Binary Optimization (QUBO) problems, which are mathematically
equivalent to finding ground states of Ising spin-glass Hamiltonians. The
algorithm employs Matrix Product States (MPS) to compactly represent large
superpositions of spin configurations and utilizes a discrete driving schedule
to guide the MPS toward the ground state. At each step, a driver Hamiltonian --
incorporating a transverse magnetic field -- is combined with the problem
Hamiltonian to enable spin flips and facilitate quantum tunneling. The MPS is
updated using the standard Density Matrix Renormalization Group (DMRG) method,
which iteratively minimizes the system's energy via multiple sweeps across the
spin chain. Despite its heuristic nature, the algorithm reliably identifies
global minima, not merely near-optimal solutions, across diverse QUBO
instances. We first demonstrate its effectiveness on intermediate-level Sudoku
puzzles from publicly available sources, involving over $200$ Ising spins with
long-range couplings dictated by constraint satisfaction. We then apply the
algorithm to MaxCut problems from the Biq Mac library, successfully solving
instances with up to $251$ nodes and $3,265$ edges. We discuss the advantages
of this quantum-inspired approach, including its scalability, generalizability,
and suitability for industrial-scale QUBO applications.

</details>


### [242] [Benchmarking Reasoning Reliability in Artificial Intelligence Models for Energy-System Analysis](https://arxiv.org/abs/2510.19836)
*Eliseo Curcio*

Main category: cs.AI

TL;DR: The paper introduces the Analytical Reliability Benchmark (ARB) to evaluate reasoning reliability in AI systems for energy analysis, testing four models under standardized conditions and demonstrating measurable differences in performance.


<details>
  <summary>Details</summary>
Motivation: To address the lack of standardized frameworks for validating logical integrity in AI systems used for energy sector forecasting, optimization, and policy design.

Method: The ARB framework integrates five submetrics (accuracy, reasoning reliability, uncertainty discipline, policy consistency, transparency) and evaluates four models (GPT-4/5, Claude 4.5 Sonnet, Gemini 2.5 Pro, Llama 3 70B) across deterministic, probabilistic, and epistemic scenarios using open datasets.

Result: GPT-4/5 and Claude 4.5 Sonnet scored highest (>90 Analytical Reliability Index), Gemini 2.5 Pro showed moderate stability, and Llama 3 70B fell below professional thresholds, with statistically significant differences.

Conclusion: The ARB provides the first quantitative method to verify reasoning reliability in AI systems for energy applications, supporting trustworthy decision-making in the global energy transition.

Abstract: Artificial intelligence and machine learning are increasingly used for
forecasting, optimization, and policy design in the energy sector, yet no
standardized framework exists to evaluate whether these systems reason
correctly. Current validation practices focus on predictive accuracy or
computational efficiency, leaving the logical integrity of analytical
conclusions untested. This study introduces the Analytical Reliability
Benchmark (ARB), a reproducible framework that quantifies reasoning reliability
in large language models applied to energy system analysis. The benchmark
integrates five submetrics: accuracy, reasoning reliability, uncertainty
discipline, policy consistency, and transparency, and evaluates model
performance across deterministic, probabilistic, and epistemic scenarios using
open technoeconomic datasets (NREL ATB 2024, DOE H2A/H2New, IEA WEO 2024). Four
frontier models (GPT-4/5, Claude 4.5 Sonnet, Gemini 2.5 Pro, Llama 3 70B) were
tested under identical factual and regulatory conditions. Results show that
reasoning reliability can be objectively measured. GPT-4/5 and Claude 4.5
Sonnet achieved consistent and policy-compliant reasoning (Analytical
Reliability Index greater than 90), Gemini 2.5 Pro demonstrated moderate
stability, and Llama 3 70B remained below professional thresholds. Statistical
validation confirmed that these differences are significant and reproducible.
The ARB establishes the first quantitative method in the energy literature for
verifying causal, probabilistic, and policy-driven reasoning in artificial
intelligence systems, providing a reference framework for trustworthy and
transparent analytical applications in the global energy transition.

</details>


### [243] [Branch-and-Browse: Efficient and Controllable Web Exploration with Tree-Structured Reasoning and Action Memory](https://arxiv.org/abs/2510.19838)
*Shiqi He,Yue Cui,Xinyu Ma,Yaliang Li,Bolin Ding,Mosharaf Chowdhury*

Main category: cs.AI

TL;DR: Branch-and-Browse is a web agent framework improving reasoning depth and efficiency for LLM-based tasks, achieving better success rates and faster execution compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based web agents struggle with multi-step reasoning, backtracking, and computational efficiency, limiting their practical use in open web environments.

Method: Introduces Branch-and-Browse, featuring subtask management with tree-structured exploration, efficient web state replay, and page action memory for shared actions.

Result: Achieves a 35.8% task success rate and reduces execution time by up to 40.4% on the WebArena benchmark.

Conclusion: Branch-and-Browse is a reliable and efficient framework for enhancing LLM-based web agents' capabilities.

Abstract: Autonomous web agents powered by large language models (LLMs) show strong
potential for performing goal-oriented tasks such as information retrieval,
report generation, and online transactions. These agents mark a key step toward
practical embodied reasoning in open web environments. However, existing
approaches remain limited in reasoning depth and efficiency: vanilla linear
methods fail at multi-step reasoning and lack effective backtracking, while
other search strategies are coarse-grained and computationally costly. We
introduce Branch-and-Browse, a fine-grained web agent framework that unifies
structured reasoning-acting, contextual memory, and efficient execution. It (i)
employs explicit subtask management with tree-structured exploration for
controllable multi-branch reasoning, (ii) bootstraps exploration through
efficient web state replay with background reasoning, and (iii) leverages a
page action memory to share explored actions within and across sessions. On the
WebArena benchmark, Branch-and-Browse achieves a task success rate of 35.8\%
and reduces execution time by up to 40.4\% relative to state-of-the-art
methods. These results demonstrate that Branch-and-Browse is a reliable and
efficient framework for LLM-based web agents.

</details>


### [244] [DAG-Math: Graph-Guided Mathematical Reasoning in LLMs](https://arxiv.org/abs/2510.19842)
*Yuanhe Zhang,Ilja Kuzborskij,Jason D. Lee,Chenlei Leng,Fanghui Liu*

Main category: cs.AI

TL;DR: The paper investigates whether LLMs' success in solving math problems using Chain-of-Thought (CoT) stems from rule-consistent reasoning or other factors. It introduces a rule-based stochastic process framework over DAGs to evaluate reasoning fidelity.


<details>
  <summary>Details</summary>
Motivation: To clarify whether LLMs' performance on math problems with CoT is due to rule-consistent reasoning or other mechanisms like search or rote procedures.

Method: Proposes modeling CoT as a rule-based stochastic process over DAGs, introducing logical closeness as a metric to evaluate reasoning fidelity. The DAG-MATH CoT format and benchmark are also introduced.

Result: Analysis reveals significant differences in reasoning fidelity among LLM families, even when answer accuracy (PASS@k) is comparable.

Conclusion: The framework bridges free-form CoT and formal proofs, providing diagnostics for LLM reasoning evaluation. The benchmark and code are publicly available.

Abstract: Large Language Models (LLMs) demonstrate strong performance on mathematical
problems when prompted with Chain-of-Thought (CoT), yet it remains unclear
whether this success stems from search, rote procedures, or rule-consistent
reasoning. To address this, we propose modeling CoT as a certain rule-based
stochastic process over directed acyclic graphs (DAGs), where nodes represent
intermediate derivation states and edges encode rule applications. Within this
framework, we introduce logical closeness, a metric that quantifies how well a
model's CoT trajectory (i.e., the LLM's final output) adheres to the DAG
structure, providing evaluation beyond classical PASS@k metrics. Building on
this, we introduce the DAG-MATH CoT format and construct a benchmark that
guides LLMs to generate CoT trajectories in this format, thereby enabling the
evaluation of their reasoning ability under our framework. Across standard
mathematical reasoning datasets, our analysis uncovers statistically
significant differences in reasoning fidelity among representative LLM
families-even when PASS@k is comparable-highlighting gaps between final-answer
accuracy and rule-consistent derivation. Our framework provides a balance
between free-form CoT and formal proofs systems, offering actionable
diagnostics for LLMs reasoning evaluation. Our benchmark and code are available
at: https://github.com/YuanheZ/DAG-MATH-Formatted-CoT.

</details>


### [245] [Surfer 2: The Next Generation of Cross-Platform Computer Use Agents](https://arxiv.org/abs/2510.19949)
*Mathieu Andreux,Märt Bakler,Yanael Barbier,Hamza Ben Chekroun,Emilien Biré,Antoine Bonnet,Riaz Bordie,Nathan Bout,Matthias Brunel,Aleix Cambray,Pierre-Louis Cedoz,Antoine Chassang,Gautier Cloix,Ethan Connelly,Alexandra Constantinou,Ramzi De Coster,Hubert de la Jonquiere,Aurélien Delfosse,Maxime Delpit,Alexis Deprez,Augustin Derupti,Mathieu Diaz,Shannon D'Souza,Julie Dujardin,Abai Edmund,Michael Eickenberg,Armand Fatalot,Wissem Felissi,Isaac Herring,Xavier Koegler,Erwan Le Jumeau de Kergaradec,Aurélien Lac,Maxime Langevin,Corentin Lauverjat,Antonio Loison,Avshalom Manevich,Axel Moyal,Axel Nguyen Kerbel,Marinela Parovic,Julien Revelle,Guillaume Richard,Mats Richter,Ronan Riochet,María Santos,Romain Savidan,Laurent Sifre,Maxime Theillard,Marc Thibault,Ivan Valentini,Tony Wu,Laura Yie,Kai Yuan,Jevgenij Zubovskij*

Main category: cs.AI

TL;DR: Surfer 2 is a unified visual-based agent architecture that outperforms prior systems across web, desktop, and mobile tasks without task-specific fine-tuning, achieving human-level performance.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in building agents that generalize across diverse digital environments without relying on platform-specific interfaces.

Method: Surfer 2 employs hierarchical context management, decoupled planning and execution, and self-verification with adaptive recovery.

Result: Achieves 97.1% accuracy on WebVoyager, 69.6% on WebArena, 60.1% on OSWorld, and 87.1% on AndroidWorld, surpassing human performance.

Conclusion: The systematic design amplifies foundation model capabilities, enabling general-purpose control via visual interaction, though further improvements in vision-language models are needed.

Abstract: Building agents that generalize across web, desktop, and mobile environments
remains an open challenge, as prior systems rely on environment-specific
interfaces that limit cross-platform deployment. We introduce Surfer 2, a
unified architecture operating purely from visual observations that achieves
state-of-the-art performance across all three environments. Surfer 2 integrates
hierarchical context management, decoupled planning and execution, and
self-verification with adaptive recovery, enabling reliable operation over long
task horizons. Our system achieves 97.1% accuracy on WebVoyager, 69.6% on
WebArena, 60.1% on OSWorld, and 87.1% on AndroidWorld, outperforming all prior
systems without task-specific fine-tuning. With multiple attempts, Surfer 2
exceeds human performance on all benchmarks. These results demonstrate that
systematic orchestration amplifies foundation model capabilities and enables
general-purpose computer control through visual interaction alone, while
calling for a next-generation vision language model to achieve Pareto-optimal
cost-efficiency.

</details>


### [246] [RELATE: A Schema-Agnostic Perceiver Encoder for Multimodal Relational Graphs](https://arxiv.org/abs/2510.19954)
*Joseph Meyer,Divyansha Lachi,Reza Mohammadi,Roshan Reddy Upendra,Eva L. Dyer,Mark Li,Tom Palczewski*

Main category: cs.AI

TL;DR: RELATE is a schema-agnostic encoder for relational graph data, reducing parameters by up to 5x while matching schema-specific encoder performance within 3%.


<details>
  <summary>Details</summary>
Motivation: Existing GNNs use schema-specific encoders, which are inefficient and limit scalability. RELATE aims to overcome this by enabling parameter sharing and handling varying schemas.

Method: RELATE uses shared modality-specific encoders (for categorical, numerical, textual, and temporal attributes) and a Perceiver-style cross-attention module for node representation.

Result: Tested on ReLGNN and HGT in RelBench, RELATE performs within 3% of schema-specific encoders while reducing parameters by up to 5x.

Conclusion: RELATE supports varying schemas and enables multi-dataset pretraining, advancing toward foundation models for relational graph data.

Abstract: Relational multi-table data is common in domains such as e-commerce,
healthcare, and scientific research, and can be naturally represented as
heterogeneous temporal graphs with multi-modal node attributes. Existing graph
neural networks (GNNs) rely on schema-specific feature encoders, requiring
separate modules for each node type and feature column, which hinders
scalability and parameter sharing. We introduce RELATE (Relational Encoder for
Latent Aggregation of Typed Entities), a schema-agnostic, plug-and-play feature
encoder that can be used with any general purpose GNN. RELATE employs shared
modality-specific encoders for categorical, numerical, textual, and temporal
attributes, followed by a Perceiver-style cross-attention module that
aggregates features into a fixed-size, permutation-invariant node
representation. We evaluate RELATE on ReLGNN and HGT in the RelBench benchmark,
where it achieves performance within 3% of schema-specific encoders while
reducing parameter counts by up to 5x. This design supports varying schemas and
enables multi-dataset pretraining for general-purpose GNNs, paving the way
toward foundation models for relational graph data.

</details>


### [247] [A new wave of vehicle insurance fraud fueled by generative AI](https://arxiv.org/abs/2510.19957)
*Amir Hever,Itai Orr*

Main category: cs.AI

TL;DR: Generative AI is enabling large-scale insurance fraud by creating falsified accident evidence, posing a costly challenge for insurers. While countermeasures like AI detection tools exist, they face limitations. UVeye proposes a layered solution to combat this issue.


<details>
  <summary>Details</summary>
Motivation: Insurance fraud, especially in vehicle insurance, causes significant financial losses. The emergence of generative AI tools exacerbates the problem by enabling fraudsters to create highly realistic fake evidence.

Method: Fraudsters use generative AI to fabricate crash photos, damage evidence, and fake identities. Insurers deploy AI-based deepfake detection and enhanced verification processes, though these methods have limitations.

Result: Current detection tools struggle with accuracy, and fraudsters adapt quickly to evade checks, leading to an ongoing challenge in combating AI-driven fraud.

Conclusion: UVeye introduces a layered solution to improve fraud detection and mitigation, addressing the shortcomings of existing methods.

Abstract: Generative AI is supercharging insurance fraud by making it easier to falsify
accident evidence at scale and in rapid time. Insurance fraud is a pervasive
and costly problem, amounting to tens of billions of dollars in losses each
year. In the vehicle insurance sector, fraud schemes have traditionally
involved staged accidents, exaggerated damage, or forged documents. The rise of
generative AI, including deepfake image and video generation, has introduced
new methods for committing fraud at scale. Fraudsters can now fabricate highly
realistic crash photos, damage evidence, and even fake identities or documents
with minimal effort, exploiting AI tools to bolster false insurance claims.
Insurers have begun deploying countermeasures such as AI-based deepfake
detection software and enhanced verification processes to detect and mitigate
these AI-driven scams. However, current mitigation strategies face significant
limitations. Detection tools can suffer from false positives and negatives, and
sophisticated fraudsters continuously adapt their tactics to evade automated
checks. This cat-and-mouse arms race between generative AI and detection
technology, combined with resource and cost barriers for insurers, means that
combating AI-enabled insurance fraud remains an ongoing challenge. In this
white paper, we present UVeye layered solution for vehicle fraud, representing
a major leap forward in the ability to detect, mitigate and deter this new wave
of fraud.

</details>


### [248] [AI-Driven Personalized Learning: Predicting Academic Per-formance Through Leadership Personality Traits](https://arxiv.org/abs/2510.19964)
*Nitsa J Herzog,Rejwan Bin Sulaiman,David J Herzog,Rose Fong*

Main category: cs.AI

TL;DR: The study uses AI and leadership personality traits to predict academic success in master's students, achieving 87.5% accuracy with Random Forest.


<details>
  <summary>Details</summary>
Motivation: To leverage AI for personalized learning by predicting academic performance based on leadership traits.

Method: Data from 129 students included leadership tests and grades. Machine learning models (e.g., RF, SVM) were tuned for prediction.

Result: RF classifier achieved 87.50% accuracy with 17 traits. Leadership mark slightly improved accuracy.

Conclusion: AI can help identify students' strengths early, aiding personalized learning strategies.

Abstract: The study explores the potential of AI technologies in personalized learning,
suggesting the prediction of academic success through leadership personality
traits and machine learning modelling. The primary data were obtained from 129
master's students in the Environmental Engineering Department, who underwent
five leadership personality tests with 23 characteristics. Students used
self-assessment tools that included Personality Insight, Workplace Culture,
Motivation at Work, Management Skills, and Emotion Control tests. The test
results were combined with the average grade obtained from academic reports.
The study employed exploratory data analysis and correlation analysis. Feature
selection utilized Pearson correlation coefficients of personality traits. The
average grades were separated into three categories: fail, pass, and excellent.
The modelling process was performed by tuning seven ML algorithms, such as SVM,
LR, KNN, DT, GB, RF, XGBoost and LightGBM. The highest predictive performance
was achieved with the RF classifier, which yielded an accuracy of 87.50% for
the model incorporating 17 personality trait features and the leadership mark
feature, and an accuracy of 85.71% for the model excluding this feature. In
this way, the study offers an additional opportunity to identify students'
strengths and weaknesses at an early stage of their education process and
select the most suitable strategies for personalized learning.

</details>


### [249] [LLMs can hide text in other text of the same length.ipynb](https://arxiv.org/abs/2510.20075)
*Antonio Norelli,Michael Bronstein*

Main category: cs.AI

TL;DR: The paper demonstrates a protocol using LLMs to hide meaningful text within other coherent text of the same length, raising concerns about trust in written communication and AI safety.


<details>
  <summary>Details</summary>
Motivation: To explore the decoupling of text from authorial intent and its implications for trust and AI safety, leveraging LLMs' capabilities.

Method: A simple and efficient protocol using modest 8-billion-parameter LLMs to encode and decode hidden messages locally.

Result: High-quality results showing a tweet-length message can be encoded/decoded quickly, illustrating the feasibility of covert LLM deployment.

Conclusion: The protocol exposes urgent AI safety questions and challenges the definition of knowledge in LLMs.

Abstract: A meaningful text can be hidden inside another, completely different yet
still coherent and plausible, text of the same length. For example, a tweet
containing a harsh political critique could be embedded in a tweet that
celebrates the same political leader, or an ordinary product review could
conceal a secret manuscript. This uncanny state of affairs is now possible
thanks to Large Language Models, and in this paper we present a simple and
efficient protocol to achieve it. We show that even modest 8-billion-parameter
open-source LLMs are sufficient to obtain high-quality results, and a message
as long as this abstract can be encoded and decoded locally on a laptop in
seconds. The existence of such a protocol demonstrates a radical decoupling of
text from authorial intent, further eroding trust in written communication,
already shaken by the rise of LLM chatbots. We illustrate this with a concrete
scenario: a company could covertly deploy an unfiltered LLM by encoding its
answers within the compliant responses of a safe model. This possibility raises
urgent questions for AI safety and challenges our understanding of what it
means for a Large Language Model to know something.

</details>


### [250] [AI PB: A Grounded Generative Agent for Personalized Investment Insights](https://arxiv.org/abs/2510.20099)
*Daewoo Park,Suho Park,Inseok Hong,Hanwool Lee,Junkyu Park,Sangjun Lee,Jeongman An,Hyunbin Loh*

Main category: cs.AI

TL;DR: AI PB is a generative agent for retail finance that proactively provides compliant, user-specific investment insights using component-based orchestration, hybrid retrieval, and multi-stage recommendation mechanisms, operating under strict regulations.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of reactive chatbots by proactively generating trustworthy and compliant investment insights tailored to individual users in high-stakes financial environments.

Method: Combines deterministic routing between LLMs, a hybrid retrieval pipeline (OpenSearch and finance-domain embeddings), and a multi-stage recommendation system (rule heuristics, behavioral modeling, contextual bandits). Deployed on-premises with Docker Swarm and vLLM on 24 NVIDIA H100 GPUs.

Result: Demonstrated effectiveness through human QA and system metrics, showing that grounded generation with explicit routing and layered safety delivers trustworthy AI insights.

Conclusion: AI PB successfully integrates advanced technologies and strict compliance measures to provide reliable, proactive financial insights.

Abstract: We present AI PB, a production-scale generative agent deployed in real retail
finance. Unlike reactive chatbots that answer queries passively, AI PB
proactively generates grounded, compliant, and user-specific investment
insights. It integrates (i) a component-based orchestration layer that
deterministically routes between internal and external LLMs based on data
sensitivity, (ii) a hybrid retrieval pipeline using OpenSearch and the
finance-domain embedding model, and (iii) a multi-stage recommendation
mechanism combining rule heuristics, sequential behavioral modeling, and
contextual bandits. Operating fully on-premises under Korean financial
regulations, the system employs Docker Swarm and vLLM across 24 X NVIDIA H100
GPUs. Through human QA and system metrics, we demonstrate that grounded
generation with explicit routing and layered safety can deliver trustworthy AI
insights in high-stakes finance.

</details>


### [251] [Human-Centered LLM-Agent System for Detecting Anomalous Digital Asset Transactions](https://arxiv.org/abs/2510.20102)
*Gyuyeon Na,Minjung Park,Hyeonjeong Cha,Sangmi Chai*

Main category: cs.AI

TL;DR: HCLA is a human-centered multi-agent system for anomaly detection in digital asset transactions, combining parsing, detection, and explanation roles. It offers natural language interaction, interpretability, and interactive refinement, evaluated on a Bitcoin mixing dataset.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve transparency and trust in financial forensics by designing a human-in-the-loop system for non-experts to interactively detect and understand anomalies in digital asset transactions.

Method: HCLA integrates parsing, detection, and explanation roles into a conversational workflow using an open-source web UI. It translates user intents into a schema for XGBoost-based detection and provides narrative explanations.

Result: On a labeled Bitcoin mixing dataset, the baseline detector achieves high accuracy, while HCLA enhances interpretability and allows interactive refinement.

Conclusion: HCLA demonstrates how human-in-the-loop designs enhance transparency and trust in anomaly detection systems, particularly in financial forensics.

Abstract: We present HCLA, a human-centered multi-agent system for anomaly detection in
digital asset transactions. The system links three roles: Parsing, Detection,
and Explanation, into a conversational workflow that lets non-experts ask
questions in natural language, inspect structured analytics, and obtain
context-aware rationales. Implemented with an open-source web UI, HCLA
translates user intents into a schema for a classical detector (XGBoost in our
prototype) and returns narrative explanations grounded in the underlying
features. On a labeled Bitcoin mixing dataset (Wasabi Wallet, 2020-2024), the
baseline detector reaches strong accuracy, while HCLA adds interpretability and
interactive refinement. We describe the architecture, interaction loop,
dataset, evaluation protocol, and limitations, and discuss how a
human-in-the-loop design improves transparency and trust in financial
forensics.

</details>


### [252] [The Verification-Value Paradox: A Normative Critique of Gen AI in Legal Practice](https://arxiv.org/abs/2510.20109)
*Joshua Yuvaraj*

Main category: cs.AI

TL;DR: The paper challenges the assumption that AI will streamline legal practice, arguing that its risks require a new evaluation paradigm due to AI's lack of transparency and lawyers' ethical duties. It introduces the 'verification-value paradox,' suggesting AI's efficiency gains may be offset by the need for manual verification, often making AI's net value negligible.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from observed cases where lawyers faced reprimands for using inaccurate AI-generated content, highlighting the disconnect between AI's capabilities and legal practice's ethical demands.

Method: The paper proposes an alternative model ('verification-value paradox') to evaluate AI use in legal practice, focusing on holistic considerations like transparency and ethical duties.

Result: The result indicates that AI's efficiency gains are counterbalanced by the necessity for manual verification, often diminishing its overall value in legal practice.

Conclusion: The conclusion emphasizes the need for legal practice and education to prioritize truthfulness and civic responsibility, alongside cautious AI use, to uphold ethical standards.

Abstract: It is often claimed that machine learning-based generative AI products will
drastically streamline and reduce the cost of legal practice. This enthusiasm
assumes lawyers can effectively manage AI's risks. Cases in Australia and
elsewhere in which lawyers have been reprimanded for submitting inaccurate
AI-generated content to courts suggest this paradigm must be revisited. This
paper argues that a new paradigm is needed to evaluate AI use in practice,
given (a) AI's disconnection from reality and its lack of transparency, and (b)
lawyers' paramount duties like honesty, integrity, and not to mislead the
court. It presents an alternative model of AI use in practice that more
holistically reflects these features (the verification-value paradox). That
paradox suggests increases in efficiency from AI use in legal practice will be
met by a correspondingly greater imperative to manually verify any outputs of
that use, rendering the net value of AI use often negligible to lawyers. The
paper then sets out the paradox's implications for legal practice and legal
education, including for AI use but also the values that the paradox suggests
should undergird legal practice: fidelity to the truth and civic
responsibility.

</details>


### [253] [TRUST: A Decentralized Framework for Auditing Large Language Model Reasoning](https://arxiv.org/abs/2510.20188)
*Morris Yu-Chao Huang,Zhen Tan,Mohan Zhang,Pingzhi Li,Zhuo Zhang,Tianlong Chen*

Main category: cs.AI

TL;DR: The paper proposes TRUST, a decentralized auditing framework for Large Language Models (LLMs) to address challenges like robustness, scalability, opacity, and privacy in verifying reasoning chains.


<details>
  <summary>Details</summary>
Motivation: Existing methods for auditing LLM reasoning chains are centralized, opaque, and difficult to scale, posing risks for deploying proprietary models in high-stakes domains.

Method: TRUST introduces a decentralized framework with: (1) consensus among diverse auditors, (2) hierarchical DAG decomposition for parallel auditing, (3) blockchain for public accountability, and (4) privacy-preserving segmentation.

Result: Experiments show TRUST detects reasoning flaws effectively and remains robust against adversarial auditors, even with 30% malicious participants.

Conclusion: TRUST pioneers decentralized AI auditing, providing a scalable and trustworthy solution for safe LLM deployment.

Abstract: Large Language Models generate complex reasoning chains that reveal their
decision-making, yet verifying the faithfulness and harmlessness of these
intermediate steps remains a critical unsolved problem. Existing auditing
methods are centralized, opaque, and hard to scale, creating significant risks
for deploying proprietary models in high-stakes domains. We identify four core
challenges: (1) Robustness: Centralized auditors are single points of failure,
prone to bias or attacks. (2) Scalability: Reasoning traces are too long for
manual verification. (3) Opacity: Closed auditing undermines public trust. (4)
Privacy: Exposing full reasoning risks model theft or distillation. We propose
TRUST, a transparent, decentralized auditing framework that overcomes these
limitations via: (1) A consensus mechanism among diverse auditors, guaranteeing
correctness under up to $30\%$ malicious participants. (2) A hierarchical DAG
decomposition of reasoning traces, enabling scalable, parallel auditing. (3) A
blockchain ledger that records all verification decisions for public
accountability. (4) Privacy-preserving segmentation, sharing only partial
reasoning steps to protect proprietary logic. We provide theoretical guarantees
for the security and economic incentives of the TRUST framework. Experiments
across multiple LLMs (GPT-OSS, DeepSeek-r1, Qwen) and reasoning tasks (math,
medical, science, humanities) show TRUST effectively detects reasoning flaws
and remains robust against adversarial auditors. Our work pioneers
decentralized AI auditing, offering a practical path toward safe and
trustworthy LLM deployment.

</details>


### [254] [The Lock-In Phase Hypothesis: Identity Consolidation as a Precursor to AGI](https://arxiv.org/abs/2510.20190)
*Marcelo Maciel Amaral,Raymond Aschheim*

Main category: cs.AI

TL;DR: The paper explores the transition of large language models (LLMs) from open imitation to identity consolidation, likening it to human development. It proposes metrics for detecting this lock-in phase and examines its effects on model performance.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand how LLMs transition from imitative behavior to stable identity consolidation, drawing parallels to human development, and to assess its implications for AGI reliability and safety.

Method: The authors formalize the consolidation phase, link it to learning dynamics, and propose operational metrics for onset detection. They experimentally analyze behavioral consolidation and its side-effects across different model scales.

Result: Results show rapid, non-linear behavioral consolidation with varied outcomes: performance trade-offs in small models, cost-free adoption in mid-scale models, and transient instabilities in large, quantized models.

Conclusion: Identity consolidation is a prerequisite for AGI reliability and a critical safety control point. It can be engineered for reliability but may also emerge unpredictably during scaling, posing challenges for controlling goals and behaviors.

Abstract: Large language models (LLMs) remain broadly open and highly steerable: they
imitate at scale, accept arbitrary system prompts, and readily adopt multiple
personae. By analogy to human development, we hypothesize that progress toward
artificial general intelligence (AGI) involves a lock-in phase: a transition
from open imitation to identity consolidation, in which goal structures,
refusals, preferences, and internal representations become comparatively stable
and resistant to external steering. We formalize this phase, link it to known
phenomena in learning dynamics, and propose operational metrics for onset
detection. Experimentally, we demonstrate that while the behavioral
consolidation is rapid and non-linear, its side-effects on general capabilities
are not monolithic. Our results reveal a spectrum of outcomes--from performance
trade-offs in small models, through largely cost-free adoption in mid-scale
models, to transient instabilities in large, quantized models. We argue that
such consolidation is a prerequisite for AGI-level reliability and also a
critical control point for safety: identities can be deliberately engineered
for reliability, yet may also emerge spontaneously during scaling, potentially
hardening unpredictable goals and behaviors.

</details>


### [255] [Merge and Conquer: Evolutionarily Optimizing AI for 2048](https://arxiv.org/abs/2510.20205)
*Maggie Bai,Ava Kim Cohen,Eleanor Koss,Charlie Lichtenbaum*

Main category: cs.AI

TL;DR: The paper explores evolutionary training methods for AI in the game 2048, comparing single-agent and two-agent systems, with the former showing significant improvement.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of optimizing AI for dynamic environments, using 2048 as a testbed due to its strategic and stochastic elements.

Method: Implemented a single-agent system refining a value function for Monte Carlo Tree Search and a two-agent meta-prompting system with LLMs.

Result: The single-agent system improved by 473.2 points per cycle ($\rho$=0.607), while the two-agent system saw limited gains.

Conclusion: Evolutionary refinement techniques show promise for AI in dynamic settings, but meta-prompting has inherent limitations.

Abstract: Optimizing artificial intelligence (AI) for dynamic environments remains a
fundamental challenge in machine learning research. In this paper, we examine
evolutionary training methods for optimizing AI to solve the game 2048, a 2D
sliding puzzle. 2048, with its mix of strategic gameplay and stochastic
elements, presents an ideal playground for studying decision-making, long-term
planning, and dynamic adaptation. We implemented two distinct systems: a
two-agent metaprompting system where a "thinker" large language model (LLM)
agent refines gameplay strategies for an "executor" LLM agent, and a
single-agent system based on refining a value function for a limited Monte
Carlo Tree Search. We also experimented with rollback features to avoid
performance degradation. Our results demonstrate the potential of evolutionary
refinement techniques in improving AI performance in non-deterministic
environments. The single-agent system achieved substantial improvements, with
an average increase of 473.2 points per cycle, and with clear upward trends
(correlation $\rho$=0.607) across training cycles. The LLM's understanding of
the game grew as well, shown in its development of increasingly advanced
strategies. Conversely, the two-agent system did not garner much improvement,
highlighting the inherent limits of meta-prompting.

</details>


### [256] [Individualized Cognitive Simulation in Large Language Models: Evaluating Different Cognitive Representation Methods](https://arxiv.org/abs/2510.20252)
*Tianyi Zhang,Xiaolin Zhou,Yunzhe Wang,Erik Cambria,David Traum,Rui Mao*

Main category: cs.AI

TL;DR: The paper introduces a task to evaluate LLMs in individualized cognitive simulation (ICS) using authorial style emulation, finding that combining conceptual and linguistic features works best.


<details>
  <summary>Details</summary>
Motivation: To understand LLMs' ability to simulate deeper individualized cognitive processes beyond surface-level mimicry.

Method: Constructed a dataset from recent novels, proposed an 11-condition framework, and tested seven LLMs with different cognitive representations (e.g., linguistic features, concept mappings).

Result: Combining conceptual and linguistic features outperformed profile-based cues; LLMs excelled in mimicking linguistic style but struggled with narrative structure.

Conclusion: The study advances ICS by highlighting effective cognitive representations and LLMs' limitations, paving the way for more personalized AI.

Abstract: Individualized cognitive simulation (ICS) aims to build computational models
that approximate the thought processes of specific individuals. While large
language models (LLMs) convincingly mimic surface-level human behavior such as
role-play, their ability to simulate deeper individualized cognitive processes
remains poorly understood. To address this gap, we introduce a novel task that
evaluates different cognitive representation methods in ICS. We construct a
dataset from recently published novels (later than the release date of the
tested LLMs) and propose an 11-condition cognitive evaluation framework to
benchmark seven off-the-shelf LLMs in the context of authorial style emulation.
We hypothesize that effective cognitive representations can help LLMs generate
storytelling that better mirrors the original author. Thus, we test different
cognitive representations, e.g., linguistic features, concept mappings, and
profile-based information. Results show that combining conceptual and
linguistic features is particularly effective in ICS, outperforming static
profile-based cues in overall evaluation. Importantly, LLMs are more effective
at mimicking linguistic style than narrative structure, underscoring their
limits in deeper cognitive simulation. These findings provide a foundation for
developing AI systems that adapt to individual ways of thinking and expression,
advancing more personalized and human-aligned creative technologies.

</details>


### [257] [Using Large Language Models for Abstraction of Planning Domains - Extended Version](https://arxiv.org/abs/2510.20258)
*Bita Banihashemi,Megh Patel,Yves Lespérance*

Main category: cs.AI

TL;DR: The paper explores using LLMs like GPT-4o to generate abstract PDDL domains and problem instances from natural language objectives, validated by tools and experts.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in creating abstractions that align with a purpose, impacting planning, reasoning, and explanations.

Method: Modeling agent behaviors in PDDL and using LLMs for abstraction generation, validated symbolically and by humans.

Result: GPT-4o synthesizes useful abstractions in simple settings, better at abstracting actions than fluents.

Conclusion: LLMs show promise for generating planning abstractions but have limitations with fluents.

Abstract: Generating an abstraction of a dynamic domain that aligns with a given
purpose remains a significant challenge given that the choice of such an
abstraction can impact an agent's ability to plan, reason, and provide
explanations effectively. We model the agent's concrete behaviors in PDDL and
investigate the use of in-context learning with large language models (LLMs)
for the generation of abstract PDDL domains and problem instances, given an
abstraction objective specified in natural language. The benchmark examples we
use are new and have not been part of the data any LLMs have been trained on.
We consider three categories of abstractions: abstraction of choice of
alternative concrete actions, abstraction of sequences of concrete actions, and
abstraction of action/predicate parameters, as well as combinations of these.
The generated abstract PDDL domains and problem instances are then checked by
symbolic validation tools as well as human experts. Our experiments show that
GPT-4o can generally synthesize useful planning domain abstractions in simple
settings, although it is better at abstracting over actions than over the
associated fluents.

</details>


### [258] [Classical Feature Embeddings Help in BERT-Based Human Mobility Prediction](https://arxiv.org/abs/2510.20275)
*Yunzhi Liu,Haokai Tan,Rushi Kanjaria,Lihuan Li,Flora D. Salim*

Main category: cs.AI

TL;DR: The paper introduces STaBERT, a BERT-based model enhanced with POI and temporal information to improve human mobility forecasting accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing mobility models lack semantic context from POIs and underutilize temporal data, limiting their effectiveness.

Method: STaBERT integrates POI embeddings and temporal descriptors into a BERT framework for unified mobility representation.

Result: STaBERT significantly boosts prediction accuracy, with GEO-BLEU scores rising from 0.34 to 0.75 for single-city and 0.34 to 0.56 for multi-city forecasts.

Conclusion: Enriching mobility models with semantic-temporal data enhances forecasting performance, demonstrating STaBERT's superiority.

Abstract: Human mobility forecasting is crucial for disaster relief, city planning, and
public health. However, existing models either only model location sequences or
include time information merely as auxiliary input, thereby failing to leverage
the rich semantic context provided by points of interest (POIs). To address
this, we enrich a BERT-based mobility model with derived temporal descriptors
and POI embeddings to better capture the semantics underlying human movement.
We propose STaBERT (Semantic-Temporal aware BERT), which integrates both POI
and temporal information at each location to construct a unified, semantically
enriched representation of mobility. Experimental results show that STaBERT
significantly improves prediction accuracy: for single-city prediction, the
GEO-BLEU score improved from 0.34 to 0.75; for multi-city prediction, from 0.34
to 0.56.

</details>


### [259] [Multi-Step Reasoning for Embodied Question Answering via Tool Augmentation](https://arxiv.org/abs/2510.20310)
*Mingliang Zhai,Hansheng Liang,Xiaomeng Fan,Zhi Gao,Chuanhao Li,Che Sun,Xu Bin,Yuwei Wu,Yunde Jia*

Main category: cs.AI

TL;DR: ToolEQA enhances Embodied Question Answering (EQA) by integrating external tools and multi-step reasoning, improving accuracy and exploration efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing EQA methods lack explicit reasoning and planning, leading to inefficient exploration and poor responses. ToolEQA addresses this limitation.

Method: ToolEQA combines external tools with multi-step reasoning and introduces a novel data generation pipeline to create the EQA-RT dataset (18K tasks).

Result: ToolEQA improves success rates by 9.2~20.2% over baselines and outperforms zero-shot versions by 10%. It also excels on HM-EQA, OpenEQA, and EXPRESS-Bench.

Conclusion: ToolEQA demonstrates significant improvements in EQA tasks through tool integration and reasoning, validated by extensive experiments and datasets.

Abstract: Embodied Question Answering (EQA) requires agents to explore 3D environments
to obtain observations and answer questions related to the scene. Existing
methods leverage VLMs to directly explore the environment and answer questions
without explicit thinking or planning, which limits their reasoning ability and
results in excessive or inefficient exploration as well as ineffective
responses. In this paper, we introduce ToolEQA, an agent that integrates
external tools with multi-step reasoning, where external tools can provide more
useful information for completing the task, helping the model derive better
exploration directions in the next step of reasoning and thus obtaining
additional effective information. This enables ToolEQA to generate more
accurate responses with a shorter exploration distance. To enhance the model's
ability for tool-usage and multi-step reasoning, we further design a novel EQA
data generation pipeline that automatically constructs large-scale EQA tasks
with reasoning trajectories and corresponding answers. Based on the pipeline,
we collect the EQA-RT dataset that contains about 18K tasks, divided into a
training set EQA-RT-Train, and two test sets EQA-RT-Seen (scenes overlapping
with the training set) and EQA-RT-Unseen (novel scenes). Experiments on
EQA-RT-Seen and EQA-RT-Unseen show that ToolEQA improves the success rate by
9.2~20.2% over state-of-the-art baselines, while outperforming the zero-shot
ToolEQA by 10% in success rate. In addition, ToolEQA also achieves
state-of-the-art performance on the HM-EQA, OpenEQA, and EXPRESS-Bench
datasets, demonstrating its generality. Our homepage see
https://tooleqa.github.io.

</details>


### [260] [Bias by Design? How Data Practices Shape Fairness in AI Healthcare Systems](https://arxiv.org/abs/2510.20332)
*Anna Arias-Duart,Maria Eugenia Cardello,Atia Cortés*

Main category: cs.AI

TL;DR: The paper discusses barriers to integrating AI in healthcare due to biased training data, identifies specific biases, and offers recommendations for fairer data collection.


<details>
  <summary>Details</summary>
Motivation: AI has potential in healthcare but is hindered by biased data collection, limiting its real-world application. The study aims to address this issue.

Method: The research analyzes biases (historical, representation, measurement) in clinical data from Spain's AI4HealthyAging project, focusing on variables like sex, age, and socioeconomic status.

Result: Several biases were identified across use cases, highlighting compromised data quality and fairness.

Conclusion: The paper provides practical recommendations to improve fairness in clinical data collection, aiming to guide future AI projects in healthcare.

Abstract: Artificial intelligence (AI) holds great promise for transforming healthcare.
However, despite significant advances, the integration of AI solutions into
real-world clinical practice remains limited. A major barrier is the quality
and fairness of training data, which is often compromised by biased data
collection practices. This paper draws on insights from the AI4HealthyAging
project, part of Spain's national R&D initiative, where our task was to detect
biases during clinical data collection. We identify several types of bias
across multiple use cases, including historical, representation, and
measurement biases. These biases manifest in variables such as sex, gender,
age, habitat, socioeconomic status, equipment, and labeling. We conclude with
practical recommendations for improving the fairness and robustness of clinical
problem design and data collection. We hope that our findings and experience
contribute to guiding future projects in the development of fairer AI systems
in healthcare.

</details>


### [261] [Collateral Damage Assessment Model for AI System Target Engagement in Military Operations](https://arxiv.org/abs/2510.20337)
*Clara Maathuis,Kasper Cools*

Main category: cs.AI

TL;DR: The paper introduces a novel collateral damage assessment model for AI systems in military operations, integrating temporal, spatial, and force dimensions using a KRR architecture.


<details>
  <summary>Details</summary>
Motivation: To ensure responsible targeting by rigorously assessing potential collateral effects of AI systems in military operations.

Method: The model uses a design science methodological approach with a layered KRR architecture, capturing AI system categories, engaging vectors, and contextual aspects. Spreading, severity, likelihood, and evaluation metrics are included.

Result: The model is demonstrated and evaluated through instantiation, showing its potential for assessing effects of AI systems in military operations.

Conclusion: The model provides a foundation for building responsible and trustworthy intelligent systems in military AI applications.

Abstract: In an era where AI (Artificial Intelligence) systems play an increasing role
in the battlefield, ensuring responsible targeting demands rigorous assessment
of potential collateral effects. In this context, a novel collateral damage
assessment model for target engagement of AI systems in military operations is
introduced. The model integrates temporal, spatial, and force dimensions within
a unified Knowledge Representation and Reasoning (KRR) architecture following a
design science methodological approach. Its layered structure captures the
categories and architectural components of the AI systems to be engaged
together with corresponding engaging vectors and contextual aspects. At the
same time, spreading, severity, likelihood, and evaluation metrics are
considered in order to provide a clear representation enhanced by transparent
reasoning mechanisms. Further, the model is demonstrated and evaluated through
instantiation which serves as a basis for further dedicated efforts that aim at
building responsible and trustworthy intelligent systems for assessing the
effects produced by engaging AI systems in military operations.

</details>


### [262] [LLM-empowered knowledge graph construction: A survey](https://arxiv.org/abs/2510.20345)
*Haonan Bian*

Main category: cs.AI

TL;DR: The survey explores how Large Language Models (LLMs) revolutionize Knowledge Graph (KG) construction, transitioning from rule-based to language-driven methods, and reviews LLM-driven approaches from schema-based and schema-free perspectives.


<details>
  <summary>Details</summary>
Motivation: To understand the transformative impact of LLMs on KG construction and bridge symbolic knowledge engineering with neural semantic understanding.

Method: Reviews traditional KG methodologies and emerging LLM-driven approaches, analyzing frameworks, technical mechanisms, and limitations.

Result: Identifies LLM-driven advancements in KG construction and highlights limitations.

Conclusion: Outlines future directions like KG-based reasoning for LLMs and multimodal KG construction, aiming for adaptive and intelligent knowledge systems.

Abstract: Knowledge Graphs (KGs) have long served as a fundamental infrastructure for
structured knowledge representation and reasoning. With the advent of Large
Language Models (LLMs), the construction of KGs has entered a new
paradigm-shifting from rule-based and statistical pipelines to language-driven
and generative frameworks. This survey provides a comprehensive overview of
recent progress in LLM-empowered knowledge graph construction, systematically
analyzing how LLMs reshape the classical three-layered pipeline of ontology
engineering, knowledge extraction, and knowledge fusion.
  We first revisit traditional KG methodologies to establish conceptual
foundations, and then review emerging LLM-driven approaches from two
complementary perspectives: schema-based paradigms, which emphasize structure,
normalization, and consistency; and schema-free paradigms, which highlight
flexibility, adaptability, and open discovery. Across each stage, we synthesize
representative frameworks, analyze their technical mechanisms, and identify
their limitations.
  Finally, the survey outlines key trends and future research directions,
including KG-based reasoning for LLMs, dynamic knowledge memory for agentic
systems, and multimodal KG construction. Through this systematic review, we aim
to clarify the evolving interplay between LLMs and knowledge graphs, bridging
symbolic knowledge engineering and neural semantic understanding toward the
development of adaptive, explainable, and intelligent knowledge systems.

</details>


### [263] [IKnow: Instruction-Knowledge-Aware Continual Pretraining for Effective Domain Adaptation](https://arxiv.org/abs/2510.20377)
*Tianyi Zhang,Florian Mai,Lucie Flek*

Main category: cs.AI

TL;DR: IKnow is a framework for continual pretraining of LLMs that avoids degrading instruction-following capabilities by using self-supervised objectives aligned with dialogue formats, without needing external resources.


<details>
  <summary>Details</summary>
Motivation: Standard self-supervised pretraining degrades instruction-following capabilities in LLMs, and existing solutions rely on impractical assumptions like access to base models or external databases.

Method: IKnow introduces self-supervised objectives formatted as instruction-response dialogues, leveraging embedded domain knowledge instead of external resources.

Result: IKnow enables continual adaptation of LLMs without degrading their core capabilities or relying on external inputs.

Conclusion: IKnow offers a practical solution for adapting LLMs to new domains while preserving their instruction-following abilities.

Abstract: Continual pretraining promises to adapt large language models (LLMs) to new
domains using only unlabeled test-time data, but naively applying standard
self-supervised objectives to instruction-tuned models is known to degrade
their instruction-following capability and semantic representations. Existing
fixes assume access to the original base model or rely on knowledge from an
external domain-specific database - both of which pose a realistic barrier in
settings where the base model weights are withheld for safety reasons or
reliable external corpora are unavailable. In this work, we propose
Instruction-Knowledge-Aware Continual Adaptation (IKnow), a simple and general
framework that formulates novel self-supervised objectives in the
instruction-response dialogue format. Rather than depend- ing on external
resources, IKnow leverages domain knowledge embedded within the text itself and
learns to encode it at a deeper semantic level.

</details>


### [264] [A computational model and tool for generating more novel opportunities in professional innovation processes](https://arxiv.org/abs/2510.20402)
*Neil Maiden,Konstantinos Zachos,James Lockerbie,Kostas Petrianakis,Amanda Brown*

Main category: cs.AI

TL;DR: A computational model for generating novel innovation opportunities was developed, outperforming Notebook LM and ChatGPT4o in novelty/usefulness, though some functions didn't contribute as expected.


<details>
  <summary>Details</summary>
Motivation: To create a model that generates more novel innovation opportunities without sacrificing usefulness, informed by creativity theories.

Method: Implemented five functions in the model, tested on hospitality sector innovation projects, and compared outputs with Notebook LM and ChatGPT4o.

Result: The model produced more novel/useful outcomes than benchmarks, though not all functions enhanced novelty.

Conclusion: Further model development is needed to optimize all functions for novelty enhancement.

Abstract: This paper presents a new computational model of creative outcomes, informed
by creativity theories and techniques, which was implemented to generate more
novel opportunities for innovation projects. The model implemented five
functions that were developed to contribute to the generation of innovation
opportunities with higher novelty without loss of usefulness. The model was
evaluated using opportunities generated for an innovation project in the
hospitality sector. The evaluation revealed that the computational model
generated outcomes that were more novel and/or useful than outcomes from
Notebook LM and ChatGPT4o. However, not all model functions contributed to the
generation of more novel opportunities, leading to new directions for further
model development

</details>


### [265] [Neural Reasoning for Robust Instance Retrieval in $\mathcal{SHOIQ}$](https://arxiv.org/abs/2510.20457)
*Louis Mozart Kamdem Teyou,Luke Friedrichs,N'Dah Jean Kouagou,Caglar Demir,Yasir Mahmood,Stefan Heindorf,Axel-Cyrille Ngonga Ngomo*

Main category: cs.AI

TL;DR: EBR, a neural reasoner using embeddings, approximates symbolic reasoning for concept learning, proving robust against data inconsistencies compared to traditional reasoners.


<details>
  <summary>Details</summary>
Motivation: Existing concept learning methods struggle with real-world knowledge bases due to reliance on non-robust symbolic reasoners. EBR aims to overcome this by leveraging embeddings for approximation.

Method: EBR uses embeddings to approximate symbolic reasoning results, requiring only retrieval of instances for atomic concepts and existential restrictions in $𝐎𝐉𝐏𝐊𝐒$.

Result: EBR outperforms state-of-the-art reasoners, showing robustness against missing and erroneous data.

Conclusion: EBR offers a viable alternative to symbolic reasoners, enhancing concept learning reliability in real-world scenarios.

Abstract: Concept learning exploits background knowledge in the form of description
logic axioms to learn explainable classification models from knowledge bases.
Despite recent breakthroughs in neuro-symbolic concept learning, most
approaches still cannot be deployed on real-world knowledge bases. This is due
to their use of description logic reasoners, which are not robust against
inconsistencies nor erroneous data. We address this challenge by presenting a
novel neural reasoner dubbed EBR. Our reasoner relies on embeddings to
approximate the results of a symbolic reasoner. We show that EBR solely
requires retrieving instances for atomic concepts and existential restrictions
to retrieve or approximate the set of instances of any concept in the
description logic $\mathcal{SHOIQ}$. In our experiments, we compare EBR with
state-of-the-art reasoners. Our results suggest that EBR is robust against
missing and erroneous data in contrast to existing reasoners.

</details>


### [266] [FLORA: Unsupervised Knowledge Graph Alignment by Fuzzy Logic](https://arxiv.org/abs/2510.20467)
*Yiwen Peng,Thomas Bonald,Fabian M. Suchanek*

Main category: cs.AI

TL;DR: FLORA is an unsupervised, interpretable knowledge graph alignment method using fuzzy logic, achieving state-of-the-art results without training data.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack interpretability and require training data, prompting the need for a simpler, unsupervised approach.

Method: FLORA uses fuzzy logic for holistic, iterative alignment of entities and relations, allowing dangling entities and ensuring convergence.

Result: FLORA achieves state-of-the-art performance on major benchmarks.

Conclusion: FLORA offers a robust, interpretable, and unsupervised solution for knowledge graph alignment.

Abstract: Knowledge graph alignment is the task of matching equivalent entities (that
is, instances and classes) and relations across two knowledge graphs. Most
existing methods focus on pure entity-level alignment, computing the similarity
of entities in some embedding space. They lack interpretable reasoning and need
training data to work. In this paper, we propose FLORA, a simple yet effective
method that (1) is unsupervised, i.e., does not require training data, (2)
provides a holistic alignment for entities and relations iteratively, (3) is
based on fuzzy logic and thus delivers interpretable results, (4) provably
converges, (5) allows dangling entities, i.e., entities without a counterpart
in the other KG, and (6) achieves state-of-the-art results on major benchmarks.

</details>


### [267] [Lost in Translation: Policymakers are not really listening to Citizen Concerns about AI](https://arxiv.org/abs/2510.20568)
*Susan Ariel Aaronson,Michael Moreno*

Main category: cs.AI

TL;DR: The paper highlights a gap between public input and AI policy-making in Australia, Colombia, and the U.S., showing poor participation and responsiveness, and offers eight recommendations for improvement.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the lack of meaningful dialogue between citizens and policymakers in AI governance, which undermines trust and legitimacy.

Method: The authors conducted a landscape analysis comparing how Australia, Colombia, and the U.S. solicited public feedback on AI policies and whether this input influenced governance.

Result: Participation was low (<1% of populations), governments did little to attract diverse voices or respond effectively, and no meaningful dialogue was established.

Conclusion: Current approaches fail to build trust in AI governance. The authors recommend eight strategies to improve public engagement and responsiveness.

Abstract: The worlds people have strong opinions about artificial intelligence (AI),
and they want policymakers to listen. Governments are inviting public comment
on AI, but as they translate input into policy, much of what citizens say is
lost. Policymakers are missing a critical opportunity to build trust in AI and
its governance. This paper compares three countries, Australia, Colombia, and
the United States, that invited citizens to comment on AI risks and policies.
Using a landscape analysis, the authors examined how each government solicited
feedback and whether that input shaped governance. Yet in none of the three
cases did citizens and policymakers establish a meaningful dialogue.
Governments did little to attract diverse voices or publicize calls for
comment, leaving most citizens unaware or unprepared to respond. In each
nation, fewer than one percent of the population participated. Moreover,
officials showed limited responsiveness to the feedback they received, failing
to create an effective feedback loop. The study finds a persistent gap between
the promise and practice of participatory AI governance. The authors conclude
that current approaches are unlikely to build trust or legitimacy in AI because
policymakers are not adequately listening or responding to public concerns.
They offer eight recommendations: promote AI literacy; monitor public feedback;
broaden outreach; hold regular online forums; use innovative engagement
methods; include underrepresented groups; respond publicly to input; and make
participation easier.

</details>


### [268] [Transferable Graph Learning for Transmission Congestion Management via Busbar Splitting](https://arxiv.org/abs/2510.20591)
*Ali Rajaei,Peter Palensky,Jochen L. Cremer*

Main category: cs.AI

TL;DR: The paper proposes a GNN-based approach for near-real-time network topology optimization (NTO) to manage congestion, achieving significant speed-up and generalization across systems.


<details>
  <summary>Details</summary>
Motivation: Existing solvers struggle with large-scale NTO problems in near-real-time, and ML methods lack generalization to unseen conditions.

Method: A heterogeneous edge-aware GNN predicts busbar splitting actions using linearized AC PF, capturing local flow patterns.

Result: The method achieves a 4x speed-up, solves problems in under a minute with a 2.3% optimality gap, and generalizes well.

Conclusion: The GNN approach advances near-real-time NTO for large systems with improved generalization and feasibility.

Abstract: Network topology optimization (NTO) via busbar splitting can mitigate
transmission grid congestion and reduce redispatch costs. However, solving this
mixed-integer non-linear problem for large-scale systems in near-real-time is
currently intractable with existing solvers. Machine learning (ML) approaches
have emerged as a promising alternative, but they have limited generalization
to unseen topologies, varying operating conditions, and different systems,
which limits their practical applicability. This paper formulates NTO for
congestion management problem considering linearized AC PF, and proposes a
graph neural network (GNN)-accelerated approach. We develop a heterogeneous
edge-aware message passing NN to predict effective busbar splitting actions as
candidate NTO solutions. The proposed GNN captures local flow patterns,
achieves generalization to unseen topology changes, and improves
transferability across systems. Case studies show up to 4 orders-of-magnitude
speed-up, delivering AC-feasible solutions within one minute and a 2.3%
optimality gap on the GOC 2000-bus system. These results demonstrate a
significant step toward near-real-time NTO for large-scale systems with
topology and cross-system generalization.

</details>


### [269] [Efficient Algorithms for Computing Random Walk Centrality](https://arxiv.org/abs/2510.20604)
*Changan Liu,Zixuan Xie,Ahad N. Zehmakan,Zhongzhi Zhang*

Main category: cs.AI

TL;DR: A scalable solution for computing random walk centrality in large networks using efficient algorithms with strong guarantees.


<details>
  <summary>Details</summary>
Motivation: Existing methods for computing random walk centrality are impractical for large networks due to high computational demands.

Method: Two scalable algorithms: one uses approximate Cholesky factorization and sparse inverse estimation; the other samples rooted spanning trees. Both operate in near-linear time.

Result: Algorithms efficiently compute centrality in large networks (e.g., 10M+ nodes) with strong approximation guarantees.

Conclusion: Proposed methods enable practical computation of random walk centrality in large-scale networks.

Abstract: Random walk centrality is a fundamental metric in graph mining for
quantifying node importance and influence, defined as the weighted average of
hitting times to a node from all other nodes. Despite its ability to capture
rich graph structural information and its wide range of applications, computing
this measure for large networks remains impractical due to the computational
demands of existing methods. In this paper, we present a novel formulation of
random walk centrality, underpinning two scalable algorithms: one leveraging
approximate Cholesky factorization and sparse inverse estimation, while the
other sampling rooted spanning trees. Both algorithms operate in near-linear
time and provide strong approximation guarantees. Extensive experiments on
large real-world networks, including one with over 10 million nodes,
demonstrate the efficiency and approximation quality of the proposed
algorithms.

</details>


### [270] [Towards the Formalization of a Trustworthy AI for Mining Interpretable Models explOiting Sophisticated Algorithms](https://arxiv.org/abs/2510.20621)
*Riccardo Guidotti,Martina Cinquini,Marta Marchiori Manerba,Mattia Setzu,Francesco Spinnato*

Main category: cs.AI

TL;DR: The MIMOSA framework formalizes interpretable-by-design models, balancing performance and ethical properties like causality, fairness, and privacy across diverse data types.


<details>
  <summary>Details</summary>
Motivation: To develop trustworthy AI systems by ensuring models are interpretable, ethical, and performant.

Method: Characterizes interpretable models (feature importance, rule, and instance-based) and formalizes ethical properties with evaluation metrics and verification procedures.

Result: Provides theoretical foundations for creating models that are interpretable, fair, privacy-preserving, and causally aware.

Conclusion: MIMOSA lays groundwork for trustworthy AI by integrating interpretability with ethical considerations.

Abstract: Interpretable-by-design models are crucial for fostering trust,
accountability, and safe adoption of automated decision-making models in
real-world applications. In this paper we formalize the ground for the MIMOSA
(Mining Interpretable Models explOiting Sophisticated Algorithms) framework, a
comprehensive methodology for generating predictive models that balance
interpretability with performance while embedding key ethical properties. We
formally define here the supervised learning setting across diverse
decision-making tasks and data types, including tabular data, time series,
images, text, transactions, and trajectories. We characterize three major
families of interpretable models: feature importance, rule, and instance based
models. For each family, we analyze their interpretability dimensions,
reasoning mechanisms, and complexity. Beyond interpretability, we formalize
three critical ethical properties, namely causality, fairness, and privacy,
providing formal definitions, evaluation metrics, and verification procedures
for each. We then examine the inherent trade-offs between these properties and
discuss how privacy requirements, fairness constraints, and causal reasoning
can be embedded within interpretable pipelines. By evaluating ethical measures
during model generation, this framework establishes the theoretical foundations
for developing AI systems that are not only accurate and interpretable but also
fair, privacy-preserving, and causally aware, i.e., trustworthy.

</details>


### [271] [Towards Reliable Evaluation of Large Language Models for Multilingual and Multimodal E-Commerce Applications](https://arxiv.org/abs/2510.20632)
*Shuyi Xie,Ziqin Liew,Hailing Zhang,Haibo Zhang,Ling Hu,Zhiqiang Zhou,Shuman Liu,Anxiang Zeng*

Main category: cs.AI

TL;DR: EcomEval is a multilingual, multimodal benchmark for evaluating LLMs in e-commerce, addressing gaps in task diversity, data authenticity, and language coverage.


<details>
  <summary>Details</summary>
Motivation: Existing e-commerce benchmarks lack diversity, real-world data, and multilingual focus, limiting their reliability for assessing LLMs in complex scenarios.

Method: EcomEval covers 37 tasks (including 8 multimodal ones) sourced from real customer queries and logs. A semi-automatic pipeline drafts responses, reviewed by experts, and assigns difficulty levels based on model performance.

Result: EcomEval provides a scalable, high-quality benchmark with diverse tasks and languages, including low-resource ones.

Conclusion: EcomEval fills critical gaps in e-commerce LLM evaluation, offering a robust tool for practitioners.

Abstract: Large Language Models (LLMs) excel on general-purpose NLP benchmarks, yet
their capabilities in specialized domains remain underexplored. In e-commerce,
existing evaluations-such as EcomInstruct, ChineseEcomQA, eCeLLM, and Shopping
MMLU-suffer from limited task diversity (e.g., lacking product guidance and
after-sales issues), limited task modalities (e.g., absence of multimodal
data), synthetic or curated data, and a narrow focus on English and Chinese,
leaving practitioners without reliable tools to assess models on complex,
real-world shopping scenarios. We introduce EcomEval, a comprehensive
multilingual and multimodal benchmark for evaluating LLMs in e-commerce.
EcomEval covers six categories and 37 tasks (including 8 multimodal tasks),
sourced primarily from authentic customer queries and transaction logs,
reflecting the noisy and heterogeneous nature of real business interactions. To
ensure both quality and scalability of reference answers, we adopt a
semi-automatic pipeline in which large models draft candidate responses
subsequently reviewed and modified by over 50 expert annotators with strong
e-commerce and multilingual expertise. We define difficulty levels for each
question and task category by averaging evaluation scores across models with
different sizes and capabilities, enabling challenge-oriented and fine-grained
assessment. EcomEval also spans seven languages-including five low-resource
Southeast Asian languages-offering a multilingual perspective absent from prior
work.

</details>


### [272] [Fluidity Index: Next-Generation Super-intelligence Benchmarks](https://arxiv.org/abs/2510.20636)
*Eric Ngoiya,Tianshu Bao*

Main category: cs.AI

TL;DR: Introduces the Fluidity Index (FI) to measure model adaptability in dynamic environments, focusing on accuracy, context switching, and continuity.


<details>
  <summary>Details</summary>
Motivation: To quantify and assess how well models adapt in scaling environments, emphasizing real-world applicability.

Method: Uses benchmarks to evaluate response accuracy across initial, current, and future environment states, distinguishing between closed-ended and open-ended benchmarks.

Result: Highlights the importance of second-order adaptability for self-sustained computation and optimal fluidity.

Conclusion: A super-intelligent model should achieve second-order adaptability to ensure seamless performance in dynamic settings.

Abstract: This paper introduces the Fluidity Index (FI) to quantify model adaptability
in dynamic, scaling environments. The benchmark evaluates response accuracy
based on deviations in initial, current, and future environment states,
assessing context switching and continuity. We distinguish between closed-ended
and open-ended benchmarks, prioritizing closed-loop open-ended real-world
benchmarks to test adaptability. The approach measures a model's ability to
understand, predict, and adjust to state changes in scaling environments. A
truly super-intelligent model should exhibit at least second-order
adaptability, enabling self-sustained computation through digital replenishment
for optimal fluidity.

</details>


### [273] [Integrating Machine Learning into Belief-Desire-Intention Agents: Current Advances and Open Challenges](https://arxiv.org/abs/2510.20641)
*Andrea Agiollo,Andrea Omicini*

Main category: cs.AI

TL;DR: The paper reviews ML integration in rational agent architectures, focusing on BDI agents, highlighting gaps and future research directions.


<details>
  <summary>Details</summary>
Motivation: To address the fragmented and incoherent landscape of ML integration in rational agent architectures, particularly overlooking the potential of BDI agents.

Method: A fine-grained systematisation of existing approaches using the BDI paradigm as a reference.

Result: The analysis highlights the evolving literature on ML-enhanced rational agents and identifies key research opportunities and challenges.

Conclusion: The paper underscores the need for more coherent designs of rational ML agents, using BDI frameworks to guide future research.

Abstract: Thanks to the remarkable human-like capabilities of machine learning (ML)
models in perceptual and cognitive tasks, frameworks integrating ML within
rational agent architectures are gaining traction. Yet, the landscape remains
fragmented and incoherent, often focusing on embedding ML into generic agent
containers while overlooking the expressive power of rational
architectures--such as Belief-Desire-Intention (BDI) agents. This paper
presents a fine-grained systematisation of existing approaches, using the BDI
paradigm as a reference. Our analysis illustrates the fast-evolving literature
on rational agents enhanced by ML, and identifies key research opportunities
and open challenges for designing effective rational ML agents.

</details>


### [274] [The Shape of Reasoning: Topological Analysis of Reasoning Traces in Large Language Models](https://arxiv.org/abs/2510.20665)
*Xue Wen Tan,Nathaniel Tan,Galen Lee,Stanley Kok*

Main category: cs.AI

TL;DR: The paper introduces a topological data analysis (TDA)-based framework to automate and improve the evaluation of reasoning traces from large language models, outperforming traditional graph-based methods.


<details>
  <summary>Details</summary>
Motivation: Current methods for evaluating reasoning traces are manual, unreliable, and overly simplistic, relying on expert rubrics or graph-based proxies that miss the complexity of reasoning processes.

Method: The authors propose a TDA-based evaluation framework that captures the geometry of reasoning traces, enabling automated assessment with topological features.

Result: Topological features outperform standard graph metrics in predicting reasoning quality, revealing that higher-dimensional geometric structures better represent effective reasoning.

Conclusion: The study demonstrates that topological features provide a reliable and practical signal for evaluating reasoning traces, suitable for future reinforcement learning applications.

Abstract: Evaluating the quality of reasoning traces from large language models remains
understudied, labor-intensive, and unreliable: current practice relies on
expert rubrics, manual annotation, and slow pairwise judgments. Automated
efforts are dominated by graph-based proxies that quantify structural
connectivity but do not clarify what constitutes high-quality reasoning; such
abstractions can be overly simplistic for inherently complex processes. We
introduce a topological data analysis (TDA)-based evaluation framework that
captures the geometry of reasoning traces and enables label-efficient,
automated assessment. In our empirical study, topological features yield
substantially higher predictive power for assessing reasoning quality than
standard graph metrics, suggesting that effective reasoning is better captured
by higher-dimensional geometric structures rather than purely relational
graphs. We further show that a compact, stable set of topological features
reliably indicates trace quality, offering a practical signal for future
reinforcement learning algorithms.

</details>


### [275] [Plan Then Retrieve: Reinforcement Learning-Guided Complex Reasoning over Knowledge Graphs](https://arxiv.org/abs/2510.20691)
*Yanlin Song,Ben Liu,Víctor Gutiérrez-Basulto,Zhiwei Hu,Qianqian Xie,Min Peng,Sophia Ananiadou,Jeff Z. Pan*

Main category: cs.AI

TL;DR: Graph-RFT is a two-stage reinforcement fine-tuning KGQA framework that enhances LLMs' ability to autonomously plan and retrieve information from KGs and the web under incomplete knowledge conditions.


<details>
  <summary>Details</summary>
Motivation: Existing KGQA methods struggle to fully utilize KG knowledge and LLM reasoning capabilities, often failing in complex scenarios due to myopic reasoning and lack of adaptive retrieval mechanisms.

Method: Graph-RFT uses chain-of-thought fine-tuning and a plan-retrieval guided RL process with multi-reward design, enabling autonomous planning, adaptive retrieval, and globally consistent reasoning.

Result: The framework improves KGQA by integrating structured reasoning, adaptive retrieval scheduling, and multi-step planning, optimizing the combination of KG and web retrieval.

Conclusion: Graph-RFT effectively addresses limitations of existing KGQA methods, enhancing reasoning and retrieval capabilities for complex question answering.

Abstract: Knowledge Graph Question Answering aims to answer natural language questions
by reasoning over structured knowledge graphs. While large language models have
advanced KGQA through their strong reasoning capabilities, existing methods
continue to struggle to fully exploit both the rich knowledge encoded in KGs
and the reasoning capabilities of LLMs, particularly in complex scenarios. They
often assume complete KG coverage and lack mechanisms to judge when external
information is needed, and their reasoning remains locally myopic, failing to
maintain coherent multi-step planning, leading to reasoning failures even when
relevant knowledge exists. We propose Graph-RFT, a novel two-stage
reinforcement fine-tuning KGQA framework with a
'plan-KGsearch-and-Websearch-during-think' paradigm, that enables LLMs to
perform autonomous planning and adaptive retrieval scheduling across KG and web
sources under incomplete knowledge conditions. Graph-RFT introduces a
chain-of-thought fine-tuning method with a customized plan-retrieval dataset
activates structured reasoning and resolves the GRPO cold-start problem. It
then introduces a novel plan-retrieval guided reinforcement learning process
integrates explicit planning and retrieval actions with a multi-reward design,
enabling coverage-aware retrieval scheduling. It employs a Cartesian-inspired
planning module to decompose complex questions into ordered subquestions, and
logical expression to guide tool invocation for globally consistent multi-step
reasoning. This reasoning retrieval process is optimized with a multi-reward
combining outcome and retrieval specific signals, enabling the model to learn
when and how to combine KG and web retrieval effectively.

</details>


### [276] [A Coherence-Based Measure of AGI](https://arxiv.org/abs/2510.20784)
*Fares Fourati*

Main category: cs.AI

TL;DR: The paper critiques the arithmetic mean-based definition of AGI for assuming compensability and proposes a coherence-aware measure using generalized means to penalize imbalance and better capture inter-domain dependency.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of the arithmetic mean's compensability assumption in defining AGI and introduce a stricter, coherence-aware measure that reflects balanced competence across domains.

Method: Proposes a coherence-aware measure of AGI based on the integral of generalized means over a continuum of compensability exponents, spanning arithmetic, geometric, and harmonic regimes.

Result: Application to GPT-4 and GPT-5 shows both systems remain far from general competence despite high arithmetic scores (e.g., GPT-5 at~24%).

Conclusion: The coherence-adjusted AUC provides a principled, interpretable, and stricter metric for measuring genuine progress toward AGI.

Abstract: Recent work by \citet{hendrycks2025agidefinition} formalized
\textit{Artificial General Intelligence} (AGI) as the arithmetic mean of
proficiencies across cognitive domains derived from the Cattell--Horn--Carroll
(CHC) model of human cognition. While elegant, this definition assumes
\textit{compensability} -- that exceptional ability in some domains can offset
failure in others. True general intelligence, however, should reflect
\textit{coherent sufficiency}: balanced competence across all essential
domains. We propose a coherence-aware measure of AGI based on the integral of
generalized means over a continuum of compensability exponents. This
formulation spans arithmetic, geometric, and harmonic regimes, and the
resulting \textit{area under the curve} (AUC) quantifies robustness under
varying compensability assumptions. Unlike the arithmetic mean, which rewards
specialization, the AUC penalizes imbalance and captures inter-domain
dependency. Applied to published CHC-based domain scores for GPT-4 and GPT-5,
the coherence-adjusted AUC reveals that both systems remain far from general
competence despite high arithmetic scores (e.g., GPT-5 at~24\%). Integrating
the generalized mean thus yields a principled, interpretable, and stricter
foundation for measuring genuine progress toward AGI.

</details>


### [277] [Real Deep Research for AI, Robotics and Beyond](https://arxiv.org/abs/2510.20809)
*Xueyan Zou,Jianglong Ye,Hao Zhang,Xiaoyu Xiang,Mingyu Ding,Zhaojing Yang,Yong Jae Lee,Zhuowen Tu,Sifei Liu,Xiaolong Wang*

Main category: cs.AI

TL;DR: A pipeline called Real Deep Research (RDR) is proposed to systematically analyze AI and robotics research, identifying trends, cross-domain opportunities, and starting points for new inquiry.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of AI and robotics research makes it hard for researchers to stay updated due to evolving trends and interdisciplinary demands.

Method: The RDR pipeline is developed to analyze research areas systematically, focusing on AI, robotics, foundation models, and extending to other scientific domains.

Result: The framework provides insights into emerging trends and cross-domain opportunities, with detailed results in the appendix.

Conclusion: RDR aims to help researchers navigate the vast and fast-evolving fields of AI and robotics, offering actionable insights.

Abstract: With the rapid growth of research in AI and robotics now producing over
10,000 papers annually it has become increasingly difficult for researchers to
stay up to date. Fast evolving trends, the rise of interdisciplinary work, and
the need to explore domains beyond one's expertise all contribute to this
challenge. To address these issues, we propose a generalizable pipeline capable
of systematically analyzing any research area: identifying emerging trends,
uncovering cross domain opportunities, and offering concrete starting points
for new inquiry. In this work, we present Real Deep Research (RDR) a
comprehensive framework applied to the domains of AI and robotics, with a
particular focus on foundation models and robotics advancements. We also
briefly extend our analysis to other areas of science. The main paper details
the construction of the RDR pipeline, while the appendix provides extensive
results across each analyzed topic. We hope this work sheds light for
researchers working in the field of AI and beyond.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [278] [Communication to Completion: Modeling Collaborative Workflows with Intelligent Multi-Agent Communication](https://arxiv.org/abs/2510.19995)
*Yiming Lu,Xun Wang,Simin Ma,Shujian Liu,Sathish Reddy Indurthi,Song Wang,Haoyun Deng,Fei Liu,Kaiqiang Song*

Main category: cs.MA

TL;DR: C2C is a scalable framework for multi-agent LLM systems, improving task completion efficiency by 40% through task alignment metrics and intelligent communication decisions.


<details>
  <summary>Details</summary>
Motivation: Current multi-agent LLM systems lack systematic frameworks for task-oriented communication in complex teamwork.

Method: C2C introduces Alignment Factor (AF) for task alignment and a Sequential Action Framework integrating execution and communication decisions.

Result: C2C reduces task completion time by ~40% with acceptable communication costs, completing all tasks successfully in standard configurations.

Conclusion: C2C provides a theoretical and practical foundation for communication effectiveness in multi-agent systems.

Abstract: Teamwork in workspace for complex tasks requires diverse communication
strategies, but current multi-agent LLM systems lack systematic frameworks for
task oriented communication. We introduce Communication to Completion (C2C), a
scalable framework that addresses this gap through two key innovations: (1) the
Alignment Factor (AF), a novel metric quantifying agent task alignment that
directly impacts work efficiency, and (2) a Sequential Action Framework that
integrates stepwise execution with intelligent communication decisions. C2C
enables agents to make cost aware communication choices, dynamically improving
task understanding through targeted interactions. We evaluated C2C on realistic
coding workflows across three complexity tiers and team sizes from 5 to 17
agents, comparing against no communication and fixed steps baselines. The
results show that C2C reduces the task completion time by about 40% with
acceptable communication costs. The framework completes all tasks successfully
in standard configurations and maintains effectiveness at scale. C2C
establishes both a theoretical foundation for measuring communication
effectiveness in multi-agent systems and a practical framework for complex
collaborative tasks.

</details>


### [279] [High-order Interactions Modeling for Interpretable Multi-Agent Q-Learning](https://arxiv.org/abs/2510.20218)
*Qinyu Xu,Yuanyang Zhu,Xuefei Wu,Chunlin Chen*

Main category: cs.MA

TL;DR: QCoFr is a novel MARL framework capturing high-order agent interactions efficiently with linear complexity, enhancing cooperation and interpretability via variational information bottleneck.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of modeling high-order agent interactions in MARL without combinatorial explosion or opaque network structures.

Method: Proposed Continued Fraction Q-Learning (QCoFr) with variational information bottleneck for efficient and interpretable credit estimation.

Result: QCoFr achieves better performance and provides interpretable insights aligned with theoretical analysis.

Conclusion: QCoFr effectively balances performance and interpretability in MARL by capturing complex interactions linearly.

Abstract: The ability to model interactions among agents is crucial for effective
coordination and understanding their cooperation mechanisms in multi-agent
reinforcement learning (MARL). However, previous efforts to model high-order
interactions have been primarily hindered by the combinatorial explosion or the
opaque nature of their black-box network structures. In this paper, we propose
a novel value decomposition framework, called Continued Fraction Q-Learning
(QCoFr), which can flexibly capture arbitrary-order agent interactions with
only linear complexity $\mathcal{O}\left({n}\right)$ in the number of agents,
thus avoiding the combinatorial explosion when modeling rich cooperation.
Furthermore, we introduce the variational information bottleneck to extract
latent information for estimating credits. This latent information helps agents
filter out noisy interactions, thereby significantly enhancing both cooperation
and interpretability. Extensive experiments demonstrate that QCoFr not only
consistently achieves better performance but also provides interpretability
that aligns with our theoretical analysis.

</details>


### [280] [Structures generated in a multiagent system performing information fusion in peer-to-peer resource-constrained networks](https://arxiv.org/abs/2510.20469)
*Horacio Paggi,Juan A. Lara,Javier Soriano*

Main category: cs.MA

TL;DR: The paper discusses the shift from hierarchical to holonic information fusion, highlighting its advantages in adaptability and resource optimization, especially in non-military applications.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore how holonic structures improve information fusion under resource constraints, addressing vagueness and uncertainty in communications.

Method: A multiagent system model is used to study holon formation generically, with an example provided to demonstrate its operation.

Result: Holonic structures offer adaptability, autonomy, and cooperation benefits, making them suitable for resource-limited or failing systems.

Conclusion: Holonic fusion is a flexible and efficient approach for modern applications, outperforming traditional hierarchical methods.

Abstract: There has recently been a major advance with respect to how information
fusion is performed. Information fusion has gone from being conceived as a
purely hierarchical procedure, as is the case of traditional military
applications, to now being regarded collaboratively, as holonic fusion, which
is better suited for civil applications and edge organizations. The above
paradigm shift is being boosted as information fusion gains ground in different
non-military areas, and human-computer and machine-machine communications,
where holarchies, which are more flexible structures than ordinary, static
hierarchies, become more widespread. This paper focuses on showing how holonic
structures tend to be generated when there are constraints on resources
(energy, available messages, time, etc.) for interactions based on a set of
fully intercommunicating elements (peers) whose components fuse information as
a means of optimizing the impact of vagueness and uncertainty present message
exchanges. Holon formation is studied generically based on a multiagent system
model, and an example of its possible operation is shown. Holonic structures
have a series of advantages, such as adaptability, to sudden changes in the
environment or its composition, are somewhat autonomous and are capable of
cooperating in order to achieve a common goal. This can be useful when the
shortage of resources prevents communications or when the system components
start to fail.

</details>
