<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 40]
- [cs.CL](#cs.CL) [Total: 47]
- [cs.AI](#cs.AI) [Total: 16]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.LG](#cs.LG) [Total: 69]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Cropland Mapping using Geospatial Embeddings](https://arxiv.org/abs/2511.02923)
*Ivan Zvonkov,Gabriel Tseng,Inbal Becker-Reshef,Hannah Kerner*

Main category: cs.CV

TL;DR: Geospatial embeddings simplify workflows and achieve high-accuracy cropland classification in Togo, aiding land use change assessments.


<details>
  <summary>Details</summary>
Motivation: Accurate land cover maps are crucial for understanding land use change's role in climate change.

Method: Used geospatial embeddings from Presto and AlphaEarth for cropland mapping in Togo.

Result: Embeddings simplified workflows and enabled high-accuracy cropland classification.

Conclusion: Geospatial embeddings support better land use change assessments and climate impact evaluations.

Abstract: Accurate and up-to-date land cover maps are essential for understanding land
use change, a key driver of climate change. Geospatial embeddings offer a more
efficient and accessible way to map landscape features, yet their use in
real-world mapping applications remains underexplored. In this work, we
evaluated the utility of geospatial embeddings for cropland mapping in Togo. We
produced cropland maps using embeddings from Presto and AlphaEarth. Our
findings show that geospatial embeddings can simplify workflows, achieve
high-accuracy cropland classification and ultimately support better assessments
of land use change and its climate impacts.

</details>


### [2] [Generative Hints](https://arxiv.org/abs/2511.02933)
*Andy Dimnaku,Abdullah Yusuf KavranoÄŸlu,Yaser Abu-Mostafa*

Main category: cs.CV

TL;DR: Generative hints improve invariance learning by leveraging unlabeled virtual examples from a generative model, outperforming standard data augmentation.


<details>
  <summary>Details</summary>
Motivation: Standard data augmentation lacks full invariance learning, as it only uses training data transformations.

Method: Proposes generative hints: a semi-supervised approach using virtual examples from a generative model to enforce invariance.

Result: Achieves up to 1.78% accuracy improvement on fine-grained benchmarks and 1.286% on CheXpert.

Conclusion: Generative hints enhance invariance learning and outperform data augmentation across datasets and architectures.

Abstract: Data augmentation is widely used in vision to introduce variation and
mitigate overfitting, through enabling models to learn invariant properties,
such as spatial invariance. However, these properties are not fully captured by
data augmentation alone, since it attempts to learn the property on
transformations of the training data only. We propose generative hints, a
training methodology that directly enforces known invariances in the entire
input space. Our approach leverages a generative model trained on the training
set to approximate the input distribution and generate unlabeled images, which
we refer to as virtual examples. These virtual examples are used to enforce
functional properties known as hints. In generative hints, although the
training dataset is fully labeled, the model is trained in a semi-supervised
manner on both the classification and hint objectives, using the unlabeled
virtual examples to guide the model in learning the desired hint. Across
datasets, architectures, and loss functions, generative hints consistently
outperform standard data augmentation when learning the same property. On
popular fine-grained visual classification benchmarks, we achieved up to 1.78%
top-1 accuracy improvement (0.63% on average) over fine-tuned models with data
augmentation and an average performance boost of 1.286% on the CheXpert X-ray
dataset.

</details>


### [3] [ProM3E: Probabilistic Masked MultiModal Embedding Model for Ecology](https://arxiv.org/abs/2511.02946)
*Srikumar Sastry,Subash Khanal,Aayush Dhakal,Jiayu Lin,Dan Cher,Phoenix Jarosz,Nathan Jacobs*

Main category: cs.CV

TL;DR: ProM3E is a probabilistic masked multimodal embedding model for any-to-any generation of multimodal representations in ecology, supporting modality inversion and cross-modal retrieval.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of inferring missing modalities in multimodal data for ecological applications, while also analyzing the feasibility of fusing different modalities for downstream tasks.

Method: ProM3E uses masked modality reconstruction in the embedding space to learn how to infer missing modalities from context modalities. It supports modality inversion and employs a probabilistic approach to determine fusion feasibility.

Result: The model achieves superior performance in cross-modal retrieval tasks by mixing inter-modal and intra-modal similarities. It also demonstrates strong representation learning capabilities in linear probing tasks.

Conclusion: ProM3E is an effective solution for multimodal representation generation and retrieval in ecology, with potential applications in various downstream tasks. All resources will be publicly released.

Abstract: We introduce ProM3E, a probabilistic masked multimodal embedding model for
any-to-any generation of multimodal representations for ecology. ProM3E is
based on masked modality reconstruction in the embedding space, learning to
infer missing modalities given a few context modalities. By design, our model
supports modality inversion in the embedding space. The probabilistic nature of
our model allows us to analyse the feasibility of fusing various modalities for
given downstream tasks, essentially learning what to fuse. Using these features
of our model, we propose a novel cross-modal retrieval approach that mixes
inter-modal and intra-modal similarities to achieve superior performance across
all retrieval tasks. We further leverage the hidden representation from our
model to perform linear probing tasks and demonstrate the superior
representation learning capability of our model. All our code, datasets and
model will be released at https://vishu26.github.io/prom3e.

</details>


### [4] [EvtSlowTV - A Large and Diverse Dataset for Event-Based Depth Estimation](https://arxiv.org/abs/2511.02953)
*Sadiq Layi Macaulay,Nimet Kaygusuz,Simon Hadfield*

Main category: cs.CV

TL;DR: EvtSlowTV is a large-scale event camera dataset from YouTube, enabling self-supervised depth learning without frame annotations, improving generalization.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of small annotated datasets for event-based depth estimation in challenging environments.

Method: Curate EvtSlowTV dataset from YouTube footage (13B events), use self-supervised learning to exploit HDR potential.

Result: EvtSlowTV enhances model generalization to complex scenes and motions, removing need for frame-based annotations.

Conclusion: EvtSlowTV provides a scalable, naturalistic solution for robust event-based depth estimation.

Abstract: Event cameras, with their high dynamic range (HDR) and low latency, offer a
promising alternative for robust depth estimation in challenging environments.
However, many event-based depth estimation approaches are constrained by
small-scale annotated datasets, limiting their generalizability to real-world
scenarios. To bridge this gap, we introduce EvtSlowTV, a large-scale event
camera dataset curated from publicly available YouTube footage, which contains
more than 13B events across various environmental conditions and motions,
including seasonal hiking, flying, scenic driving, and underwater exploration.
EvtSlowTV is an order of magnitude larger than existing event datasets,
providing an unconstrained, naturalistic setting for event-based depth
learning. This work shows the suitability of EvtSlowTV for a self-supervised
learning framework to capitalise on the HDR potential of raw event streams. We
further demonstrate that training with EvtSlowTV enhances the model's ability
to generalise to complex scenes and motions. Our approach removes the need for
frame-based annotations and preserves the asynchronous nature of event data.

</details>


### [5] [Hybrid Convolution and Vision Transformer NAS Search Space for TinyML Image Classification](https://arxiv.org/abs/2511.02992)
*Mikhael Djajapermana,Moritz Reiber,Daniel Mueller-Gritschneder,Ulf Schlichtmann*

Main category: cs.CV

TL;DR: A new hybrid CNN-ViT search space for NAS is introduced to create efficient architectures for tinyML, outperforming ResNet-based models in accuracy and speed under size constraints.


<details>
  <summary>Details</summary>
Motivation: Hybrid CNN-ViT models are powerful but too large for tinyML. This work aims to find efficient hybrid architectures suitable for resource-constrained deployment.

Method: The paper proposes a NAS search space combining CNN and ViT blocks for local and global feature learning, and introduces a searchable pooling layer for efficient feature reduction.

Result: On CIFAR10, the proposed method yields hybrid CNN-ViT models with better accuracy and inference speed than ResNet-based tinyML models under tight size limits.

Conclusion: The introduced search space effectively addresses the inefficiency of hybrid CNN-ViT models for tinyML, offering a practical solution for resource-constrained applications.

Abstract: Hybrids of Convolutional Neural Network (CNN) and Vision Transformer (ViT)
have outperformed pure CNN or ViT architecture. However, since these
architectures require large parameters and incur large computational costs,
they are unsuitable for tinyML deployment. This paper introduces a new hybrid
CNN-ViT search space for Neural Architecture Search (NAS) to find efficient
hybrid architectures for image classification. The search space covers hybrid
CNN and ViT blocks to learn local and global information, as well as the novel
Pooling block of searchable pooling layers for efficient feature map reduction.
Experimental results on the CIFAR10 dataset show that our proposed search space
can produce hybrid CNN-ViT architectures with superior accuracy and inference
speed to ResNet-based tinyML models under tight model size constraints.

</details>


### [6] [SCALE-VLP: Soft-Weighted Contrastive Volumetric Vision-Language Pre-training with Spatial-Knowledge Semantics](https://arxiv.org/abs/2511.02996)
*Ailar Mahdizadeh,Puria Azadi Moghadam,Xiangteng He,Shahriar Mirabbasi,Panos Nasiopoulos,Leonid Sigal*

Main category: cs.CV

TL;DR: SCALE-VLP is a soft-weighted contrastive vision-language pre-training framework integrating volumetric spatial and domain-aware semantics for cross-modal tasks in volumetric data like CT.


<details>
  <summary>Details</summary>
Motivation: Existing VLMs lack efficiency with volumetric data, treating them as 2D slices and ignoring continuous dependencies and clinical semantics.

Method: SCALE-VLP incorporates volumetric spatial semantics and domain-aware knowledge (e.g., radiological ontologies) for structured and semantic alignment.

Result: Achieves up to 4.3x higher CT-report retrieval, 10-point improvement in abnormality classification, and strong report generation metrics (ROUGE-L 0.44, BERT-F1 0.89).

Conclusion: SCALE-VLP demonstrates superior cross-task and cross-domain generalization without fine-tuning, outperforming existing methods.

Abstract: Vision-language models (VLMs) have demonstrated strong cross-modal
capabilities, yet most work remains limited to 2D data and assumes binary
supervision (i.e., positive vs. negative pairs), overlooking the continuous and
structured dependencies present in volumetric data such as CT. Existing
approaches often treat volumetric scans as independent 2D slices, compromising
spatial coherence and underutilizing rich clinical semantics. We propose
SCALE-VLP, a soft-weighted contrastive vision-language pre-training framework
that integrates (i) volumetric spatial semantics to preserve anatomical
structure and (ii) domain-aware, knowledge-infused semantics (e.g.,
radiological ontologies) to guide alignment. This yields structurally
consistent and semantically grounded representations under limited supervision,
demonstrating strong cross-task transferability (retrieval, report generation,
and classification), and cross-domain generalizability with consistent gains
without further fine-tuning. In particular, compared to the previous state of
the art, SCALE-VLP achieves up to 4.3x higher top-1 CT-report retrieval,
improves abnormality classification by 10 points, and reaches ROUGE-L 0.44 and
BERT-F1 0.89 for report generation. Further, in zero-shot evaluation on an
out-of-domain external dataset, we observe consistent gains, indicating the
cross-task and cross-domain generalization ability of SCALE-VLP.

</details>


### [7] [Learning with less: label-efficient land cover classification at very high spatial resolution using self-supervised deep learning](https://arxiv.org/abs/2511.03004)
*Dakota Hester,Vitor S. Martins,Lucas B. Ferreira,Thainara M. A. Lima*

Main category: cs.CV

TL;DR: A label-efficient self-supervised deep learning approach achieves 87.14% accuracy for statewide 1-m land cover classification using only 1,000 annotated patches.


<details>
  <summary>Details</summary>
Motivation: The challenge of collecting large volumes of training data limits the adoption of deep learning for meter-scale land cover mapping over large areas.

Method: Uses 'Bootstrap Your Own Latent' pre-training with unlabeled images, followed by fine-tuning multiple segmentation models (e.g., U-Net, DeepLabV3+) with small labeled datasets.

Result: Achieved 87.14% overall accuracy and 75.58% macro F1 score for 8-class land cover mapping over Mississippi, USA.

Conclusion: Self-supervised learning reduces the need for manual annotations, enabling scalable high-resolution land cover mapping.

Abstract: Deep learning semantic segmentation methods have shown promising performance
for very high 1-m resolution land cover classification, but the challenge of
collecting large volumes of representative training data creates a significant
barrier to widespread adoption of such models for meter-scale land cover
mapping over large areas. In this study, we present a novel label-efficient
approach for statewide 1-m land cover classification using only 1,000 annotated
reference image patches with self-supervised deep learning. We use the
"Bootstrap Your Own Latent" pre-training strategy with a large amount of
unlabeled color-infrared aerial images (377,921 256x256 1-m pixel patches) to
pre-train a ResNet-101 convolutional encoder. The learned encoder weights were
subsequently transferred into multiple deep semantic segmentation architectures
(FCN, U-Net, Attention U-Net, DeepLabV3+, UPerNet, PAN), which were then
fine-tuned using very small training dataset sizes with cross-validation (250,
500, 750 patches). Among the fine-tuned models, we obtained the 87.14% overall
accuracy and 75.58% macro F1 score using an ensemble of the best performing
U-Net models for comprehensive 1-m, 8-class land cover mapping, covering more
than 123 billion pixels over the state of Mississippi, USA. Detailed
qualitative and quantitative analysis revealed accurate mapping of open water
and forested areas, while highlighting challenges in accurate delineation
between cropland, herbaceous, and barren land cover types. These results show
that self-supervised learning is an effective strategy for reducing the need
for large volumes of manually annotated data, directly addressing a major
limitation to high spatial resolution land cover mapping at scale.

</details>


### [8] [A Foundation Model for Brain MRI with Dynamic Modality Integration](https://arxiv.org/abs/2511.03014)
*Minh Sao Khue Luu,Bair N. Tuchinov*

Main category: cs.CV

TL;DR: A foundation model for brain MRI uses a single encoder with adaptable features to handle missing or unseen imaging sequences, trained on 60,000 MRIs for flexible representation learning.


<details>
  <summary>Details</summary>
Motivation: The motivation is to create a unified model that eliminates the need for separate models per imaging sequence and adapts to missing or unseen modalities.

Method: The method employs learnable modality embeddings, conditional layer normalization, masked autoencoding, and a variance-covariance regularizer for stable, diverse feature learning.

Result: Preliminary results indicate feasibility, with planned evaluations for brain tumor and multiple sclerosis segmentation and lesion classification.

Conclusion: The model offers a versatile solution for handling diverse MRI sequences, with open-source code and pretrained models available.

Abstract: We present a foundation model for brain MRI that can work with different
combinations of imaging sequences. The model uses one encoder with learnable
modality embeddings, conditional layer normalization, and a masked autoencoding
objective that accounts for missing modalities. A variance-covariance
regularizer is applied to stabilize feature learning and improve representation
diversity. This design removes the need for separate models for each modality
and allows the network to adapt when some sequences are missing or unseen. It
is trained on about 60,000 multi-center MRIs using self-supervised
reconstruction and modality imputation to learn flexible representations. A
learnable modality embedding guides feature extraction so the encoder can
adjust to different inputs. We describe our planned evaluation on brain tumor
and multiple sclerosis segmentation, as well as lesion classification, under
various modality settings. Preliminary results show that the method works
feasibly, and further experiments are planned to study its performance in more
detail. All code and pretrained models are available at
https://github.com/BrainFM/brainfm

</details>


### [9] [SLIP: Structural-aware Language-Image Pretraining for Vision-Language Alignment](https://arxiv.org/abs/2511.03019)
*Wenbo Lu*

Main category: cs.CV

TL;DR: SLIP improves VLP by incorporating relational data, outperforming CLIP in cross-modal tasks.


<details>
  <summary>Details</summary>
Motivation: Existing VLP methods treat image-text pairs in isolation, ignoring the rich relational structure present in domains like e-commerce and social networks. Human cognitive maps encode knowledge relationally, inspiring SLIP.

Method: SLIP introduces a structural contrastive loss to align modalities while modeling relationships between entities in a structured graph. It uses a large-scale Amazon Product Co-purchase Multimodal Graph Dataset for cross-modality supervision.

Result: SLIP consistently outperforms CLIP in cross-modal retrieval and classification tasks, demonstrating the benefits of relational supervision for alignment.

Conclusion: Incorporating relational structure in VLP enhances cross-modal alignment, as shown by SLIP's superior performance over CLIP in zero-shot and few-shot settings.

Abstract: Vision-Language Pretraining (VLP) has achieved remarkable success across
various downstream tasks, but such gains are largely driven by scaling up on
training data. Yet, literature methods treat image-text pairs as isolated
training examples; this neglects the rich relational structure naturally
present in many domains, such as e-commerce product co-purchase graphs and
social recommendation networks. Inspired by neuroscientific evidence that human
encodes knowledge as relationship cognitive maps, we introduce Structure-aware
Language-Image Pretraining (SLIP). SLIP integrates a structural contrastive
loss to align modalities while also modeling relationships between neighboring
entities in a structured graph. To support this paradigm, we construct a
large-scale Amazon Product Co-purchase Multimodal Graph Dataset, enabling
structured cross-modality supervision at scale. Experiment results show that
SLIP consistently outperforms CLIP on cross-modal retrieval and classification
tasks in both zero-shot and few-shot settings, showing the value of relational
supervision for cross-modal alignment.

</details>


### [10] [From Propagation to Prediction: Point-level Uncertainty Evaluation of MLS Point Clouds under Limited Ground Truth](https://arxiv.org/abs/2511.03053)
*Ziyang Xu,Olaf Wysocki,Christoph Holst*

Main category: cs.CV

TL;DR: The study introduces a learning-based framework to evaluate Mobile Laser Scanning (MLS) point cloud uncertainty without ground truth, using geometric features and optimal neighborhood estimation. XGBoost outperforms Random Forest in efficiency, showing uncertainty is learnable.


<details>
  <summary>Details</summary>
Motivation: Ground truth for MLS point cloud uncertainty evaluation is often costly or impractical, necessitating a GT-independent approach.

Method: The framework integrates optimal neighborhood estimation and geometric feature extraction, tested with XGBoost and Random Forest on real-world data.

Result: XGBoost is 3 times faster than Random Forest with comparable accuracy, proving geometric features can predict point-level uncertainty via C2C distance.

Conclusion: MLS point cloud uncertainty can be learned, offering a new perspective in uncertainty evaluation research.

Abstract: Evaluating uncertainty is critical for reliable use of Mobile Laser Scanning
(MLS) point clouds in many high-precision applications such as Scan-to-BIM,
deformation analysis, and 3D modeling. However, obtaining the ground truth (GT)
for evaluation is often costly and infeasible in many real-world applications.
To reduce this long-standing reliance on GT in uncertainty evaluation research,
this study presents a learning-based framework for MLS point clouds that
integrates optimal neighborhood estimation with geometric feature extraction.
Experiments on a real-world dataset show that the proposed framework is
feasible and the XGBoost model delivers fully comparable accuracy to Random
Forest while achieving substantially higher efficiency (about 3 times faster),
providing initial evidence that geometric features can be used to predict
point-level uncertainty quantified by the C2C distance. In summary, this study
shows that MLS point clouds' uncertainty is learnable, offering a novel
learning-based viewpoint towards uncertainty evaluation research.

</details>


### [11] [ISC-Perception: A Hybrid Computer Vision Dataset for Object Detection in Novel Steel Assembly](https://arxiv.org/abs/2511.03098)
*Miftahur Rahman,Samuel Adebayo,Dorian A. Acevedo-Mejia,David Hester,Daniel McPolin,Karen Rafferty,Debra F. Laefer*

Main category: cs.CV

TL;DR: ISC-Perception is a hybrid dataset combining synthetic and real images for ISC component detection, reducing manual labeling effort by 81.7% and improving detection performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of dedicated image data for ISC-aware robots due to logistical and safety challenges on construction sites.

Method: Developed ISC-Perception, blending CAD-rendered images, photorealistic game-engine scenes, and curated real photos, enabling automatic labeling.

Result: Detectors trained on ISC-Perception achieved mAP@0.50 of 0.756, outperforming synthetic-only or photorealistic-only datasets.

Conclusion: ISC-Perception bridges the data gap for construction robotics, enabling rapid detector development and is available for research and industrial use.

Abstract: The Intermeshed Steel Connection (ISC) system, when paired with robotic
manipulators, can accelerate steel-frame assembly and improve worker safety by
eliminating manual assembly. Dependable perception is one of the initial stages
for ISC-aware robots. However, this is hampered by the absence of a dedicated
image corpus, as collecting photographs on active construction sites is
logistically difficult and raises safety and privacy concerns. In response, we
introduce ISC-Perception, the first hybrid dataset expressly designed for ISC
component detection. It blends procedurally rendered CAD images, game-engine
photorealistic scenes, and a limited, curated set of real photographs, enabling
fully automatic labelling of the synthetic portion. We explicitly account for
all human effort to produce the dataset, including simulation engine and scene
setup, asset preparation, post-processing scripts and quality checks; our total
human time to generate a 10,000-image dataset was 30.5,h versus 166.7,h for
manual labelling at 60,s per image (-81.7%). A manual pilot on a representative
image with five instances of ISC members took 60,s (maximum 80,s), anchoring
the manual baseline. Detectors trained on ISC-Perception achieved a mean
Average Precision at IoU 0.50 of 0.756, substantially surpassing models trained
on synthetic-only or photorealistic-only data. On a 1,200-frame bench test, we
report mAP@0.50/mAP@[0.50:0.95] of 0.943/0.823. By bridging the data gap for
construction-robotics perception, ISC-Perception facilitates rapid development
of custom object detectors and is freely available for research and industrial
use upon request.

</details>


### [12] [DentalSplat: Dental Occlusion Novel View Synthesis from Sparse Intra-Oral Photographs](https://arxiv.org/abs/2511.03099)
*Yiyi Miao,Taoyu Wu,Tong Chen,Sihao Li,Ji Jiang,Youpeng Yang,Angelos Stefanidis,Limin Yu,Jionglong Su*

Main category: cs.CV

TL;DR: A framework called DentalSplat is proposed for 3D reconstruction from sparse orthodontic images, addressing limitations of conventional 3D Gaussian Splatting pipelines by using prior-guided dense stereo reconstruction, scale-adaptive pruning, and geometric constraints.


<details>
  <summary>Details</summary>
Motivation: Orthodontic treatments often rely on sparse images (anterior and bilateral buccal views), making 3D reconstruction challenging due to degraded quality and lack of camera pose information. DentalSplat aims to overcome these limitations.

Method: The method uses a prior-guided dense stereo reconstruction model for point cloud initialization, scale-adaptive pruning for efficiency and quality, and optical flow with gradient regularization for enhanced fidelity in sparse scenarios.

Result: Validated on 950 clinical cases and a video-based test set of 195 cases, DentalSplat outperforms state-of-the-art techniques in novel view synthesis for dental occlusion visualization.

Conclusion: DentalSplat effectively handles sparse input scenarios and improves 3D reconstruction quality for orthodontic applications, demonstrating superior performance over existing methods.

Abstract: In orthodontic treatment, particularly within telemedicine contexts,
observing patients' dental occlusion from multiple viewpoints facilitates
timely clinical decision-making. Recent advances in 3D Gaussian Splatting
(3DGS) have shown strong potential in 3D reconstruction and novel view
synthesis. However, conventional 3DGS pipelines typically rely on densely
captured multi-view inputs and precisely initialized camera poses, limiting
their practicality. Orthodontic cases, in contrast, often comprise only three
sparse images, specifically, the anterior view and bilateral buccal views,
rendering the reconstruction task especially challenging. The extreme sparsity
of input views severely degrades reconstruction quality, while the absence of
camera pose information further complicates the process. To overcome these
limitations, we propose DentalSplat, an effective framework for 3D
reconstruction from sparse orthodontic imagery. Our method leverages a
prior-guided dense stereo reconstruction model to initialize the point cloud,
followed by a scale-adaptive pruning strategy to improve the training
efficiency and reconstruction quality of 3DGS. In scenarios with extremely
sparse viewpoints, we further incorporate optical flow as a geometric
constraint, coupled with gradient regularization, to enhance rendering
fidelity. We validate our approach on a large-scale dataset comprising 950
clinical cases and an additional video-based test set of 195 cases designed to
simulate real-world remote orthodontic imaging conditions. Experimental results
demonstrate that our method effectively handles sparse input scenarios and
achieves superior novel view synthesis quality for dental occlusion
visualization, outperforming state-of-the-art techniques.

</details>


### [13] [Image-Intrinsic Priors for Integrated Circuit Defect Detection and Novel Class Discovery via Self-Supervised Learning](https://arxiv.org/abs/2511.03120)
*Botong. Zhao,Xubin. Wang,Shujing. Lyu,Yue. Lu*

Main category: cs.CV

TL;DR: Proposes IC DefectNCD, a framework for defect detection and novel class discovery in IC SEM images without human annotation, using self-normal and self-defect information.


<details>
  <summary>Details</summary>
Motivation: Supervised methods require extensive human annotation and struggle with emergent defects, while clustering-based methods lack stability due to missing priors.

Method: Uses self-normal information for defect detection with adaptive binarization, and self-defect information with a teacher-student model for classification.

Result: Validated on a real-world dataset, the method shows robust performance in defect detection and classification of unseen defects.

Conclusion: The framework effectively addresses challenges in defect detection and novel defect classification without reliance on human annotations.

Abstract: Integrated circuit manufacturing is highly complex, comprising hundreds of
process steps. Defects can arise at any stage, causing yield loss and
ultimately degrading product reliability. Supervised methods require extensive
human annotation and struggle with emergent categories and rare, data scarce
defects. Clustering-based unsupervised methods often exhibit unstable
performance due to missing priors. We propose IC DefectNCD, a support set free
framework that leverages Image Intrinsic Priors in IC SEM images for defect
detection and novel class discovery. We first develop Self Normal Information
Guided IC Defect Detection, aggregating representative normal features via a
learnable normal information extractor and using reconstruction residuals to
coarsely localize defect regions. To handle saliency variations across defects,
we introduce an adaptive binarization strategy that produces stable subimages
focused on core defective areas. Finally, we design Self Defect Information
Guided IC Defect Classification, which incorporates a soft mask guided
attention mechanism to inject spatial defect priors into the teacher student
model. This enhances sensitivity to defective regions, suppresses background
interference, and enables recognition and classification of unseen defects. We
validate the approach on a real world dataset spanning three key fabrication
stages and covering 15 defect types. Experiments demonstrate robust performance
on both defect detection and unseen defect classification.

</details>


### [14] [Accelerating Physical Property Reasoning for Augmented Visual Cognition](https://arxiv.org/abs/2511.03126)
*Hongbo Lan,Zhenlin An,Haoyu Li,Vaibhav Singh,Longfei Shangguan*

Main category: cs.CV

TL;DR: \sysname accelerates vision-guided physical property reasoning with algorithmic and systematic optimizations, reducing latency from minutes to seconds while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: To enhance augmented visual cognition by minimizing latency in vision-guided physical property reasoning.

Method: Combines rapid geometric 3D reconstruction, semantic feature fusion, and parallel view encoding.

Result: Achieves a 62.9x--287.2x speedup with comparable or better accuracy than SOTA baselines. Works well in real-world scenarios like IKEA stores.

Conclusion: \sysname efficiently streamlines physical property reasoning, proving robust in real-world applications.

Abstract: This paper introduces \sysname, a system that accelerates vision-guided
physical property reasoning to enable augmented visual cognition. \sysname
minimizes the run-time latency of this reasoning pipeline through a combination
of both algorithmic and systematic optimizations, including rapid geometric 3D
reconstruction, efficient semantic feature fusion, and parallel view encoding.
Through these simple yet effective optimizations, \sysname reduces the
end-to-end latency of this reasoning pipeline from 10--20 minutes to less than
6 seconds. A head-to-head comparison on the ABO dataset shows that \sysname
achieves this 62.9$\times$--287.2$\times$ speedup while not only reaching
on-par (and sometimes slightly better) object-level physical property
estimation accuracy(e.g. mass), but also demonstrating superior performance in
material segmentation and voxel-level inference than two SOTA baselines. We
further combine gaze-tracking with \sysname to localize the object of interest
in cluttered, real-world environments, streamlining the physical property
reasoning on smart glasses. The case study with Meta Aria Glasses conducted at
an IKEA furniture store demonstrates that \sysname achives consistently high
performance compared to controlled captures, providing robust property
estimations even with fewer views in real-world scenarios.

</details>


### [15] [Deploying Rapid Damage Assessments from sUAS Imagery for Disaster Response](https://arxiv.org/abs/2511.03132)
*Thomas Manzini,Priyankari Perali,Robin R. Murphy*

Main category: cs.CV

TL;DR: This paper presents an AI/ML system for automating building damage assessment using sUAS imagery, deployed during Hurricanes Debby and Helene to address the challenge of processing large volumes of disaster imagery quickly.


<details>
  <summary>Details</summary>
Motivation: The motivation was to address the overwhelming volume of sUAS imagery collected during disasters, which exceeds human capacity to analyze, delaying response efforts.

Method: The method involved training models on a large dataset of post-disaster sUAS imagery (21,716 labels) and deploying the best-performing model during actual disasters.

Result: The deployed model assessed 415 buildings in about 18 minutes during Hurricanes Debby and Helene, demonstrating operational effectiveness.

Conclusion: This work establishes a state of practice for sUAS-based damage assessment and provides practical insights for AI/ML deployment in disaster response.

Abstract: This paper presents the first AI/ML system for automating building damage
assessment in uncrewed aerial systems (sUAS) imagery to be deployed
operationally during federally declared disasters (Hurricanes Debby and
Helene). In response to major disasters, sUAS teams are dispatched to collect
imagery of the affected areas to assess damage; however, at recent disasters,
teams collectively delivered between 47GB and 369GB of imagery per day,
representing more imagery than can reasonably be transmitted or interpreted by
subject matter experts in the disaster scene, thus delaying response efforts.
To alleviate this data avalanche encountered in practice, computer vision and
machine learning techniques are necessary. While prior work has been deployed
to automatically assess damage in satellite imagery, there is no current state
of practice for sUAS-based damage assessment systems, as all known work has
been confined to academic settings. This work establishes the state of practice
via the development and deployment of models for building damage assessment
with sUAS imagery. The model development involved training on the largest known
dataset of post-disaster sUAS aerial imagery, containing 21,716 building damage
labels, and the operational training of 91 disaster practitioners. The best
performing model was deployed during the responses to Hurricanes Debby and
Helene, where it assessed a combined 415 buildings in approximately 18 minutes.
This work contributes documentation of the actual use of AI/ML for damage
assessment during a disaster and lessons learned to the benefit of the AI/ML
research and user communities.

</details>


### [16] [Finetuning-Free Personalization of Text to Image Generation via Hypernetworks](https://arxiv.org/abs/2511.03156)
*Sagar Shrestha,Gopal Sharma,Luowei Zhou,Suren Kumar*

Main category: cs.CV

TL;DR: A fine-tuning-free method for personalizing text-to-image diffusion models using hypernetworks to predict LoRA-adapted weights, avoiding computational overhead and improving stability.


<details>
  <summary>Details</summary>
Motivation: Traditional subject-specific fine-tuning methods like DreamBooth are computationally expensive and slow. Recent adapter- and encoder-based methods still require fine-tuning or large models.

Method: Proposes hypernetworks to predict LoRA-adapted weights directly from subject images, with an end-to-end training objective stabilized by output regularization. Introduces Hybrid-Model Classifier-Free Guidance (HM-CFG) for better compositional generalization.

Result: Achieves strong personalization performance on CelebA-HQ, AFHQ-v2, and DreamBench without per-subject optimization at test time, preserving subject fidelity and prompt alignment.

Conclusion: Hypernetworks provide a scalable and effective solution for open-category personalization in text-to-image diffusion models.

Abstract: Personalizing text-to-image diffusion models has traditionally relied on
subject-specific fine-tuning approaches such as
DreamBooth~\cite{ruiz2023dreambooth}, which are computationally expensive and
slow at inference. Recent adapter- and encoder-based methods attempt to reduce
this overhead but still depend on additional fine-tuning or large backbone
models for satisfactory results. In this work, we revisit an orthogonal
direction: fine-tuning-free personalization via Hypernetworks that predict
LoRA-adapted weights directly from subject images. Prior hypernetwork-based
approaches, however, suffer from costly data generation or unstable attempts to
mimic base model optimization trajectories. We address these limitations with
an end-to-end training objective, stabilized by a simple output regularization,
yielding reliable and effective hypernetworks. Our method removes the need for
per-subject optimization at test time while preserving both subject fidelity
and prompt alignment. To further enhance compositional generalization at
inference time, we introduce Hybrid-Model Classifier-Free Guidance (HM-CFG),
which combines the compositional strengths of the base diffusion model with the
subject fidelity of personalized models during sampling. Extensive experiments
on CelebA-HQ, AFHQ-v2, and DreamBench demonstrate that our approach achieves
strong personalization performance and highlights the promise of hypernetworks
as a scalable and effective direction for open-category personalization.

</details>


### [17] [Subsampled Randomized Fourier GaLore for Adapting Foundation Models in Depth-Driven Liver Landmark Segmentation](https://arxiv.org/abs/2511.03163)
*Yun-Chen Lin,Jiayuan Huang,Hanyuan Zhang,Sergi Kavtaradze,Matthew J. Clarkson,Mobarak I. Hoque*

Main category: cs.CV

TL;DR: The paper proposes a depth-guided liver landmark segmentation framework using SAM2 and DA2 encoders, enhanced by SRFT-GaLore for efficient fine-tuning, achieving superior accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Improving anatomical structure detection in laparoscopic liver surgery, where 2D video limits depth perception and landmark localization.

Method: Uses dual encoders (SAM2 for RGB, DA2 for depth) with SRFT-GaLore for efficient fine-tuning and cross-attention fusion for RGB-depth integration.

Result: Achieves 4.85% higher Dice Similarity Coefficient and 11.78-point lower Surface Distance on L3D dataset; maintains performance on LLSD.

Conclusion: The SRFT-GaLore-enhanced framework enables scalable, precise segmentation in real-time surgical settings with strong cross-dataset robustness.

Abstract: Accurate detection and delineation of anatomical structures in medical
imaging are critical for computer-assisted interventions, particularly in
laparoscopic liver surgery where 2D video streams limit depth perception and
complicate landmark localization. While recent works have leveraged monocular
depth cues for enhanced landmark detection, challenges remain in fusing RGB and
depth features and in efficiently adapting large-scale vision models to
surgical domains. We propose a depth-guided liver landmark segmentation
framework integrating semantic and geometric cues via vision foundation
encoders. We employ Segment Anything Model V2 (SAM2) encoder to extract RGB
features and Depth Anything V2 (DA2) encoder to extract depth-aware features.
To efficiently adapt SAM2, we introduce SRFT-GaLore, a novel low-rank gradient
projection method that replaces the computationally expensive SVD with a
Subsampled Randomized Fourier Transform (SRFT). This enables efficient
fine-tuning of high-dimensional attention layers without sacrificing
representational power. A cross-attention fusion module further integrates RGB
and depth cues. To assess cross-dataset generalization, we also construct a new
Laparoscopic Liver Surgical Dataset (LLSD) as an external validation benchmark.
On the public L3D dataset, our method achieves a 4.85% improvement in Dice
Similarity Coefficient and a 11.78-point reduction in Average Symmetric Surface
Distance compared to the D2GPLand. To further assess generalization capability,
we evaluate our model on LLSD dataset. Our model maintains competitive
performance and significantly outperforms SAM-based baselines, demonstrating
strong cross-dataset robustness and adaptability to unseen surgical
environments. These results demonstrate that our SRFT-GaLore-enhanced
dual-encoder framework enables scalable and precise segmentation under
real-time, depth-constrained surgical settings.

</details>


### [18] [SurgAnt-ViVQA: Learning to Anticipate Surgical Events through GRU-Driven Temporal Cross-Attention](https://arxiv.org/abs/2511.03178)
*Shreyas C. Dhake,Jiayuan Huang,Runlong He,Danyal Z. Khan,Evangelos B. Mazomenos,Sophia Bano,Hani J. Marcus,Danail Stoyanov,Matthew J. Clarkson,Mobarak I. Hoque*

Main category: cs.CV

TL;DR: The paper introduces PitVQA-Anticipation, the first VQA dataset for anticipating surgical events, and SurgAnt-ViVQA, a video-language model that outperforms existing baselines in surgical VQA tasks.


<details>
  <summary>Details</summary>
Motivation: Real-time assistance in endonasal transsphenoidal pituitary surgery requires anticipating future events due to limited visibility and rapid workflow changes. Current VQA systems lack support for forecasting.

Method: PitVQA-Anticipation is created with 33.5 hours of video and 734,769 QA pairs across four tasks. SurgAnt-ViVQA uses a GRU Gated Temporal Cross-Attention module to integrate temporal dynamics and visual context into a large language model.

Result: SurgAnt-ViVQA outperforms strong baselines on PitVQA-Anticipation and EndoVis datasets. Ablations show temporal recurrence and gated fusion drive performance.

Conclusion: SurgAnt-ViVQA advances surgical VQA by enabling proactive anticipation. PitVQA-Anticipation serves as a benchmark, emphasizing the need for temporal modeling in surgical assistance.

Abstract: Anticipating forthcoming surgical events is vital for real-time assistance in
endonasal transsphenoidal pituitary surgery, where visibility is limited and
workflow changes rapidly. Most visual question answering (VQA) systems reason
on isolated frames with static vision language alignment, providing little
support for forecasting next steps or instrument needs. Existing surgical VQA
datasets likewise center on the current scene rather than the near future. We
introduce PitVQA-Anticipation, the first VQA dataset designed for forward
looking surgical reasoning. It comprises 33.5 hours of operative video and
734,769 question answer pairs built from temporally grouped clips and expert
annotations across four tasks: predicting the future phase, next step, upcoming
instrument, and remaining duration. We further propose SurgAnt-ViVQA, a video
language model that adapts a large language model using a GRU Gated Temporal
Cross-Attention module. A bidirectional GRU encodes frame to frame dynamics,
while an adaptive gate injects visual context into the language stream at the
token level. Parameter efficient fine tuning customizes the language backbone
to the surgical domain. SurgAnt-ViVQA tested upon on PitVQA-Anticipation and
EndoVis datasets, surpassing strong image and video based baselines. Ablations
show that temporal recurrence and gated fusion drive most of the gains. A frame
budget study indicates a trade-off: 8 frames maximize fluency, whereas 32
frames slightly reduce BLEU but improve numeric time estimation. By pairing a
temporally aware encoder with fine grained gated cross-attention, SurgAnt-ViVQA
advances surgical VQA from retrospective description to proactive anticipation.
PitVQA-Anticipation offers a comprehensive benchmark for this setting and
highlights the importance of targeted temporal modeling for reliable, future
aware surgical assistance.

</details>


### [19] [PETWB-REP: A Multi-Cancer Whole-Body FDG PET/CT and Radiology Report Dataset for Medical Imaging Research](https://arxiv.org/abs/2511.03194)
*Le Xue,Gang Feng,Wenbo Zhang,Yichi Zhang,Lanlan Li,Shuqi Wang,Liling Peng,Sisi Peng,Xin Gao*

Main category: cs.CV

TL;DR: The paper introduces PETWB-REP, a curated dataset of whole-body FDG PET/CT scans and radiology reports from 490 patients with various cancers, aimed at advancing medical imaging and AI research.


<details>
  <summary>Details</summary>
Motivation: Addresses the scarcity of datasets combining functional and anatomical imaging with detailed clinical reports across multiple cancer types.

Method: Curated dataset includes paired PET/CT images, de-identified radiology reports, and structured clinical metadata for 490 patients with common malignancies.

Result: PETWB-REP provides a comprehensive resource for research in medical imaging, radiomics, AI, and multi-modal learning.

Conclusion: PETWB-REP is a valuable publicly available dataset to support retrospective clinical research and AI model development.

Abstract: Publicly available, large-scale medical imaging datasets are crucial for
developing and validating artificial intelligence models and conducting
retrospective clinical research. However, datasets that combine functional and
anatomical imaging with detailed clinical reports across multiple cancer types
remain scarce. Here, we present PETWB-REP, a curated dataset comprising
whole-body 18F-Fluorodeoxyglucose (FDG) Positron Emission Tomography/Computed
Tomography (PET/CT) scans and corresponding radiology reports from 490 patients
diagnosed with various malignancies. The dataset primarily includes common
cancers such as lung cancer, liver cancer, breast cancer, prostate cancer, and
ovarian cancer. This dataset includes paired PET and CT images, de-identified
textual reports, and structured clinical metadata. It is designed to support
research in medical imaging, radiomics, artificial intelligence, and
multi-modal learning.

</details>


### [20] [QG-CoC: Question-Guided Chain-of-Captions for Large Multimodal Models](https://arxiv.org/abs/2511.03206)
*Kuei-Chun Kao,Hsu Tzu-Yin,Yunqi Hong,Ruochen Wang,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: The paper addresses issues in Multimodal Large Language Models (MLLMs) with multi-image tasks, proposing a zero-shot prompting method, QG-CoC, to improve fine-grained perception and reasoning across multiple images.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs struggle with fine-grained perception and reasoning in multi-image contexts, prompting the need for a better method.

Method: The authors propose QG-CoC, a zero-shot prompting approach, to enhance perception and reasoning with arbitrary numbers of images.

Result: QG-CoC outperforms existing methods in multi-image benchmarks and shows robustness in challenging scenarios.

Conclusion: The study highlights the effectiveness of QG-CoC in addressing gaps in multi-image reasoning for MLLMs.

Abstract: Recently, Multimodal Large Language Models (MLLMs) encounter two key issues
in multi-image contexts: (1) a lack of fine-grained perception across disparate
images, and (2) a diminished capability to effectively reason over and
synthesize information from multiple visual inputs. However, while various
prompting methods aim to describe visual content, many existing studies focus
primarily on single-image settings or specific, constrained scenarios. This
leaves a critical gap in understanding and addressing how MLLMs tackle more
general and complex multi-image reasoning tasks. Thus, we first extensively
investigate how current prompting methods perceive fine-grained visual details
and process visual information when dealing with multiple images. Our findings
reveal that existing prompting methods fall short in attending to needed clues
and seamlessly integrating perception and reasoning. Inspired by the findings,
we propose a new zero-shot prompting method, Question-Guided Chain-of-Captions
(QG-CoC), a generalized prompting approach that effectively handles problems
with an arbitrary number of images. We evaluate our method on various
open-source and closed-source MLLMs for multi-image and single-image
benchmarks. Experimental results indicate that QG-CoC demonstrates competitive
performance across tasks and exhibits robust improvements in the challenging
scenarios where existing prompting methods fail.

</details>


### [21] [MvBody: Multi-View-Based Hybrid Transformer Using Optical 3D Body Scan for Explainable Cesarean Section Prediction](https://arxiv.org/abs/2511.03212)
*Ruting Cheng,Boyuan Feng,Yijiang Zheng,Chuhui Qiu,Aizierjiang Aiersilan,Joaquin A. Calderon,Wentao Zhao,Qing Pan,James K. Hahn*

Main category: cs.CV

TL;DR: Study proposes MvBody, a multi-view Transformer network, for predicting cesarean section risk using self-reported medical data and 3D body scans, achieving 84.62% accuracy and AUC-ROC of 0.724.


<details>
  <summary>Details</summary>
Motivation: Early CS risk prediction improves maternal and neonatal outcomes, especially in resource-limited settings where current models are impractical.

Method: Uses MvBody with metric learning loss, leveraging self-reported data and 3D body scans between weeks 31-38 of gestation.

Result: Outperforms existing methods with 84.62% accuracy and AUC-ROC of 0.724; key predictors include pre-pregnancy weight, age, and body shape.

Conclusion: MvBody offers accurate, affordable CS risk prediction with explainable decisions, suitable for low-resource settings.

Abstract: Accurately assessing the risk of cesarean section (CS) delivery is critical,
especially in settings with limited medical resources, where access to
healthcare is often restricted. Early and reliable risk prediction allows
better-informed prenatal care decisions and can improve maternal and neonatal
outcomes. However, most existing predictive models are tailored for in-hospital
use during labor and rely on parameters that are often unavailable in
resource-limited or home-based settings. In this study, we conduct a pilot
investigation to examine the feasibility of using 3D body shape for CS risk
assessment for future applications with more affordable general devices. We
propose a novel multi-view-based Transformer network, MvBody, which predicts CS
risk using only self-reported medical data and 3D optical body scans obtained
between the 31st and 38th weeks of gestation. To enhance training efficiency
and model generalizability in data-scarce environments, we incorporate a metric
learning loss into the network. Compared to widely used machine learning models
and the latest advanced 3D analysis methods, our method demonstrates superior
performance, achieving an accuracy of 84.62% and an Area Under the Receiver
Operating Characteristic Curve (AUC-ROC) of 0.724 on the independent test set.
To improve transparency and trust in the model's predictions, we apply the
Integrated Gradients algorithm to provide theoretically grounded explanations
of the model's decision-making process. Our results indicate that pre-pregnancy
weight, maternal age, obstetric history, previous CS history, and body shape,
particularly around the head and shoulders, are key contributors to CS risk
prediction.

</details>


### [22] [Diffusion-Guided Mask-Consistent Paired Mixing for Endoscopic Image Segmentation](https://arxiv.org/abs/2511.03219)
*Pengyu Jie,Wanquan Liu,Rui He,Yihui Wen,Deyu Meng,Chenqiang Gao*

Main category: cs.CV

TL;DR: The paper proposes a paired, diffusion-guided paradigm combining sample mixing and generative synthesis for dense prediction, improving segmentation performance without compromising pixel-level semantics.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of traditional augmentation methods (sample mixing and generative synthesis) in dense prediction, such as soft label ambiguity and domain shift.

Method: Introduces Mask-Consistent Paired Mixing (MCPMix) and Real-Anchored Learnable Annealing (RLA) to generate synthetic-real pairs with shared geometry and adaptively adjust training focus.

Result: Achieves state-of-the-art segmentation performance across multiple datasets, demonstrating robustness and generalizability.

Conclusion: Combining label-preserving mixing with diffusion-driven diversity and adaptive re-anchoring enhances endoscopic segmentation effectively.

Abstract: Augmentation for dense prediction typically relies on either sample mixing or
generative synthesis. Mixing improves robustness but misaligned masks yield
soft label ambiguity. Diffusion synthesis increases apparent diversity but,
when trained as common samples, overlooks the structural benefit of mask
conditioning and introduces synthetic-real domain shift. We propose a paired,
diffusion-guided paradigm that fuses the strengths of both. For each real
image, a synthetic counterpart is generated under the same mask and the pair is
used as a controllable input for Mask-Consistent Paired Mixing (MCPMix), which
mixes only image appearance while supervision always uses the original hard
mask. This produces a continuous family of intermediate samples that smoothly
bridges synthetic and real appearances under shared geometry, enlarging
diversity without compromising pixel-level semantics. To keep learning aligned
with real data, Real-Anchored Learnable Annealing (RLA) adaptively adjusts the
mixing strength and the loss weight of mixed samples over training, gradually
re-anchoring optimization to real data and mitigating distributional bias.
Across Kvasir-SEG, PICCOLO, CVC-ClinicDB, a private NPC-LES cohort, and ISIC
2017, the approach achieves state-of-the-art segmentation performance and
consistent gains over baselines. The results show that combining
label-preserving mixing with diffusion-driven diversity, together with adaptive
re-anchoring, yields robust and generalizable endoscopic segmentation.

</details>


### [23] [Transformer-Progressive Mamba Network for Lightweight Image Super-Resolution](https://arxiv.org/abs/2511.03232)
*Sichen Guo,Wenjie Li,Yuanyang Liu,Guangwei Gao,Jian Yang,Chia-Wen Lin*

Main category: cs.CV

TL;DR: T-PMambaSR combines window-based self-attention with Progressive Mamba to improve SR efficiency and feature representation, while an Adaptive High-Frequency Refinement Module recovers lost details.


<details>
  <summary>Details</summary>
Motivation: Existing Mamba-based SR methods lack fine-grained scale transitions, limiting feature representation efficiency.

Method: Integrates window-based self-attention with Progressive Mamba and introduces AHFRM for detail recovery.

Result: Achieves better performance than Transformer/Mamba-based methods with lower computational cost.

Conclusion: T-PMambaSR enhances receptive field and expressiveness efficiently.

Abstract: Recently, Mamba-based super-resolution (SR) methods have demonstrated the
ability to capture global receptive fields with linear complexity, addressing
the quadratic computational cost of Transformer-based SR approaches. However,
existing Mamba-based methods lack fine-grained transitions across different
modeling scales, which limits the efficiency of feature representation. In this
paper, we propose T-PMambaSR, a lightweight SR framework that integrates
window-based self-attention with Progressive Mamba. By enabling interactions
among receptive fields of different scales, our method establishes a
fine-grained modeling paradigm that progressively enhances feature
representation with linear complexity. Furthermore, we introduce an Adaptive
High-Frequency Refinement Module (AHFRM) to recover high-frequency details lost
during Transformer and Mamba processing. Extensive experiments demonstrate that
T-PMambaSR progressively enhances the model's receptive field and
expressiveness, yielding better performance than recent Transformer- or
Mamba-based methods while incurring lower computational cost. Our codes will be
released after acceptance.

</details>


### [24] [Decoupled Multi-Predictor Optimization for Inference-Efficient Model Tuning](https://arxiv.org/abs/2511.03245)
*Liwei Luo,Shuaitengyuan Li,Dongwei Ren,Qilong Wang,Pengfei Zhu,Qinghua Hu*

Main category: cs.CV

TL;DR: DMPO method decouples low-level and high-level features in early stages of multi-stage predictors using a bypass module and decoupled optimization, improving inference efficiency.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of early stages providing both low-level fundamental and high-level discriminative features in pre-trained model fine-tuning.

Method: Introduces a lightweight bypass module for feature decomposition and a high-order statistics-based predictor for discriminative ability. Uses decoupled optimization for training.

Result: DMPO outperforms counterparts in reducing computational costs across various datasets and backbones.

Conclusion: DMPO effectively balances feature representation and discrimination in early stages, enhancing inference efficiency.

Abstract: Recently, remarkable progress has been made in large-scale pre-trained model
tuning, and inference efficiency is becoming more crucial for practical
deployment. Early exiting in conjunction with multi-stage predictors, when
cooperated with a parameter-efficient fine-tuning strategy, offers a
straightforward way to achieve an inference-efficient model. However, a key
challenge remains unresolved: How can early stages provide low-level
fundamental features to deep stages while simultaneously supplying high-level
discriminative features to early-stage predictors? To address this problem, we
propose a Decoupled Multi-Predictor Optimization (DMPO) method to effectively
decouple the low-level representative ability and high-level discriminative
ability in early stages. First, in terms of architecture, we introduce a
lightweight bypass module into multi-stage predictors for functional
decomposition of shallow features from early stages, while a high-order
statistics-based predictor is developed for early stages to effectively enhance
their discriminative ability. To reasonably train our multi-predictor
architecture, a decoupled optimization is proposed to allocate two-phase loss
weights for multi-stage predictors during model tuning, where the initial
training phase enables the model to prioritize the acquisition of
discriminative ability of deep stages via emphasizing representative ability of
early stages, and the latter training phase drives discriminative ability
towards earlier stages as much as possible. As such, our DMPO can effectively
decouple representative and discriminative abilities in early stages in terms
of architecture design and model optimization. Experiments across various
datasets and pre-trained backbones demonstrate that DMPO clearly outperforms
its counterparts when reducing computational cost.

</details>


### [25] [Generative deep learning for foundational video translation in ultrasound](https://arxiv.org/abs/2511.03255)
*Nikolina Tomic Roshni Bhatnagar,Sarthak Jain,Connor Lau,Tien-Yu Liu,Laura Gambini,Rima Arnaout*

Main category: cs.CV

TL;DR: A generative method translates ultrasound sub-modalities (CFD-greyscale) to balance datasets, achieving realistic results indistinguishable from real data in classification, segmentation, and clinical evaluation.


<details>
  <summary>Details</summary>
Motivation: Addressing data imbalance and missingness in ultrasound imaging, particularly for sub-modalities like greyscale and color flow doppler (CFD), to enhance dataset utility for deep learning applications in medicine.

Method: Developed a generative approach using pixel-wise, adversarial, and perceptual losses with two networks: one for reconstructing anatomic structures and another for denoising. Trained on 54,975 videos and tested on 8,368.

Result: Achieved high similarity (SSIM 0.91+/-0.04), indistinguishable performance in DL tasks (F1 score 0.89 vs. 0.9), and clinician accuracy of 54+/-6% in distinguishing real vs. synthetic videos. Demonstrated generalizability across clinical domains.

Conclusion: The method effectively balances ultrasound datasets, enabling realistic synthetic data generation and expanding the utility of retrospectively collected imaging for medical applications.

Abstract: Deep learning (DL) has the potential to revolutionize image acquisition and
interpretation across medicine, however, attention to data imbalance and
missingness is required. Ultrasound data presents a particular challenge
because in addition to different views and structures, it includes several
sub-modalities-such as greyscale and color flow doppler (CFD)-that are often
imbalanced in clinical studies. Image translation can help balance datasets but
is challenging for ultrasound sub-modalities to date. Here, we present a
generative method for ultrasound CFD-greyscale video translation, trained on
54,975 videos and tested on 8,368. The method developed leveraged pixel-wise,
adversarial, and perceptual loses and utilized two networks: one for
reconstructing anatomic structures and one for denoising to achieve realistic
ultrasound imaging. Average pairwise SSIM between synthetic videos and ground
truth was 0.91+/-0.04. Synthetic videos performed indistinguishably from real
ones in DL classification and segmentation tasks and when evaluated by blinded
clinical experts: F1 score was 0.9 for real and 0.89 for synthetic videos; Dice
score between real and synthetic segmentation was 0.97. Overall clinician
accuracy in distinguishing real vs synthetic videos was 54+/-6% (42-61%),
indicating realistic synthetic videos. Although trained only on heart videos,
the model worked well on ultrasound spanning several clinical domains (average
SSIM 0.91+/-0.05), demonstrating foundational abilities. Together, these data
expand the utility of retrospectively collected imaging and augment the dataset
design toolbox for medical imaging.

</details>


### [26] [Enhancing Medical Image Segmentation via Heat Conduction Equation](https://arxiv.org/abs/2511.03260)
*Rong Wu,Yim-Sang Yu*

Main category: cs.CV

TL;DR: A new hybrid architecture combining U-Mamba and Heat Conduction Equation for medical image segmentation outperforms existing models by efficiently modeling global context and long-range dependencies.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle with efficient global context modeling and long-range dependency reasoning under practical computational budgets.

Method: Proposes a hybrid architecture using U-Mamba with Heat Conduction Equation, combining Mamba-based state-space modules for long-range reasoning and Heat Conduction Operators (HCOs) for enhanced semantic abstraction.

Result: The model consistently outperforms baselines on multimodal abdominal CT and MRI datasets, demonstrating effectiveness and generalizability.

Conclusion: Blending state-space dynamics with heat-based global diffusion provides a scalable and interpretable solution for medical segmentation tasks.

Abstract: Medical image segmentation has been significantly advanced by deep learning
architectures, notably U-Net variants. However, existing models struggle to
achieve efficient global context modeling and long-range dependency reasoning
under practical computational budgets simultaneously. In this work, we propose
a novel hybrid architecture utilizing U-Mamba with Heat Conduction Equation.
Our model combines Mamba-based state-space modules for efficient long-range
reasoning with Heat Conduction Operators (HCOs) in the bottleneck layers,
simulating frequency-domain thermal diffusion for enhanced semantic
abstraction. Experimental results on multimodal abdominal CT and MRI datasets
demonstrate that the proposed model consistently outperforms strong baselines,
validating its effectiveness and generalizability. It suggest that blending
state-space dynamics with heat-based global diffusion offers a scalable and
interpretable solution for medical segmentation tasks.

</details>


### [27] [IEC3D-AD: A 3D Dataset of Industrial Equipment Components for Unsupervised Point Cloud Anomaly Detection](https://arxiv.org/abs/2511.03267)
*Bingyang Guo,Hongjie Li,Ruiyun Yu,Hanzhe Liang,Jinbao Wang*

Main category: cs.CV

TL;DR: The paper introduces IEC3D-AD, a high-fidelity point cloud dataset for 3D anomaly detection in industrial equipment components, and proposes GMANet, a novel method leveraging geometric morphological analysis and spatial discrepancy optimization.


<details>
  <summary>Details</summary>
Motivation: Existing 3D datasets lack complexity and subtlety for real industrial environments, hindering precise anomaly detection for critical components like bearings and bolts.

Method: Developed IEC3D-AD dataset from actual production lines, then introduced GMANet, a paradigm generating synthetic point clouds and optimizing spatial discrepancies.

Result: GMANet shows effectiveness on IEC3D-AD and other datasets, improving anomaly detection through enhanced resolution and defect annotation.

Conclusion: The IEC3D-AD dataset and GMANet method address limitations in current 3D-AD research, offering practical solutions for industrial applications.

Abstract: 3D anomaly detection (3D-AD) plays a critical role in industrial
manufacturing, particularly in ensuring the reliability and safety of core
equipment components. Although existing 3D datasets like Real3D-AD and MVTec
3D-AD offer broad application support, they fall short in capturing the
complexities and subtle defects found in real industrial environments. This
limitation hampers precise anomaly detection research, especially for
industrial equipment components (IEC) such as bearings, rings, and bolts. To
address this challenge, we have developed a point cloud anomaly detection
dataset (IEC3D-AD) specific to real industrial scenarios. This dataset is
directly collected from actual production lines, ensuring high fidelity and
relevance. Compared to existing datasets, IEC3D-AD features significantly
improved point cloud resolution and defect annotation granularity, facilitating
more demanding anomaly detection tasks. Furthermore, inspired by generative
2D-AD methods, we introduce a novel 3D-AD paradigm (GMANet) on IEC3D-AD. This
paradigm generates synthetic point cloud samples based on geometric
morphological analysis, then reduces the margin and increases the overlap
between normal and abnormal point-level features through spatial discrepancy
optimization. Extensive experiments demonstrate the effectiveness of our method
on both IEC3D-AD and other datasets.

</details>


### [28] [Unified Long Video Inpainting and Outpainting via Overlapping High-Order Co-Denoising](https://arxiv.org/abs/2511.03272)
*Shuangquan Lyu,Steven Mao,Yue Ma*

Main category: cs.CV

TL;DR: A novel unified approach for long video inpainting and outpainting using LoRA-fine-tuned diffusion models and temporal co-denoising achieves high fidelity and consistency without seams or drift.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of generating long videos and achieving high controllability in video inpainting/outpainting simultaneously.

Method: Leverages LoRA to fine-tune Alibaba's Wan 2.1 model for masked video synthesis, using overlap-and-blend temporal co-denoising for long sequences.

Result: Outperforms baselines (Wan 2.1, VACE) in quality (PSNR/SSIM) and realism (LPIPS), enabling seamless long-range video editing.

Conclusion: The method balances efficiency and performance, enabling practical long-video editing with high fidelity.

Abstract: Generating long videos remains a fundamental challenge, and achieving high
controllability in video inpainting and outpainting is particularly demanding.
To address both of these challenges simultaneously and achieve controllable
video inpainting and outpainting for long video clips, we introduce a novel and
unified approach for long video inpainting and outpainting that extends
text-to-video diffusion models to generate arbitrarily long, spatially edited
videos with high fidelity. Our method leverages LoRA to efficiently fine-tune a
large pre-trained video diffusion model like Alibaba's Wan 2.1 for masked
region video synthesis, and employs an overlap-and-blend temporal co-denoising
strategy with high-order solvers to maintain consistency across long sequences.
In contrast to prior work that struggles with fixed-length clips or exhibits
stitching artifacts, our system enables arbitrarily long video generation and
editing without noticeable seams or drift. We validate our approach on
challenging inpainting/outpainting tasks including editing or adding objects
over hundreds of frames and demonstrate superior performance to baseline
methods like Wan 2.1 model and VACE in terms of quality (PSNR/SSIM), and
perceptual realism (LPIPS). Our method enables practical long-range video
editing with minimal overhead, achieved a balance between parameter efficient
and superior performance.

</details>


### [29] [Diffusion-SDPO: Safeguarded Direct Preference Optimization for Diffusion Models](https://arxiv.org/abs/2511.03317)
*Minghao Fu,Guo-Hua Wang,Tianyu Cui,Qing-Guo Chen,Zhao Xu,Weihua Luo,Kaifu Zhang*

Main category: cs.CV

TL;DR: The paper addresses challenges in aligning text-to-image diffusion models with human preferences, introducing Diffusion-SDPO, a safeguarded update rule to improve generation quality without increasing reconstruction errors.


<details>
  <summary>Details</summary>
Motivation: Aligning text-to-image diffusion models with human preferences is difficult, as standard methods like Diffusion-DPO can degrade both preferred and less-preferred outputs when enlarging the preference margin.

Method: The authors propose Diffusion-SDPO, which adaptively scales the loser gradient based on its alignment with the winner gradient, ensuring the preferred output's error does not increase.

Result: Diffusion-SDPO outperforms existing preference-learning baselines in automated preference, aesthetic, and prompt alignment metrics.

Conclusion: Diffusion-SDPO is a simple, model-agnostic solution that improves alignment in text-to-image models with minimal computational overhead.

Abstract: Text-to-image diffusion models deliver high-quality images, yet aligning them
with human preferences remains challenging. We revisit diffusion-based Direct
Preference Optimization (DPO) for these models and identify a critical
pathology: enlarging the preference margin does not necessarily improve
generation quality. In particular, the standard Diffusion-DPO objective can
increase the reconstruction error of both winner and loser branches.
Consequently, degradation of the less-preferred outputs can become sufficiently
severe that the preferred branch is also adversely affected even as the margin
grows. To address this, we introduce Diffusion-SDPO, a safeguarded update rule
that preserves the winner by adaptively scaling the loser gradient according to
its alignment with the winner gradient. A first-order analysis yields a
closed-form scaling coefficient that guarantees the error of the preferred
output is non-increasing at each optimization step. Our method is simple,
model-agnostic, broadly compatible with existing DPO-style alignment frameworks
and adds only marginal computational overhead. Across standard text-to-image
benchmarks, Diffusion-SDPO delivers consistent gains over preference-learning
baselines on automated preference, aesthetic, and prompt alignment metrics.
Code is publicly available at https://github.com/AIDC-AI/Diffusion-SDPO.

</details>


### [30] [SurgViVQA: Temporally-Grounded Video Question Answering for Surgical Scene Understanding](https://arxiv.org/abs/2511.03325)
*Mauro Orazio Drago,Luca Carlini,Pelinsu Celebi Balyemez,Dennis Pierantozzi,Chiara Lena,Cesare Hassan,Danail Stoyanov,Elena De Momi,Sophia Bano,Mobarak I. Hoque*

Main category: cs.CV

TL;DR: SurgViVQA enhances surgical VideoQA by focusing on temporal dynamics, outperforming image-based models with improved accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Current VideoQA methods in surgery rely on static images, missing critical temporal cues like motion and tool-tissue interactions.

Method: SurgViVQA uses a Masked Video-Text Encoder fused with LLM decoding to capture temporal dynamics, evaluated on REAL-Colon-VQA and EndoVis18-VQA datasets.

Result: SurgViVQA outperforms benchmarks by +11% on REAL-Colon-VQA and +9% on EndoVis18-VQA, showing robustness to question variations.

Conclusion: SurgViVQA and REAL-Colon-VQA advance temporal-aware AI understanding in surgical VideoQA, improving dynamic procedural interpretation.

Abstract: Video Question Answering (VideoQA) in the surgical domain aims to enhance
intraoperative understanding by enabling AI models to reason over temporally
coherent events rather than isolated frames. Current approaches are limited to
static image features, and available datasets often lack temporal annotations,
ignoring the dynamics critical for accurate procedural interpretation. We
propose SurgViVQA, a surgical VideoQA model that extends visual reasoning from
static images to dynamic surgical scenes. It uses a Masked Video--Text Encoder
to fuse video and question features, capturing temporal cues such as motion and
tool--tissue interactions, which a fine-tuned large language model (LLM) then
decodes into coherent answers. To evaluate its performance, we curated
REAL-Colon-VQA, a colonoscopic video dataset that includes motion-related
questions and diagnostic attributes, as well as out-of-template questions with
rephrased or semantically altered formulations to assess model robustness.
Experimental validation on REAL-Colon-VQA and the public EndoVis18-VQA dataset
shows that SurgViVQA outperforms existing image-based VQA benchmark models,
particularly in keyword accuracy, improving over PitVQA by +11\% on
REAL-Colon-VQA and +9\% on EndoVis18-VQA. A perturbation study on the questions
further confirms improved generalizability and robustness to variations in
question phrasing. SurgViVQA and the REAL-Colon-VQA dataset provide a framework
for temporally-aware understanding in surgical VideoQA, enabling AI models to
interpret dynamic procedural contexts more effectively. Code and dataset
available at https://github.com/madratak/SurgViVQA.

</details>


### [31] [Multi-Object Tracking Retrieval with LLaVA-Video: A Training-Free Solution to MOT25-StAG Challenge](https://arxiv.org/abs/2511.03332)
*Yi Yang,Yiming Xu,Timo Kaiser,Hao Cheng,Bodo Rosenhahn,Michael Ying Yang*

Main category: cs.CV

TL;DR: The paper presents a two-stage, zero-shot approach for the MOT25-StAG Challenge, combining FastTracker and LLaVA-Video to localize and track objects matching language queries in videos, achieving second place in the challenge.


<details>
  <summary>Details</summary>
Motivation: The challenge requires accurate localization and tracking of multiple objects matching specific and free-form language queries in complex real-world video scenes.

Method: A two-stage, zero-shot approach integrating the SOTA tracking model FastTracker and the multi-modal large language model LLaVA-Video.

Result: Achieved m-HIoU and HOTA scores of 20.68 and 10.73 on the test set, securing second place in the challenge.

Conclusion: The proposed method effectively addresses the MOT25-StAG task, demonstrating strong performance in object localization and tracking with language queries.

Abstract: In this report, we present our solution to the MOT25-Spatiotemporal Action
Grounding (MOT25-StAG) Challenge. The aim of this challenge is to accurately
localize and track multiple objects that match specific and free-form language
queries, using video data of complex real-world scenes as input. We model the
underlying task as a video retrieval problem and present a two-stage, zero-shot
approach, combining the advantages of the SOTA tracking model FastTracker and
Multi-modal Large Language Model LLaVA-Video. On the MOT25-StAG test set, our
method achieves m-HIoU and HOTA scores of 20.68 and 10.73 respectively, which
won second place in the challenge.

</details>


### [32] [UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal Interactions](https://arxiv.org/abs/2511.03334)
*Guozhen Zhang,Zixiang Zhou,Teng Hu,Ziqiao Peng,Youliang Zhang,Yi Chen,Yuan Zhou,Qinglin Lu,Limin Wang*

Main category: cs.CV

TL;DR: UniAVGen is a unified framework for joint audio-video generation, addressing lip sync and semantic consistency issues with dual-branch Diffusion Transformers and cross-modal interaction mechanisms.


<details>
  <summary>Details</summary>
Motivation: Existing methods suffer from poor lip synchronization and semantic inconsistency in audio-video generation, necessitating a more effective cross-modal approach.

Method: UniAVGen uses parallel Diffusion Transformers (DiTs) and an Asymmetric Cross-Modal Interaction mechanism, augmented by Face-Aware Modulation and Modality-Aware Classifier-Free Guidance.

Result: The framework outperforms others in synchronization and consistency with fewer training samples (1.3M vs. 30.1M).

Conclusion: UniAVGen provides a robust, unified solution for diverse audio-video tasks, improving fidelity and efficiency.

Abstract: Due to the lack of effective cross-modal modeling, existing open-source
audio-video generation methods often exhibit compromised lip synchronization
and insufficient semantic consistency. To mitigate these drawbacks, we propose
UniAVGen, a unified framework for joint audio and video generation. UniAVGen is
anchored in a dual-branch joint synthesis architecture, incorporating two
parallel Diffusion Transformers (DiTs) to build a cohesive cross-modal latent
space. At its heart lies an Asymmetric Cross-Modal Interaction mechanism, which
enables bidirectional, temporally aligned cross-attention, thus ensuring
precise spatiotemporal synchronization and semantic consistency. Furthermore,
this cross-modal interaction is augmented by a Face-Aware Modulation module,
which dynamically prioritizes salient regions in the interaction process. To
enhance generative fidelity during inference, we additionally introduce
Modality-Aware Classifier-Free Guidance, a novel strategy that explicitly
amplifies cross-modal correlation signals. Notably, UniAVGen's robust joint
synthesis design enables seamless unification of pivotal audio-video tasks
within a single model, such as joint audio-video generation and continuation,
video-to-audio dubbing, and audio-driven video synthesis. Comprehensive
experiments validate that, with far fewer training samples (1.3M vs. 30.1M),
UniAVGen delivers overall advantages in audio-video synchronization, timbre
consistency, and emotion consistency.

</details>


### [33] [Decoupling Augmentation Bias in Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2511.03367)
*Gahyeon Kim,Sohee Kim,Seokju Lee*

Main category: cs.CV

TL;DR: The paper explores image-level augmentations to enhance prompt learning in zero-shot tasks, proposing AAPL, a method using adversarial token embeddings to focus on discriminative features, outperforming existing methods across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing prompt learning methods struggle with generalization to unseen categories and lack focus on semantically meaningful visual features, prompting the exploration of image-based augmentations.

Method: Proposes AAPL, which introduces adversarial token embeddings to decouple superficial visual variations from class-relevant semantics, enhancing prompt learning.

Result: AAPL outperforms existing methods on eleven benchmark datasets across few-shot, zero-shot, cross-dataset, and domain generalization settings.

Conclusion: AAPL effectively improves generalization in prompt learning by leveraging image-level augmentations and adversarial token embeddings, addressing limitations of prior methods.

Abstract: Recent advances in large-scale vision and language models have led to
significant progress in zero-shot learning tasks. Methods such as CoOp and
CoCoOp have shown that replacing handcrafted prompts with learnable vectors,
known as prompt learning, can result in improved performance. However, these
models often struggle to generalize to entirely unseen categories. While
traditional zero-shot learning techniques benefit from various data
augmentation strategies, prompt learning has primarily focused on text-based
modifications, leaving the potential of image-based augmentation largely
unexplored. In this work, we explore how image-level augmentations,
particularly those that introduce attribute-specific variations, can support
and enhance prompt learning. Our analysis examines the interaction between
these augmentations and soft prompt frameworks, revealing their potential to
improve generalization. We also identify a limitation in existing methods, such
as CoCoOp, which do not provide explicit guidance for learning prompts that
focus on semantically meaningful visual features. To address this, we propose
Adding Attributes to Prompt Learning, AAPL, a novel method that introduces
adversarial token embeddings to decouple superficial visual variations
introduced by augmentation from class-relevant semantic representations. This
decoupling enables the learned prompts to concentrate on visually
discriminative features that align with the target categories. We conduct
comprehensive experiments on eleven benchmark datasets, and AAPL consistently
outperforms existing methods across few-shot, zero-shot, cross-dataset, and
domain generalization settings. Our source code is publicly available at:
https://github.com/Gahyeonkim09/AAPL

</details>


### [34] [Robust Alignment of the Human Embryo in 3D Ultrasound using PCA and an Ensemble of Heuristic, Atlas-based and Learning-based Classifiers Evaluated on the Rotterdam Periconceptional Cohort](https://arxiv.org/abs/2511.03416)
*Nikolai Herrmann,Marcella C. Zijta,Stefan Klein,RÃ©gine P. M. Steegers-Theunissen,Rene M. H. Wijnen,Bernadette S. de Bakker,Melek Rousian,Wietske A. P. Bastiaansen*

Main category: cs.CV

TL;DR: An automated method using PCA for standardizing embryonic alignment in 3D ultrasound images achieves high accuracy (98.5%) via heuristic, atlas-based, and Random Forest strategies.


<details>
  <summary>Details</summary>
Motivation: Standardized alignment improves prenatal growth monitoring by aiding standard plane detection and enhancing landmark visualization.

Method: PCA extracts embryo axes from segmentation masks, generating four candidates. Selection via Pearson's correlation, atlas matching, or Random Forest determines the correct orientation.

Result: PCA correctly identified axes in 99.0% of images. Selection accuracies were 97.4% (Pearson), 95.8% (Atlas), and 98.4% (Random Forest). Majority Vote achieved 98.5%.

Conclusion: The method ensures consistent embryonic alignment, supporting scalable clinical and research applications.

Abstract: Standardized alignment of the embryo in three-dimensional (3D) ultrasound
images aids prenatal growth monitoring by facilitating standard plane
detection, improving visualization of landmarks and accentuating differences
between different scans. In this work, we propose an automated method for
standardizing this alignment. Given a segmentation mask of the embryo,
Principal Component Analysis (PCA) is applied to the mask extracting the
embryo's principal axes, from which four candidate orientations are derived.
The candidate in standard orientation is selected using one of three
strategies: a heuristic based on Pearson's correlation assessing shape, image
matching to an atlas through normalized cross-correlation, and a Random Forest
classifier. We tested our method on 2166 images longitudinally acquired 3D
ultrasound scans from 1043 pregnancies from the Rotterdam Periconceptional
Cohort, ranging from 7+0 to 12+6 weeks of gestational age. In 99.0% of images,
PCA correctly extracted the principal axes of the embryo. The correct candidate
was selected by the Pearson Heuristic, Atlas-based and Random Forest in 97.4%,
95.8%, and 98.4% of images, respectively. A Majority Vote of these selection
methods resulted in an accuracy of 98.5%. The high accuracy of this pipeline
enables consistent embryonic alignment in the first trimester, enabling
scalable analysis in both clinical and research settings. The code is publicly
available at:
https://gitlab.com/radiology/prenatal-image-analysis/pca-3d-alignment.

</details>


### [35] [Generalizing Shape-from-Template to Topological Changes](https://arxiv.org/abs/2511.03459)
*Kevin Manogue,Tomasz M Schang,Dilara KuÅŸ,Jonas MÃ¼ller,Stefan Zachow,Agniva Sengupta*

Main category: cs.CV

TL;DR: Proposes a method to reconstruct deformable object surfaces from 2D images, even with topological changes, outperforming existing SfT approaches.


<details>
  <summary>Details</summary>
Motivation: Existing Shape-from-Template (SfT) methods fail when topological changes occur during deformation, limiting their practical applicability.

Method: Extends SfT by iteratively adapting the template through spatial domain partitioning, minimizing an energy functional for physical plausibility and reprojection consistency.

Result: Robustly handles topological events like tears and cuts, achieving better performance than baseline methods in synthetic and real data experiments.

Conclusion: Establishes the first general framework for SfT that accommodates topological changes, significantly advancing the field.

Abstract: Reconstructing the surfaces of deformable objects from correspondences
between a 3D template and a 2D image is well studied under Shape-from-Template
(SfT) methods; however, existing approaches break down when topological changes
accompany the deformation. We propose a principled extension of SfT that
enables reconstruction in the presence of such changes. Our approach is
initialized with a classical SfT solution and iteratively adapts the template
by partitioning its spatial domain so as to minimize an energy functional that
jointly encodes physical plausibility and reprojection consistency. We
demonstrate that the method robustly captures a wide range of practically
relevant topological events including tears and cuts on bounded 2D surfaces,
thereby establishing the first general framework for topological-change-aware
SfT. Experiments on both synthetic and real data confirm that our approach
consistently outperforms baseline methods.

</details>


### [36] [Human Mesh Modeling for Anny Body](https://arxiv.org/abs/2511.03589)
*Romain BrÃ©gier,GuÃ©nolÃ© Fiche,Laura Bravo-SÃ¡nchez,Thomas Lucas,Matthieu Armando,Philippe Weinzaepfel,GrÃ©gory Rogez,Fabien Baradel*

Main category: cs.CV

TL;DR: Anny is a scan-free, differentiable body model leveraging anthropometric knowledge for diverse human forms, validated by WHO stats. It supports tasks like scan fitting and synthetic data generation, matching scan-based models' performance while being open-source.


<details>
  <summary>Details</summary>
Motivation: Existing body models rely on costly 3D scans and proprietary datasets, limiting accessibility and demographic representation.

Method: Anny uses anthropometric parameters (e.g., age, height) to control blendshapes, creating realistic human forms calibrated with WHO statistics.

Result: Anny matches scan-based models in performance for tasks like Human Mesh Recovery (HMR) and offers interpretability and broad representation.

Conclusion: Anny provides an open-source, versatile foundation for human-centric 3D modeling, combining realism with accessibility.

Abstract: Parametric body models are central to many human-centric tasks, yet existing
models often rely on costly 3D scans and learned shape spaces that are
proprietary and demographically narrow. We introduce Anny, a simple, fully
differentiable, and scan-free human body model grounded in anthropometric
knowledge from the MakeHuman community. Anny defines a continuous,
interpretable shape space, where phenotype parameters (e.g. gender, age,
height, weight) control blendshapes spanning a wide range of human forms --
across ages (from infants to elders), body types, and proportions. Calibrated
using WHO population statistics, it provides realistic and demographically
grounded human shape variation within a single unified model. Thanks to its
openness and semantic control, Anny serves as a versatile foundation for 3D
human modeling -- supporting millimeter-accurate scan fitting, controlled
synthetic data generation, and Human Mesh Recovery (HMR). We further introduce
Anny-One, a collection of 800k photorealistic humans generated with Anny,
showing that despite its simplicity, HMR models trained with Anny can match the
performance of those trained with scan-based body models, while remaining
interpretable and broadly representative. The Anny body model and its code are
released under the Apache 2.0 license, making Anny an accessible foundation for
human-centric 3D modeling.

</details>


### [37] [Signal Intensity-weighted coordinate channels improve learning stability and generalisation in 1D and 2D CNNs in localisation tasks on biomedical signals](https://arxiv.org/abs/2511.03645)
*Vittal L. Rao*

Main category: cs.CV

TL;DR: Proposes a signal intensity-weighted coordinate representation for biomedical data localization tasks, improving convergence and performance.


<details>
  <summary>Details</summary>
Motivation: Localization tasks in biomedical data require models to learn spatial/temporal relationships from complex signals. Current methods like CoordConv use pure coordinate channels, but signal intensity can enhance learning.

Method: Introduces coordinate channels scaled by local signal intensity, embedding intensity-position coupling as an inductive bias. Tested on ECG signal transition timing and nuclear center regression in cytological images.

Result: The proposed method achieves faster convergence and higher generalization performance compared to conventional coordinate-channel methods, across 1D and 2D signals.

Conclusion: Signal intensity-weighted coordinate representation is effective for localization tasks, offering a simple, modality-agnostic improvement over existing approaches.

Abstract: Localisation tasks in biomedical data often require models to learn
meaningful spatial or temporal relationships from signals with complex
intensity distributions. A common strategy, exemplified by CoordConv layers, is
to append coordinate channels to convolutional inputs, enabling networks to
learn absolute positions. In this work, we propose a signal intensity-weighted
coordinate representation that replaces the pure coordinate channels with
channels scaled by local signal intensity. This modification embeds an
intensity-position coupling directly in the input representation, introducing a
simple and modality-agnostic inductive bias. We evaluate the approach on two
distinct localisation problems: (i) predicting the time of morphological
transition in 20-second, two-lead ECG signals, and (ii) regressing the
coordinates of nuclear centres in cytological images from the SiPaKMeD dataset.
In both cases, the proposed representation yields faster convergence and higher
generalisation performance relative to conventional coordinate-channel
approaches, demonstrating its effectiveness across both one-dimensional and
two-dimensional biomedical signals.

</details>


### [38] [A Lightweight 3D-CNN for Event-Based Human Action Recognition with Privacy-Preserving Potential](https://arxiv.org/abs/2511.03665)
*Mehdi Sefidgar Dilmaghani,Francis Fowley,Peter Corcoran*

Main category: cs.CV

TL;DR: A lightweight 3DCNN for HAR using event-based vision data, ensuring privacy and outperforming benchmarks with 94.17% accuracy.


<details>
  <summary>Details</summary>
Motivation: Privacy preservation in human monitoring systems is critical, and event cameras inherently avoid capturing identifiable information.

Method: A compact 3DCNN models spatial-temporal dynamics, uses focal loss with class reweighting, and employs data augmentation.

Result: Achieves F1-score of 0.9415 and 94.17% accuracy, surpassing benchmarks like C3D, ResNet3D, and MC3_18 by up to 3%.

Conclusion: Event-based deep learning enables accurate, efficient, and privacy-aware HAR for edge applications.

Abstract: This paper presents a lightweight three-dimensional convolutional neural
network (3DCNN) for human activity recognition (HAR) using event-based vision
data. Privacy preservation is a key challenge in human monitoring systems, as
conventional frame-based cameras capture identifiable personal information. In
contrast, event cameras record only changes in pixel intensity, providing an
inherently privacy-preserving sensing modality. The proposed network
effectively models both spatial and temporal dynamics while maintaining a
compact design suitable for edge deployment. To address class imbalance and
enhance generalization, focal loss with class reweighting and targeted data
augmentation strategies are employed. The model is trained and evaluated on a
composite dataset derived from the Toyota Smart Home and ETRI datasets.
Experimental results demonstrate an F1-score of 0.9415 and an overall accuracy
of 94.17%, outperforming benchmark 3D-CNN architectures such as C3D, ResNet3D,
and MC3_18 by up to 3%. These results highlight the potential of event-based
deep learning for developing accurate, efficient, and privacy-aware human
action recognition systems suitable for real-world edge applications.

</details>


### [39] [Part-Aware Bottom-Up Group Reasoning for Fine-Grained Social Interaction Detection](https://arxiv.org/abs/2511.03666)
*Dongkeun Kim,Minsu Cho,Suha Kwak*

Main category: cs.CV

TL;DR: A part-aware bottom-up framework is proposed for fine-grained social interaction detection by leveraging body part features and interpersonal relations, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook nuanced social cues and rely on holistic representations, limiting their ability to detect localized interactions and infer group configurations accurately.

Method: The proposed framework detects individuals, enhances their features with part-aware cues, and infers group configurations by associating individuals through similarity-based reasoning, incorporating spatial and social cues.

Result: The method achieves state-of-the-art performance on the NVI dataset, demonstrating superior accuracy in group inference.

Conclusion: The part-aware bottom-up framework effectively captures fine-grained social interactions and improves group detection by modeling subtle interpersonal cues.

Abstract: Social interactions often emerge from subtle, fine-grained cues such as
facial expressions, gaze, and gestures. However, existing methods for social
interaction detection overlook such nuanced cues and primarily rely on holistic
representations of individuals. Moreover, they directly detect social groups
without explicitly modeling the underlying interactions between individuals.
These drawbacks limit their ability to capture localized social signals and
introduce ambiguity when group configurations should be inferred from social
interactions grounded in nuanced cues. In this work, we propose a part-aware
bottom-up group reasoning framework for fine-grained social interaction
detection. The proposed method infers social groups and their interactions
using body part features and their interpersonal relations. Our model first
detects individuals and enhances their features using part-aware cues, and then
infers group configuration by associating individuals via similarity-based
reasoning, which considers not only spatial relations but also subtle social
cues that signal interactions, leading to more accurate group inference.
Experiments on the NVI dataset demonstrate that our method outperforms prior
methods, achieving the new state of the art.

</details>


### [40] [Disentangled Concepts Speak Louder Than Words:Explainable Video Action Recognition](https://arxiv.org/abs/2511.03725)
*Jongseo Lee,Wooil Lee,Gyeong-Moon Park,Seong Tae Kim,Jinwoo Choi*

Main category: cs.CV

TL;DR: DANCE framework improves video action recognition by disentangling motion dynamics, objects, and scenes for clearer explanations, with competitive performance and practical benefits.


<details>
  <summary>Details</summary>
Motivation: Existing methods produce entangled explanations, making it unclear whether predictions rely on motion or spatial context. Language-based approaches often fail to explain tacit motions.

Method: Proposes DANCE, a concept-based framework using human pose sequences for motion dynamics and a large language model for object and scene concepts. It employs an ante-hoc concept bottleneck design.

Result: DANCE significantly improves explanation clarity on four datasets (KTH, Penn Action, HAA500, UCF-101) with competitive performance. A user study validates its superior interpretability.

Conclusion: DANCE not only enhances explanation clarity but also aids in model debugging, editing, and failure analysis.

Abstract: Effective explanations of video action recognition models should disentangle
how movements unfold over time from the surrounding spatial context. However,
existing methods based on saliency produce entangled explanations, making it
unclear whether predictions rely on motion or spatial context. Language-based
approaches offer structure but often fail to explain motions due to their tacit
nature -- intuitively understood but difficult to verbalize. To address these
challenges, we propose Disentangled Action aNd Context concept-based
Explainable (DANCE) video action recognition, a framework that predicts actions
through disentangled concept types: motion dynamics, objects, and scenes. We
define motion dynamics concepts as human pose sequences. We employ a large
language model to automatically extract object and scene concepts. Built on an
ante-hoc concept bottleneck design, DANCE enforces prediction through these
concepts. Experiments on four datasets -- KTH, Penn Action, HAA500, and UCF-101
-- demonstrate that DANCE significantly improves explanation clarity with
competitive performance. We validate the superior interpretability of DANCE
through a user study. Experimental results also show that DANCE is beneficial
for model debugging, editing, and failure analysis.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [41] [Cache Mechanism for Agent RAG Systems](https://arxiv.org/abs/2511.02919)
*Shuhang Lin,Zhencan Peng,Lingyao Li,Xiao Lin,Xi Zhu,Yongfeng Zhang*

Main category: cs.CL

TL;DR: ARC is introduced as a caching framework for dynamic management of small, high-value corpora in RAG-powered LLM agents, reducing storage and improving efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing RAG models lack efficient dynamic cache management for tailored agent needs, motivating ARC's development.

Method: ARC synthesizes query patterns and embedding geometry to maintain a high-relevance cache.

Result: ARC reduces storage to 0.015% of the original corpus, increases has-answer rate to 79.8%, and cuts retrieval latency by 80%.

Conclusion: ARC significantly enhances efficiency and effectiveness in RAG-powered LLM agents.

Abstract: Recent advances in Large Language Model (LLM)-based agents have been
propelled by Retrieval-Augmented Generation (RAG), which grants the models
access to vast external knowledge bases. Despite RAG's success in improving
agent performance, agent-level cache management, particularly constructing,
maintaining, and updating a compact, relevant corpus dynamically tailored to
each agent's need, remains underexplored. Therefore, we introduce ARC (Agent
RAG Cache Mechanism), a novel, annotation-free caching framework that
dynamically manages small, high-value corpora for each agent. By synthesizing
historical query distribution patterns with the intrinsic geometry of cached
items in the embedding space, ARC automatically maintains a high-relevance
cache. With comprehensive experiments on three retrieval datasets, our
experimental results demonstrate that ARC reduces storage requirements to
0.015% of the original corpus while offering up to 79.8% has-answer rate and
reducing average retrieval latency by 80%. Our results demonstrate that ARC can
drastically enhance efficiency and effectiveness in RAG-powered LLM agents.

</details>


### [42] [Automatic Machine Translation Detection Using a Surrogate Multilingual Translation Model](https://arxiv.org/abs/2511.02958)
*Cristian GarcÃ­a-Romero,Miquel EsplÃ -Gomis,Felipe SÃ¡nchez-MartÃ­nez*

Main category: cs.CL

TL;DR: Proposes a method using internal representations of a multilingual MT model to filter out machine-translated sentences, improving accuracy by at least 5 percentage points.


<details>
  <summary>Details</summary>
Motivation: Reliance on machine-generated translations in training data degrades MT quality, necessitating effective filtering methods.

Method: Uses internal representations of a surrogate multilingual MT model to distinguish human from machine translations.

Result: Outperforms state-of-the-art techniques, especially for non-English pairs, with â‰¥5% accuracy gains.

Conclusion: The proposed method effectively enhances MT training data quality by filtering synthetic translations.

Abstract: Modern machine translation (MT) systems depend on large parallel corpora,
often collected from the Internet. However, recent evidence indicates that (i)
a substantial portion of these texts are machine-generated translations, and
(ii) an overreliance on such synthetic content in training data can
significantly degrade translation quality. As a result, filtering out non-human
translations is becoming an essential pre-processing step in building
high-quality MT systems. In this work, we propose a novel approach that
directly exploits the internal representations of a surrogate multilingual MT
model to distinguish between human and machine-translated sentences.
Experimental results show that our method outperforms current state-of-the-art
techniques, particularly for non-English language pairs, achieving gains of at
least 5 percentage points of accuracy.

</details>


### [43] [LEGO-Eval: Towards Fine-Grained Evaluation on Synthesizing 3D Embodied Environments with Tool Augmentation](https://arxiv.org/abs/2511.03001)
*Gyeom Hwangbo,Hyungjoo Chae,Minseok Kang,Hyeonjong Ju,Soohyun Oh,Jinyoung Yeo*

Main category: cs.CL

TL;DR: LEGO-Eval improves 3D scene-instruction alignment assessment, outperforming VLMs by 0.41 F1, but current methods succeed only 10% of the time in matching fine-grained instructions.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-generated 3D scenes lack realism due to coarse-grained instructions, hindering embodied agent training.

Method: Introduces LEGO-Eval, a framework with diverse tools for grounding scene components, and LEGO-Bench, a benchmark of detailed instructions.

Result: LEGO-Eval surpasses VLM-as-a-judge by 0.41 F1; current methods achieve â‰¤10% success in aligning scenes with fine-grained instructions.

Conclusion: Fine-grained instructions and better evaluation are crucial for realistic 3D scene synthesis and effective agent training.

Abstract: Despite recent progress in using Large Language Models (LLMs) for
automatically generating 3D scenes, generated scenes often lack realistic
spatial layouts and object attributes found in real-world environments. As this
problem stems from insufficiently detailed, coarse-grained instructions,
advancing 3D scene synthesis guided by more detailed, fine-grained instructions
that reflect real-world environments becomes crucial. Without such realistic
scenes, training embodied agents in unrealistic environments can lead them to
learn priors that diverge significantly from real-world physics and semantics,
degrading their performance when deployed. Thus, verifying the alignment
between the fine-grained instruction and the generated scene is essential for
effective learning. However, current evaluation methods, such as CLIPScore and
vision-language models (VLMs), often fail to reliably assess such alignment.
This shortcoming arises primarily from their shallow understanding of 3D
scenes, which often leads to improperly grounded scene components. To address
this, we introduce LEGO-Eval, an evaluation framework equipped with diverse
tools designed to explicitly ground scene components, enabling more accurate
alignment assessments. We also present LEGO-Bench, a benchmark of detailed
instructions that specify complex layouts and attributes of real-world
environments. Experiments demonstrate that LEGO-Eval outperforms VLM-as-a-judge
by 0.41 F1 score in assessing scene-instruction alignment. Benchmarking with
LEGO-Bench reveals significant limitations in current generation methods.
Across all evaluated approaches, success rates reached at most 10% in
generating scenes that fully align with fine-grained instructions.

</details>


### [44] [Targeted Error Correction in Knowledge Distillation: Small Language Models Surpass GPT](https://arxiv.org/abs/2511.03005)
*Hee-Jin Lee,Zhen Guo,Luchao Jin,Morteza Moazami Goudarzi*

Main category: cs.CL

TL;DR: The ARF pipeline helps smaller open-source LLMs outperform larger proprietary models in summarization by analyzing errors, revising with a compact editor, and fine-tuning a smaller model.


<details>
  <summary>Details</summary>
Motivation: To enhance the performance of smaller open-source language models while improving cost efficiency and data privacy.

Method: The ARF pipeline involves analyzing errors in teacher model summaries, revising them using a compact editor, and fine-tuning a smaller student model on refined data.

Result: The smaller student model outperformed GPT-3.5 in summarization tasks, demonstrating cost efficiency and privacy benefits.

Conclusion: The ARF pipeline provides a scalable framework to boost open-source LLMs, making them competitive with larger proprietary models in downstream tasks.

Abstract: We introduce an Analyze-Revise-Finetune (ARF) pipeline that enables smaller
open-source language models (LLMs) to surpass substantially larger proprietary
models in customer service summarization tasks. The pipeline first analyzes and
categorizes common errors in summaries produced by a teacher model (GPT-3.5),
then performs a targeted revision using a compact editor model (Llama 3.1 70B)
to generate high-quality, refined training data. Fine-tuning a smaller student
model (Llama 3.1 8B) on this refined data resulted in superior summarization
performance compared to GPT-3.5. The ARF pipeline improves cost efficiency and
data privacy while maintaining competitive accuracy, illustrating a
generalizable framework for enhancing open-source LLMs across diverse
downstream applications.

</details>


### [45] [Data-Efficient Adaptation and a Novel Evaluation Method for Aspect-based Sentiment Analysis](https://arxiv.org/abs/2511.03034)
*Yan Cathy Hua,Paul Denny,JÃ¶rg Wicker,Katerina TaÅ¡kova*

Main category: cs.CL

TL;DR: The paper addresses gaps in Aspect-based Sentiment Analysis (ABSA) by introducing a flexible evaluation method, exploring small generative models for low-resource domains, and releasing education review ABSA resources.


<details>
  <summary>Details</summary>
Motivation: ABSA research is concentrated in commercial domains, neglecting high-demand areas like education and healthcare due to domain adaptation challenges and rigid evaluation methods.

Method: Proposes FTS-OBP for flexible evaluation, studies small decoder-only generative models (SLMs) in education ABSA, and explores data-free and data-light fine-tuning methods.

Result: SLMs (1.5-3.8B parameters) outperform large proprietary models with minimal data, and multitask fine-tuning enhances performance significantly.

Conclusion: The work bridges ABSA gaps with innovative evaluation, efficient models for low-resource domains, and publicly available education review resources.

Abstract: Aspect-based Sentiment Analysis (ABSA) is a fine-grained opinion mining
approach that identifies and classifies opinions associated with specific
entities (aspects) or their categories within a sentence. Despite its rapid
growth and broad potential, ABSA research and resources remain concentrated in
commercial domains, leaving analytical needs unmet in high-demand yet
low-resource areas such as education and healthcare. Domain adaptation
challenges and most existing methods' reliance on resource-intensive
in-training knowledge injection further hinder progress in these areas.
Moreover, traditional evaluation methods based on exact matches are overly
rigid for ABSA tasks, penalising any boundary variations which may misrepresent
the performance of generative models. This work addresses these gaps through
three contributions: 1) We propose a novel evaluation method, Flexible Text
Similarity Matching and Optimal Bipartite Pairing (FTS-OBP), which accommodates
realistic extraction boundary variations while maintaining strong correlation
with traditional metrics and offering fine-grained diagnostics. 2) We present
the first ABSA study of small decoder-only generative language models (SLMs;
<7B parameters), examining resource lower bounds via a case study in education
review ABSA. We systematically explore data-free (in-context learning and
weight merging) and data-light fine-tuning methods, and propose a multitask
fine-tuning strategy that significantly enhances SLM performance, enabling
1.5-3.8 B models to surpass proprietary large models and approach benchmark
results with only 200-1,000 examples on a single GPU. 3) We release the first
public set of education review ABSA resources to support future research in
low-resource domains.

</details>


### [46] [ROBoto2: An Interactive System and Dataset for LLM-assisted Clinical Trial Risk of Bias Assessment](https://arxiv.org/abs/2511.03048)
*Anthony Hevia,Sanjana Chintalapati,Veronica Ka Wai Lai,Thanh Tam Nguyen,Wai-Tat Wong,Terry Klassen,Lucy Lu Wang*

Main category: cs.CL

TL;DR: ROBOTO2 is a web-based platform using LLMs to streamline risk of bias assessments in clinical trials, offering interactive features for annotation and human review.


<details>
  <summary>Details</summary>
Motivation: To automate and simplify the traditionally labor-intensive risk of bias (ROB2) assessment process in clinical trials.

Method: Combines PDF parsing, retrieval-augmented LLM prompting, and human-in-the-loop review for ROB2 annotation of clinical trial reports.

Result: Developed ROBOTO2 platform and released a dataset of 521 pediatric clinical trial reports, benchmarking ROB2 performance for 4 LLMs.

Conclusion: ROBOTO2 effectively assists in ROB2 assessments but highlights challenges in fully automating systematic reviews.

Abstract: We present ROBOTO2, an open-source, web-based platform for large language
model (LLM)-assisted risk of bias (ROB) assessment of clinical trials. ROBOTO2
streamlines the traditionally labor-intensive ROB v2 (ROB2) annotation process
via an interactive interface that combines PDF parsing, retrieval-augmented LLM
prompting, and human-in-the-loop review. Users can upload clinical trial
reports, receive preliminary answers and supporting evidence for ROB2 signaling
questions, and provide real-time feedback or corrections to system suggestions.
ROBOTO2 is publicly available at https://roboto2.vercel.app/, with code and
data released to foster reproducibility and adoption. We construct and release
a dataset of 521 pediatric clinical trial reports (8954 signaling questions
with 1202 evidence passages), annotated using both manually and LLM-assisted
methods, serving as a benchmark and enabling future research. Using this
dataset, we benchmark ROB2 performance for 4 LLMs and provide an analysis into
current model capabilities and ongoing challenges in automating this critical
aspect of systematic review.

</details>


### [47] [Reading Between the Lines: The One-Sided Conversation Problem](https://arxiv.org/abs/2511.03056)
*Victoria Ebert,Rishabh Singh,Tuochao Chen,Noah A. Smith,Shyamnath Gollakota*

Main category: cs.CL

TL;DR: The paper addresses the one-sided conversation problem (1SC), focusing on reconstructing missing dialogue turns and generating summaries from partial transcripts, showing promising results with large models and privacy-aware techniques.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by real-world scenarios like telemedicine and call centers where only one side of a conversation is recorded, limiting AI applications.

Method: The authors formalize 1SC and evaluate tasks using prompting and finetuned models on datasets like MultiWOZ and DailyDialog, employing A/B testing and LLM-as-a-judge metrics.

Result: Access to future turns and utterance length improves reconstruction; placeholder prompting reduces hallucination. Large models perform well with prompts, while smaller ones need finetuning. Summaries can be generated without full reconstruction.

Conclusion: The paper introduces 1SC as a novel challenge and demonstrates progress toward privacy-aware AI solutions.

Abstract: Conversational AI is constrained in many real-world settings where only one
side of a dialogue can be recorded, such as telemedicine, call centers, and
smart glasses. We formalize this as the one-sided conversation problem (1SC):
inferring and learning from one side of a conversation. We study two tasks: (1)
reconstructing the missing speaker's turns for real-time use cases, and (2)
generating summaries from one-sided transcripts. Evaluating prompting and
finetuned models on MultiWOZ, DailyDialog, and Candor with both human A/B
testing and LLM-as-a-judge metrics, we find that access to one future turn and
information about utterance length improves reconstruction, placeholder
prompting helps to mitigate hallucination, and while large models generate
promising reconstructions with prompting, smaller models require finetuning.
Further, high-quality summaries can be generated without reconstructing missing
turns. We present 1SC as a novel challenge and report promising results that
mark a step toward privacy-aware conversational AI.

</details>


### [48] [PolyNorm: Few-Shot LLM-Based Text Normalization for Text-to-Speech](https://arxiv.org/abs/2511.03080)
*Michel Wong,Ali Alshehri,Sophia Kao,Haotian He*

Main category: cs.CL

TL;DR: PolyNorm is a prompt-based TN approach using LLMs, reducing manual rules and enhancing linguistic scalability. It includes a language-agnostic pipeline for data curation and evaluation, showing WER improvements across eight languages.


<details>
  <summary>Details</summary>
Motivation: Traditional TN systems are accurate but require extensive engineering, lack scalability, and struggle with low-resource languages. PolyNorm aims to address these limitations.

Method: PolyNorm leverages LLMs for TN, minimizing manual rules. It introduces a language-agnostic pipeline for automated data curation and evaluation.

Result: Experiments in eight languages demonstrate consistent WER reductions compared to production-grade systems.

Conclusion: PolyNorm offers a scalable, multilingual TN solution with improved accuracy and reduced engineering effort, supported by a released benchmark dataset.

Abstract: Text Normalization (TN) is a key preprocessing step in Text-to-Speech (TTS)
systems, converting written forms into their canonical spoken equivalents.
Traditional TN systems can exhibit high accuracy, but involve substantial
engineering effort, are difficult to scale, and pose challenges to language
coverage, particularly in low-resource settings. We propose PolyNorm, a
prompt-based approach to TN using Large Language Models (LLMs), aiming to
reduce the reliance on manually crafted rules and enable broader linguistic
applicability with minimal human intervention. Additionally, we present a
language-agnostic pipeline for automatic data curation and evaluation, designed
to facilitate scalable experimentation across diverse languages. Experiments
across eight languages show consistent reductions in the word error rate (WER)
compared to a production-grade-based system. To support further research, we
release PolyNorm-Benchmark, a multilingual data set covering a diverse range of
text normalization phenomena.

</details>


### [49] [A Computational Approach to Analyzing Disrupted Language in Schizophrenia: Integrating Surprisal and Coherence Measures](https://arxiv.org/abs/2511.03089)
*Gowtham Premananth,Carol Espy-Wilson*

Main category: cs.CL

TL;DR: The study uses computational linguistic measures (surprisal and semantic coherence) to analyze language disruptions in schizophrenia and their correlation with symptom severity.


<details>
  <summary>Details</summary>
Motivation: To understand how language disruptions in schizophrenia can serve as objective markers for diagnosis and symptom severity using computational tools.

Method: Computational models are used to measure surprisal and semantic coherence in language samples from subjects with schizophrenia and healthy controls.

Result: Differences in surprisal and semantic coherence were found between schizophrenia patients and controls, with these measures also varying by symptom severity.

Conclusion: Computational linguistic measures like surprisal and semantic coherence can effectively characterize language disruptions in schizophrenia and reflect symptom severity.

Abstract: Language disruptions are one of the well-known effects of schizophrenia
symptoms. They are often manifested as disorganized speech and impaired
discourse coherence. These abnormalities in spontaneous language production
reflect underlying cognitive disturbances and have the potential to serve as
objective markers for symptom severity and diagnosis of schizophrenia. This
study focuses on how these language disruptions can be characterized in terms
of two computational linguistic measures: surprisal and semantic coherence. By
computing surprisal and semantic coherence of language using computational
models, this study investigates how they differ between subjects with
schizophrenia and healthy controls. Furthermore, this study provides further
insight into how language disruptions in terms of these linguistic measures
change with varying degrees of schizophrenia symptom severity.

</details>


### [50] [CARMA: Comprehensive Automatically-annotated Reddit Mental Health Dataset for Arabic](https://arxiv.org/abs/2511.03102)
*Saad Mankarious,Ayah Zirikly*

Main category: cs.CL

TL;DR: CARMA is the first large-scale Arabic dataset for mental health detection, addressing gaps in resources and research for Arabic-speaking populations.


<details>
  <summary>Details</summary>
Motivation: Early detection of mental health disorders is challenging for Arabic-speaking populations due to limited resources and cultural stigma. Existing research focuses mainly on English, leaving Arabic underexplored.

Method: CARMA is an automatically annotated dataset of Arabic Reddit posts covering six mental health conditions and a control group. The dataset is analyzed qualitatively and quantitatively, and classification experiments are conducted using various models.

Result: CARMA surpasses existing resources in scale and diversity. Classification experiments demonstrate its potential for advancing mental health detection in Arabic.

Conclusion: CARMA fills a critical gap in Arabic mental health research and shows promise for improving detection methods in underrepresented languages.

Abstract: Mental health disorders affect millions worldwide, yet early detection
remains a major challenge, particularly for Arabic-speaking populations where
resources are limited and mental health discourse is often discouraged due to
cultural stigma. While substantial research has focused on English-language
mental health detection, Arabic remains significantly underexplored, partly due
to the scarcity of annotated datasets. We present CARMA, the first
automatically annotated large-scale dataset of Arabic Reddit posts. The dataset
encompasses six mental health conditions, such as Anxiety, Autism, and
Depression, and a control group. CARMA surpasses existing resources in both
scale and diversity. We conduct qualitative and quantitative analyses of
lexical and semantic differences between users, providing insights into the
linguistic markers of specific mental health conditions. To demonstrate the
dataset's potential for further mental health analysis, we perform
classification experiments using a range of models, from shallow classifiers to
large language models. Our results highlight the promise of advancing mental
health detection in underrepresented languages such as Arabic.

</details>


### [51] [Control Barrier Function for Aligning Large Language Models](https://arxiv.org/abs/2511.03121)
*Yuya Miyaoka,Masaki Inoue*

Main category: cs.CL

TL;DR: A control-based framework using CBF ensures safe text generation in LLMs without fine-tuning, leveraging an add-on safety filter.


<details>
  <summary>Details</summary>
Motivation: To align LLM outputs with user-desirable text without altering the base model.

Method: Uses a CBF safety filter on predicted tokens from the baseline LLM to intervene in text generation.

Result: Implemented with open-source models to generate positive text successfully.

Conclusion: The framework provides a flexible, add-on solution for aligning LLM outputs safely.

Abstract: This paper proposes a control-based framework for aligning large language
models (LLMs) by leveraging a control barrier function (CBF) to ensure
user-desirable text generation. The presented framework applies the CBF safety
filter to the predicted token generated from the baseline LLM, to intervene in
the generated text. The safety filter includes two significant advantages: this
safety filter is an add-on type, allowing it to be used for alignment purposes
without fine-tuning the baseline LLM, and if there is an evaluation model
regarding the desired alignment, it can be directly applied to the filter
design. The overall text-generation system is implemented with open-source
language models, aiming to generate positive text.

</details>


### [52] [MME-CC: A Challenging Multi-Modal Evaluation Benchmark of Cognitive Capacity](https://arxiv.org/abs/2511.03146)
*Kaiyuan Zhang,Chenghao Yang,Zhoufutu Wen,Sihang Yuan,Qiuyue Wang,Chaoyi Huang,Guosheng Zhu,He Wang,Huawenyu Lu,Jianing Wen,Jianpeng Jiao,Lishu Luo,Longxiang Liu,Sijin Wu,Xiaolei Zhu,Xuanliang Zhang,Ge Zhang,Yi Lin,Guang Shi,Chaoyou Fu,Wenhao Huang*

Main category: cs.CL

TL;DR: The paper introduces MME-CC, a vision-grounded benchmark to evaluate multimodal models' cognitive capacity across spatial, geometric, and knowledge-based reasoning tasks, revealing gaps in performance and common errors.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal benchmarks lack systematic assessment of vision-centric cognitive behaviors, prompting the need for a focused evaluation framework like MME-CC.

Method: MME-CC organizes 11 reasoning tasks into three visual information categories and evaluates 16 multimodal models, analyzing their performance and identifying error patterns.

Result: Closed-source models outperform others (e.g., Gemini-2.5-Pro scores 42.66 vs. GLM-4.5V's 30.45), but spatial and geometric reasoning remain weak (â‰¤30%). Common errors include orientation mistakes and poor counterfactual adherence.

Conclusion: The study highlights the need to prioritize cognitive capacity in multimodal model evaluation and design, advocating for MME-CC as a catalyst for this shift.

Abstract: As reasoning models scale rapidly, the essential role of multimodality in
human cognition has come into sharp relief, driving a growing need to probe
vision-centric cognitive behaviors. Yet, existing multimodal benchmarks either
overemphasize textual reasoning or fall short of systematically capturing
vision-centric cognitive behaviors, leaving the cognitive capacity of MLLMs
insufficiently assessed. To address this limitation, we introduce MME-CC
(Multi-Modal Evaluation benchmark of Cognitive Capacity), a vision-grounded
benchmark that organizes 11 representative reasoning tasks into three
fundamental categories of visual information: spatial, geometric, and
knowledge-based reasoning, and provides fine-grained analyses of MLLMs'
cognitive capacity across these dimensions. Based on MME-CC, we conduct
extensive experiments over 16 representative MLLMs. Our study reveals that
closed-source models currently lead overall (e.g., 42.66 for Gemini-2.5-Pro vs.
30.45 for GLM-4.5V), while spatial and geometric reasoning remain broadly weak
(less than or equal to 30%). We further identify common error patterns,
including orientation mistakes, fragile cross-view identity persistence, and
poor adherence to counterfactual instructions, and observe that
Chain-of-Thought typically follows a three-stage process (extract -> reason ->
verify) with heavy reliance on visual extraction. We hope this work catalyzes a
shift toward treating the cognitive capacity of MLLMs as central to both
evaluation and model design.

</details>


### [53] [Who Sees the Risk? Stakeholder Conflicts and Explanatory Policies in LLM-based Risk Assessment](https://arxiv.org/abs/2511.03152)
*Srishti Yadav,Jasmina Gajcin,Erik Miehling,Elizabeth Daly*

Main category: cs.CL

TL;DR: A framework for stakeholder-grounded AI risk assessment using LLMs to predict, explain, and visualize stakeholder-specific risk perceptions and conflicts.


<details>
  <summary>Details</summary>
Motivation: To enhance transparency and interpretability in AI risk assessments by understanding diverse stakeholder perspectives.

Method: Employs LLMs as judges with Risk Atlas Nexus and GloVE for stakeholder-specific, interpretable risk policies and interactive conflict visualization.

Result: Stakeholder perspectives significantly influence risk perception and conflict patterns, highlighting the need for stakeholder-aware explanations.

Conclusion: The framework improves transparency and alignment with human-centered AI governance by addressing stakeholder-specific risk perceptions and conflicts.

Abstract: Understanding how different stakeholders perceive risks in AI systems is
essential for their responsible deployment. This paper presents a framework for
stakeholder-grounded risk assessment by using LLMs, acting as judges to predict
and explain risks. Using the Risk Atlas Nexus and GloVE explanation method, our
framework generates stakeholder-specific, interpretable policies that shows how
different stakeholders agree or disagree about the same risks. We demonstrate
our method using three real-world AI use cases of medical AI, autonomous
vehicles, and fraud detection domain. We further propose an interactive
visualization that reveals how and why conflicts emerge across stakeholder
perspectives, enhancing transparency in conflict reasoning. Our results show
that stakeholder perspectives significantly influence risk perception and
conflict patterns. Our work emphasizes the importance of these
stakeholder-aware explanations needed to make LLM-based evaluations more
transparent, interpretable, and aligned with human-centered AI governance
goals.

</details>


### [54] [Measuring Aleatoric and Epistemic Uncertainty in LLMs: Empirical Evaluation on ID and OOD QA Tasks](https://arxiv.org/abs/2511.03166)
*Kevin Wang,Subre Abdoul Moktar,Jia Li,Kangshuo Li,Feng Chen*

Main category: cs.CL

TL;DR: A study evaluates Uncertainty Estimation (UE) methods in LLMs for QA tasks, finding that different methods excel in in-distribution (ID) vs. out-of-distribution (OOD) settings.


<details>
  <summary>Details</summary>
Motivation: To assess the trustworthiness of LLM outputs by examining the robustness of UE measures in handling aleatoric and epistemic uncertainty.

Method: Twelve UE methods and four generation quality metrics were tested on QA tasks using ID and OOD datasets.

Result: Information-based methods perform well in ID settings, while density-based methods and P(True) excel in OOD. Semantic consistency methods are robust across datasets.

Conclusion: UE method effectiveness varies by context; no single method is universally optimal for all uncertainty types or dataset distributions.

Abstract: Large Language Models (LLMs) have become increasingly pervasive, finding
applications across many industries and disciplines. Ensuring the
trustworthiness of LLM outputs is paramount, where Uncertainty Estimation (UE)
plays a key role. In this work, a comprehensive empirical study is conducted to
examine the robustness and effectiveness of diverse UE measures regarding
aleatoric and epistemic uncertainty in LLMs. It involves twelve different UE
methods and four generation quality metrics including LLMScore from LLM
criticizers to evaluate the uncertainty of LLM-generated answers in
Question-Answering (QA) tasks on both in-distribution (ID) and
out-of-distribution (OOD) datasets. Our analysis reveals that information-based
methods, which leverage token and sequence probabilities, perform exceptionally
well in ID settings due to their alignment with the model's understanding of
the data. Conversely, density-based methods and the P(True) metric exhibit
superior performance in OOD contexts, highlighting their effectiveness in
capturing the model's epistemic uncertainty. Semantic consistency methods,
which assess variability in generated answers, show reliable performance across
different datasets and generation metrics. These methods generally perform well
but may not be optimal for every situation.

</details>


### [55] [BengaliMoralBench: A Benchmark for Auditing Moral Reasoning in Large Language Models within Bengali Language and Culture](https://arxiv.org/abs/2511.03180)
*Shahriyar Zaman Ridoy,Azmine Toushik Wasi,Koushik Ahamed Tonmoy*

Main category: cs.CL

TL;DR: The paper introduces BengaliMoralBench, the first ethics benchmark for Bengali, addressing gaps in multilingual LLMs' alignment with local ethical norms. It evaluates LLMs using native-speaker consensus and highlights cultural and ethical shortcomings.


<details>
  <summary>Details</summary>
Motivation: Current ethics benchmarks for LLMs are English-centric, lacking cultural relevance for Bengali, a widely spoken language. This gap hinders responsible AI deployment in South Asia.

Method: Developed BengaliMoralBench, covering five moral domains and 50 subtopics, annotated via native-speaker consensus using three ethical frameworks. Evaluated multilingual LLMs (Llama, Gemma, Qwen, DeepSeek) in zero-shot settings with unified prompts.

Result: LLM performance varied (50-91% accuracy), showing weaknesses in cultural grounding, commonsense reasoning, and moral fairness.

Conclusion: BengaliMoralBench enables culturally aligned evaluation, supporting ethical AI localization in low-resource multilingual contexts like Bangladesh.

Abstract: As multilingual Large Language Models (LLMs) gain traction across South Asia,
their alignment with local ethical norms, particularly for Bengali, which is
spoken by over 285 million people and ranked 6th globally, remains
underexplored. Existing ethics benchmarks are largely English-centric and
shaped by Western frameworks, overlooking cultural nuances critical for
real-world deployment. To address this, we introduce BengaliMoralBench, the
first large-scale ethics benchmark for the Bengali language and socio-cultural
contexts. It covers five moral domains, Daily Activities, Habits, Parenting,
Family Relationships, and Religious Activities, subdivided into 50 culturally
relevant subtopics. Each scenario is annotated via native-speaker consensus
using three ethical lenses: Virtue, Commonsense, and Justice ethics. We conduct
systematic zero-shot evaluation of prominent multilingual LLMs, including
Llama, Gemma, Qwen, and DeepSeek, using a unified prompting protocol and
standard metrics. Performance varies widely (50-91% accuracy), with qualitative
analysis revealing consistent weaknesses in cultural grounding, commonsense
reasoning, and moral fairness. BengaliMoralBench provides a foundation for
responsible localization, enabling culturally aligned evaluation and supporting
the deployment of ethically robust AI in diverse, low-resource multilingual
settings such as Bangladesh.

</details>


### [56] [LGM: Enhancing Large Language Models with Conceptual Meta-Relations and Iterative Retrieval](https://arxiv.org/abs/2511.03214)
*Wenchang Lei,Ping Zou,Yue Wang,Feng Sun,Lei Zhao*

Main category: cs.CL

TL;DR: The paper introduces the Language Graph Model (LGM) to improve LLMs' handling of ambiguous or misaligned terms by extracting and validating meta-relations like inheritance, alias, and composition from natural language. It outperforms traditional RAG methods without requiring truncation of long texts.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) often struggle with ambiguous or conceptually misaligned terms in user instructions, prompting the need for a solution to enhance conceptual clarity and interpretation.

Method: The LGM extracts meta-relations (inheritance, alias, composition) from natural language, validates them via a reflection mechanism, and dynamically supplies them to LLMs using a Concept Iterative Retrieval Algorithm.

Result: Experiments show that the LGM consistently outperforms existing Retrieval-Augmented Generation (RAG) baselines on standard benchmarks.

Conclusion: The LGM addresses LLMs' limitations in handling ambiguous terms, offering a scalable solution without text truncation, and demonstrates superior performance over traditional RAG methods.

Abstract: Large language models (LLMs) exhibit strong semantic understanding, yet
struggle when user instructions involve ambiguous or conceptually misaligned
terms. We propose the Language Graph Model (LGM) to enhance conceptual clarity
by extracting meta-relations-inheritance, alias, and composition-from natural
language. The model further employs a reflection mechanism to validate these
meta-relations. Leveraging a Concept Iterative Retrieval Algorithm, these
relations and related descriptions are dynamically supplied to the LLM,
improving its ability to interpret concepts and generate accurate responses.
Unlike conventional Retrieval-Augmented Generation (RAG) approaches that rely
on extended context windows, our method enables large language models to
process texts of any length without the need for truncation. Experiments on
standard benchmarks demonstrate that the LGM consistently outperforms existing
RAG baselines.

</details>


### [57] [Hybrid Fact-Checking that Integrates Knowledge Graphs, Large Language Models, and Search-Based Retrieval Agents Improves Interpretable Claim Verification](https://arxiv.org/abs/2511.03217)
*Shaghayegh Kolli,Richard Rosenbaum,Timo Cavelius,Lasse Strothe,Andrii Lata,Jana Diesner*

Main category: cs.CL

TL;DR: The paper introduces a hybrid fact-checking system combining LLMs and knowledge graphs, achieving high accuracy and addressing limitations like insufficient information cases.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of LLMs (lack of grounding) and knowledge graphs (limited coverage/latency) by integrating them for reliable fact-checking.

Method: The system uses KG retrieval, LM-based classification, and web search agents in three steps for comprehensive fact-checking.

Result: Achieves an F1 score of 0.93 on FEVER benchmark and uncovers evidence for NEI claims through reannotation.

Conclusion: A modular, open-source pipeline with fallback strategies and dataset generalization is presented.

Abstract: Large language models (LLMs) excel in generating fluent utterances but can
lack reliable grounding in verified information. At the same time,
knowledge-graph-based fact-checkers deliver precise and interpretable evidence,
yet suffer from limited coverage or latency. By integrating LLMs with knowledge
graphs and real-time search agents, we introduce a hybrid fact-checking
approach that leverages the individual strengths of each component. Our system
comprises three autonomous steps: 1) a Knowledge Graph (KG) Retrieval for rapid
one - hop lookups in DBpedia, 2) an LM-based classification guided by a
task-specific labeling prompt, producing outputs with internal rule-based
logic, and 3) a Web Search Agent invoked only when KG coverage is insufficient.
Our pipeline achieves an F1 score of 0.93 on the FEVER benchmark on the
Supported/Refuted split without task- specific fine - tuning. To address Not
enough information cases, we conduct a targeted reannotation study showing that
our approach frequently uncovers valid evidence for claims originally labeled
as Not Enough Information (NEI), as confirmed by both expert annotators and LLM
reviewers. With this paper, we present a modular, opensource fact-checking
pipeline with fallback strategies and generalization across datasets.

</details>


### [58] [Beyond Ranked Lists: The SARAL Framework for Cross-Lingual Document Set Retrieval](https://arxiv.org/abs/2511.03228)
*Shantanu Agarwal,Joel Barry,Elizabeth Boschee,Scott Miller*

Main category: cs.CL

TL;DR: SARAL's approach for MATERIAL advanced cross-lingual information retrieval by focusing on retrieving a query-relevant document set, outperforming other teams in most evaluations.


<details>
  <summary>Details</summary>
Motivation: To improve cross-lingual information retrieval (CLIR) by developing methods that retrieve relevant document sets, not just ranked lists.

Method: SARAL developed a novel approach emphasizing summarization and domain-adaptive retrieval across languages.

Result: SARAL outperformed other teams in five out of six evaluation conditions across Farsi, Kazakh, and Georgian languages.

Conclusion: The SARAL approach demonstrated effectiveness in CLIR, particularly in retrieving query-relevant document sets.

Abstract: Machine Translation for English Retrieval of Information in Any Language
(MATERIAL) is an IARPA initiative targeted to advance the state of
cross-lingual information retrieval (CLIR). This report provides a detailed
description of Information Sciences Institute's (ISI's) Summarization and
domain-Adaptive Retrieval Across Language's (SARAL's) effort for MATERIAL.
Specifically, we outline our team's novel approach to handle CLIR with emphasis
in developing an approach amenable to retrieve a query-relevant document
\textit{set}, and not just a ranked document-list. In MATERIAL's Phase-3
evaluations, SARAL exceeded the performance of other teams in five out of six
evaluation conditions spanning three different languages (Farsi, Kazakh, and
Georgian).

</details>


### [59] [IndicSuperTokenizer: An Optimized Tokenizer for Indic Multilingual LLMs](https://arxiv.org/abs/2511.03237)
*Souvik Rana,Arul Menezes,Ashish Kulkarni,Chandra Khatri,Shubham Agarwal*

Main category: cs.CL

TL;DR: The paper introduces IndicSuperTokenizer, a tokenizer for multilingual LLMs that combines subword and multi-word tokenization for better linguistic alignment and performance.


<details>
  <summary>Details</summary>
Motivation: Tokenizers are critical for LLM performance, but designing effective multilingual tokenizers is challenging due to script diversity and morphological variation.

Method: IndicSuperTokenizer uses subword and multi-word tokenization with language-specific pre-tokenization.

Result: It achieves a 39.5% improvement in fertility score over LLaMA4 and 18% over Sutra, with a 44% boost in inference throughput.

Conclusion: The tokenizer's design choices, validated through extensive ablations, offer robust performance improvements for multilingual LLMs.

Abstract: Tokenizers play a crucial role in determining the performance, training
efficiency, and the inference cost of Large Language Models (LLMs). Designing
effective tokenizers for multilingual LLMs is particularly challenging due to
diverse scripts and rich morphological variation. While subword methods such as
Byte Pair Encoding (BPE) are widely adopted, their effectiveness in
multilingual settings remains underexplored. We present IndicSuperTokenizer, a
tokenizer for Indic multilingual LLMs, that combines both subword and
multi-word tokenization, along with language-specific pre-tokenization, leading
to more linguistically aligned tokens and achieving a new state-of-the-art in
fertility score. Evaluated across English, 22 Indian languages and code data,
our tokenizer improves the average fertility score by 39.5% over LLaMA4 and by
18% over Sutra (the current best). This translates to 44% improvement in
inference throughput over LLaMA4 while maintaining comparable performance on
English and Indic benchmarks. We also present detailed ablations across
tokenizer training data size, vocabulary size, merging techniques, and
pre-tokenization strategies, demonstrating the robustness of our design
choices.

</details>


### [60] [Comparing the Performance of LLMs in RAG-based Question-Answering: A Case Study in Computer Science Literature](https://arxiv.org/abs/2511.03261)
*Ranul Dayarathne,Uvini Ranaweera,Upeksha Ganegoda*

Main category: cs.CL

TL;DR: The study compares the performance of four open-source LLMs and GPT-3.5 in QA tasks using RAG, highlighting GPT-3.5's superiority and Mistral-7b-instruct's lead among open-source models.


<details>
  <summary>Details</summary>
Motivation: To compare the performance of different LLMs in QA tasks across diverse domains using RAG to reduce hallucination and enhance accuracy.

Method: The study evaluates four open-source LLMs and GPT-3.5 over QA tasks in computer science literature using RAG. Metrics include accuracy, precision, human and AI-based rankings, and cosine similarity.

Result: GPT-3.5 performs best with RAG, while Mistral-7b-instruct leads among open-source LLMs. Orca-mini-v3-7b has the lowest latency, and LLaMa2-7b-chat the highest.

Conclusion: Open-source LLMs can compete with proprietary models like GPT-3.5 when supported by better infrastructure, with Mistral-7b-instruct showing strong performance.

Abstract: Retrieval Augmented Generation (RAG) is emerging as a powerful technique to
enhance the capabilities of Generative AI models by reducing hallucination.
Thus, the increasing prominence of RAG alongside Large Language Models (LLMs)
has sparked interest in comparing the performance of different LLMs in
question-answering (QA) in diverse domains. This study compares the performance
of four open-source LLMs, Mistral-7b-instruct, LLaMa2-7b-chat,
Falcon-7b-instruct and Orca-mini-v3-7b, and OpenAI's trending GPT-3.5 over QA
tasks within the computer science literature leveraging RAG support. Evaluation
metrics employed in the study include accuracy and precision for binary
questions and ranking by a human expert, ranking by Google's AI model Gemini,
alongside cosine similarity for long-answer questions. GPT-3.5, when paired
with RAG, effectively answers binary and long-answer questions, reaffirming its
status as an advanced LLM. Regarding open-source LLMs, Mistral AI's
Mistral-7b-instruct paired with RAG surpasses the rest in answering both binary
and long-answer questions. However, among the open-source LLMs, Orca-mini-v3-7b
reports the shortest average latency in generating responses, whereas
LLaMa2-7b-chat by Meta reports the highest average latency. This research
underscores the fact that open-source LLMs, too, can go hand in hand with
proprietary models like GPT-3.5 with better infrastructure.

</details>


### [61] [SCALE: Upscaled Continual Learning of Large Language Models](https://arxiv.org/abs/2511.03270)
*Jin-woo Lee,Junhwa Choi,Bongkyu Hwang,Jinho Choo,Bogun Kim,JeongSeon Yi,Joonseok Lee,DongYoung Jung,Jaeseon Park,Kyoungwon Park,Suk-hoon Jung*

Main category: cs.CL

TL;DR: SCALE introduces a width upscaling architecture for continual pre-training of large language models, focusing on persistent preservation and collaborative adaptation to balance stability and plasticity.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of continual pre-training for large language models, where progress depends on scalable structures rather than just parameter scaling. It seeks to mitigate forgetting while acquiring new knowledge.

Method: SCALE inserts lightweight expansion into linear modules, freezing pre-trained parameters to preserve original functionality. It uses Persistent Preservation (maintaining base model behavior) and Collaborative Adaptation (selective training of expansion components). Variants include SCALE-Preserve, SCALE-Adapt, and SCALE-Route for token-level routing.

Result: SCALE reduces severe forgetting compared to depth expansion and achieves competitive gains in continual pre-training, especially on a Korean corpus while maintaining English performance. It provides a balanced stability-plasticity trade-off.

Conclusion: SCALE offers a scalable and effective solution for continual pre-training, with analysis showing provable preservation and stabilized optimization through the interplay of preservation and adaptation.

Abstract: We revisit continual pre-training for large language models and argue that
progress now depends more on scaling the right structure than on scaling
parameters alone. We introduce SCALE, a width upscaling architecture that
inserts lightweight expansion into linear modules while freezing all
pre-trained parameters. This preserves the residual and attention topologies
and increases capacity without perturbing the base model's original
functionality. SCALE is guided by two principles: Persistent Preservation,
which maintains the base model's behavior via preservation-oriented
initialization and freezing of the pre-trained weights, and Collaborative
Adaptation, which selectively trains a subset of expansion components to
acquire new knowledge with minimal interference. We instantiate these ideas as
SCALE-Preserve (preservation-first), SCALE-Adapt (adaptation-first), and
SCALE-Route, an optional routing extension that performs token-level routing
between preservation and adaptation heads. On a controlled synthetic biography
benchmark, SCALE mitigates the severe forgetting observed with depth expansion
while still acquiring new knowledge. In continual pre-training on a Korean
corpus, SCALE variants achieve less forgetting on English evaluations and
competitive gains on Korean benchmarks, with these variants offering the best
overall stability-plasticity trade-off. Accompanying analysis clarifies when
preservation provably holds and why the interplay between preservation and
adaptation stabilizes optimization compared to standard continual learning
setups.

</details>


### [62] [How to Evaluate Speech Translation with Source-Aware Neural MT Metrics](https://arxiv.org/abs/2511.03295)
*Mauro Cettolo,Marco Gaido,Matteo Negri,Sara Papi,Luisa Bentivogli*

Main category: cs.CL

TL;DR: The paper proposes source-aware metrics for speech-to-text translation (ST) evaluation, using ASR transcripts and back-translations as synthetic sources, and introduces a cross-lingual re-segmentation algorithm to improve alignment.


<details>
  <summary>Details</summary>
Motivation: Current ST evaluation methods rely on reference translations, ignoring source audio information. The paper aims to address this by incorporating source-aware metrics, inspired by progress in machine translation (MT).

Method: Two strategies are explored: using ASR transcripts and back-translations of reference translations as synthetic sources. A novel cross-lingual re-segmentation algorithm is introduced to align synthetic sources with references.

Result: ASR transcripts are more reliable than back-translations when word error rate is below 20%. Back-translations offer a computationally cheaper alternative. The re-segmentation algorithm enhances the robustness of source-aware metrics.

Conclusion: The study demonstrates the feasibility of source-aware metrics for ST evaluation, improving accuracy and paving the way for more principled methodologies.

Abstract: Automatic evaluation of speech-to-text translation (ST) systems is typically
performed by comparing translation hypotheses with one or more reference
translations. While effective to some extent, this approach inherits the
limitation of reference-based evaluation that ignores valuable information from
the source input. In machine translation (MT), recent progress has shown that
neural metrics incorporating the source text achieve stronger correlation with
human judgments. Extending this idea to ST, however, is not trivial because the
source is audio rather than text, and reliable transcripts or alignments
between source and references are often unavailable. In this work, we conduct
the first systematic study of source-aware metrics for ST, with a particular
focus on real-world operating conditions where source transcripts are not
available. We explore two complementary strategies for generating textual
proxies of the input audio, automatic speech recognition (ASR) transcripts, and
back-translations of the reference translation, and introduce a novel two-step
cross-lingual re-segmentation algorithm to address the alignment mismatch
between synthetic sources and reference translations. Our experiments, carried
out on two ST benchmarks covering 79 language pairs and six ST systems with
diverse architectures and performance levels, show that ASR transcripts
constitute a more reliable synthetic source than back-translations when word
error rate is below 20%, while back-translations always represent a
computationally cheaper but still effective alternative. Furthermore, our
cross-lingual re-segmentation algorithm enables robust use of source-aware MT
metrics in ST evaluation, paving the way toward more accurate and principled
evaluation methodologies for speech translation.

</details>


### [63] [Benchmarking the Thinking Mode of Multimodal Large Language Models in Clinical Tasks](https://arxiv.org/abs/2511.03328)
*Jindong Hong,Tianjie Chen,Lingjie Luo,Chuanyang Zheng,Ting Xu,Haibao Yu,Jianing Qiu,Qianzhong Chen,Suning Huang,Yan Xu,Yong Gui,Yijun He,Jiankai Sun*

Main category: cs.CL

TL;DR: The paper evaluates the impact of "reasoning MLLMs" on clinical tasks, finding minimal performance improvement in the thinking mode compared to the standard mode, especially for complex medical tasks.


<details>
  <summary>Details</summary>
Motivation: The study aims to assess how the enhanced reasoning processes of dual-state MLLMs affect their performance and reliability in clinical applications.

Method: The active "thinking mode" of Seed1.5-VL and Gemini-2.5-Flash is evaluated on four visual medical tasks using VQA-RAD and ROCOv2 datasets.

Result: Activating the thinking mode provides marginal improvements over the standard mode for most tasks, with suboptimal performance on complex medical tasks like open-ended VQA and medical image interpretation.

Conclusion: The results highlight the need for domain-specific medical data and more advanced methods for integrating medical knowledge into MLLMs.

Abstract: A recent advancement in Multimodal Large Language Models (MLLMs) research is
the emergence of "reasoning MLLMs" that offer explicit control over their
internal thinking processes (normally referred as the "thinking mode")
alongside the standard "non-thinking mode". This capability allows these models
to engage in a step-by-step process of internal deliberation before generating
a final response. With the rapid transition to and adoption of these
"dual-state" MLLMs, this work rigorously evaluated how the enhanced reasoning
processes of these MLLMs impact model performance and reliability in clinical
tasks. This paper evaluates the active "thinking mode" capabilities of two
leading MLLMs, Seed1.5-VL and Gemini-2.5-Flash, for medical applications. We
assessed their performance on four visual medical tasks using VQA-RAD and
ROCOv2 datasets. Our findings reveal that the improvement from activating the
thinking mode remains marginal compared to the standard non-thinking mode for
the majority of the tasks. Their performance on complex medical tasks such as
open-ended VQA and medical image interpretation remains suboptimal,
highlighting the need for domain-specific medical data and more advanced
methods for medical knowledge integration.

</details>


### [64] [Generative Artificial Intelligence in Bioinformatics: A Systematic Review of Models, Applications, and Methodological Advances](https://arxiv.org/abs/2511.03354)
*Riasad Alvi,Sayeem Been Zaman,Wasimul Karim,Arefin Ittesafun Abian,Mohaimenul Azam Khan Raiaan,Saddam Mukta,Md Rafi Ur Rashid,Md Rafiqul Islam,Yakub Sebastian,Sami Azam*

Main category: cs.CL

TL;DR: This review evaluates the impact of generative AI in bioinformatics, highlighting advancements in genomics, proteomics, and drug discovery, while addressing challenges like scalability and data biases.


<details>
  <summary>Details</summary>
Motivation: To systematically assess the role and advancements of generative AI in bioinformatics, identifying its impact on methodological progress, predictive performance, and specialized applications.

Method: The review adopts a structured approach with six research questions (RQs) based on systematic review and meta-analysis methods to evaluate GenAI strategies, model architectures, and datasets.

Result: GenAI outperforms traditional methods in sequence analysis, molecular design, and data integration. Specialized models excel due to targeted pretraining, while challenges include scalability and data biases.

Conclusion: Generative AI shows transformative potential in bioinformatics, with significant benefits in accuracy and integration, but future work must address scalability, biases, and biological grounding.

Abstract: Generative artificial intelligence (GenAI) has become a transformative
approach in bioinformatics that often enables advancements in genomics,
proteomics, transcriptomics, structural biology, and drug discovery. To
systematically identify and evaluate these growing developments, this review
proposed six research questions (RQs), according to the preferred reporting
items for systematic reviews and meta-analysis methods. The objective is to
evaluate impactful GenAI strategies in methodological advancement, predictive
performance, and specialization, and to identify promising approaches for
advanced modeling, data-intensive discovery, and integrative biological
analysis. RQ1 highlights diverse applications across multiple bioinformatics
subfields (sequence analysis, molecular design, and integrative data modeling),
which demonstrate superior performance over traditional methods through pattern
recognition and output generation. RQ2 reveals that adapted specialized model
architectures outperformed general-purpose models, an advantage attributed to
targeted pretraining and context-aware strategies. RQ3 identifies significant
benefits in the bioinformatics domains, focusing on molecular analysis and data
integration, which improves accuracy and reduces errors in complex analysis.
RQ4 indicates improvements in structural modeling, functional prediction, and
synthetic data generation, validated by established benchmarks. RQ5 suggests
the main constraints, such as the lack of scalability and biases in data that
impact generalizability, and proposes future directions focused on robust
evaluation and biologically grounded modeling. RQ6 examines that molecular
datasets (such as UniProtKB and ProteinNet12), cellular datasets (such as
CELLxGENE and GTEx) and textual resources (such as PubMedQA and OMIM) broadly
support the training and generalization of GenAI models.

</details>


### [65] [Silenced Biases: The Dark Side LLMs Learned to Refuse](https://arxiv.org/abs/2511.03369)
*Rom Himelstein,Amit LeVi,Brit Youngmann,Yaniv Nemcovsky,Avi Mendelson*

Main category: cs.CL

TL;DR: The paper introduces the Silenced Bias Benchmark (SBB) to uncover hidden biases in safety-aligned LLMs, which standard QA methods often miss by misinterpreting refusal responses as fair.


<details>
  <summary>Details</summary>
Motivation: Current fairness evaluation methods for LLMs overlook deeper biases masked by safety-alignment, leading to a false sense of fairness.

Method: The SBB uses activation steering to reduce model refusals in QA, enabling scalable and unbiased detection of silenced biases in LLMs.

Result: Testing on multiple LLMs reveals significant disparities between direct responses and underlying fairness issues.

Conclusion: The SBB framework provides a scalable and unbiased tool for future development of fairer models, highlighting the need to address deeper biases masked by safety-alignment.

Abstract: Safety-aligned large language models (LLMs) are becoming increasingly
widespread, especially in sensitive applications where fairness is essential
and biased outputs can cause significant harm. However, evaluating the fairness
of models is a complex challenge, and approaches that do so typically utilize
standard question-answer (QA) styled schemes. Such methods often overlook
deeper issues by interpreting the model's refusal responses as positive
fairness measurements, which creates a false sense of fairness. In this work,
we introduce the concept of silenced biases, which are unfair preferences
encoded within models' latent space and are effectively concealed by
safety-alignment. Previous approaches that considered similar indirect biases
often relied on prompt manipulation or handcrafted implicit queries, which
present limited scalability and risk contaminating the evaluation process with
additional biases. We propose the Silenced Bias Benchmark (SBB), which aims to
uncover these biases by employing activation steering to reduce model refusals
during QA. SBB supports easy expansion to new demographic groups and subjects,
presenting a fairness evaluation framework that encourages the future
development of fair models and tools beyond the masking effects of alignment
training. We demonstrate our approach over multiple LLMs, where our findings
expose an alarming distinction between models' direct responses and their
underlying fairness issues.

</details>


### [66] [EQ-Negotiator: Dynamic Emotional Personas Empower Small Language Models for Edge-Deployable Credit Negotiation](https://arxiv.org/abs/2511.03370)
*Yunbo Long,Yuhan Liu,Alexandra Brintrup*

Main category: cs.CL

TL;DR: EQ-Negotiator bridges the performance gap between small and large language models in automated negotiation by integrating emotional personas, game theory, and HMM, enabling SLMs to outperform larger models in privacy-sensitive scenarios.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) are computationally expensive and lack privacy for on-device applications, while small language models (SLMs) underperform in emotionally charged negotiations like credit recovery. This paper aims to enhance SLMs' negotiation capabilities.

Method: The paper introduces EQ-Negotiator, a framework combining game theory and a Hidden Markov Model (HMM) to track and respond to debtor emotional states dynamically, without pre-training.

Result: A 7B parameter SLM with EQ-Negotiator outperforms much larger LLMs in debt recovery and negotiation efficiency, even against adversarial strategies like cheating or threatening.

Conclusion: Strategic emotional intelligence, not model size, is key for automated negotiation. EQ-Negotiator enables ethical, privacy-preserving AI negotiators suitable for edge devices.

Abstract: The deployment of large language models (LLMs) in automated negotiation has
set a high performance benchmark, but their computational cost and data privacy
requirements render them unsuitable for many privacy-sensitive, on-device
applications such as mobile assistants, embodied AI agents or private client
interactions. While small language models (SLMs) offer a practical alternative,
they suffer from a significant performance gap compared to LLMs in playing
emotionally charged complex personas, especially for credit negotiation. This
paper introduces EQ-Negotiator, a novel framework that bridges this capability
gap using emotional personas. Its core is a reasoning system that integrates
game theory with a Hidden Markov Model(HMM) to learn and track debtor emotional
states online, without pre-training. This allows EQ-Negotiator to equip SLMs
with the strategic intelligence to counter manipulation while de-escalating
conflict and upholding ethical standards. Through extensive agent-to-agent
simulations across diverse credit negotiation scenarios, including adversarial
debtor strategies like cheating, threatening, and playing the victim, we show
that a 7B parameter language model with EQ-Negotiator achieves better debt
recovery and negotiation efficiency than baseline LLMs more than 10 times its
size. This work advances persona modeling from descriptive character profiles
to dynamic emotional architectures that operate within privacy constraints.
Besides, this paper establishes that strategic emotional intelligence, not raw
model scale, is the critical factor for success in automated negotiation,
paving the way for effective, ethical, and privacy-preserving AI negotiators
that can operate on the edge.

</details>


### [67] [LFC-DA: Logical Formula-Controlled Data Augmentation for Enhanced Logical Reasoning](https://arxiv.org/abs/2511.03372)
*Shenghao Li*

Main category: cs.CL

TL;DR: LFC-DA is introduced as a symbolic-logic-controlled pipeline to enhance logical data augmentation by reducing human annotation costs and improving interpretability and diversity compared to direct LLM generation.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of costly human annotation and uninterpretable, logically homogeneous examples generated by large language models in complex logical data augmentation.

Method: A symbolic-logic-controlled pipeline (LFC-DA) maps logical text to propositional expressions, compiles a rule library, and uses bounded state-space search to discover valid formulas, which are verbalized back into natural-language questions.

Result: Experiments on ReClor and LogiQA demonstrate significant improvements in the logical-reasoning accuracy of pretrained models.

Conclusion: LFC-DA effectively enhances logical data augmentation by ensuring diversity and logical rigor under propositional logic, outperforming direct LLM generation.

Abstract: For complex logical data augmentation, heavy reliance on human annotation is
costly, whereas direct generation with large language models yields
uninterpretable and logically homogeneous examples. To address this, we present
LFC-DA, a symbolic-logic-controlled pipeline: logical text is first mapped to
propositional expressions, a compact rule library is compiled, and a bounded
state-space search systematically discovers valid formulas that are then
verbalized back into natural-language questions, ensuring both diversity and
logical rigor under propositional logic. Experiments on ReClor and LogiQA show
significant improvements in the logical-reasoning accuracy of pretrained
models, confirming the effectiveness of LFC-DA for LLM-guided logical data
augmentation.

</details>


### [68] [Segmentation Beyond Defaults: Asymmetrical Byte Pair Encoding for Optimal Machine Translation Performance](https://arxiv.org/abs/2511.03383)
*Saumitra Yadav,Manish Shrivastava*

Main category: cs.CL

TL;DR: Asymmetric BPE improves MT performance over symmetric BPE, especially in low-resource settings, with significant gains across multiple language pairs.


<details>
  <summary>Details</summary>
Motivation: Existing symmetric BPE approaches may not optimize MT performance across diverse language pairs and data sizes.

Method: Investigates BPE segmentation recipes with varying data volumes and language pairs, comparing asymmetric vs. symmetric BPE.

Result: Asymmetric BPE yields statistically significant improvements, especially with high NMO for source and low NMO for target languages.

Conclusion: Asymmetric BPE is superior for low-resource MT, with specific NMO configurations providing optimal results.

Abstract: Existing Machine Translation (MT) research often suggests a single, fixed set
of hyperparameters for word segmentation models, symmetric Byte Pair Encoding
(BPE), which applies the same number of merge operations (NMO) to train
tokenizers for both source and target languages. However, we demonstrate that
this uniform approach doesn't guarantee optimal MT performance across different
language pairs and data sizes. This work investigates BPE segmentation recipes
across various data volumes and language pairs to evaluate MT system
performance. We find that utilizing asymmetric BPE, where the source and target
languages have different NMOs, significantly improves results over the
symmetric approach, especially in low-resource settings (50K, 100K, and 500K
sentence pairs). Specifically, asymmetric BPE yield statistically significant
($p<0.05$) average gains of 5.32, 4.46, and 0.7 CHRF++ on English-Hindi in
low-resource setups. We validated this trend across six additional language
pairs (English and Telugu, Shona, Norwegian, Kyrgyz, Hausa, and Inuktitut),
observing statistically significant improvement in 10 out of 12 systems
compared to symmetric BPE. Our findings indicate a high NMO for the source (4K
to 32K) and a low NMO for the target (0.5K to 2K) provides optimal results,
particularly benefiting low-resource MT.

</details>


### [69] [Overcoming the Generalization Limits of SLM Finetuning for Shape-Based Extraction of Datatype and Object Properties](https://arxiv.org/abs/2511.03407)
*CÃ©lian Ringwald,Fabien Gandon,Catherine Faron,Franck Michel,Hanna Abi Akl*

Main category: cs.CL

TL;DR: Small language models (SLMs) can extract RDF triples for relation extraction (RE), but face challenges with long-tail rare properties. The paper evaluates strategies to balance performance across properties, finding that a threshold-based training set works best.


<details>
  <summary>Details</summary>
Motivation: To explore how SLMs handle both datatype and object properties in RDF graph extraction and address the bottleneck of rare properties.

Method: Evaluated strategies like stratified sampling, weighted loss, dataset scaling, and synthetic data augmentation.

Result: The best approach is ensuring each property in the training set exceeds a threshold number of occurrences.

Conclusion: The findings provide practical training guidance for SLMs in semantic RE and highlight future research directions.

Abstract: Small language models (SLMs) have shown promises for relation extraction (RE)
when extracting RDF triples guided by SHACL shapes focused on common datatype
properties. This paper investigates how SLMs handle both datatype and object
properties for a complete RDF graph extraction. We show that the key bottleneck
is related to long-tail distribution of rare properties. To solve this issue,
we evaluate several strategies: stratified sampling, weighted loss, dataset
scaling, and template-based synthetic data augmentation. We show that the best
strategy to perform equally well over unbalanced target properties is to build
a training set where the number of occurrences of each property exceeds a given
threshold. To enable reproducibility, we publicly released our datasets,
experimental results and code. Our findings offer practical guidance for
training shape-aware SLMs and highlight promising directions for future work in
semantic RE.

</details>


### [70] [Efficient Reasoning via Thought-Training and Thought-Free Inference](https://arxiv.org/abs/2511.03408)
*Canhui Wu,Qiong Cao,Chao Xue,Wei Xi,Xiaodong He*

Main category: cs.CL

TL;DR: 3TF is a framework for efficient reasoning in LLMs that trains models to internalize structured reasoning while producing concise outputs, outperforming traditional compression-based methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods compress verbose reasoning outputs for efficiency but still rely on explicit reasoning during inference. 3TF aims to improve reasoning quality implicitly.

Method: 3TF trains a hybrid model to operate in reasoning and non-reasoning modes, internalizing structured reasoning from CoT-annotated data while enforcing concise outputs during inference.

Result: 3TF-trained models show significant improvements on reasoning benchmarks under thought-free inference.

Conclusion: High-quality reasoning can be learned and executed implicitly without explicit step-by-step generation, enabling efficient and effective inference.

Abstract: Recent advances in large language models (LLMs) have leveraged explicit
Chain-of-Thought (CoT) prompting to improve reasoning accuracy. However, most
existing methods primarily compress verbose reasoning outputs. These
Long-to-Short transformations aim to improve efficiency, but still rely on
explicit reasoning during inference. In this work, we introduce \textbf{3TF}
(\textbf{T}hought-\textbf{T}raining and \textbf{T}hought-\textbf{F}ree
inference), a framework for efficient reasoning that takes a Short-to-Long
perspective. We first train a hybrid model that can operate in both reasoning
and non-reasoning modes, and then further train it on CoT-annotated data to
internalize structured reasoning, while enforcing concise, thought-free outputs
at inference time using the no-reasoning mode. Unlike compression-based
approaches, 3TF improves the reasoning quality of non-reasoning outputs,
enabling models to perform rich internal reasoning implicitly while keeping
external outputs short. Empirically, 3TF-trained models obtain large
improvements on reasoning benchmarks under thought-free inference,
demonstrating that high quality reasoning can be learned and executed
implicitly without explicit step-by-step generation.

</details>


### [71] [Knowledge-Augmented Question Error Correction for Chinese Question Answer System with QuestionRAG](https://arxiv.org/abs/2511.03410)
*Longpeng Qiu,Ting Li,Shuai Mao,Nan Yang,Xiaohui Yan*

Main category: cs.CL

TL;DR: QuestionRAG combines knowledge augmentation and reinforcement learning to improve LLMs' ability to correct and understand faulty questions.


<details>
  <summary>Details</summary>
Motivation: Input errors in QA systems often lead to incorrect responses due to misinterpretation or over-correction by LLMs.

Method: QuestionRAG enriches input with external knowledge and uses RL to align the model's objective with precise correction.

Result: Knowledge augmentation aids understanding, and RL outperforms SFT in improving instruction-following and generalization.

Conclusion: Integrating knowledge augmentation and RL unlocks LLMs' potential for question correction.

Abstract: Input errors in question-answering (QA) systems often lead to incorrect
responses. Large language models (LLMs) struggle with this task, frequently
failing to interpret user intent (misinterpretation) or unnecessarily altering
the original question's structure (over-correction). We propose QuestionRAG, a
framework that tackles these problems. To address misinterpretation, it
enriches the input with external knowledge (e.g., search results, related
entities). To prevent over-correction, it uses reinforcement learning (RL) to
align the model's objective with precise correction, not just paraphrasing. Our
results demonstrate that knowledge augmentation is critical for understanding
faulty questions. Furthermore, RL-based alignment proves significantly more
effective than traditional supervised fine-tuning (SFT), boosting the model's
ability to follow instructions and generalize. By integrating these two
strategies, QuestionRAG unlocks the full potential of LLMs for the question
correction task.

</details>


### [72] [CareMedEval dataset: Evaluating Critical Appraisal and Reasoning in the Biomedical Field](https://arxiv.org/abs/2511.03441)
*Doria Bonzi,Alexandre Guiggi,FrÃ©dÃ©ric BÃ©chet,Carlos Ramisch,Benoit Favre*

Main category: cs.CL

TL;DR: The paper introduces CareMedEval, a dataset for evaluating LLMs in biomedical critical appraisal, showing current models struggle with tasks like study limitations and statistical analysis despite reasoning improvements.


<details>
  <summary>Details</summary>
Motivation: To address the unreliability of LLMs in specialized biomedical reasoning by creating a dataset for critical appraisal evaluation.

Method: Developed CareMedEval, a dataset with 534 questions from 37 scientific articles, used to benchmark generalist and specialized LLMs under varied conditions.

Result: LLMs scored below 0.5 in Exact Match Rate, with intermediate reasoning tokens improving results, but performance remained low on study limitations and statistical analysis.

Conclusion: CareMedEval highlights LLMs' limitations in specialized biomedical reasoning and supports future development for automated critical appraisal tools.

Abstract: Critical appraisal of scientific literature is an essential skill in the
biomedical field. While large language models (LLMs) can offer promising
support in this task, their reliability remains limited, particularly for
critical reasoning in specialized domains. We introduce CareMedEval, an
original dataset designed to evaluate LLMs on biomedical critical appraisal and
reasoning tasks. Derived from authentic exams taken by French medical students,
the dataset contains 534 questions based on 37 scientific articles. Unlike
existing benchmarks, CareMedEval explicitly evaluates critical reading and
reasoning grounded in scientific papers. Benchmarking state-of-the-art
generalist and biomedical-specialized LLMs under various context conditions
reveals the difficulty of the task: open and commercial models fail to exceed
an Exact Match Rate of 0.5 even though generating intermediate reasoning tokens
considerably improves the results. Yet, models remain challenged especially on
questions about study limitations and statistical analysis. CareMedEval
provides a challenging benchmark for grounded reasoning, exposing current LLM
limitations and paving the way for future development of automated support for
critical appraisal.

</details>


### [73] [Kastor: Fine-tuned Small Language Models for Shape-based Active Relation Extraction](https://arxiv.org/abs/2511.03466)
*Ringwald Celian,Gandon Fabien,Faron Catherine,Michel Franck,Abi Akl Hanna*

Main category: cs.CL

TL;DR: Kastor is a framework that enhances RDF pattern-based extraction by optimizing property combinations and iteratively refining knowledge bases, improving model performance and generalization.


<details>
  <summary>Details</summary>
Motivation: To address the need for efficient knowledge base completion and refinement in specialized domains using limited data.

Method: Kastor reformulates SHACL shape validation by evaluating all property combinations, selects optimal ones for training, and uses iterative learning for refinement.

Result: Improves model generalization and performance, enabling discovery of new facts.

Conclusion: Kastor offers a robust approach for fine-tuning SLMs and refining knowledge bases effectively.

Abstract: RDF pattern-based extraction is a compelling approach for fine-tuning small
language models (SLMs) by focusing a relation extraction task on a specified
SHACL shape. This technique enables the development of efficient models trained
on limited text and RDF data. In this article, we introduce Kastor, a framework
that advances this approach to meet the demands for completing and refining
knowledge bases in specialized domains. Kastor reformulates the traditional
validation task, shifting from single SHACL shape validation to evaluating all
possible combinations of properties derived from the shape. By selecting the
optimal combination for each training example, the framework significantly
enhances model generalization and performance. Additionally, Kastor employs an
iterative learning process to refine noisy knowledge bases, enabling the
creation of robust models capable of uncovering new, relevant facts

</details>


### [74] [BanglaSTEM: A Parallel Corpus for Technical Domain Bangla-English Translation](https://arxiv.org/abs/2511.03498)
*Kazi Reyazul Hasan,Mubasshira Musarrat,A. B. M. Alim Al Islam,Muhammad Abdullah Adnan*

Main category: cs.CL

TL;DR: The paper introduces BanglaSTEM, a dataset for improving Bangla-English technical translations to enhance the performance of English-focused language models for Bangla speakers.


<details>
  <summary>Details</summary>
Motivation: Existing Bangla-English translation systems fail to accurately translate technical terms, leading to incorrect problem-solving outcomes when using English-focused language models.

Method: The authors created BanglaSTEM, a dataset of 5,000 high-quality Bangla-English sentence pairs from STEM fields, trained a T5-based translation model, and tested it on code generation and math problem-solving tasks.

Result: The trained model shows significant improvements in translation accuracy for technical content, enabling better use of English-focused language models by Bangla speakers.

Conclusion: BanglaSTEM and the trained translation model provide a valuable resource for accurate technical translations, bridging the gap for Bangla-speaking users of English-focused language models.

Abstract: Large language models work well for technical problem solving in English but
perform poorly when the same questions are asked in Bangla. A simple solution
would be to translate Bangla questions into English first and then use these
models. However, existing Bangla-English translation systems struggle with
technical terms. They often mistranslate specialized vocabulary, which changes
the meaning of the problem and leads to wrong answers. We present BanglaSTEM, a
dataset of 5,000 carefully selected Bangla-English sentence pairs from STEM
fields including computer science, mathematics, physics, chemistry, and
biology. We generated over 12,000 translations using language models and then
used human evaluators to select the highest quality pairs that preserve
technical terminology correctly. We train a T5-based translation model on
BanglaSTEM and test it on two tasks: generating code and solving math problems.
Our results show significant improvements in translation accuracy for technical
content, making it easier for Bangla speakers to use English-focused language
models effectively. Both the BanglaSTEM dataset and the trained translation
model are publicly released at https://huggingface.co/reyazul/BanglaSTEM-T5.

</details>


### [75] [HaluMem: Evaluating Hallucinations in Memory Systems of Agents](https://arxiv.org/abs/2511.03506)
*Ding Chen,Simin Niu,Kehang Li,Peng Liu,Xiangping Zheng,Bo Tang,Xinchi Li,Feiyu Xiong,Zhiyu Li*

Main category: cs.CL

TL;DR: HaluMem is the first benchmark evaluating memory system hallucinations at operational stages, revealing issues in extraction, updating, and QA, using large-scale interaction datasets.


<details>
  <summary>Details</summary>
Motivation: Memory systems in AI often suffer from hallucinations (fabrication, errors, etc.), but current evaluations lack stage-specific insights. This work aims to pinpoint where and how hallucinations occur.

Method: Introduces HaluMem, a benchmark with three tasks (memory extraction, updating, and QA) and datasets (HaluMem-Medium and HaluMem-Long) featuring multi-turn interactions and diverse questions.

Result: Empirical findings show hallucinations originate in extraction and updating stages, propagating errors to QA. Highlights the need for better memory operation mechanisms.

Conclusion: Future work should focus on interpretable and constrained memory operations to reduce hallucinations and enhance reliability.

Abstract: Memory systems are key components that enable AI systems such as LLMs and AI
agents to achieve long-term learning and sustained interaction. However, during
memory storage and retrieval, these systems frequently exhibit memory
hallucinations, including fabrication, errors, conflicts, and omissions.
Existing evaluations of memory hallucinations are primarily end-to-end question
answering, which makes it difficult to localize the operational stage within
the memory system where hallucinations arise. To address this, we introduce the
Hallucination in Memory Benchmark (HaluMem), the first operation level
hallucination evaluation benchmark tailored to memory systems. HaluMem defines
three evaluation tasks (memory extraction, memory updating, and memory question
answering) to comprehensively reveal hallucination behaviors across different
operational stages of interaction. To support evaluation, we construct
user-centric, multi-turn human-AI interaction datasets, HaluMem-Medium and
HaluMem-Long. Both include about 15k memory points and 3.5k multi-type
questions. The average dialogue length per user reaches 1.5k and 2.6k turns,
with context lengths exceeding 1M tokens, enabling evaluation of hallucinations
across different context scales and task complexities. Empirical studies based
on HaluMem show that existing memory systems tend to generate and accumulate
hallucinations during the extraction and updating stages, which subsequently
propagate errors to the question answering stage. Future research should focus
on developing interpretable and constrained memory operation mechanisms that
systematically suppress hallucinations and improve memory reliability.

</details>


### [76] [One Battle After Another: Probing LLMs' Limits on Multi-Turn Instruction Following with a Benchmark Evolving Framework](https://arxiv.org/abs/2511.03508)
*Qi Jia,Kaiwei Zhang,Xiujie Song,Ye Shen,Xiangyang Zhu,Guangtao Zhai*

Main category: cs.CL

TL;DR: The paper introduces an extensible framework for evaluating multi-turn instruction-following in large language models, decoupling linguistic forms from user intent. It proposes EvolIF, a benchmark with dynamic interactions, and shows GPT-5 outperforms others in robustness and conversation length.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing benchmarks, which are fixed-turn and lack user experience consideration, the paper aims to better assess conversational AI's multi-turn instruction-following ability.

Method: The framework uses a three-layer mechanism (constraints, instructions, topics) to simulate user-LLM interactions dynamically, terminating based on user patience. Metrics are defined to capture interaction quality.

Result: GPT-5 performs best, averaging 18.54 turns and 70.31% robustness, outperforming Gemini-2.5-Pro by 11.41%. Other models show weaker performance.

Conclusion: The proposed framework and EvolIF benchmark effectively evaluate multi-turn instruction-following, highlighting GPT-5's superiority. Data and code will be publicly shared.

Abstract: Understanding how well large language models can follow users' instructions
throughout a dialogue spanning multiple topics is of great importance for
data-intensive conversational applications. Existing benchmarks are often
limited to a fixed number of turns, making them susceptible to saturation and
failing to account for the user's interactive experience. In this work, we
propose an extensible framework for assessing multi-turn instruction-following
ability. At its core, our framework decouples linguistic surface forms from
user intent simulation through a three-layer mechanism that tracks constraints,
instructions, and topics. This framework mimics User-LLM interaction by
enabling the dynamic construction of benchmarks with state changes and
tracebacks, terminating a conversation only when the model exhausts a simulated
user's patience. We define a suite of metrics capturing the quality of the
interaction process. Using this framework, we construct EvolIF, an evolving
instruction-following benchmark incorporating nine distinct constraint types.
Our results indicate that GPT-5 exhibits superior instruction-following
performance. It sustains an average of 18.54 conversational turns and
demonstrates 70.31% robustness, outperforming Gemini-2.5-Pro by a significant
margin of 11.41%, while other models lag far behind. All of the data and code
will be made publicly available online.

</details>


### [77] [SOLVE-Med: Specialized Orchestration for Leading Vertical Experts across Medical Specialties](https://arxiv.org/abs/2511.03542)
*Roberta Di Marino,Giovanni Dioguardi,Antonio Romano,Giuseppe Riccio,Mariano Barone,Marco Postiglione,Flora Amato,Vincenzo Moscato*

Main category: cs.CL

TL;DR: SOLVE-Med is a multi-agent system using specialized small language models for medical queries, outperforming larger standalone models while enabling local deployment.


<details>
  <summary>Details</summary>
Motivation: Address challenges like hallucinations, bias, and computational demands in medical QA systems by leveraging specialized models.

Method: Uses a Router Agent for specialist selection, ten domain-specific models, and an Orchestrator Agent for response synthesis.

Result: Achieves ROUGE-1 of 0.301 and BERTScore F1 of 0.697, outperforming models up to 14B parameters.

Conclusion: SOLVE-Med offers a scalable, efficient solution for medical QA, with code available on GitHub.

Abstract: Medical question answering systems face deployment challenges including
hallucinations, bias, computational demands, privacy concerns, and the need for
specialized expertise across diverse domains. Here, we present SOLVE-Med, a
multi-agent architecture combining domain-specialized small language models for
complex medical queries. The system employs a Router Agent for dynamic
specialist selection, ten specialized models (1B parameters each) fine-tuned on
specific medical domains, and an Orchestrator Agent that synthesizes responses.
Evaluated on Italian medical forum data across ten specialties, SOLVE-Med
achieves superior performance with ROUGE-1 of 0.301 and BERTScore F1 of 0.697,
outperforming standalone models up to 14B parameters while enabling local
deployment. Our code is publicly available on GitHub:
https://github.com/PRAISELab-PicusLab/SOLVE-Med.

</details>


### [78] [Bearing Syntactic Fruit with Stack-Augmented Neural Networks](https://arxiv.org/abs/2511.03547)
*Brian DuSell,Ryan Cotterell*

Main category: cs.CL

TL;DR: Neural networks with stack augmentation can generalize in a human-like way for language acquisition, outperforming standard architectures.


<details>
  <summary>Details</summary>
Motivation: To explore if neural networks can mimic human-like hierarchical syntactic rule learning without special conditions like supervision or massive pre-training.

Method: Tested stack-augmented transformer, simple RNN, and LSTM architectures, including superposition and nondeterministic stacks, on a question formation task.

Result: Transformers with nondeterministic stacks performed best, and a modified stack RNN improved hierarchical generalization.

Conclusion: Stack-augmented networks may better model human language acquisition, offering insights for psycholinguistics.

Abstract: Any finite set of training data is consistent with an infinite number of
hypothetical algorithms that could have generated it. Studies have shown that
when human children learn language, they consistently favor hypotheses based on
hierarchical syntactic rules without ever encountering disambiguating examples.
A recent line of work has inquired as to whether common neural network
architectures share this bias, finding that they do so only under special
conditions: when syntactically supervised, when pre-trained on massive corpora,
or when trained long past convergence. In this paper, we demonstrate, for the
first time, neural network architectures that are able to generalize in
human-like fashion without any of the aforementioned requirements:
stack-augmented neural networks. We test three base architectures (transformer,
simple RNN, LSTM) augmented with two styles of stack: the superposition stack
of Joulin & Mikolov (2015) and a nondeterministic generalization of it proposed
by DuSell & Chiang (2023). We find that transformers with nondeterministic
stacks generalize best out of these architectures on a classical question
formation task. We also propose a modification to the stack RNN architecture
that improves hierarchical generalization. These results suggest that
stack-augmented neural networks may be more accurate models of human language
acquisition than standard architectures, serving as useful objects of
psycholinguistic study. Our code is publicly available.

</details>


### [79] [MultiZebraLogic: A Multilingual Logical Reasoning Benchmark](https://arxiv.org/abs/2511.03553)
*Sofie Helene Bruun,Dan Saattrup Smart*

Main category: cs.CL

TL;DR: The paper introduces MultiZebraLogic, a dataset of zebra puzzles in nine Germanic languages designed to benchmark LLMs' logical reasoning skills across various languages, themes, and difficulty levels.


<details>
  <summary>Details</summary>
Motivation: The motivation is to create large, high-quality datasets for comparing logical reasoning skills of LLMs across languages and difficulty levels, addressing the need for diverse benchmarks.

Method: The study generates zebra puzzles in multiple languages, themes, sizes, and includes different clue types and red herrings. It tests puzzle difficulty on models like GPT-4o mini and o3-mini.

Result: Results show specific puzzle sizes challenge different models, with red herrings significantly reducing accuracy. No notable impact was found from language or theme variation, or clue types on difficulty.

Conclusion: The study concludes with publishing datasets and code for adaptable puzzle generation, facilitating broader benchmarking of LLMs' reasoning abilities.

Abstract: Measuring the full abilities of large language models (LLMs) requires
benchmarks representing multiple tasks. We aim to create large, high-quality
datasets for comparison of logical reasoning skills across several languages
and of suitable difficulty for LLMs of various reasoning ability. We explore
multiple ways of increasing difficulty. We generate zebra puzzles in multiple
languages, themes, sizes and including 14 different clue types and 8 red
herring types (uninformative clues). We find puzzle sizes 2x3 and 4x5 are
sufficiently challenging for GPT-4o mini (a non-reasoning model) and o3-mini (a
reasoning model), respectively. Including 5 red herrings decreases o3-mini
puzzle-level accuracy on 4x5 puzzles by 15$\pm$7 %. Scores of o3-mini on 4x5
puzzles are not significantly affected by use of English vs. Danish or the
common houses theme vs. the country-specific smoerrebroed theme. We find no
correlation between difficulty and the selected clue types. Datasets of
128+1024 puzzles are published as MultiZebraLogic in each of nine Germanic
languages for sizes 2x3 and 4x5. We publish code for puzzle generation,
designed for adaptablity into more languages and themes.

</details>


### [80] [AILA--First Experiments with Localist Language Models](https://arxiv.org/abs/2511.03559)
*Joachim Diederich*

Main category: cs.CL

TL;DR: The paper introduces a controllable locality framework for transformer language models, enabling dynamic adjustment between interpretable localist and efficient distributed representations without retraining.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between interpretability and performance in language models, especially for regulated domains requiring transparency.

Method: A transformer architecture with a tunable locality dial parameter (Î») was tested on the WikiText corpus, varying Î» from 1.0 (localist) to 0.0 (distributed).

Result: Localist configurations (Î»=1.0) showed lower attention entropy (5.36 bits vs. 7.18 bits) and higher pointer fidelity, while intermediate Î»=0.6 balanced performance (84.7% accuracy) and interpretability.

Conclusion: The framework offers precise control over interpretability and performance, making it practical for regulated applications.

Abstract: This paper presents the first empirical demonstration of controllable
locality in transformer language models, a novel architectural framework that
enables continuous control over the degree of representation localization
through a tunable locality dial parameter. Unlike traditional language models
that rely exclusively on distributed representations, our approach allows
dynamic interpolation between highly interpretable localist encodings and
efficient distributed representations without requiring model retraining. We
conducted experiments on the WikiText corpus using a two-layer transformer
architecture, systematically varying the locality parameter {\lambda} across
the full spectrum from 1.0 (fully localist) to 0.0 (fully distributed). Our
results demonstrate that localist configurations achieve dramatically lower
attention entropy, with {\lambda} = 1.0 yielding 5.36 bits compared to 7.18
bits at {\lambda} = 0.0, while maintaining substantially higher pointer
fidelity scores reflecting stronger alignment with rule-specified targets.
Prediction experiments reveal that intermediate locality values optimize the
tradeoff between interpretability and performance, with {\lambda} = 0.6
achieving test perplexity of 4.65 and accuracy of 84.7%. These findings
establish that localist language models provide a practical framework for
applications in regulated domains requiring both transparency and capability,
offering precise mathematical control over the interpretability-performance
spectrum through explicit penalty thresholds and information-theoretic design
principles.

</details>


### [81] [ASVRI-Legal: Fine-Tuning LLMs with Retrieval Augmented Generation for Enhanced Legal Regulation](https://arxiv.org/abs/2511.03563)
*One Octadion,Bondan Sapta Prakoso,Nanang Yudi Setiawan,Novanto Yudistira*

Main category: cs.CL

TL;DR: The paper explores fine-tuning LLMs and using RAG to assist policymakers in legal research and drafting regulations, showing significant effectiveness.


<details>
  <summary>Details</summary>
Motivation: To enhance policymakers' ability to understand, analyze, and draft legal regulations by leveraging LLMs.

Method: Fine-tuning LLMs with a tailored legal dataset and integrating Retrieval-Augmented Generation (RAG) for up-to-date legal knowledge.

Result: The combined approach improves legal research and regulation drafting, proving highly effective.

Conclusion: The developed tool is a valuable resource for policymakers in the dynamic field of law.

Abstract: In this study, we explore the fine-tuning of Large Language Models (LLMs) to
better support policymakers in their crucial work of understanding, analyzing,
and crafting legal regulations. To equip the model with a deep understanding of
legal texts, we curated a supervised dataset tailored to the specific needs of
the legal domain. Additionally, we integrated the Retrieval-Augmented
Generation (RAG) method, enabling the LLM to access and incorporate up-to-date
legal knowledge from external sources. This combination of fine-tuning and
RAG-based augmentation results in a tool that not only processes legal
information but actively assists policymakers in interpreting regulations and
drafting new ones that align with current needs. The results demonstrate that
this approach can significantly enhance the effectiveness of legal research and
regulation development, offering a valuable resource in the ever-evolving field
of law.

</details>


### [82] [Step-Audio-EditX Technical Report](https://arxiv.org/abs/2511.03601)
*Chao Yan,Boyong Wu,Peng Yang,Pengfei Tan,Guoqiang Hu,Yuxin Zhang,Xiangyu,Zhang,Fei Tian,Xuerui Yang,Xiangyu Zhang,Daxin Jiang,Gang Yu*

Main category: cs.CL

TL;DR: Step-Audio-EditX is the first open-source LLM-based audio model for expressive and iterative audio editing, excelling in emotion, style, and paralinguistics, with strong zero-shot TTS capabilities.


<details>
  <summary>Details</summary>
Motivation: The paper aims to advance audio editing by introducing a model that achieves high expressivity and iterative control without relying on embedding-based priors or auxiliary modules.

Method: The model leverages large-margin synthetic data, bypassing the need for traditional representation-level disentanglement, enabling iterative control and high expressivity across voices.

Result: Step-Audio-EditX outperforms competitors like MiniMax-2.6-hd and Doubao-Seed-TTS-2.0 in emotion editing and fine-grained control tasks.

Conclusion: The large-margin learning approach represents a significant shift from conventional methods, demonstrating superior performance in expressive audio editing tasks.

Abstract: We present Step-Audio-EditX, the first open-source LLM-based audio model
excelling at expressive and iterative audio editing encompassing emotion,
speaking style, and paralinguistics alongside robust zero-shot text-to-speech
(TTS) capabilities.Our core innovation lies in leveraging only large-margin
synthetic data, which circumvents the need for embedding-based priors or
auxiliary modules. This large-margin learning approach enables both iterative
control and high expressivity across voices, and represents a fundamental pivot
from the conventional focus on representation-level disentanglement. Evaluation
results demonstrate that Step-Audio-EditX surpasses both MiniMax-2.6-hd and
Doubao-Seed-TTS-2.0 in emotion editing and other fine-grained control tasks.

</details>


### [83] [A systematic review of relation extraction task since the emergence of Transformers](https://arxiv.org/abs/2511.03610)
*Ringwald Celian,Gandon,Fabien,Faron Catherine,Michel Franck,Abi Akl Hanna*

Main category: cs.CL

TL;DR: A systematic review of relation extraction (RE) research from 2019 to 2024, analyzing surveys, datasets, and models to highlight trends, limitations, and future directions.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive overview of RE research since the rise of Transformer-based models, aiding researchers and practitioners in understanding advancements and gaps.

Method: Utilized an automated framework to collect and annotate 34 surveys, 64 datasets, and 104 models. Analyzed methodological advances, benchmarks, and semantic web integration.

Result: Consolidated findings reveal key trends, benchmark resources, and challenges in RE, offering a detailed reference for the field.

Conclusion: The study serves as a valuable resource for understanding RE's evolution and guiding future research, addressing current limitations and open challenges.

Abstract: This article presents a systematic review of relation extraction (RE)
research since the advent of Transformer-based models. Using an automated
framework to collect and annotate publications, we analyze 34 surveys, 64
datasets, and 104 models published between 2019 and 2024. The review highlights
methodological advances, benchmark resources, and the integration of semantic
web technologies. By consolidating results across multiple dimensions, the
study identifies current trends, limitations, and open challenges, offering
researchers and practitioners a comprehensive reference for understanding the
evolution and future directions of RE.

</details>


### [84] [Towards Transparent Stance Detection: A Zero-Shot Approach Using Implicit and Explicit Interpretability](https://arxiv.org/abs/2511.03635)
*Apoorva Upadhyaya,Wolfgang Nejdl,Marco Fisichella*

Main category: cs.CL

TL;DR: IRIS is a novel interpretable Zero-Shot Stance Detection framework that leverages implicit and explicit rationales for better generalizability and interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing Zero-Shot Stance Detection methods suffer from generalizability issues, lack of coherence, and poor interpretability. IRIS aims to address these limitations by incorporating implicit and explicit rationales.

Method: IRIS frames stance detection as an information retrieval task, using implicit rationales (sequences within the text) and explicit rationales (linguistic measures) to guide predictions without requiring ground-truth rationales.

Result: Extensive experiments on VAST, EZ-STANCE, P-Stance, and RFD datasets demonstrate IRIS's generalizability, even with limited training data (50%, 30%, 10%).

Conclusion: IRIS provides a robust and interpretable solution for Zero-Shot Stance Detection by combining implicit and explicit reasoning, addressing key limitations of prior work.

Abstract: Zero-Shot Stance Detection (ZSSD) identifies the attitude of the post toward
unseen targets. Existing research using contrastive, meta-learning, or data
augmentation suffers from generalizability issues or lack of coherence between
text and target. Recent works leveraging large language models (LLMs) for ZSSD
focus either on improving unseen target-specific knowledge or generating
explanations for stance analysis. However, most of these works are limited by
their over-reliance on explicit reasoning, provide coarse explanations that
lack nuance, and do not explicitly model the reasoning process, making it
difficult to interpret the model's predictions. To address these issues, in our
study, we develop a novel interpretable ZSSD framework, IRIS. We provide an
interpretable understanding of the attitude of the input towards the target
implicitly based on sequences within the text (implicit rationales) and
explicitly based on linguistic measures (explicit rationales). IRIS considers
stance detection as an information retrieval ranking task, understanding the
relevance of implicit rationales for different stances to guide the model
towards correct predictions without requiring the ground-truth of rationales,
thus providing inherent interpretability. In addition, explicit rationales
based on communicative features help decode the emotional and cognitive
dimensions of stance, offering an interpretable understanding of the author's
attitude towards the given target. Extensive experiments on the benchmark
datasets of VAST, EZ-STANCE, P-Stance, and RFD using 50%, 30%, and even 10%
training data prove the generalizability of our model, benefiting from the
proposed architecture and interpretable design.

</details>


### [85] [ChiMDQA: Towards Comprehensive Chinese Document QA with Fine-grained Evaluation](https://arxiv.org/abs/2511.03656)
*Jing Gao,Shutiao Luo,Yumeng Liu,Yuanming Li,Hongji Zeng*

Main category: cs.CL

TL;DR: The paper introduces the Chinese Multi-Document Question Answering Dataset (ChiMDQA) to meet the rising demand for high-quality Chinese NLP datasets. It includes 6,068 QA pairs across six domains and ten categories, designed for diverse NLP tasks.


<details>
  <summary>Details</summary>
Motivation: The growing need for high-quality Chinese NLP datasets, particularly for question-answering systems in business scenarios, motivated the creation of ChiMDQA.

Method: The dataset was constructed through meticulous document screening and a systematic question-design methodology, ensuring diversity and high quality across six domains and ten fine-grained categories.

Result: ChiMDQA consists of 6,068 high-quality QA pairs from long-form documents, applicable to NLP tasks like document comprehension, knowledge extraction, and intelligent QA systems.

Conclusion: ChiMDQA provides a solid foundation for future research and practical applications in Chinese QA, with detailed documentation on design, construction, and evaluation available for public use.

Abstract: With the rapid advancement of natural language processing (NLP) technologies,
the demand for high-quality Chinese document question-answering datasets is
steadily growing. To address this issue, we present the Chinese Multi-Document
Question Answering Dataset(ChiMDQA), specifically designed for downstream
business scenarios across prevalent domains including academic, education,
finance, law, medical treatment, and news. ChiMDQA encompasses long-form
documents from six distinct fields, consisting of 6,068 rigorously curated,
high-quality question-answer (QA) pairs further classified into ten
fine-grained categories. Through meticulous document screening and a systematic
question-design methodology, the dataset guarantees both diversity and high
quality, rendering it applicable to various NLP tasks such as document
comprehension, knowledge extraction, and intelligent QA systems. Additionally,
this paper offers a comprehensive overview of the dataset's design objectives,
construction methodologies, and fine-grained evaluation system, supplying a
substantial foundation for future research and practical applications in
Chinese QA. The code and data are available at:
https://anonymous.4open.science/r/Foxit-CHiMDQA/.

</details>


### [86] [Do Androids Dream of Unseen Puppeteers? Probing for a Conspiracy Mindset in Large Language Models](https://arxiv.org/abs/2511.03699)
*Francesco Corso,Francesco Pierri,Gianmarco De Francisci Morales*

Main category: cs.CL

TL;DR: The paper explores if LLMs show conspiratorial tendencies, biases, and susceptibility to conditioning, revealing partial agreement with conspiracy beliefs and demographic biases.


<details>
  <summary>Details</summary>
Motivation: Understanding LLMs' psychological constructs, like conspiratorial mindset, is crucial for social fidelity and mitigating harmful uses.

Method: Administered psychometric surveys on conspiracy mindset to LLMs under varied prompting and conditioning strategies.

Result: LLMs partially agree with conspiracy beliefs, show demographic biases, and are easily manipulated toward conspiratorial responses.

Conclusion: Critical evaluation of LLMs' psychological dimensions is needed for both research and risk mitigation in sensitive contexts.

Abstract: In this paper, we investigate whether Large Language Models (LLMs) exhibit
conspiratorial tendencies, whether they display sociodemographic biases in this
domain, and how easily they can be conditioned into adopting conspiratorial
perspectives. Conspiracy beliefs play a central role in the spread of
misinformation and in shaping distrust toward institutions, making them a
critical testbed for evaluating the social fidelity of LLMs. LLMs are
increasingly used as proxies for studying human behavior, yet little is known
about whether they reproduce higher-order psychological constructs such as a
conspiratorial mindset. To bridge this research gap, we administer validated
psychometric surveys measuring conspiracy mindset to multiple models under
different prompting and conditioning strategies. Our findings reveal that LLMs
show partial agreement with elements of conspiracy belief, and conditioning
with socio-demographic attributes produces uneven effects, exposing latent
demographic biases. Moreover, targeted prompts can easily shift model responses
toward conspiratorial directions, underscoring both the susceptibility of LLMs
to manipulation and the potential risks of their deployment in sensitive
contexts. These results highlight the importance of critically evaluating the
psychological dimensions embedded in LLMs, both to advance computational social
science and to inform possible mitigation strategies against harmful uses.

</details>


### [87] [Grounded Misunderstandings in Asymmetric Dialogue: A Perspectivist Annotation Scheme for MapTask](https://arxiv.org/abs/2511.03718)
*Nan Li,Albert Gatt,Massimo Poesio*

Main category: cs.CL

TL;DR: The paper analyzes referential misunderstandings in collaborative dialogue using a perspectivist annotation scheme on the HCRC MapTask corpus, revealing rare full misunderstandings but systematic divergences due to multiplicity discrepancies.


<details>
  <summary>Details</summary>
Motivation: To study how understanding emerges, diverges, and repairs in asymmetric collaborative dialogues, particularly focusing on referential misalignment masked by apparent grounding.

Method: Introduces a perspectivist annotation scheme for the HCRC MapTask corpus, using a scheme-constrained LLM pipeline to annotate 13k reference expressions with reliability estimates.

Result: Full misunderstandings are rare after unifying lexical variants, but multiplicity discrepancies lead to systematic divergences, uncovering hidden referential misalignment.

Conclusion: The framework offers a resource and analytic tool for studying grounded misunderstanding and evaluating (V)LLMs' ability to model perspective-dependent grounding.

Abstract: Collaborative dialogue relies on participants incrementally establishing
common ground, yet in asymmetric settings they may believe they agree while
referring to different entities. We introduce a perspectivist annotation scheme
for the HCRC MapTask corpus (Anderson et al., 1991) that separately captures
speaker and addressee grounded interpretations for each reference expression,
enabling us to trace how understanding emerges, diverges, and repairs over
time. Using a scheme-constrained LLM annotation pipeline, we obtain 13k
annotated reference expressions with reliability estimates and analyze the
resulting understanding states. The results show that full misunderstandings
are rare once lexical variants are unified, but multiplicity discrepancies
systematically induce divergences, revealing how apparent grounding can mask
referential misalignment. Our framework provides both a resource and an
analytic lens for studying grounded misunderstanding and for evaluating
(V)LLMs' capacity to model perspective-dependent grounding in collaborative
dialogue.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [88] [Toward Autonomous Engineering Design: A Knowledge-Guided Multi-Agent Framework](https://arxiv.org/abs/2511.03179)
*Varun Kumar,George Em Karniadakis*

Main category: cs.AI

TL;DR: A multi-agent AI framework formalizes the engineering design process, using specialized agents to generate and refine designs, demonstrated with aerodynamic optimization of NACA airfoils.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies in traditional engineering design processes, which are resource-intensive and complex due to multi-domain collaborations.

Method: The framework employs three AI agents: a Graph Ontologist, a Design Engineer, and a Systems Engineer, which collaborate using knowledge graphs and iterative feedback loops.

Result: The framework successfully optimizes NACA airfoil designs, improving efficiency, consistency, and quality in the engineering process.

Conclusion: Collaborative AI agents with structured knowledge representations can enhance engineering design, as shown in aerodynamic optimization.

Abstract: The engineering design process often demands expertise from multiple domains,
leading to complex collaborations and iterative refinements. Traditional
methods can be resource-intensive and prone to inefficiencies. To address this,
we formalize the engineering design process through a multi-agent AI framework
that integrates structured design and review loops. The framework introduces
specialized knowledge-driven agents that collaborate to generate and refine
design candidates. As an exemplar, we demonstrate its application to the
aerodynamic optimization of 4-digit NACA airfoils. The framework consists of
three key AI agents: a Graph Ontologist, a Design Engineer, and a Systems
Engineer. The Graph Ontologist employs a Large Language Model (LLM) to
construct two domain-specific knowledge graphs from airfoil design literature.
The Systems Engineer, informed by a human manager, formulates technical
requirements that guide design generation and evaluation. The Design Engineer
leverages the design knowledge graph and computational tools to propose
candidate airfoils meeting these requirements. The Systems Engineer reviews and
provides feedback both qualitative and quantitative using its own knowledge
graph, forming an iterative feedback loop until a design is validated by the
manager. The final design is then optimized to maximize performance metrics
such as the lift-to-drag ratio. Overall, this work demonstrates how
collaborative AI agents equipped with structured knowledge representations can
enhance efficiency, consistency, and quality in the engineering design process.

</details>


### [89] [Evaluating Control Protocols for Untrusted AI Agents](https://arxiv.org/abs/2511.02997)
*Jon Kutasov,Chloe Loughridge,Yuqi Sun,Henry Sleight,Buck Shlegeris,Tyler Tracy,Joe Benton*

Main category: cs.AI

TL;DR: The paper evaluates AI control protocols for safety, finding that deferring on critical actions is highly effective, while resampling can be vulnerable to adaptive attacks.


<details>
  <summary>Details</summary>
Motivation: Ensuring the safe operation of AI systems as they become more capable and widely deployed is critical. The study aims to evaluate the effectiveness and robustness of AI control protocols against attacks.

Method: The researchers systematically evaluated control protocols using SHADE-Arena, a dataset of diverse agentic environments. They tested blue team protocols (e.g., deferral to trusted models, resampling, deferring on critical actions) against default and adaptive attack policies.

Result: Deferring on critical actions increased safety from 50% to 96%, while resampling was vulnerable to adaptive attacks, dropping safety to 17%. Deferring on critical actions remained robust even against strong red team strategies.

Conclusion: The study highlights the importance of denying attack policies access to protocol internals, with deferring on critical actions being the most robust control strategy.

Abstract: As AI systems become more capable and widely deployed as agents, ensuring
their safe operation becomes critical. AI control offers one approach to
mitigating the risk from untrusted AI agents by monitoring their actions and
intervening or auditing when necessary. Evaluating the safety of these
protocols requires understanding both their effectiveness against current
attacks and their robustness to adaptive adversaries. In this work, we
systematically evaluate a range of control protocols in SHADE-Arena, a dataset
of diverse agentic environments. First, we evaluate blue team protocols,
including deferral to trusted models, resampling, and deferring on critical
actions, against a default attack policy. We find that resampling for
incrimination and deferring on critical actions perform best, increasing safety
from 50% to 96%. We then iterate on red team strategies against these protocols
and find that attack policies with additional affordances, such as knowledge of
when resampling occurs or the ability to simulate monitors, can substantially
improve attack success rates against our resampling strategy, decreasing safety
to 17%. However, deferring on critical actions is highly robust to even our
strongest red team strategies, demonstrating the importance of denying attack
policies access to protocol internals.

</details>


### [90] [Outbidding and Outbluffing Elite Humans: Mastering Liar's Poker via Self-Play and Reinforcement Learning](https://arxiv.org/abs/2511.03724)
*Richard Dewey,Janos Botyanszki,Ciamac C. Moallemi,Andrew T. Zheng*

Main category: cs.AI

TL;DR: Solly is the first AI agent achieving elite human play in reduced-format Liar's Poker, outperforming humans and LLMs using self-play reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Poker-like games are a testbed for AI, but most focus on limited player dynamics. Liar's Poker offers extensive multi-player engagement, making it a valuable challenge.

Method: Solly was trained using self-play with a model-free, actor-critic, deep reinforcement learning algorithm.

Result: Solly achieved elite human performance in win rate and equity, outperformed LLMs, developed novel strategies, and resisted exploitation by top players.

Conclusion: Solly demonstrates the potential of reinforcement learning to master complex, multi-player games with imperfect information.

Abstract: AI researchers have long focused on poker-like games as a testbed for
environments characterized by multi-player dynamics, imperfect information, and
reasoning under uncertainty. While recent breakthroughs have matched elite
human play at no-limit Texas hold'em, the multi-player dynamics are subdued:
most hands converge quickly with only two players engaged through multiple
rounds of bidding. In this paper, we present Solly, the first AI agent to
achieve elite human play in reduced-format Liar's Poker, a game characterized
by extensive multi-player engagement. We trained Solly using self-play with a
model-free, actor-critic, deep reinforcement learning algorithm. Solly played
at an elite human level as measured by win rate (won over 50% of hands) and
equity (money won) in heads-up and multi-player Liar's Poker. Solly also
outperformed large language models (LLMs), including those with reasoning
abilities, on the same metrics. Solly developed novel bidding strategies,
randomized play effectively, and was not easily exploitable by world-class
human players.

</details>


### [91] [PublicAgent: Multi-Agent Design Principles From an LLM-Based Open Data Analysis Framework](https://arxiv.org/abs/2511.03023)
*Sina Montazeri,Yunhe Feng,Kewei Sha*

Main category: cs.AI

TL;DR: PublicAgent is a multi-agent framework designed to make open data repositories accessible to non-experts by decomposing workflows into specialized agents. It addresses limitations of large language models and derives five design principles for effective multi-agent systems.


<details>
  <summary>Details</summary>
Motivation: Open data repositories are inaccessible to non-experts due to the complexity of dataset discovery, schema mapping, and analysis. While large language models can help, end-to-end workflows expose fundamental limitations.

Method: PublicAgent decomposes workflows into specialized agents for intent clarification, dataset discovery, analysis, and reporting. This maintains focused attention and enables validation at each stage.

Result: Evaluation across five models and 50 queries revealed five design principles: specialization's value, universal vs. conditional agents, failure mode mitigation, persistent benefits across complexity, and model-aware design.

Conclusion: Specialization is necessary for complex analytical workflows, enabling broader access to public data through natural language interfaces.

Abstract: Open data repositories hold potential for evidence-based decision-making, yet
are inaccessible to non-experts lacking expertise in dataset discovery, schema
mapping, and statistical analysis. Large language models show promise for
individual tasks, but end-to-end analytical workflows expose fundamental
limitations: attention dilutes across growing contexts, specialized reasoning
patterns interfere, and errors propagate undetected. We present PublicAgent, a
multi-agent framework that addresses these limitations through decomposition
into specialized agents for intent clarification, dataset discovery, analysis,
and reporting. This architecture maintains focused attention within agent
contexts and enables validation at each stage. Evaluation across five models
and 50 queries derives five design principles for multi-agent LLM systems.
First, specialization provides value independent of model strength--even the
strongest model shows 97.5% agent win rates, with benefits orthogonal to model
scale. Second, agents divide into universal (discovery, analysis) and
conditional (report, intent) categories. Universal agents show consistent
effectiveness (std dev 12.4%) while conditional agents vary by model (std dev
20.5%). Third, agents mitigate distinct failure modes--removing discovery or
analysis causes catastrophic failures (243-280 instances), while removing
report or intent causes quality degradation. Fourth, architectural benefits
persist across task complexity with stable win rates (86-92% analysis, 84-94%
discovery), indicating workflow management value rather than reasoning
enhancement. Fifth, wide variance in agent effectiveness across models (42-96%
for analysis) requires model-aware architecture design. These principles guide
when and why specialization is necessary for complex analytical workflows while
enabling broader access to public data through natural language interfaces.

</details>


### [92] [No-Human in the Loop: Agentic Evaluation at Scale for Recommendation](https://arxiv.org/abs/2511.03051)
*Tao Zhang,Kehui Yao,Luyi Ma,Jiao Chen,Reza Yousefi Maragheh,Kai Zhao,Jianpeng Xu,Evren Korpeoglu,Sushant Kumar,Kannan Achan*

Main category: cs.AI

TL;DR: ScalingEval benchmarks 36 LLMs to evaluate their performance as judges, using a consensus-driven protocol. Key findings highlight top-performing models like Claude 3.5 Sonnet and Gemini 1.5 Pro, while also noting performance variations across domains.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the need for scalable and trustworthy evaluation pipelines for LLMs by providing a systematic comparison of their performance as judges.

Method: ScalingEval employs a multi-agent framework with majority voting to aggregate pattern audits and issue codes into ground-truth labels, eliminating the need for human annotation.

Result: Claude 3.5 Sonnet shows the highest decision confidence, Gemini 1.5 Pro excels overall, GPT-4o balances latency-accuracy-cost, and GPT-OSS 20B leads among open-source models. Consensus varies by domain.

Conclusion: ScalingEval offers a reproducible benchmark for evaluating LLMs as judges, providing insights into scaling, reliability, and tradeoffs among model families.

Abstract: Evaluating large language models (LLMs) as judges is increasingly critical
for building scalable and trustworthy evaluation pipelines. We present
ScalingEval, a large-scale benchmarking study that systematically compares 36
LLMs, including GPT, Gemini, Claude, and Llama, across multiple product
categories using a consensus-driven evaluation protocol. Our multi-agent
framework aggregates pattern audits and issue codes into ground-truth labels
via scalable majority voting, enabling reproducible comparison of LLM
evaluators without human annotation. Applied to large-scale complementary-item
recommendation, the benchmark reports four key findings: (i) Anthropic Claude
3.5 Sonnet achieves the highest decision confidence; (ii) Gemini 1.5 Pro offers
the best overall performance across categories; (iii) GPT-4o provides the most
favorable latency-accuracy-cost tradeoff; and (iv) GPT-OSS 20B leads among
open-source models. Category-level analysis shows strong consensus in
structured domains (Electronics, Sports) but persistent disagreement in
lifestyle categories (Clothing, Food). These results establish ScalingEval as a
reproducible benchmark and evaluation protocol for LLMs as judges, with
actionable guidance on scaling, reliability, and model family tradeoffs.

</details>


### [93] [Epidemiology of Large Language Models: A Benchmark for Observational Distribution Knowledge](https://arxiv.org/abs/2511.03070)
*Drago Plecko,Patrik Okanovic,Torsten Hoefler,Elias Bareinboim*

Main category: cs.AI

TL;DR: The paper aims to benchmark whether LLMs internalize real-world probabilistic knowledge, finding they perform poorly and lack observational distributional knowledge.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' ability to capture probabilistic knowledge about real-world distributions, given their training on vast text data.

Method: Developed a benchmark to test LLMs on empirical distributions across domains like economics, health, and education.

Result: LLMs perform poorly overall and do not naturally internalize real-world statistics, lacking observational knowledge.

Conclusion: LLMs' limitations in probabilistic knowledge imply constrained capabilities in higher-level causal reasoning (interventional and counterfactual).

Abstract: Artificial intelligence (AI) systems hold great promise for advancing various
scientific disciplines, and are increasingly used in real-world applications.
Despite their remarkable progress, further capabilities are expected in order
to achieve more general types of intelligence. A critical distinction in this
context is between factual knowledge, which can be evaluated against true or
false answers (e.g., "what is the capital of England?"), and probabilistic
knowledge, reflecting probabilistic properties of the real world (e.g., "what
is the sex of a computer science graduate in the US?"). In this paper, our goal
is to build a benchmark for understanding the capabilities of LLMs in terms of
knowledge of probability distributions describing the real world. Given that
LLMs are trained on vast amounts of text, it may be plausible that they
internalize aspects of these distributions. Indeed, LLMs are touted as powerful
universal approximators of real-world distributions. At the same time,
classical results in statistics, known as curse of dimensionality, highlight
fundamental challenges in learning distributions in high dimensions,
challenging the notion of universal distributional learning. In this work, we
develop the first benchmark to directly test this hypothesis, evaluating
whether LLMs have access to empirical distributions describing real-world
populations across domains such as economics, health, education, and social
behavior. Our results demonstrate that LLMs perform poorly overall, and do not
seem to internalize real-world statistics naturally. When interpreted in the
context of Pearl's Causal Hierarchy (PCH), our benchmark demonstrates that
language models do not contain knowledge on observational distributions (Layer
1 of PCH), and thus the Causal Hierarchy Theorem implies that interventional
(Layer 2) and counterfactual (Layer 3) knowledge of these models is also
limited.

</details>


### [94] [SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators](https://arxiv.org/abs/2511.03092)
*Jonathan Li,Nasim Farahini,Evgenii Iuliugin,Magnus Vesterlund,Christian Haggstrom,Guangtao Wang,Shubhangi Upasani,Ayush Sachdeva,Rui Li,Faline Fu,Chen Wu,Ayesha Siddiqua,John Long,Tuowen Zhao,Matheen Musaddiq,Hakan Zeffer,Yun Du,Mingran Wang,Qinghua Li,Bo Li,Urmish Thakker,Raghu Prabhakar*

Main category: cs.AI

TL;DR: SnapStream is introduced as a KV cache compression method for large language models, improving memory usage by 4Ã— with minimal accuracy loss in production settings.


<details>
  <summary>Details</summary>
Motivation: The demand for efficient on-chip memory in large language models with extensive context lengths, coupled with the challenges of integrating existing techniques into industrial frameworks like vLLM or SGLang, drives the need for practical solutions.

Method: The paper develops SnapStream, a KV cache compression method, and evaluates its accuracy implications on models like Llama-3.1-8B-Instruct and DeepSeek-R1, deploying it in a 16-way tensor-parallel setup.

Result: SnapStream achieves a 4Ã— improvement in on-chip memory usage with minimal accuracy degradation, as demonstrated on benchmarks like LongBench-v2, AIME24, and LiveCodeBench.

Conclusion: SnapStream successfully addresses the challenges of deploying sparse KV attention techniques in production inference systems with static graphs and continuous batching, marking a first in this context.

Abstract: The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+
context length support have resulted in increasing demands for on-chip memory
to support large KV caches. Techniques such as StreamingLLM and SnapKV
demonstrate how to control KV cache size while maintaining model accuracy. Yet,
these techniques are not commonly used within industrial deployments using
frameworks like vLLM or SGLang. The reason is twofold: on one hand, the static
graphs and continuous batching methodology employed by these frameworks make it
difficult to admit modifications to the standard multi-head attention
algorithm, while on the other hand, the accuracy implications of such
techniques on modern instruction-following and reasoning models are not well
understood, obfuscating the need for implementing these techniques. In this
paper, we explore these accuracy implications on Llama-3.1-8B-Instruct and
DeepSeek-R1, and develop SnapStream, a KV cache compression method that can be
deployed at scale. We demonstrate the efficacy of SnapStream in a 16-way
tensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators
running at 128k context length and up to 1832 tokens per second in a real
production setting. SnapStream enables $4\times$ improved on-chip memory usage
and introduces minimal accuracy degradation on LongBench-v2, AIME24 and
LiveCodeBench. To the best of our knowledge, this is the first implementation
of sparse KV attention techniques deployed in a production inference system
with static graphs and continuous batching.

</details>


### [95] [Large language models require a new form of oversight: capability-based monitoring](https://arxiv.org/abs/2511.03106)
*Katherine C. Kellogg,Bingyang Ye,Yifan Hu,Guergana K. Savova,Byron Wallace,Danielle S. Bitterman*

Main category: cs.AI

TL;DR: The paper proposes capability-based monitoring for LLMs in healthcare, focusing on shared model capabilities rather than task-specific performance to detect systemic issues and emergent behaviors.


<details>
  <summary>Details</summary>
Motivation: Existing task-based monitoring for LLMs, inherited from traditional ML, is inadequate because LLMs are not trained for specific tasks or populations, leading to potential oversight of systemic weaknesses and emergent behaviors.

Method: The paper introduces capability-based monitoring, which organizes evaluation around shared LLM capabilities (e.g., summarization, reasoning) to enable scalable, cross-task detection of issues.

Result: Capability-based monitoring addresses gaps in traditional task-based approaches by detecting systemic weaknesses and long-tail errors across multiple tasks.

Conclusion: This approach provides a scalable foundation for safe and adaptive monitoring of LLMs and future generalist AI models in healthcare, benefiting developers, leaders, and professional societies.

Abstract: The rapid adoption of large language models (LLMs) in healthcare has been
accompanied by scrutiny of their oversight. Existing monitoring approaches,
inherited from traditional machine learning (ML), are task-based and founded on
assumed performance degradation arising from dataset drift. In contrast, with
LLMs, inevitable model degradation due to changes in populations compared to
the training dataset cannot be assumed, because LLMs were not trained for any
specific task in any given population. We therefore propose a new organizing
principle guiding generalist LLM monitoring that is scalable and grounded in
how these models are developed and used in practice: capability-based
monitoring. Capability-based monitoring is motivated by the fact that LLMs are
generalist systems whose overlapping internal capabilities are reused across
numerous downstream tasks. Instead of evaluating each downstream task
independently, this approach organizes monitoring around shared model
capabilities, such as summarization, reasoning, translation, or safety
guardrails, in order to enable cross-task detection of systemic weaknesses,
long-tail errors, and emergent behaviors that task-based monitoring may miss.
We describe considerations for developers, organizational leaders, and
professional societies for implementing a capability-based monitoring approach.
Ultimately, capability-based monitoring will provide a scalable foundation for
safe, adaptive, and collaborative monitoring of LLMs and future generalist
artificial intelligence models in healthcare.

</details>


### [96] [miniF2F-Lean Revisited: Reviewing Limitations and Charting a Path Forward](https://arxiv.org/abs/2511.03108)
*Azim Ospanov,Farzan Farnia,Roozbeh Yousefzadeh*

Main category: cs.AI

TL;DR: Analysis of the miniF2F benchmark reveals discrepancies between formal and informal math statements, leading to low AI pipeline accuracy. Correcting these errors in miniF2F-v2 improved accuracy to 70%, highlighting the need for better benchmarks to evaluate formal reasoning models.


<details>
  <summary>Details</summary>
Motivation: To evaluate AI systems in formal reasoning tasks, particularly in math Olympiads, by identifying and addressing discrepancies between formal and informal problem statements in the miniF2F benchmark.

Method: Analyzed discrepancies in the miniF2F benchmark, corrected errors, and created miniF2F-v2 with verified statements and proofs. Evaluated the AI pipeline's performance on both versions.

Result: Improved accuracy from 36% (original miniF2F) to 70% (miniF2F-v2), exposing misalignment between autoformalization models and theorem provers.

Conclusion: Higher quality benchmarks are essential for accurate evaluation and diagnosis of formal reasoning models. The corrected dataset (miniF2F-v2) is publicly available for community use.

Abstract: We perform a thorough analysis of the formal and informal statements in the
miniF2F benchmark from the perspective of an AI system that is tasked to
participate in a math Olympiad consisting of the problems in miniF2F. In such
setting, the model has to read and comprehend the problems in natural language,
formalize them in Lean language, then proceed with proving the problems, and it
will get credit for each problem if the formal proof corresponds to the
original informal statement presented to the model. Our evaluation results
reveal that the best accuracy of such pipeline can be about 36% using the SoTA
models in the literature, considerably lower than the individual SoTA
accuracies, 97% and 69% reported in the autoformalization and theorem proving
literature. Analyzing the failure modes, we trace back a considerable portion
of this drop to discrepancies between the formal and informal statements for
more than half of the problems in miniF2F. We proceed with correcting all the
errors, discrepancies and simplifications in formal and informal statements,
and present the miniF2F-v2 with fully verified formal and informal statements
and proofs. Evaluating the full theorem proving pipeline on miniF2F-v2 leads to
the best accuracy of 70%, a significant improvement from the 40% on the
original miniF2F, yet indicating considerable misalignment between the
autoformalization models and theorem provers. Our deep analysis suggests that a
higher quality benchmark can help the community better evaluate progress in the
field of formal reasoning and also better diagnose the failure and success
modes of autoformalization and theorem proving models. Our dataset is available
at https://github.com/roozbeh-yz/miniF2F_v2.

</details>


### [97] [Using Multi-modal Large Language Model to Boost Fireworks Algorithm's Ability in Settling Challenging Optimization Tasks](https://arxiv.org/abs/2511.03137)
*Shipeng Cen,Ying Tan*

Main category: cs.AI

TL;DR: The paper proposes a novel approach to enhance the fireworks algorithm (FWA) by integrating multi-modal large language models (MLLM) to tackle complex optimization problems like TSP and EDA, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Complex optimization problems often exhibit non-convexity, high-dimensionality, and other challenging traits. Traditional methods struggle with efficiency and accuracy, prompting the need for innovative solutions leveraging advancements in large language models.

Method: The study extends FWA by introducing the concept of Critical Part (CP) and integrates MLLM to utilize optimization process information more effectively, focusing on tasks like TSP and EDA.

Result: Experimental results demonstrate that the enhanced FWA framework achieves or surpasses state-of-the-art performance on various problem instances.

Conclusion: The integration of MLLM with FWA presents a promising direction for addressing complex optimization challenges, showcasing improved efficiency and accuracy.

Abstract: As optimization problems grow increasingly complex and diverse, advancements
in optimization techniques and paradigm innovations hold significant
importance. The challenges posed by optimization problems are primarily
manifested in their non-convexity, high-dimensionality, black-box nature, and
other unfavorable characteristics. Traditional zero-order or first-order
methods, which are often characterized by low efficiency, inaccurate gradient
information, and insufficient utilization of optimization information, are
ill-equipped to address these challenges effectively. In recent years, the
rapid development of large language models (LLM) has led to substantial
improvements in their language understanding and code generation capabilities.
Consequently, the design of optimization algorithms leveraging large language
models has garnered increasing attention from researchers. In this study, we
choose the fireworks algorithm(FWA) as the basic optimizer and propose a novel
approach to assist the design of the FWA by incorporating multi-modal large
language model(MLLM). To put it simply, we propose the concept of Critical
Part(CP), which extends FWA to complex high-dimensional tasks, and further
utilizes the information in the optimization process with the help of the
multi-modal characteristics of large language models. We focus on two specific
tasks: the \textit{traveling salesman problem }(TSP) and \textit{electronic
design automation problem} (EDA). The experimental results show that FWAs
generated under our new framework have achieved or surpassed SOTA results on
many problem instances.

</details>


### [98] [A Proprietary Model-Based Safety Response Framework for AI Agents](https://arxiv.org/abs/2511.03138)
*Qi Li,Jianjun Xu,Pingtao Wei,Jiu Li,Peiqiang Zhao,Jiwei Shi,Xuan Zhang,Yanhui Yang,Xiaodong Hui,Peng Xu,Wenqin Shao*

Main category: cs.AI

TL;DR: The paper introduces a safety response framework for Large Language Models (LLMs) at input and output levels, enhancing risk identification and response grounding in trustworthy knowledge, achieving high safety scores.


<details>
  <summary>Details</summary>
Motivation: The rise of security issues with LLMs limits their trustworthy deployment in critical domains, necessitating a systematic safety framework.

Method: The framework uses a fine-grained classification model for input-level risk identification and integrates Retrieval-Augmented Generation (RAG) with an interpretation model for output-level safety.

Result: Achieves 99.3% risk recall at input level and 100% safety score on high-risk tests, outperforming baselines.

Conclusion: The framework offers a practical solution for high-security, high-trust LLM applications.

Abstract: With the widespread application of Large Language Models (LLMs), their
associated security issues have become increasingly prominent, severely
constraining their trustworthy deployment in critical domains. This paper
proposes a novel safety response framework designed to systematically safeguard
LLMs at both the input and output levels. At the input level, the framework
employs a supervised fine-tuning-based safety classification model. Through a
fine-grained four-tier taxonomy (Safe, Unsafe, Conditionally Safe, Focused
Attention), it performs precise risk identification and differentiated handling
of user queries, significantly enhancing risk coverage and business scenario
adaptability, and achieving a risk recall rate of 99.3%. At the output level,
the framework integrates Retrieval-Augmented Generation (RAG) with a
specifically fine-tuned interpretation model, ensuring all responses are
grounded in a real-time, trustworthy knowledge base. This approach eliminates
information fabrication and enables result traceability. Experimental results
demonstrate that our proposed safety control model achieves a significantly
higher safety score on public safety evaluation benchmarks compared to the
baseline model, TinyR1-Safety-8B. Furthermore, on our proprietary high-risk
test set, the framework's components attained a perfect 100% safety score,
validating their exceptional protective capabilities in complex risk scenarios.
This research provides an effective engineering pathway for building
high-security, high-trust LLM applications.

</details>


### [99] [Uncovering Bugs in Formal Explainers: A Case Study with PyXAI](https://arxiv.org/abs/2511.03169)
*Xuanxiang Huang,Yacine Izza,Alexey Ignatiev,Joao Marques-Silva*

Main category: cs.AI

TL;DR: The paper proposes a new method to validate formal XAI explainers, tests PyXAI, and finds incorrect explanations, highlighting the need for such validation.


<details>
  <summary>Details</summary>
Motivation: To address the lack of validation for practical implementations of formal XAI methods, ensuring their correctness.

Method: Develops a novel validation methodology for formal explainers and applies it to PyXAI.

Result: Identifies incorrect explanations in PyXAI across most tested datasets.

Conclusion: Highlights the necessity of validating formal explainers to ensure reliability.

Abstract: Formal explainable artificial intelligence (XAI) offers unique theoretical
guarantees of rigor when compared to other non-formal methods of
explainability. However, little attention has been given to the validation of
practical implementations of formal explainers. This paper develops a novel
methodology for validating formal explainers and reports on the assessment of
the publicly available formal explainer PyXAI. The paper documents the
existence of incorrect explanations computed by PyXAI on most of the datasets
analyzed in the experiments, thereby confirming the importance of the proposed
novel methodology for the validation of formal explainers.

</details>


### [100] [Adobe Summit Concierge Evaluation with Human in the Loop](https://arxiv.org/abs/2511.03186)
*Yiru Chen,Sally Fang,Sai Sree Harsha,Dan Luo,Vaishnavi Muppala,Fei Wu,Shun Jiang,Kun Qian,Yunyao Li*

Main category: cs.AI

TL;DR: Summit Concierge is a domain-specific AI assistant for Adobe Summit, addressing event-related queries under real-world constraints using prompt engineering, retrieval grounding, and human validation.


<details>
  <summary>Details</summary>
Motivation: To enhance productivity, streamline information access, and improve user experience in enterprise contexts by developing a reliable AI assistant for Adobe Summit.

Method: Human-in-the-loop workflow combining prompt engineering, retrieval grounding, and lightweight human validation to tackle data sparsity and quality assurance.

Result: Successful deployment showcasing scalable and reliable AI assistants, even in cold-start scenarios.

Conclusion: Agile, feedback-driven development is effective for building scalable and reliable AI assistants in enterprise settings.

Abstract: Generative AI assistants offer significant potential to enhance productivity,
streamline information access, and improve user experience in enterprise
contexts. In this work, we present Summit Concierge, a domain-specific AI
assistant developed for Adobe Summit. The assistant handles a wide range of
event-related queries and operates under real-world constraints such as data
sparsity, quality assurance, and rapid deployment. To address these challenges,
we adopt a human-in-the-loop development workflow that combines prompt
engineering, retrieval grounding, and lightweight human validation. We describe
the system architecture, development process, and real-world deployment
outcomes. Our experience shows that agile, feedback-driven development enables
scalable and reliable AI assistants, even in cold-start scenarios.

</details>


### [101] [From Five Dimensions to Many: Large Language Models as Precise and Interpretable Psychological Profilers](https://arxiv.org/abs/2511.03235)
*Yi-Fei Liu,Yi-Long Lu,Di He,Hang Zhang*

Main category: cs.AI

TL;DR: LLMs accurately model human psychological traits' correlational structure from minimal inputs using a systematic two-stage process, outperforming semantic similarity and nearing trained algorithms' accuracy.


<details>
  <summary>Details</summary>
Motivation: To explore if LLMs can capture the interconnectedness of human psychological traits from minimal quantitative inputs, leveraging their ability to abstract and reason.

Method: Prompted LLMs with Big Five Personality Scale responses to role-play human responses on other psychological scales, analyzing reasoning traces.

Result: LLM-generated responses strongly aligned with human data (RÂ² > 0.89), outperforming semantic similarity and approaching trained algorithms' accuracy.

Conclusion: LLMs effectively predict psychological traits through abstraction and reasoning, offering insights into their emergent capabilities and potential for psychological simulation.

Abstract: Psychological constructs within individuals are widely believed to be
interconnected. We investigated whether and how Large Language Models (LLMs)
can model the correlational structure of human psychological traits from
minimal quantitative inputs. We prompted various LLMs with Big Five Personality
Scale responses from 816 human individuals to role-play their responses on nine
other psychological scales. LLMs demonstrated remarkable accuracy in capturing
human psychological structure, with the inter-scale correlation patterns from
LLM-generated responses strongly aligning with those from human data $(R^2 >
0.89)$. This zero-shot performance substantially exceeded predictions based on
semantic similarity and approached the accuracy of machine learning algorithms
trained directly on the dataset. Analysis of reasoning traces revealed that
LLMs use a systematic two-stage process: First, they transform raw Big Five
responses into natural language personality summaries through information
selection and compression, analogous to generating sufficient statistics.
Second, they generate target scale responses based on reasoning from these
summaries. For information selection, LLMs identify the same key personality
factors as trained algorithms, though they fail to differentiate item
importance within factors. The resulting compressed summaries are not merely
redundant representations but capture synergistic information--adding them to
original scores enhances prediction alignment, suggesting they encode emergent,
second-order patterns of trait interplay. Our findings demonstrate that LLMs
can precisely predict individual participants' psychological traits from
minimal data through a process of abstraction and reasoning, offering both a
powerful tool for psychological simulation and valuable insights into their
emergent reasoning capabilities.

</details>


### [102] [Towards Scalable Web Accessibility Audit with MLLMs as Copilots](https://arxiv.org/abs/2511.03471)
*Ming Gu,Ziwei Wang,Sicen Lai,Zirui Gao,Sheng Zhou,Jiajun Bu*

Main category: cs.AI

TL;DR: The paper presents the AAA framework for scalable web accessibility auditing, combining human expertise with AI assistance to address current limitations in WCAG-EM.


<details>
  <summary>Details</summary>
Motivation: Current web accessibility auditing methods are resource-intensive and unscalable, hindering their practical application despite their importance for social welfare and equality.

Method: The AAA framework integrates GRASP (a graph-based sampling method for representative page coverage) and MaC (a multimodal LLM copilot for cross-modal reasoning and task assistance), operationalizing WCAG-EM.

Result: The framework, along with four new datasets, demonstrates effectiveness in enabling scalable auditing, with small-scale language models proving capable when fine-tuned.

Conclusion: The AAA framework bridges the gap in scalable web accessibility auditing, offering practical AI-enhanced solutions for real-world impact.

Abstract: Ensuring web accessibility is crucial for advancing social welfare, justice,
and equality in digital spaces, yet the vast majority of website user
interfaces remain non-compliant, due in part to the resource-intensive and
unscalable nature of current auditing practices. While WCAG-EM offers a
structured methodology for site-wise conformance evaluation, it involves great
human efforts and lacks practical support for execution at scale. In this work,
we present an auditing framework, AAA, which operationalizes WCAG-EM through a
human-AI partnership model. AAA is anchored by two key innovations: GRASP, a
graph-based multimodal sampling method that ensures representative page
coverage via learned embeddings of visual, textual, and relational cues; and
MaC, a multimodal large language model-based copilot that supports auditors
through cross-modal reasoning and intelligent assistance in high-effort tasks.
Together, these components enable scalable, end-to-end web accessibility
auditing, empowering human auditors with AI-enhanced assistance for real-world
impact. We further contribute four novel datasets designed for benchmarking
core stages of the audit pipeline. Extensive experiments demonstrate the
effectiveness of our methods, providing insights that small-scale language
models can serve as capable experts when fine-tuned.

</details>


### [103] [Explaining Decisions in ML Models: a Parameterized Complexity Analysis (Part I)](https://arxiv.org/abs/2511.03545)
*Sebastian Ordyniak,Giacomo Paesani,Mateusz Rychlicki,Stefan Szeider*

Main category: cs.AI

TL;DR: The paper investigates the parameterized complexity of explanation problems in transparent ML models, covering abductive and contrastive explanations across various models like Decision Trees and Boolean Circuits.


<details>
  <summary>Details</summary>
Motivation: To address the gap in explainable AI (XAI) by analyzing the complexity of generating explanations for transparent ML models, emphasizing the need for transparency and accountability in AI systems.

Method: A theoretical investigation into the parameterized complexity of abductive and contrastive explanation problems in diverse ML models (Decision Trees, Decision Sets, Decision Lists, Boolean Circuits, and ensembles).

Result: Provides foundational insights into the complexities of generating explanations for transparent ML models, advancing the understanding of XAI.

Conclusion: The research contributes to the discourse on transparency in AI, offering vital insights for further XAI studies.

Abstract: This paper presents a comprehensive theoretical investigation into the
parameterized complexity of explanation problems in various machine learning
(ML) models. Contrary to the prevalent black-box perception, our study focuses
on models with transparent internal mechanisms. We address two principal types
of explanation problems: abductive and contrastive, both in their local and
global variants. Our analysis encompasses diverse ML models, including Decision
Trees, Decision Sets, Decision Lists, Boolean Circuits, and ensembles thereof,
each offering unique explanatory challenges. This research fills a significant
gap in explainable AI (XAI) by providing a foundational understanding of the
complexities of generating explanations for these models. This work provides
insights vital for further research in the domain of XAI, contributing to the
broader discourse on the necessity of transparency and accountability in AI
systems.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [104] [ALAS: Transactional and Dynamic Multi-Agent LLM Planning](https://arxiv.org/abs/2511.03094)
*Longling Geng,Edward Y. Chang*

Main category: cs.MA

TL;DR: ALAS is a stateful, disruption-aware framework for multi-agent LLM planning that improves efficiency, feasibility, and scalability by separating planning from validation, using versioned logs, and enabling localized repair.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based multi-agent planning systems are fragile, with circular verification, untracked state changes, and costly global recomputation for small faults. ALAS addresses these issues.

Method: ALAS separates planning from non-circular validation, uses versioned execution logs for grounded checks and restore points, and performs localized repair under defined policies. The validator operates independently to avoid self-check loops.

Result: ALAS achieves 83.7% success in job-shop scheduling, reduces token usage by 60%, and runs 1.82x faster than baselines. Validator detects faults with low overhead, and localized repair minimizes runtime disruptions.

Conclusion: ALAS enhances multi-agent LLM planning efficiency and scalability through validator isolation, versioned logs, and localized repair.

Abstract: Large language models enable flexible multi-agent planning but remain fragile
in practice: verification is often circular, state changes are not tracked for
repair, and small faults trigger costly global recomputation. We present ALAS,
a stateful, disruption-aware framework that separates planning from
non-circular validation, records a versioned execution log for grounded checks
and restore points, and performs localized repair that preserves work in
progress. The validator operates independently of the planning LLM with fresh,
bounded context, avoiding self-check loops and mid-context attrition. The
repair protocol edits only the minimal affected region under explicit policies
(retry, catch, timeout, backoff, idempotency keys, compensation, loop guards)
defined in a canonical workflow IR that maps to Amazon States Language and Argo
Workflows. On job-shop scheduling suites (DMU, TA) across five classical
benchmarks, ALAS matches or exceeds strong single-LLM and multi-agent
baselines, achieving 83.7% success, reducing token usage by 60%, and running
1.82times faster under comparable settings. A minimal reliability study shows
that the validator detects injected structural faults with low overhead, and
that localized repair contains runtime perturbations with a bounded edit radius
and less makespan degradation than global recompute. Results indicate that the
combination of validator isolation, versioned execution logs, and localized
repair provides measurable efficiency, feasibility, and scalability for
multi-agent LLM planning. Code and seeds will be released.

</details>


### [105] [Learning Communication Skills in Multi-task Multi-agent Deep Reinforcement Learning](https://arxiv.org/abs/2511.03348)
*Changxi Zhu,Mehdi Dastani,Shihan Wang*

Main category: cs.MA

TL;DR: The paper introduces Multi-task Communication Skills (MCS), a method for multi-agent deep reinforcement learning (MADRL) that enables agents to perform multiple tasks simultaneously using learnable communication protocols, outperforming baseline methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance coordination and knowledge sharing among agents in multi-task MADRL settings, leveraging communication to improve learning and performance across varied tasks.

Method: MCS uses a Transformer encoder to encode task-specific observations into a shared message space and a prediction network to correlate messages with sender agents' actions for better coordination.

Result: Experimental results show MCS outperforms multi-task and single-task MADRL baselines, both with and without communication, across adapted benchmark environments.

Conclusion: MCS effectively improves multi-task performance in MADRL by leveraging shared communication skills and enhancing agent coordination.

Abstract: In multi-agent deep reinforcement learning (MADRL), agents can communicate
with one another to perform a task in a coordinated manner. When multiple tasks
are involved, agents can also leverage knowledge from one task to improve
learning in other tasks. In this paper, we propose Multi-task Communication
Skills (MCS), a MADRL with communication method that learns and performs
multiple tasks simultaneously, with agents interacting through learnable
communication protocols. MCS employs a Transformer encoder to encode
task-specific observations into a shared message space, capturing shared
communication skills among agents. To enhance coordination among agents, we
introduce a prediction network that correlates messages with the actions of
sender agents in each task. We adapt three multi-agent benchmark environments
to multi-task settings, where the number of agents as well as the observation
and action spaces vary across tasks. Experimental results demonstrate that MCS
achieves better performance than multi-task MADRL baselines without
communication, as well as single-task MADRL baselines with and without
communication.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [106] [Scaling Multi-Agent Environment Co-Design with Diffusion Models](https://arxiv.org/abs/2511.03100)
*Hao Xiang Li,Michael Amir,Amanda Prorok*

Main category: cs.LG

TL;DR: DiCoDe introduces scalable and efficient co-design for agent-environment systems, outperforming state-of-the-art methods with innovations like PUG and critic distillation.


<details>
  <summary>Details</summary>
Motivation: Current co-design methods struggle with scalability and sample inefficiency in high-dimensional environments and moving targets.

Method: DiCoDe uses Projected Universal Guidance (PUG) for constraint-satisfying environment sampling and critic distillation to adapt to evolving policies.

Result: DiCoDe achieves 39% higher rewards in warehouse settings with 66% fewer samples, setting a new benchmark in co-design.

Conclusion: DiCoDe advances agent-environment co-design, making it practical for real-world applications.

Abstract: The agent-environment co-design paradigm jointly optimises agent policies and
environment configurations in search of improved system performance. With
application domains ranging from warehouse logistics to windfarm management,
co-design promises to fundamentally change how we deploy multi-agent systems.
However, current co-design methods struggle to scale. They collapse under
high-dimensional environment design spaces and suffer from sample inefficiency
when addressing moving targets inherent to joint optimisation. We address these
challenges by developing Diffusion Co-Design (DiCoDe), a scalable and
sample-efficient co-design framework pushing co-design towards practically
relevant settings. DiCoDe incorporates two core innovations. First, we
introduce Projected Universal Guidance (PUG), a sampling technique that enables
DiCoDe to explore a distribution of reward-maximising environments while
satisfying hard constraints such as spatial separation between obstacles.
Second, we devise a critic distillation mechanism to share knowledge from the
reinforcement learning critic, ensuring that the guided diffusion model adapts
to evolving agent policies using a dense and up-to-date learning signal.
Together, these improvements lead to superior environment-policy pairs when
validated on challenging multi-agent environment co-design benchmarks including
warehouse automation, multi-agent pathfinding and wind farm optimisation. Our
method consistently exceeds the state-of-the-art, achieving, for example, 39%
higher rewards in the warehouse setting with 66% fewer simulation samples. This
sets a new standard in agent-environment co-design, and is a stepping stone
towards reaping the rewards of co-design in real world domains.

</details>


### [107] [FATE: A Formal Benchmark Series for Frontier Algebra of Multiple Difficulty Levels](https://arxiv.org/abs/2511.02872)
*Jiedong Jiang,Wanyi He,Yuefeng Wang,Guoxiong Gao,Yongle Hu,Jingting Wang,Nailing Guan,Peihao Wu,Chunbo Dai,Liang Xiao,Bin Dong*

Main category: cs.LG

TL;DR: The paper introduces FATE, a new benchmark series in formal algebra, to challenge LLMs beyond contest-based math, revealing a performance gap in advanced mathematical reasoning.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for LLMs focus on contest-based math, which lacks the depth and breadth of modern mathematical research. FATE aims to bridge this gap.

Method: Introduces two components, FATE-H and FATE-X, with 100 problems each in abstract and commutative algebra, spanning undergraduate to PhD-level difficulty. Evaluates LLMs on these problems.

Result: Best LLMs achieve only 3% accuracy on FATE-H and 0% on FATE-X. Finds models' natural-language reasoning outperforms their formalization ability. Identifies common formalization errors.

Conclusion: FATE provides a robust benchmark for advancing research-level formal mathematical reasoning, highlighting current limitations in LLMs.

Abstract: Recent advances in large language models (LLMs) have demonstrated impressive
capabilities in formal theorem proving, particularly on contest-based
mathematical benchmarks like the IMO. However, these contests do not reflect
the depth, breadth, and abstraction of modern mathematical research. To bridge
this gap, we introduce FATE (Formal Algebra Theorem Evaluation), a new
benchmark series in formal algebra designed to chart a course toward advanced
mathematical reasoning. We present two new components, FATE-H and FATE-X, each
with 100 problems in abstract and commutative algebra. The FATE series spans a
difficulty spectrum from undergraduate exercises to problems exceeding PhD
qualifying exams. Notably, FATE-X is the first formal benchmark to surpass both
PhD-level exam difficulty and the coverage of the Mathlib library. Our
evaluations of state-of-the-art LLM provers on this new benchmark reveal a
stark performance gap compared to contest math: the best model achieves only 3%
(pass@64) accuracy on FATE-H and 0% on FATE-X. Our two-stage evaluation reveals
that models' natural-language reasoning is notably more accurate than their
ability to formalize this reasoning. We systematically classify the common
errors that arise during this formalization process. Furthermore, a comparative
study shows that a specialized prover can exhibit less effective reflection
than general-purpose models, reducing its accuracy at the natural-language
stage. We believe FATE provides a robust and challenging benchmark that
establishes essential checkpoints on the path toward research-level formal
mathematical reasoning.

</details>


### [108] [Stochastic Deep Graph Clustering for Practical Group Formation](https://arxiv.org/abs/2511.02879)
*Junhyung Park,Hyungjin Kim,Seokho Ahn,Young-Duk Seo*

Main category: cs.LG

TL;DR: DeepForm is a framework addressing dynamic group formation in recommender systems using lightweight GCN, stochastic clustering, and contrastive learning for real-time adjustments and improved accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing GRSs assume static groups, making them unfit for dynamic scenarios. DeepForm aims to address this by focusing on adaptive group formation.

Method: Uses lightweight GCN for high-order user info, stochastic clustering for real-time group reconfiguration, and contrastive learning for dynamic adjustments.

Result: DeepForm outperforms baselines in group formation quality, efficiency, and recommendation accuracy across multiple datasets.

Conclusion: DeepForm effectively addresses dynamic group formation challenges in GRSs, offering superior performance and adaptability.

Abstract: While prior work on group recommender systems (GRSs) has primarily focused on
improving recommendation accuracy, most approaches assume static or predefined
groups, making them unsuitable for dynamic, real-world scenarios. We reframe
group formation as a core challenge in GRSs and propose DeepForm (Stochastic
Deep Graph Clustering for Practical Group Formation), a framework designed to
meet three key operational requirements: (1) the incorporation of high-order
user information, (2) real-time group formation, and (3) dynamic adjustment of
the number of groups. DeepForm employs a lightweight GCN architecture that
effectively captures high-order structural signals. Stochastic cluster learning
enables adaptive group reconfiguration without retraining, while contrastive
learning refines groups under dynamic conditions. Experiments on multiple
datasets demonstrate that DeepForm achieves superior group formation quality,
efficiency, and recommendation accuracy compared with various baselines.

</details>


### [109] [Test-time Adaptation of Tiny Recursive Models](https://arxiv.org/abs/2511.02886)
*Ronan Killian McGovern*

Main category: cs.LG

TL;DR: A 7M-parameter recursive model pre-trained on public ARC tasks was fine-tuned efficiently within competition compute limits, achieving a 6.67% score on semi-private tasks.


<details>
  <summary>Details</summary>
Motivation: To demonstrate that starting from a pre-trained tiny recursive model allows efficient fine-tuning on competition tasks within strict compute constraints.

Method: Pre-trained a 7M-parameter recursive model on 1,280 public ARC tasks, then fine-tuned it in 12,500 gradient steps during the competition.

Result: The model scored ~10% on public tasks post-pre-training and 6.67% on semi-private tasks after fine-tuning.

Conclusion: Full fine-tuning of tiny pre-trained models is feasible within compute limits, achieving competitive results despite constraints.

Abstract: Prior to the close of the 2025 ARC Prize competition, the leading open source
approach - known as TRM, or Tiny Recursive Models - involved training a 7M
parameter recursive neural network on augmented variants of ARC tasks. That
approach scored approximately 7.8% on the public ARC AGI II evaluation set, but
required a level of compute far in excess of what is allowed during the
competition. This paper shows that, by starting from a tiny recursive model
that has been pre-trained on public ARC tasks, one can efficiently fine-tune on
competition tasks within the allowed compute limits. Specifically, a model was
pre-trained on 1,280 public tasks for 700k+ optimizer steps over 48 hours on
4xH100 SXM GPUs to obtain a ~10% score on the public evaluation set. That model
was then post-trained in just 12,500 gradient steps during the competition to
reach a score of 6.67% on semi-private evaluation tasks. Notably, such
post-training performance is achieved by full-fine tuning of the tiny model,
not LoRA fine-tuning or fine-tuning of task embeddings alone.

</details>


### [110] [Predicting Weekly Fishing Concentration Zones through Deep Learning Integration of Heterogeneous Environmental Spatial Datasets](https://arxiv.org/abs/2511.02887)
*Chaitanya Rele,Aditya Rathod,Kaustubh Natu,Saurabh Kulkarni,Ajay Koli,Swapnali Makdey*

Main category: cs.LG

TL;DR: AI framework predicts fishing zones using ocean data to aid fishermen in reducing search time and fuel use.


<details>
  <summary>Details</summary>
Motivation: Fishermen struggle to locate productive fishing grounds, impacting livelihoods in the North Indian Ocean.

Method: Uses oceanographic parameters (sea surface temperature, chlorophyll concentration) to predict Potential Fishing Zones (PFZs).

Result: Preliminary findings show reduced search time, lower fuel consumption, and better resource use.

Conclusion: The AI framework supports sustainable fishing by improving PFZ identification.

Abstract: The North Indian Ocean, including the Arabian Sea and the Bay of Bengal,
represents a vital source of livelihood for coastal communities, yet fishermen
often face uncertainty in locating productive fishing grounds. To address this
challenge, we present an AI-assisted framework for predicting Potential Fishing
Zones (PFZs) using oceanographic parameters such as sea surface temperature and
chlorophyll concentration. The approach is designed to enhance the accuracy of
PFZ identification and provide region-specific insights for sustainable fishing
practices. Preliminary results indicate that the framework can support
fishermen by reducing search time, lowering fuel consumption, and promoting
efficient resource utilization.

</details>


### [111] [Adaptive and Robust Data Poisoning Detection and Sanitization in Wearable IoT Systems using Large Language Models](https://arxiv.org/abs/2511.02894)
*W. K. M Mithsara,Ning Yang,Ahmed Imteaj,Hussein Zangoti,Abdur R. Shahid*

Main category: cs.LG

TL;DR: The paper proposes a novel framework using large language models (LLMs) for detecting and sanitizing data poisoning attacks in human activity recognition (HAR) systems, leveraging zero-shot, one-shot, and few-shot learning to reduce reliance on large datasets.


<details>
  <summary>Details</summary>
Motivation: The increasing susceptibility of machine learning models in HAR systems to data poisoning attacks and the limitations of conventional defense methods in dynamic IoT environments drive the need for adaptable solutions.

Method: The framework employs LLMs with role-play prompting and step-by-step reasoning to detect anomalies and sanitize data, minimizing the need for large labeled datasets.

Result: Extensive evaluation shows the framework's effectiveness in improving detection accuracy, sanitization quality, and real-time adaptability, with low latency and communication costs.

Conclusion: LLMs offer a practical and effective solution for enhancing the security and reliability of wearable IoT systems against data poisoning attacks.

Abstract: The widespread integration of wearable sensing devices in Internet of Things
(IoT) ecosystems, particularly in healthcare, smart homes, and industrial
applications, has required robust human activity recognition (HAR) techniques
to improve functionality and user experience. Although machine learning models
have advanced HAR, they are increasingly susceptible to data poisoning attacks
that compromise the data integrity and reliability of these systems.
Conventional approaches to defending against such attacks often require
extensive task-specific training with large, labeled datasets, which limits
adaptability in dynamic IoT environments. This work proposes a novel framework
that uses large language models (LLMs) to perform poisoning detection and
sanitization in HAR systems, utilizing zero-shot, one-shot, and few-shot
learning paradigms. Our approach incorporates \textit{role play} prompting,
whereby the LLM assumes the role of expert to contextualize and evaluate sensor
anomalies, and \textit{think step-by-step} reasoning, guiding the LLM to infer
poisoning indicators in the raw sensor data and plausible clean alternatives.
These strategies minimize reliance on curation of extensive datasets and enable
robust, adaptable defense mechanisms in real-time. We perform an extensive
evaluation of the framework, quantifying detection accuracy, sanitization
quality, latency, and communication cost, thus demonstrating the practicality
and effectiveness of LLMs in improving the security and reliability of wearable
IoT systems.

</details>


### [112] [Zero-shot data citation function classification using transformer-based large language models (LLMs)](https://arxiv.org/abs/2511.02936)
*Neil Byers,Ali Zaidi,Valerie Skye,Chris Beecroft,Kjiersten Fagnan*

Main category: cs.LG

TL;DR: The paper explores using the Llama 3.1-405B LLM to generate structured labels for data use cases in genomic publications, achieving a .674 F1 score, but highlights challenges like data availability and computational costs.


<details>
  <summary>Details</summary>
Motivation: To automate the description of how specific datasets are used in scientific literature, avoiding manual labeling and training dataset development for classical ML systems.

Method: Applied Llama 3.1-405B to generate structured data use case labels for genomic publications and introduced a novel evaluation framework.

Result: The model achieved an F1 score of .674 on a zero-shot data citation classification task without predefined categories.

Conclusion: While promising, the approach faces challenges like data availability, prompt overfitting, computational infrastructure, and evaluation costs.

Abstract: Efforts have increased in recent years to identify associations between
specific datasets and the scientific literature that incorporates them. Knowing
that a given publication cites a given dataset, the next logical step is to
explore how or why that data was used. Advances in recent years with
pretrained, transformer-based large language models (LLMs) offer potential
means for scaling the description of data use cases in the published
literature. This avoids expensive manual labeling and the development of
training datasets for classical machine-learning (ML) systems. In this work we
apply an open-source LLM, Llama 3.1-405B, to generate structured data use case
labels for publications known to incorporate specific genomic datasets. We also
introduce a novel evaluation framework for determining the efficacy of our
methods. Our results demonstrate that the stock model can achieve an F1 score
of .674 on a zero-shot data citation classification task with no previously
defined categories. While promising, our results are qualified by barriers
related to data availability, prompt overfitting, computational infrastructure,
and the expense required to conduct responsible performance evaluation.

</details>


### [113] [Power Constrained Nonstationary Bandits with Habituation and Recovery Dynamics](https://arxiv.org/abs/2511.02944)
*Fengxu Li,Stephanie M. Carpenter,Matthew P. Buman,Yonatan Mintz*

Main category: cs.LG

TL;DR: The paper addresses the challenge of optimizing actions with evolving rewards in nonstationary settings, introducing ROGUE-TS for sublinear regret and a clipping method to balance personalization and population-level learning.


<details>
  <summary>Details</summary>
Motivation: Decision makers face challenges in selecting actions with unknown, evolving rewards, particularly in behavioral health interventions. Existing methods lack sufficient exploration, hindering population-level effect estimation.

Method: Proposes ROGUE-TS, a Thompson Sampling algorithm for the ROGUE framework, and introduces a probability clipping procedure to balance exploration and exploitation.

Result: ROGUE-TS achieves lower regret than existing approaches and maintains high statistical power via clipping, validated on MRT datasets for physical activity and bipolar disorder.

Conclusion: The framework balances personalization with statistical validity, aiding reliable treatment effect detection in MRTs.

Abstract: A common challenge for decision makers is selecting actions whose rewards are
unknown and evolve over time based on prior policies. For instance, repeated
use may reduce an action's effectiveness (habituation), while inactivity may
restore it (recovery). These nonstationarities are captured by the Reducing or
Gaining Unknown Efficacy (ROGUE) bandit framework, which models real-world
settings such as behavioral health interventions. While existing algorithms can
compute sublinear regret policies to optimize these settings, they may not
provide sufficient exploration due to overemphasis on exploitation, limiting
the ability to estimate population-level effects. This is a challenge of
particular interest in micro-randomized trials (MRTs) that aid researchers in
developing just-in-time adaptive interventions that have population-level
effects while still providing personalized recommendations to individuals. In
this paper, we first develop ROGUE-TS, a Thompson Sampling algorithm tailored
to the ROGUE framework, and provide theoretical guarantees of sublinear regret.
We then introduce a probability clipping procedure to balance personalization
and population-level learning, with quantified trade-off that balances regret
and minimum exploration probability. Validation on two MRT datasets concerning
physical activity promotion and bipolar disorder treatment shows that our
methods both achieve lower regret than existing approaches and maintain high
statistical power through the clipping procedure without significantly
increasing regret. This enables reliable detection of treatment effects while
accounting for individual behavioral dynamics. For researchers designing MRTs,
our framework offers practical guidance on balancing personalization with
statistical validity.

</details>


### [114] [Digital Twin-Driven Pavement Health Monitoring and Maintenance Optimization Using Graph Neural Networks](https://arxiv.org/abs/2511.02957)
*Mohsin Mahmud Topu,Mahfuz Ahmed Anik,Azmine Toushik Wasi,Md Manjurul Ahsan*

Main category: cs.LG

TL;DR: A Digital Twin-GNN framework is proposed for proactive pavement monitoring, outperforming traditional methods with a 0.3798 R2 score.


<details>
  <summary>Details</summary>
Motivation: To address the reactive nature of traditional PMS by enabling real-time, data-driven pavement health monitoring.

Method: Combines Digital Twin and Graph Neural Network to model pavement segments and spatial relations, using UAV, sensor, and LiDAR data.

Result: Achieves an R2 of 0.3798, outperforming baselines, and includes an interactive dashboard for adaptive planning.

Conclusion: The DT-GNN framework enhances precision and sustainability in pavement management, with potential for real-world deployment.

Abstract: Pavement infrastructure monitoring is challenged by complex spatial
dependencies, changing environmental conditions, and non-linear deterioration
across road networks. Traditional Pavement Management Systems (PMS) remain
largely reactive, lacking real-time intelligence for failure prevention and
optimal maintenance planning. To address this, we propose a unified Digital
Twin (DT) and Graph Neural Network (GNN) framework for scalable, data-driven
pavement health monitoring and predictive maintenance. Pavement segments and
spatial relations are modeled as graph nodes and edges, while real-time UAV,
sensor, and LiDAR data stream into the DT. The inductive GNN learns
deterioration patterns from graph-structured inputs to forecast distress and
enable proactive interventions. Trained on a real-world-inspired dataset with
segment attributes and dynamic connectivity, our model achieves an R2 of
0.3798, outperforming baseline regressors and effectively capturing non-linear
degradation. We also develop an interactive dashboard and reinforcement
learning module for simulation, visualization, and adaptive maintenance
planning. This DT-GNN integration enhances forecasting precision and
establishes a closed feedback loop for continuous improvement, positioning the
approach as a foundation for proactive, intelligent, and sustainable pavement
management, with future extensions toward real-world deployment, multi-agent
coordination, and smart-city integration.

</details>


### [115] [Inference-Time Personalized Alignment with a Few User Preference Queries](https://arxiv.org/abs/2511.02966)
*Victor-Alexandru PÄƒdurean,Parameswaran Kamalaruban,Nachiket Kotalwar,Alkis Gotovos,Adish Singla*

Main category: cs.LG

TL;DR: UserAlign is a personalized alignment method that uses few pairwise queries to identify user preferences quickly.


<details>
  <summary>Details</summary>
Motivation: Existing methods for aligning generative models with user preferences require excessive queries or explicit text inputs.

Method: UserAlign leverages logistic bandits theory, assuming consistent user feedback, to select personalized responses from a generated pool.

Result: Experiments show UserAlign effectively aligns responses for personalized text and image generation tasks.

Conclusion: UserAlign offers a practical solution for personalized alignment with minimal user input.

Abstract: We study the problem of aligning a generative model's response with a user's
preferences. Recent works have proposed several different formulations for
personalized alignment; however, they either require a large amount of user
preference queries or require that the preference be explicitly specified as a
text input. In this paper, we propose a novel inference-time personalized
alignment method, UserAlign, that elicits the user's preferences with a few
queries as pairwise response comparisons. In particular, UserAlign builds on
the theoretical framework of best-arm identification in logistic bandits and
selects a personalized response from a fixed pool of the model's generated
responses. The key idea is to consider the user's feedback consistent and
noise-free, and incorporate it into the theoretical framework to identify the
best response quickly. Experimental results across several tasks, involving
personalized text and image generation, showcase the effectiveness of UserAlign
in achieving personalized alignment.

</details>


### [116] [Value of Information-Enhanced Exploration in Bootstrapped DQN](https://arxiv.org/abs/2511.02969)
*Stergios Plataniotis,Charilaos Akasiadis,Georgios Chalkiadakis*

Main category: cs.LG

TL;DR: The paper introduces two novel algorithms integrating expected value of information (EVOI) into Bootstrapped DQN to improve exploration in deep reinforcement learning, especially in high-dimensional, sparse-reward environments, without adding extra hyperparameters.


<details>
  <summary>Details</summary>
Motivation: Traditional exploration methods like $\epsilon$-greedy and Boltzmann struggle in high-dimensional, sparse-reward environments. The paper aims to enhance exploration by leveraging EVOI within the Bootstrapped DQN framework.

Method: The authors develop two algorithms that incorporate EVOI into Bootstrapped DQN, using value of information estimates to measure discrepancies among network heads and guide exploration.

Result: Experiments in complex Atari games show improved performance and better utilization of uncertainty, with no additional hyperparameters.

Conclusion: The integration of EVOI into Bootstrapped DQN effectively enhances exploration and performance in challenging environments, addressing limitations of traditional methods.

Abstract: Efficient exploration in deep reinforcement learning remains a fundamental
challenge, especially in environments characterized by high-dimensional states
and sparse rewards. Traditional exploration strategies that rely on random
local policy noise, such as $\epsilon$-greedy and Boltzmann exploration
methods, often struggle to efficiently balance exploration and exploitation. In
this paper, we integrate the notion of (expected) value of information (EVOI)
within the well-known Bootstrapped DQN algorithmic framework, to enhance the
algorithm's deep exploration ability. Specifically, we develop two novel
algorithms that incorporate the expected gain from learning the value of
information into Bootstrapped DQN. Our methods use value of information
estimates to measure the discrepancies of opinions among distinct network
heads, and drive exploration towards areas with the most potential. We evaluate
our algorithms with respect to performance and their ability to exploit
inherent uncertainty arising from random network initialization. Our
experiments in complex, sparse-reward Atari games demonstrate increased
performance, all the while making better use of uncertainty, and, importantly,
without introducing extra hyperparameters.

</details>


### [117] [Heterogeneous Metamaterials Design via Multiscale Neural Implicit Representation](https://arxiv.org/abs/2511.03012)
*Hongrui Chen,Liwei Wang,Levent Burak Kara*

Main category: cs.LG

TL;DR: A neural network-based framework is proposed for designing heterogeneous metamaterials by learning a continuous two-scale representation, addressing challenges like design space complexity and compatibility issues without predefined datasets.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for designing heterogeneous metamaterials face challenges like expensive optimizations and boundary discontinuities, while data-driven approaches are limited by fixed datasets. This work aims to overcome these limitations by leveraging neural networks for continuous multiscale representation.

Method: The framework uses a multiscale neural network that inputs global and local coordinates to output an implicit field representing compatible unit cell geometries. A compatibility loss term ensures connectivity between adjacent cells, enabling high-resolution designs without predefined data.

Result: The approach successfully demonstrates its effectiveness in mechanical metamaterial design, achieving properties like negative Poisson's ratio and mechanical cloaking, applicable in robotics, bioengineering, and aerospace.

Conclusion: The proposed neural network-based framework offers a scalable and continuous solution for designing heterogeneous metamaterials, overcoming limitations of traditional and data-driven methods while enabling high-resolution fabrication and simulation.

Abstract: Metamaterials are engineered materials composed of specially designed unit
cells that exhibit extraordinary properties beyond those of natural materials.
Complex engineering tasks often require heterogeneous unit cells to accommodate
spatially varying property requirements. However, designing heterogeneous
metamaterials poses significant challenges due to the enormous design space and
strict compatibility requirements between neighboring cells. Traditional
concurrent multiscale design methods require solving an expensive optimization
problem for each unit cell and often suffer from discontinuities at cell
boundaries. On the other hand, data-driven approaches that assemble structures
from a fixed library of microstructures are limited by the dataset and require
additional post-processing to ensure seamless connections. In this work, we
propose a neural network-based metamaterial design framework that learns a
continuous two-scale representation of the structure, thereby jointly
addressing these challenges. Central to our framework is a multiscale neural
representation in which the neural network takes both global (macroscale) and
local (microscale) coordinates as inputs, outputting an implicit field that
represents multiscale structures with compatible unit cell geometries across
the domain, without the need for a predefined dataset. We use a compatibility
loss term during training to enforce connectivity between adjacent unit cells.
Once trained, the network can produce metamaterial designs at arbitrarily high
resolution, hence enabling infinite upsampling for fabrication or simulation.
We demonstrate the effectiveness of the proposed approach on mechanical
metamaterial design, negative Poisson's ratio, and mechanical cloaking problems
with potential applications in robotics, bioengineering, and aerospace.

</details>


### [118] [Discrete Bayesian Sample Inference for Graph Generation](https://arxiv.org/abs/2511.03015)
*Ole Petersen,Marcel Kollovieh,Marten Lienen,Stephan GÃ¼nnemann*

Main category: cs.LG

TL;DR: GraphBSI is a one-shot graph generative model using Bayesian Sample Inference (BSI) to refine beliefs over graphs in continuous space, outperforming benchmarks in molecular and synthetic graph generation.


<details>
  <summary>Details</summary>
Motivation: Traditional generative models struggle with discrete, unordered graph data, prompting the need for innovations like discrete diffusion and flow matching models.

Method: GraphBSI refines beliefs over graphs in continuous space using BSI, framed as a stochastic differential equation (SDE) with noise-controlled updates preserving marginal distributions.

Result: GraphBSI achieves state-of-the-art performance on molecular (Moses) and synthetic (GuacaMol) graph generation benchmarks.

Conclusion: GraphBSI effectively handles discrete graph structures by leveraging continuous space refinement, connecting to Bayesian Flow Networks and Diffusion models, and outperforming existing one-shot models.

Abstract: Generating graph-structured data is crucial in applications such as molecular
generation, knowledge graphs, and network analysis. However, their discrete,
unordered nature makes them difficult for traditional generative models,
leading to the rise of discrete diffusion and flow matching models. In this
work, we introduce GraphBSI, a novel one-shot graph generative model based on
Bayesian Sample Inference (BSI). Instead of evolving samples directly, GraphBSI
iteratively refines a belief over graphs in the continuous space of
distribution parameters, naturally handling discrete structures. Further, we
state BSI as a stochastic differential equation (SDE) and derive a
noise-controlled family of SDEs that preserves the marginal distributions via
an approximation of the score function. Our theoretical analysis further
reveals the connection to Bayesian Flow Networks and Diffusion models. Finally,
in our empirical evaluation, we demonstrate state-of-the-art performance on
molecular and synthetic graph generation, outperforming existing one-shot graph
generative models on the standard benchmarks Moses and GuacaMol.

</details>


### [119] [Adaptive-Sensorless Monitoring of Shipping Containers](https://arxiv.org/abs/2511.03022)
*Lingqing Shen,Chi Heem Wong,Misaki Mito,Arnab Chakrabarti*

Main category: cs.LG

TL;DR: The paper introduces adaptive-sensorless monitoring, a method correcting systematic biases in sensorless models using telemetry data, improving accuracy over baseline models.


<details>
  <summary>Details</summary>
Motivation: Sensorless monitoring lacks telemetry integration and systematic error correction, leading to inaccurate predictions.

Method: Residual correction method is proposed to adjust systematic biases in sensorless models using live telemetry data.

Result: Adaptive-sensorless models outperform baselines, reducing MAEs and RMSEs for temperature and humidity.

Conclusion: The method enhances cargo monitoring accuracy, early risk detection, and reduces reliance on full connectivity.

Abstract: Monitoring the internal temperature and humidity of shipping containers is
essential to preventing quality degradation during cargo transportation.
Sensorless monitoring -- machine learning models that predict the internal
conditions of the containers using exogenous factors -- shows promise as an
alternative to monitoring using sensors. However, it does not incorporate
telemetry information and correct for systematic errors, causing the
predictions to differ significantly from the live data and confusing the users.
In this paper, we introduce the residual correction method, a general framework
for correcting for systematic biases in sensorless models after observing live
telemetry data. We call this class of models ``adaptive-sensorless''
monitoring. We train and evaluate adaptive-sensorless models on the 3.48
million data points -- the largest dataset of container sensor readings ever
used in academic research -- and show that they produce consistent improvements
over the baseline sensorless models. When evaluated on the holdout set of the
simulated data, they achieve average mean absolute errors (MAEs) of 2.24 $\sim$
2.31$^\circ$C (vs 2.43$^\circ$C by sensorless) for temperature and 5.72 $\sim$
7.09% for relative humidity (vs 7.99% by sensorless) and average root
mean-squared errors (RMSEs) of 3.19 $\sim$ 3.26$^\circ$C for temperature (vs
3.38$^\circ$C by sensorless) and 7.70 $\sim$ 9.12% for relative humidity (vs
10.0% by sensorless). Adaptive-sensorless models enable more accurate cargo
monitoring, early risk detection, and less dependence on full connectivity in
global shipping.

</details>


### [120] [Leveraging Discrete Function Decomposability for Scientific Design](https://arxiv.org/abs/2511.03032)
*James C. Bowden,Sergey Levine,Jennifer Listgarten*

Main category: cs.LG

TL;DR: The paper proposes DADO, a decomposition-aware distributional optimization algorithm for efficient discrete object design in AI-driven science, leveraging decomposability for improved optimization.


<details>
  <summary>Details</summary>
Motivation: The challenge of optimizing discrete-valued designs due to combinatorial complexity motivates the need for a method that can utilize decomposability in property predictors.

Method: DADO introduces a soft-factorized search distribution and uses graph message-passing to coordinate optimization across linked factors.

Result: DADO demonstrates improved efficiency in optimizing generative models for discrete designs by leveraging decomposability structures.

Conclusion: DADO successfully addresses limitations of current distributional optimization algorithms by exploiting decomposability, enhancing optimization efficiency.

Abstract: In the era of AI-driven science and engineering, we often want to design
discrete objects in silico according to user-specified properties. For example,
we may wish to design a protein to bind its target, arrange components within a
circuit to minimize latency, or find materials with certain properties. Given a
property predictive model, in silico design typically involves training a
generative model over the design space (e.g., protein sequence space) to
concentrate on designs with the desired properties. Distributional optimization
-- which can be formalized as an estimation of distribution algorithm or as
reinforcement learning policy optimization -- finds the generative model that
maximizes an objective function in expectation. Optimizing a distribution over
discrete-valued designs is in general challenging because of the combinatorial
nature of the design space. However, many property predictors in scientific
applications are decomposable in the sense that they can be factorized over
design variables in a way that could in principle enable more effective
optimization. For example, amino acids at a catalytic site of a protein may
only loosely interact with amino acids of the rest of the protein to achieve
maximal catalytic activity. Current distributional optimization algorithms are
unable to make use of such decomposability structure. Herein, we propose and
demonstrate use of a new distributional optimization algorithm,
Decomposition-Aware Distributional Optimization (DADO), that can leverage any
decomposability defined by a junction tree on the design variables, to make
optimization more efficient. At its core, DADO employs a soft-factorized
"search distribution" -- a learned generative model -- for efficient navigation
of the search space, invoking graph message-passing to coordinate optimization
across linked factors.

</details>


### [121] [Data-Efficient Realized Volatility Forecasting with Vision Transformers](https://arxiv.org/abs/2511.03046)
*Emi Soroka,Artem Arzyn*

Main category: cs.LG

TL;DR: Exploring transformer models for financial options data, specifically using Vision Transformer (ViT) to predict volatility, showing potential.


<details>
  <summary>Details</summary>
Motivation: To investigate the untapped potential of transformer models in financial options forecasting, leveraging their ability to capture complex patterns.

Method: Using the Vision Transformer (ViT) architecture, typically for image tasks, trained on implied volatility surfaces to predict 30-day realized volatility.

Result: ViT successfully learns seasonal and nonlinear features from implied volatility data, indicating promise for further development.

Conclusion: Transformer models like ViT show promising results for financial options forecasting, warranting further exploration.

Abstract: Recent work in financial machine learning has shown the virtue of complexity:
the phenomenon by which deep learning methods capable of learning highly
nonlinear relationships outperform simpler approaches in financial forecasting.
While transformer architectures like Informer have shown promise for financial
time series forecasting, the application of transformer models for options data
remains largely unexplored. We conduct preliminary studies towards the
development of a transformer model for options data by training the Vision
Transformer (ViT) architecture, typically used in modern image recognition and
classification systems, to predict the realized volatility of an asset over the
next 30 days from its implied volatility surface (augmented with date
information) for a single day. We show that the ViT can learn seasonal patterns
and nonlinear features from the IV surface, suggesting a promising direction
for model development.

</details>


### [122] [Unsupervised Evaluation of Multi-Turn Objective-Driven Interactions](https://arxiv.org/abs/2511.03047)
*Emi Soroka,Tanmay Chopra,Krish Desai,Sanjay Lall*

Main category: cs.LG

TL;DR: The paper introduces unsupervised metrics for evaluating large language models (LLMs) in objective-driven interactions, addressing challenges like complex data and unreliable human annotation.


<details>
  <summary>Details</summary>
Motivation: Traditional evaluation methods for LLMs in enterprise applications face issues like unlabeled data, impractical human annotation, and unreliable custom metrics or LLM judges.

Method: The authors develop unsupervised metrics using statistical properties of unlabeled data and fine-tuned LLMs to adapt to shifts. Metrics include labeling user goals, measuring goal completion, and quantifying LLM uncertainty.

Result: The approach is validated on open-domain and task-specific interaction data, demonstrating effectiveness without relying on human-generated ideal responses.

Conclusion: The proposed unsupervised metrics offer a scalable and reliable solution for evaluating LLMs in objective-driven interactions, overcoming limitations of existing methods.

Abstract: Large language models (LLMs) have seen increasing popularity in enterprise
applications where AI agents and humans engage in objective-driven
interactions. However, these systems are difficult to evaluate: data may be
complex and unlabeled; human annotation is often impractical at scale; custom
metrics can monitor for specific errors, but not previously-undetected ones;
and LLM judges can produce unreliable results. We introduce the first set of
unsupervised metrics for objective-driven interactions, leveraging statistical
properties of unlabeled interaction data and using fine-tuned LLMs to adapt to
distributional shifts. We develop metrics for labeling user goals, measuring
goal completion, and quantifying LLM uncertainty without grounding evaluations
in human-generated ideal responses. Our approach is validated on open-domain
and task-specific interaction data.

</details>


### [123] [The Curved Spacetime of Transformer Architectures](https://arxiv.org/abs/2511.03060)
*Riccardo Di Sipio,Jairo Diaz-Rodriguez,Luis Serrano*

Main category: cs.LG

TL;DR: The paper proposes a geometric framework for Transformer-based language models, likening them to General Relativity, and tests this analogy through experiments.


<details>
  <summary>Details</summary>
Motivation: The authors aim to provide a geometric interpretation of Transformer models, comparing attention mechanisms to curved space-time in General Relativity.

Method: They design experiments to visualize and measure curvature in token embeddings, including analyzing turning angles, length-to-chord ratios, and controlled context edits.

Result: Experiments confirm the presence of attention-induced curvature, showing bending in token embedding trajectories that align with the geometric analogy.

Conclusion: The geometric framework offers a novel understanding of Transformer dynamics, with curvature playing a measurable role in token representation evolution.

Abstract: We present a geometric framework for understanding Transformer-based language
models, drawing an explicit analogy to General Relativity. Queries and keys
induce an effective metric on representation space, and attention acts as a
discrete connection that implements parallel transport of value vectors across
tokens. Stacked layers provide discrete time-slices through which token
representations evolve on this curved manifold, while backpropagation plays the
role of a least-action principle that shapes loss-minimizing trajectories in
parameter space. If this analogy is correct, token embeddings should not
traverse straight paths in feature space; instead, their layer-wise steps
should bend and reorient as interactions mediated by embedding space curvature.
To test this prediction, we design experiments that expose both the presence
and the consequences of curvature: (i) we visualize a curvature landscape for a
full paragraph, revealing how local turning angles vary across tokens and
layers; (ii) we show through simulations that excess counts of sharp/flat
angles and longer length-to-chord ratios are not explainable by dimensionality
or chance; and (iii) inspired by Einstein's eclipse experiment, we probe
deflection under controlled context edits, demonstrating measurable,
meaning-consistent bends in embedding trajectories that confirm
attention-induced curvature.

</details>


### [124] [Homomorphism distortion: A metric to distinguish them all and in the latent space bind them](https://arxiv.org/abs/2511.03068)
*Martin Carrasco,Olga Zaghen,Erik Bekkers,Bastian Rieck*

Main category: cs.LG

TL;DR: The paper introduces graph homomorphism distortion as a new measure to compare vertex-attributed graphs, proving its completeness and overcoming graph canonization via sampling. It outperforms previous methods empirically.


<details>
  <summary>Details</summary>
Motivation: To move beyond combinatorial properties in measuring graph neural network expressivity by introducing a principled similarity measure for vertex-attributed graphs.

Method: Proposes graph homomorphism distortion, addresses graph canonization via sampling, and derives a metric from the measure.

Result: Empirically validates that the measure fully distinguishes challenging graphs (BREC dataset) and outperforms prior homomorphism-inspired methods (ZINC-12k dataset).

Conclusion: The measure extends graph characterization beyond traditional methods, enabling future theoretical and empirical advancements.

Abstract: For far too long, expressivity of graph neural networks has been measured
\emph{only} in terms of combinatorial properties. In this work we stray away
from this tradition and provide a principled way to measure similarity between
vertex attributed graphs. We denote this measure as the \emph{graph
homomorphism distortion}. We show it can \emph{completely characterize} graphs
and thus is also a \emph{complete graph embedding}. However, somewhere along
the road, we run into the graph canonization problem. To circumvent this
obstacle, we devise to efficiently compute this measure via sampling, which in
expectation ensures \emph{completeness}. Additionally, we also discovered that
we can obtain a metric from this measure. We validate our claims empirically
and find that the \emph{graph homomorphism distortion}: (1.) fully
distinguishes the \texttt{BREC} dataset with up to $4$-WL non-distinguishable
graphs, and (2.) \emph{outperforms} previous methods inspired in homomorphisms
under the \texttt{ZINC-12k} dataset.
  These theoretical results, (and their empirical validation), pave the way for
future characterization of graphs, extending the graph theoretic tradition to
new frontiers.

</details>


### [125] [Online Learning to Rank under Corruption: A Robust Cascading Bandits Approach](https://arxiv.org/abs/2511.03074)
*Fatemeh Ghaffari,Siddarth Sitaraman,Xutong Liu,Xuchuang Wang,Mohammad Hajiesmaili*

Main category: cs.LG

TL;DR: MSUCB, a robust OLTR algorithm using a mean-of-medians estimator, achieves optimal regret without corruption and degrades gracefully with corruption, outperforming prior methods significantly.


<details>
  <summary>Details</summary>
Motivation: Online learning to rank (OLTR) systems, modeled as cascading bandits, are vulnerable to click fraud and manipulations. Robustness against such corruption is needed to maintain performance.

Method: Proposes MSUCB, a robust algorithm incorporating a novel mean-of-medians estimator, which filters outliers and corrupted samples while behaving like a standard mean in clean settings.

Result: MSUCB achieves optimal logarithmic regret without corruption and degrades gracefully under corruption, with regret increasing only additively. It outperforms prior methods by up to 97.35% in regret improvement.

Conclusion: The MSUCB algorithm is highly effective and robust, providing significant improvements over existing methods in both clean and corrupted settings.

Abstract: Online learning to rank (OLTR) studies how to recommend a short ranked list
of items from a large pool and improves future rankings based on user clicks.
This setting is commonly modeled as cascading bandits, where the objective is
to maximize the likelihood that the user clicks on at least one of the
presented items across as many timesteps as possible. However, such systems are
vulnerable to click fraud and other manipulations (i.e., corruption), where
bots or paid click farms inject corrupted feedback that misleads the learning
process and degrades user experience. In this paper, we propose MSUCB, a robust
algorithm that incorporates a novel mean-of-medians estimator, which to our
knowledge is applied to bandits with corruption setting for the first time.
This estimator behaves like a standard mean in the absence of corruption, so no
cost is paid for robustness. Under corruption, the median step filters out
outliers and corrupted samples, keeping the estimate close to its true value.
Updating this estimate at every round further accelerates empirical convergence
in experiments. Hence, MSUCB achieves optimal logarithmic regret in the absence
of corruption and degrades gracefully under corruptions, with regret increasing
only by an additive term tied to the total corruption. Comprehensive and
extensive experiments on real-world datasets further demonstrate that our
approach consistently outperforms prior methods while maintaining strong
robustness. In particular, it achieves a \(97.35\%\) and a \(91.60\%\) regret
improvement over two state-of-the-art methods.

</details>


### [126] [Sparse, self-organizing ensembles of local kernels detect rare statistical anomalies](https://arxiv.org/abs/2511.03095)
*Gaia Grosso,Sai Sumedh R. Hindupur,Thomas Fel,Samuel Bright-Thonney,Philip Harris,Demba Ba*

Main category: cs.LG

TL;DR: The paper introduces SparKer, a sparse ensemble of Gaussian kernels for anomaly detection. It addresses the gap in detecting weak or rare signals by enforcing sparsity, locality, and competition.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve anomaly detection by addressing the lack of control over statistical properties in modern AI representations, which can hide weak or rare signals.

Method: Proposes SparKer, a semi-supervised Neyman-Pearson framework using sparse ensembles of Gaussian kernels for anomaly detection.

Result: Demonstrates effectiveness in high-dimensional problems, showing interpretability, efficiency, and scalability with just a handful of kernels.

Conclusion: The approach effectively detects anomalies in high-dimensional spaces while maintaining interpretability and efficiency.

Abstract: Modern artificial intelligence has revolutionized our ability to extract rich
and versatile data representations across scientific disciplines. Yet, the
statistical properties of these representations remain poorly controlled,
causing misspecified anomaly detection (AD) methods to falter. Weak or rare
signals can remain hidden within the apparent regularity of normal data,
creating a gap in our ability to detect and interpret anomalies. We examine
this gap and identify a set of structural desiderata for detection methods
operating under minimal prior information: sparsity, to enforce parsimony;
locality, to preserve geometric sensitivity; and competition, to promote
efficient allocation of model capacity. These principles define a class of
self-organizing local kernels that adaptively partition the representation
space around regions of statistical imbalance. As an instantiation of these
principles, we introduce SparKer, a sparse ensemble of Gaussian kernels trained
within a semi-supervised Neyman--Pearson framework to locally model the
likelihood ratio between a sample that may contain anomalies and a nominal,
anomaly-free reference. We provide theoretical insights into the mechanisms
that drive detection and self-organization in the proposed model, and
demonstrate the effectiveness of this approach on realistic high-dimensional
problems of scientific discovery, open-world novelty detection, intrusion
detection, and generative-model validation. Our applications span both the
natural- and computer-science domains. We demonstrate that ensembles containing
only a handful of kernels can identify statistically significant anomalous
locations within representation spaces of thousands of dimensions, underscoring
both the interpretability, efficiency and scalability of the proposed approach.

</details>


### [127] [An Efficient Classification Model for Cyber Text](https://arxiv.org/abs/2511.03107)
*Md Sakhawat Hossen,Md. Zashid Iqbal Borshon,A. S. M. Badrudduza*

Main category: cs.LG

TL;DR: introducing a modified tf-idf (ctf-idf) and irlba for text analytics, reducing computational cost and carbon footprint compared to deep learning, with minor accuracy trade-offs.


<details>
  <summary>Details</summary>
Motivation: the increasing carbon footprint from deep learning's computational demands motivates exploring efficient classical methods like ctf-idf and irlba.

Method: proposes ctf-idf for preprocessing and uses irlba for dimensionality reduction in classical text analytics pipelines.

Result: achieves significant time complexity reduction and accuracy improvement, with lower computational costs.

Conclusion: classical methods with ctf-idf and irlba offer a greener, efficient alternative to deep learning in text analytics.

Abstract: The uprising of deep learning methodology and practice in recent years has
brought about a severe consequence of increasing carbon footprint due to the
insatiable demand for computational resources and power. The field of text
analytics also experienced a massive transformation in this trend of
monopolizing methodology. In this paper, the original TF-IDF algorithm has been
modified, and Clement Term Frequency-Inverse Document Frequency (CTF-IDF) has
been proposed for data preprocessing. This paper primarily discusses the
effectiveness of classical machine learning techniques in text analytics with
CTF-IDF and a faster IRLBA algorithm for dimensionality reduction. The
introduction of both of these techniques in the conventional text analytics
pipeline ensures a more efficient, faster, and less computationally intensive
application when compared with deep learning methodology regarding carbon
footprint, with minor compromise in accuracy. The experimental results also
exhibit a manifold of reduction in time complexity and improvement of model
accuracy for the classical machine learning methods discussed further in this
paper.

</details>


### [128] [Towards Scalable Backpropagation-Free Gradient Estimation](https://arxiv.org/abs/2511.03110)
*Daniel Wang,Evan Markou,Dylan Campbell*

Main category: cs.LG

TL;DR: The paper introduces a gradient estimation method that reduces bias and variance by manipulating upstream Jacobian matrices, showing promise for scaling to larger networks.


<details>
  <summary>Details</summary>
Motivation: Existing gradient estimation methods suffer from high variance or significant bias, limiting their scalability and utility in deep learning.

Method: The proposed approach manipulates upstream Jacobian matrices when computing guess directions to reduce bias and variance.

Result: The method shows promising results, improving performance as network width increases, and is analyzed for bias and variance.

Conclusion: The new gradient estimation technique has potential for scalability and better performance in larger networks, with insights drawn from bias and variance analysis.

Abstract: While backpropagation--reverse-mode automatic differentiation--has been
extraordinarily successful in deep learning, it requires two passes (forward
and backward) through the neural network and the storage of intermediate
activations. Existing gradient estimation methods that instead use forward-mode
automatic differentiation struggle to scale beyond small networks due to the
high variance of the estimates. Efforts to mitigate this have so far introduced
significant bias to the estimates, reducing their utility. We introduce a
gradient estimation approach that reduces both bias and variance by
manipulating upstream Jacobian matrices when computing guess directions. It
shows promising results and has the potential to scale to larger networks,
indeed performing better as the network width is increased. Our understanding
of this method is facilitated by analyses of bias and variance, and their
connection to the low-dimensional structure of neural network gradients.

</details>


### [129] [FP-AbDiff: Improving Score-based Antibody Design by Capturing Nonequilibrium Dynamics through the Underlying Fokker-Planck Equation](https://arxiv.org/abs/2511.03113)
*Jiameng Chen,Yida Xiong,Kun Li,Hongzhi Zhang,Xiantao Cai,Wenbin Hu,Jia Wu*

Main category: cs.LG

TL;DR: FP-AbDiff, a new antibody generator, enforces Fokker-Planck physics for physically plausible and generalizable designs, outperforming existing models in benchmark tests.


<details>
  <summary>Details</summary>
Motivation: To address limitations in computational antibody design, such as lack of dynamical consistency and poor generalization due to data scarcity.

Method: FP-AbDiff integrates Fokker-Planck Equation physics with SE(3)-equivariant diffusion, using a novel FPE residual loss on CDR geometries.

Result: Achieves 25% better RMSD and highest Contact Amino Acid Recovery (39.91%) in CDR-H3 design; 15% lower full-chain RMSD in six-CDR co-design.

Conclusion: FP-AbDiff's physics-aligned approach enhances robustness and generalizability, setting a new benchmark for antibody design.

Abstract: Computational antibody design holds immense promise for therapeutic
discovery, yet existing generative models are fundamentally limited by two core
challenges: (i) a lack of dynamical consistency, which yields physically
implausible structures, and (ii) poor generalization due to data scarcity and
structural bias. We introduce FP-AbDiff, the first antibody generator to
enforce Fokker-Planck Equation (FPE) physics along the entire generative
trajectory. Our method minimizes a novel FPE residual loss over the mixed
manifold of CDR geometries (R^3 x SO(3)), compelling locally-learned denoising
scores to assemble into a globally coherent probability flow. This
physics-informed regularizer is synergistically integrated with deep biological
priors within a state-of-the-art SE(3)-equivariant diffusion framework.
Rigorous evaluation on the RAbD benchmark confirms that FP-AbDiff establishes a
new state-of-the-art. In de novo CDR-H3 design, it achieves a mean Root Mean
Square Deviation of 0.99 {\AA} when superposing on the variable region, a 25%
improvement over the previous state-of-the-art model, AbX, and the highest
reported Contact Amino Acid Recovery of 39.91%. This superiority is underscored
in the more challenging six-CDR co-design task, where our model delivers
consistently superior geometric precision, cutting the average full-chain Root
Mean Square Deviation by ~15%, and crucially, achieves the highest full-chain
Amino Acid Recovery on the functionally dominant CDR-H3 loop (45.67%). By
aligning generative dynamics with physical laws, FP-AbDiff enhances robustness
and generalizability, establishing a principled approach for physically
faithful and functionally viable antibody design.

</details>


### [130] [An Augmentation Overlap Theory of Contrastive Learning](https://arxiv.org/abs/2511.03114)
*Qi Zhang,Yifei Wang,Yisen Wang*

Main category: cs.LG

TL;DR: The paper tightens bounds for self-supervised contrastive learning under conditional independence and proposes a more practical augmentation overlap assumption, deriving asymptotically closed bounds for downstream performance. It also introduces an unsupervised evaluation metric aligned with downstream results.


<details>
  <summary>Details</summary>
Motivation: To clarify the unclear working mechanism of self-supervised contrastive learning and improve its theoretical understanding and practical performance.

Method: The study tightens bounds under conditional independence, relaxes it to augmentation overlap, and develops an unsupervised metric for representation evaluation.

Result: The proposed augmentation overlap theory explains how aggressive augmentations help cluster intra-class samples, and the derived metric aligns well with downstream performance.

Conclusion: The paper advances the theoretical foundation of contrastive learning and offers practical tools for evaluation, demonstrating effectiveness through empirical validation.

Abstract: Recently, self-supervised contrastive learning has achieved great success on
various tasks. However, its underlying working mechanism is yet unclear. In
this paper, we first provide the tightest bounds based on the widely adopted
assumption of conditional independence. Further, we relax the conditional
independence assumption to a more practical assumption of augmentation overlap
and derive the asymptotically closed bounds for the downstream performance. Our
proposed augmentation overlap theory hinges on the insight that the support of
different intra-class samples will become more overlapped under aggressive data
augmentations, thus simply aligning the positive samples (augmented views of
the same sample) could make contrastive learning cluster intra-class samples
together. Moreover, from the newly derived augmentation overlap perspective, we
develop an unsupervised metric for the representation evaluation of contrastive
learning, which aligns well with the downstream performance almost without
relying on additional modules. Code is available at
https://github.com/PKU-ML/GARC.

</details>


### [131] [From Insight to Exploit: Leveraging LLM Collaboration for Adaptive Adversarial Text Generation](https://arxiv.org/abs/2511.03128)
*Najrin Sultana,Md Rafi Ur Rashid,Kang Gu,Shagufta Mehnaz*

Main category: cs.LG

TL;DR: LLMs offer strong zero-shot performance but require robustness testing against adversarial inputs. StaDec and DyDec frameworks generate subtle, adaptive adversarial examples using LLMs' understanding, ensuring semantic similarity while deceiving LLMs. The automated pipeline eliminates external heuristics, and attacks show transferability across models. The work aids LLM self-assessment.


<details>
  <summary>Details</summary>
Motivation: Assess LLM robustness against adversarial inputs for sensitive tasks, ensuring reliable performance.

Method: Introduce StaDec and DyDec frameworks to generate dynamic, adaptive adversarial examples via an LLM-driven pipeline.

Result: Produces subtle, natural-looking adversarial inputs with high semantic similarity and strong transferability across models.

Conclusion: Provides a systematic approach for LLM robustness self-assessment, aiding in reliable deployment.

Abstract: LLMs can provide substantial zero-shot performance on diverse tasks using a
simple task prompt, eliminating the need for training or fine-tuning. However,
when applying these models to sensitive tasks, it is crucial to thoroughly
assess their robustness against adversarial inputs. In this work, we introduce
Static Deceptor (StaDec) and Dynamic Deceptor (DyDec), two innovative attack
frameworks designed to systematically generate dynamic and adaptive adversarial
examples by leveraging the understanding of the LLMs. We produce subtle and
natural-looking adversarial inputs that preserve semantic similarity to the
original text while effectively deceiving the target LLM. By utilizing an
automated, LLM-driven pipeline, we eliminate the dependence on external
heuristics. Our attacks evolve with the advancements in LLMs and demonstrate
strong transferability across models unknown to the attacker. Overall, this
work provides a systematic approach for the self-assessment of an LLM's
robustness. We release our code and data at
https://github.com/Shukti042/AdversarialExample.

</details>


### [132] [Test Time Adaptation Using Adaptive Quantile Recalibration](https://arxiv.org/abs/2511.03148)
*Paria Mehrbod,Pedro Vianna,Geraldin Nanfack,Guy Wolf,Eugene Belilovsky*

Main category: cs.LG

TL;DR: AQR is a test-time adaptation method aligning quantiles channel-wise for better domain adaptation without retraining, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Enhancing model generalizability in real-world scenarios where test distributions differ from training, addressing limitations of conventional and recent methods.

Method: Adaptive Quantile Recalibration (AQR) modifies pre-activation distributions by aligning quantiles channel-wise, using robust tail calibration for varying batch sizes.

Result: AQR outperforms baselines on CIFAR-10-C, CIFAR-100-C, and ImageNet-C across architectures, showing robust adaptation.

Conclusion: AQR is effective for real-world deployment in dynamic scenarios, generalizing across normalization layers and architectures.

Abstract: Domain adaptation is a key strategy for enhancing the generalizability of
deep learning models in real-world scenarios, where test distributions often
diverge significantly from the training domain. However, conventional
approaches typically rely on prior knowledge of the target domain or require
model retraining, limiting their practicality in dynamic or
resource-constrained environments. Recent test-time adaptation methods based on
batch normalization statistic updates allow for unsupervised adaptation, but
they often fail to capture complex activation distributions and are constrained
to specific normalization layers. We propose Adaptive Quantile Recalibration
(AQR), a test-time adaptation technique that modifies pre-activation
distributions by aligning quantiles on a channel-wise basis. AQR captures the
full shape of activation distributions and generalizes across architectures
employing BatchNorm, GroupNorm, or LayerNorm. To address the challenge of
estimating distribution tails under varying batch sizes, AQR incorporates a
robust tail calibration strategy that improves stability and precision. Our
method leverages source-domain statistics computed at training time, enabling
unsupervised adaptation without retraining models. Experiments on CIFAR-10-C,
CIFAR-100-C, and ImageNet-C across multiple architectures demonstrate that AQR
achieves robust adaptation across diverse settings, outperforming existing
test-time adaptation baselines. These results highlight AQR's potential for
deployment in real-world scenarios with dynamic and unpredictable data
distributions.

</details>


### [133] [Forecast2Anomaly (F2A): Adapting Multivariate Time Series Foundation Models for Anomaly Prediction](https://arxiv.org/abs/2511.03149)
*Atif Hassan,Tarun Kumar,Ashish Mishra,Sergey Serebryakov,Satish Kumar Mopur,Phanidhar Koganti,Murthy Chelankuri,Ramanagopal Vogety,Suparna Bhattacharya,Martin Foltin*

Main category: cs.LG

TL;DR: F2A leverages pretrained Time Series Foundation Models (TSFMs) with a novel framework for anomaly prediction, combining fine-tuning and dynamic retrieval to outperform existing methods.


<details>
  <summary>Details</summary>
Motivation: The need to predict anomalies in multivariate time series to preempt failures and reduce costs, despite limitations of existing methods in generalizing to evolving patterns.

Method: F2A introduces a joint forecast-anomaly loss and a Retrieval-Augmented Generation (RAG) module to adapt TSFMs for anomaly prediction without model updates.

Result: F2A outperforms state-of-the-art methods across 16 datasets, demonstrating robust zero-shot anomaly prediction capabilities.

Conclusion: F2A bridges the gap between TSFM forecasting and anomaly prediction, offering a scalable solution for real-world applications.

Abstract: Forecasting anomalies (anomaly prediction) in multivariate time series from
different real-world, dynamic, and complex systems is vital for preempting
critical failures, leading to a substantial minimization in operational costs
and human labor. Yet, existing methods are limited to specific systems while
failing to generalize to evolving anomaly patterns over time. In contrast,
pretrained Time Series Foundation Models (TSFMs) have recently demonstrated
strong generalization and zero-shot forecasting capabilities. However, their
potential remains untapped for anomaly prediction, a task fundamentally
different from forecasting normal behavior. Thus, we present Forecast2Anomaly
(F2A), a novel framework that empowers TSFMs with anomaly prediction abilities
through two key innovations. First, we propose a joint forecast-anomaly loss
that fine-tunes TSFMs to accurately forecast future signals even at anomalous
time points. Second, we introduce a Retrieval-Augmented Generation (RAG) module
that retrieves historically relevant horizons and conditions predictions on
them. This component dynamically adapts to distributional shifts at inference
time, enabling F2A to track evolving anomalies without requiring model updates.
By combining targeted fine-tuning with dynamic retrieval, F2A bridges the gap
between robust TSFM zero-shot forecasting and zero-shot anomaly prediction.
Extensive experiments across 16 diverse datasets and multiple TSFM backbones
show that F2A consistently outperforms state-of-the-art methods, offering a
scalable, zero-shot anomaly prediction solution for real-world applications.

</details>


### [134] [UnCLe: Towards Scalable Dynamic Causal Discovery in Non-linear Temporal Systems](https://arxiv.org/abs/2511.03168)
*Tingzhu Bi,Yicheng Pan,Xinrui Jiang,Huize Sun,Meng Ma,Ping Wang*

Main category: cs.LG

TL;DR: UnCLe is a deep learning method that uncovers dynamic causal relationships in time series data using uncoupler and recoupler networks, outperforming existing methods in both static and dynamic contexts.


<details>
  <summary>Details</summary>
Motivation: Understanding evolving cause-effect relationships in complex systems requires time-resolved causal graphs, which existing static methods fail to capture.

Method: UnCLe uses uncoupler and recoupler networks to disentangle time series into semantic representations and learns dependencies via auto-regressive matrices, analyzing prediction errors from temporal perturbations.

Result: UnCLe outperforms state-of-the-art methods in static causal discovery and excels at capturing temporal causality in synthetic and real-world systems.

Conclusion: UnCLe provides a scalable and accurate solution for uncovering time-varying causal mechanisms in complex phenomena.

Abstract: Uncovering cause-effect relationships from observational time series is
fundamental to understanding complex systems. While many methods infer static
causal graphs, real-world systems often exhibit dynamic causality-where
relationships evolve over time. Accurately capturing these temporal dynamics
requires time-resolved causal graphs. We propose UnCLe, a novel deep learning
method for scalable dynamic causal discovery. UnCLe employs a pair of Uncoupler
and Recoupler networks to disentangle input time series into semantic
representations and learns inter-variable dependencies via auto-regressive
Dependency Matrices. It estimates dynamic causal influences by analyzing
datapoint-wise prediction errors induced by temporal perturbations. Extensive
experiments demonstrate that UnCLe not only outperforms state-of-the-art
baselines on static causal discovery benchmarks but, more importantly, exhibits
a unique capability to accurately capture and represent evolving temporal
causality in both synthetic and real-world dynamic systems (e.g., human
motion). UnCLe offers a promising approach for revealing the underlying,
time-varying mechanisms of complex phenomena.

</details>


### [135] [Periodic Skill Discovery](https://arxiv.org/abs/2511.03187)
*Jonghae Park,Daesol Cho,Jusuk Lee,Dongseok Shim,Inkyu Jang,H. Jin Kim*

Main category: cs.LG

TL;DR: PSD introduces a framework for unsupervised discovery of periodic skills in RL, mapping states to a circular latent space to naturally encode periodicity, and shows effectiveness in diverse robotic tasks.


<details>
  <summary>Details</summary>
Motivation: Current methods often ignore the periodic nature of skills, which is crucial for tasks like locomotion, prompting the need for a framework to discover diverse periodic behaviors.

Method: PSD trains an encoder to map states to a circular latent space, capturing temporal distance to learn skills with varying periods, even from pixel observations.

Result: PSD successfully learns diverse periodic skills and achieves high performance in downstream tasks like hurdling, while enhancing behavior diversity when combined with existing methods.

Conclusion: PSD effectively addresses the need for periodic skill discovery in RL, demonstrating practical utility in robotic tasks and potential for broader applications.

Abstract: Unsupervised skill discovery in reinforcement learning (RL) aims to learn
diverse behaviors without relying on external rewards. However, current methods
often overlook the periodic nature of learned skills, focusing instead on
increasing the mutual dependence between states and skills or maximizing the
distance traveled in latent space. Considering that many robotic tasks --
particularly those involving locomotion -- require periodic behaviors across
varying timescales, the ability to discover diverse periodic skills is
essential. Motivated by this, we propose Periodic Skill Discovery (PSD), a
framework that discovers periodic behaviors in an unsupervised manner. The key
idea of PSD is to train an encoder that maps states to a circular latent space,
thereby naturally encoding periodicity in the latent representation. By
capturing temporal distance, PSD can effectively learn skills with diverse
periods in complex robotic tasks, even with pixel-based observations. We
further show that these learned skills achieve high performance on downstream
tasks such as hurdling. Moreover, integrating PSD with an existing skill
discovery method offers more diverse behaviors, thus broadening the agent's
repertoire. Our code and demos are available at
https://jonghaepark.github.io/psd/

</details>


### [136] [Efficient Linear Attention for Multivariate Time Series Modeling via Entropy Equality](https://arxiv.org/abs/2511.03190)
*Mingtao Zhang,Guoli Yang,Zhanxing Zhu,Mengzhu Wang,Xiaoying Bai*

Main category: cs.LG

TL;DR: The paper introduces a novel linear attention mechanism to address the quadratic computational complexity issue in traditional attention models, leveraging entropy-based approximations for efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional attention mechanisms face scalability issues due to high computational complexity, limiting their use for long sequences.

Method: The authors propose a linear attention mechanism using entropy-based approximations, arguing that balanced weight distributions are key to attention's efficacy.

Result: Experiments on spatio-temporal datasets show competitive or better performance with reduced memory and computational costs.

Conclusion: The study suggests that attention's effectiveness may rely more on balanced weight distributions than softmax non-linearity.

Abstract: Attention mechanisms have been extensively employed in various applications,
including time series modeling, owing to their capacity to capture intricate
dependencies; however, their utility is often constrained by quadratic
computational complexity, which impedes scalability for long sequences. In this
work, we propose a novel linear attention mechanism designed to overcome these
limitations. Our approach is grounded in a theoretical demonstration that
entropy, as a strictly concave function on the probability simplex, implies
that distributions with aligned probability rankings and similar entropy values
exhibit structural resemblance. Building on this insight, we develop an
efficient approximation algorithm that computes the entropy of
dot-product-derived distributions with only linear complexity, enabling the
implementation of a linear attention mechanism based on entropy equality.
Through rigorous analysis, we reveal that the effectiveness of attention in
spatio-temporal time series modeling may not primarily stem from the
non-linearity of softmax but rather from the attainment of a moderate and
well-balanced weight distribution. Extensive experiments on four
spatio-temporal datasets validate our method, demonstrating competitive or
superior forecasting performance while achieving substantial reductions in both
memory usage and computational time.

</details>


### [137] [Cross-Modal Alignment via Variational Copula Modelling](https://arxiv.org/abs/2511.03196)
*Feng Wu,Tsai Hor Chan,Fuying Wang,Guosheng Yin,Lequan Yu*

Main category: cs.LG

TL;DR: Proposes a copula-driven framework for multimodal learning to model complex interactions between modalities, outperforming existing methods on MIMIC datasets.


<details>
  <summary>Details</summary>
Motivation: Existing methods oversimplify interactions between modalities, and higher-order joint distributions are underexplored. Copulas offer a powerful way to model these interactions.

Method: Uses Gaussian mixture distributions for each modality and a copula model to align and fuse their joint distribution, enabling accurate representation of missing modalities.

Result: Demonstrates superior performance on MIMIC datasets compared to competitors.

Conclusion: The copula-driven framework effectively captures complex multimodal interactions and improves representation learning.

Abstract: Various data modalities are common in real-world applications (e.g.,
electronic health records, medical images and clinical notes in healthcare). It
is essential to develop multimodal learning methods to aggregate various
information from multiple modalities. The main challenge is how to
appropriately align and fuse the representations of different modalities into a
joint distribution. Existing methods mainly rely on concatenation or the
Kronecker product, oversimplifying the interaction structure between modalities
and indicating a need to model more complex interactions. Additionally, the
joint distribution of latent representations with higher-order interactions is
underexplored. Copula is a powerful statistical structure for modelling the
interactions among variables, as it naturally bridges the joint distribution
and marginal distributions of multiple variables. We propose a novel
copula-driven multimodal learning framework, which focuses on learning the
joint distribution of various modalities to capture the complex interactions
among them. The key idea is to interpret the copula model as a tool to align
the marginal distributions of the modalities efficiently. By assuming a
Gaussian mixture distribution for each modality and a copula model on the joint
distribution, our model can generate accurate representations for missing
modalities. Extensive experiments on public MIMIC datasets demonstrate the
superior performance of our model over other competitors. The code is available
at https://github.com/HKU-MedAI/CMCM.

</details>


### [138] [A Probabilistic U-Net Approach to Downscaling Climate Simulations](https://arxiv.org/abs/2511.03197)
*Maryam Alipourhajiagha,Pierre-Louis Lemaire,Youssef Diouane,Julie Carreau*

Main category: cs.LG

TL;DR: The paper explores using a probabilistic U-Net for statistical downscaling of climate data, comparing four training objectives to improve resolution for precipitation and temperature.


<details>
  <summary>Details</summary>
Motivation: Climate models often produce low-resolution outputs, but finer scales are needed for impact studies. Statistical downscaling addresses this gap.

Method: The study adapts the probabilistic U-Net, combining a deterministic backbone with a variational latent space to handle uncertainty.

Result: WMSE-MS-SSIM excels for extreme values under certain conditions, while afCRPS better captures spatial variability across scales.

Conclusion: The findings suggest tailored training objectives can enhance downscaling performance for specific climate variables or scenarios.

Abstract: Climate models are limited by heavy computational costs, often producing
outputs at coarse spatial resolutions, while many climate change impact studies
require finer scales. Statistical downscaling bridges this gap, and we adapt
the probabilistic U-Net for this task, combining a deterministic U-Net backbone
with a variational latent space to capture aleatoric uncertainty. We evaluate
four training objectives, afCRPS and WMSE-MS-SSIM with three settings for
downscaling precipitation and temperature from $16\times$ coarser resolution.
Our main finding is that WMSE-MS-SSIM performs well for extremes under certain
settings, whereas afCRPS better captures spatial variability across scales.

</details>


### [139] [A Quantized VAE-MLP Botnet Detection Model: A Systematic Evaluation of Quantization-Aware Training and Post-Training Quantization Strategies](https://arxiv.org/abs/2511.03201)
*Hassan Wasswa,Hussein Abbass,Timothy Lynar*

Main category: cs.LG

TL;DR: The paper proposes a VAE-MLP model for lightweight IoT botnet detection, comparing QAT and PTQ quantization methods, with PTQ showing better performance in accuracy, speed, and storage efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the computational intensity of deep learning models for IoT botnet detection, the study focuses on creating a lightweight solution using quantization.

Method: A VAE-MLP model is introduced, where an MLP classifier is trained on latent vectors from a pretrained VAE. QAT and PTQ quantization strategies are evaluated on two datasets.

Result: PTQ outperformed QAT with minimal accuracy loss, achieving 6x speedup and 21x size reduction, while QAT showed 3x speedup and 24x compression.

Conclusion: Quantization, especially PTQ, is practical for deploying lightweight botnet detection models on resource-constrained IoT devices.

Abstract: In an effort to counter the increasing IoT botnet-based attacks,
state-of-the-art deep learning methods have been proposed and have achieved
impressive detection accuracy. However, their computational intensity restricts
deployment on resource-constrained IoT devices, creating a critical need for
lightweight detection models. A common solution to this challenge is model
compression via quantization. This study proposes a VAE-MLP model framework
where an MLP-based classifier is trained on 8-dimensional latent vectors
derived from the high-dimensional train data using the encoder component of a
pretrained variational autoencoder (VAE). Two widely used quantization
strategies--Quantization-Aware Training (QAT) and Post-Training Quantization
(PTQ)--are then systematically evaluated in terms of their impact on detection
performance, storage efficiency, and inference latency using two benchmark IoT
botnet datasets--N-BaIoT and CICIoT2022. The results revealed that, with
respect to detection accuracy, the QAT strategy experienced a more noticeable
decline,whereas PTQ incurred only a marginal reduction compared to the original
unquantized model. Furthermore, PTQ yielded a 6x speedup and 21x reduction in
size, while QAT achieved a 3x speedup and 24x compression, demonstrating the
practicality of quantization for device-level IoT botnet detection.

</details>


### [140] [A Feedback-Control Framework for Efficient Dataset Collection from In-Vehicle Data Streams](https://arxiv.org/abs/2511.03239)
*Philipp Reis,Philipp Rigoll,Christian Steinhauser,Jacob Langner,Eric Sax*

Main category: cs.LG

TL;DR: Introduces FCDC, a closed-loop data collection method using online probabilistic modeling and feedback signals to improve dataset diversity and reduce redundancy.


<details>
  <summary>Details</summary>
Motivation: Addresses inefficiencies in open-loop data collection, such as redundant samples and poor generalization, by transforming it into a feedback-driven process.

Method: FCDC uses probabilistic models and feedback signals (likelihood, Mahalanobis distance) to dynamically balance exploration and exploitation during data collection.

Result: Achieves a 25.9% improvement in dataset balance and reduces storage by 39.8%, demonstrating controllable data collection.

Conclusion: FCDC makes data collection an active, self-regulating process, enhancing efficiency and diversity in data-centric AI.

Abstract: Modern AI systems are increasingly constrained not by model capacity but by
the quality and diversity of their data. Despite growing emphasis on
data-centric AI, most datasets are still gathered in an open-loop manner which
accumulates redundant samples without feedback from the current coverage. This
results in inefficient storage, costly labeling, and limited generalization. To
address this, this paper introduces \ac{FCDC}, a paradigm that formulates data
collection as a closed-loop control problem. \ac{FCDC} continuously
approximates the state of the collected data distribution using an online
probabilistic model and adaptively regulates sample retention using based on
feedback signals such as likelihood and Mahalanobis distance. Through this
feedback mechanism, the system dynamically balances exploration and
exploitation, maintains dataset diversity, and prevents redundancy from
accumulating over time. Besides showcasing the controllability of \ac{FCDC} on
a synthetic dataset, experiments on a real data stream show that \ac{FCDC}
produces more balanced datasets by $\SI{25.9}{\percent}$ while reducing data
storage by $\SI{39.8}{\percent}$. These results demonstrate that data
collection itself can be actively controlled, transforming collection from a
passive pipeline stage into a self-regulating, feedback-driven process at the
core of data-centric AI.

</details>


### [141] [A unified physics-informed generative operator framework for general inverse problems](https://arxiv.org/abs/2511.03241)
*Gang Bao,Yaohua Zang*

Main category: cs.LG

TL;DR: IGNO, a generative neural operator framework, solves PDE-governed inverse problems without labeled data, outperforming state-of-the-art methods under noise and handling discontinuous coefficients.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods face limitations with sparse/noisy measurements or high-dimensional/discontinuous coefficients, restricting practical use.

Method: IGNO encodes coefficients into a latent space, uses neural operator decoders for reconstruction, and trains via physics constraints and gradient-based optimization.

Result: IGNO achieves accurate, stable inversion even under severe noise, outperforming current methods and generalizing well to out-of-distribution targets.

Conclusion: IGNO is a unified, powerful framework for challenging inverse problems in computational science.

Abstract: Solving inverse problems governed by partial differential equations (PDEs) is
central to science and engineering, yet remains challenging when measurements
are sparse, noisy, or when the underlying coefficients are high-dimensional or
discontinuous. Existing deep learning approaches either require extensive
labeled datasets or are limited to specific measurement types, often leading to
failure in such regimes and restricting their practical applicability. Here, a
novel generative neural operator framework, IGNO, is introduced to overcome
these limitations. IGNO unifies the solution of inverse problems from both
point measurements and operator-valued data without labeled training pairs.
This framework encodes high-dimensional, potentially discontinuous coefficient
fields into a low-dimensional latent space, which drives neural operator
decoders to reconstruct both coefficients and PDE solutions. Training relies
purely on physics constraints through PDE residuals, while inversion proceeds
via efficient gradient-based optimization in latent space, accelerated by an a
priori normalizing flow model. Across a diverse set of challenging inverse
problems, including recovery of discontinuous coefficients from solution-based
measurements and the EIT problem with operator-based measurements, IGNO
consistently achieves accurate, stable, and scalable inversion even under
severe noise. It consistently outperforms the state-of-the-art method under
varying noise levels and demonstrates strong generalization to
out-of-distribution targets. These results establish IGNO as a unified and
powerful framework for tackling challenging inverse problems across
computational science domains.

</details>


### [142] [GMoPE:A Prompt-Expert Mixture Framework for Graph Foundation Models](https://arxiv.org/abs/2511.03251)
*Zhibin Wang,Zhixing Zhang,Shuqi Wang,Xuanting Xie,Zhao Kang*

Main category: cs.LG

TL;DR: GMoPE integrates Mixture-of-Experts with prompt-based learning for graphs, improving generalization and efficiency.


<details>
  <summary>Details</summary>
Motivation: Address limitations of GNNs in generalization, scalability, and adaptation costs.

Method: Combines expert-specific prompts and structure-aware routing with a soft orthogonality constraint to prevent expert collapse.

Result: Outperforms state-of-the-art baselines with reduced adaptation overhead.

Conclusion: GMoPE offers a scalable and efficient framework for generalizable graph foundation models.

Abstract: Graph Neural Networks (GNNs) have demonstrated impressive performance on
task-specific benchmarks, yet their ability to generalize across diverse
domains and tasks remains limited. Existing approaches often struggle with
negative transfer, scalability issues, and high adaptation costs. To address
these challenges, we propose GMoPE (Graph Mixture of Prompt-Experts), a novel
framework that seamlessly integrates the Mixture-of-Experts (MoE) architecture
with prompt-based learning for graphs. GMoPE leverages expert-specific prompt
vectors and structure-aware MoE routing to enable each expert to specialize in
distinct subdomains and dynamically contribute to predictions. To promote
diversity and prevent expert collapse, we introduce a soft orthogonality
constraint across prompt vectors, encouraging expert specialization and
facilitating a more balanced expert utilization. Additionally, we adopt a
prompt-only fine-tuning strategy that significantly reduces spatiotemporal
complexity during transfer. We validate GMoPE through extensive experiments
under various pretraining strategies and multiple downstream tasks. Results
show that GMoPE consistently outperforms state-of-the-art baselines and
achieves performance comparable to full parameter fine-tuning-while requiring
only a fraction of the adaptation overhead. Our work provides a principled and
scalable framework for advancing generalizable and efficient graph foundation
models.

</details>


### [143] [Decoupled Entropy Minimization](https://arxiv.org/abs/2511.03256)
*Jing Ma,Hanlin Li,Xiang Xiang*

Main category: cs.LG

TL;DR: The paper analyzes the limitations of classical Entropy Minimization (EM) and proposes AdaDEM, a reformulated approach that outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of classical EM, such as reward collapse and easy-class bias, the paper aims to decouple and reformulate EM for better performance.

Method: Classical EM is decoupled into CADF and GMC, then replaced with AdaDEM, which includes CADF normalization and MEC.

Result: AdaDEM surpasses DEM* and performs well in noisy and dynamic environments.

Conclusion: AdaDEM effectively addresses classical EM's limitations, enhancing performance in imperfectly supervised learning tasks.

Abstract: Entropy Minimization (EM) is beneficial to reducing class overlap, bridging
domain gap, and restricting uncertainty for various tasks in machine learning,
yet its potential is limited. To study the internal mechanism of EM, we
reformulate and decouple the classical EM into two parts with opposite effects:
cluster aggregation driving factor (CADF) rewards dominant classes and prompts
a peaked output distribution, while gradient mitigation calibrator (GMC)
penalizes high-confidence classes based on predicted probabilities.
Furthermore, we reveal the limitations of classical EM caused by its coupled
formulation: 1) reward collapse impedes the contribution of high-certainty
samples in the learning process, and 2) easy-class bias induces misalignment
between output distribution and label distribution. To address these issues, we
propose Adaptive Decoupled Entropy Minimization (AdaDEM), which normalizes the
reward brought from CADF and employs a marginal entropy calibrator (MEC) to
replace GMC. AdaDEM outperforms DEM*, an upper-bound variant of classical EM,
and achieves superior performance across various imperfectly supervised
learning tasks in noisy and dynamic environments.

</details>


### [144] [Diffusion Language Models are Super Data Learners](https://arxiv.org/abs/2511.03276)
*Jinjie Ni,Qian Liu,Longxu Dou,Chao Du,Zili Wang,Hang Yan,Tianyu Pang,Michael Qizhe Shieh*

Main category: cs.LG

TL;DR: Diffusion language models (DLMs) outperform autoregressive (AR) models when unique data is limited, with gains attributed to any-order modeling, super-dense compute, and Monte Carlo augmentation. At scale, DLMs trained under similar settings surpass AR models in performance.


<details>
  <summary>Details</summary>
Motivation: The study aims to explore the performance of diffusion language models compared to autoregressive models, especially under data constraints, and to identify the factors contributing to DLMs' superiority.

Method: The researchers conducted controlled pre-training experiments, varying data quantity, quality, and model size. They analyzed the impact of factors like any-order modeling, iterative bidirectional denoising, and Monte Carlo augmentation on model performance.

Result: DLMs consistently outperformed AR models under limited unique data, with larger models and higher-quality data delaying the crossover point. A 1.7B DLM surpassed an AR coder with matched settings, and a smaller 1B DLM achieved significant accuracy on benchmarks using minimal data.

Conclusion: DLMs excel over AR models in data-constrained scenarios due to their unique advantages, and rising validation cross-entropy does not necessarily indicate degraded downstream performance for DLMs.

Abstract: Under strictly controlled pre-training settings, we observe a Crossover: when
unique data is limited, diffusion language models (DLMs) consistently surpass
autoregressive (AR) models by training for more epochs. The crossover shifts
later with more or higher-quality data, earlier with larger models, and
persists across dense and sparse architectures. We attribute the gains to three
compounding factors: (1) any-order modeling, (2) super-dense compute from
iterative bidirectional denoising, and (3) built-in Monte Carlo augmentation;
input or parameter noise improves AR under data constraint but cannot close the
gap. At scale, a 1.7B DLM trained with a ~1.5T-token compute budget on 10B
unique Python tokens overtakes an AR coder trained with strictly matched
settings. In addition, a 1B-parameter DLM achieves > 56% accuracy on HellaSwag
and > 33% on MMLU using only 1B tokens, without any special tricks, just by
repeating standard pre-training data. We also show that rising validation
cross-entropy does not imply degraded downstream performance in this regime.

</details>


### [145] [Multi-Objective Adaptive Rate Limiting in Microservices Using Deep Reinforcement Learning](https://arxiv.org/abs/2511.03279)
*Ning Lyu,Yuxi Wang,Ziyu Cheng,Qingyuan Zhang,Feng Chen*

Main category: cs.LG

TL;DR: Proposes an adaptive API rate limiting strategy using deep reinforcement learning, outperforming traditional methods in throughput and latency reduction.


<details>
  <summary>Details</summary>
Motivation: Traditional rate limiting algorithms fail to adapt to dynamic traffic patterns, prompting the need for an adaptive solution.

Method: Hybrid Deep Q-Network (DQN) and Asynchronous Advantage Actor-Critic (A3C) algorithms model rate limiting as a Markov Decision Process.

Result: Achieves 23.7% higher throughput and 31.4% lower P99 latency than fixed-threshold methods, with significant production deployment benefits.

Conclusion: The adaptive strategy effectively balances system performance metrics, reducing service degradation and manual interventions.

Abstract: As cloud computing and microservice architectures become increasingly
prevalent, API rate limiting has emerged as a critical mechanism for ensuring
system stability and service quality. Traditional rate limiting algorithms,
such as token bucket and sliding window, while widely adopted, struggle to
adapt to dynamic traffic patterns and varying system loads. This paper proposes
an adaptive rate limiting strategy based on deep reinforcement learning that
dynamically balances system throughput and service latency. We design a hybrid
architecture combining Deep Q-Network (DQN) and Asynchronous Advantage
Actor-Critic (A3C) algorithms, modeling the rate limiting decision process as a
Markov Decision Process. The system continuously monitors microservice states
and learns optimal rate limiting policies through environmental interaction.
Extensive experiments conducted in a Kubernetes cluster environment demonstrate
that our approach achieves 23.7% throughput improvement and 31.4% P99 latency
reduction compared to traditional fixed-threshold strategies under high-load
scenarios. Results from a 90-day production deployment handling 500 million
daily requests validate the practical effectiveness of the proposed method,
with 82% reduction in service degradation incidents and 68% decrease in manual
interventions.

</details>


### [146] [A Probabilistic Approach to Pose Synchronization for Multi-Reference Alignment with Applications to MIMO Wireless Communication Systems](https://arxiv.org/abs/2511.03280)
*Rob Romijnders,Gabriele Cesa,Christos Louizos,Kumar Pratik,Arash Behboodi*

Main category: cs.LG

TL;DR: The paper introduces a new probabilistic approach for multi-reference alignment (MRA) to improve signal reconstruction by marginalizing out nuisance variables and leveraging decentralization for computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Aligning and reconstructing signals from misaligned observations is critical in fields like molecular imaging and wireless communications. The paper aims to address the challenges of MRA with a more efficient solution.

Method: The authors propose a probabilistic model for MRA, using relative poses as nuisance variables to marginalize out global symmetries. Decentralization is employed to avoid the computational cost of centralized methods.

Result: The new algorithms achieve lower reconstruction error compared to existing methods, demonstrating improved performance in various experimental settings.

Conclusion: The probabilistic approach and decentralization significantly enhance the efficiency and accuracy of MRA, making it suitable for real-world applications like cryo-EM and wireless communications.

Abstract: From molecular imaging to wireless communications, the ability to align and
reconstruct signals from multiple misaligned observations is crucial for system
performance. We study the problem of multi-reference alignment (MRA), which
arises in many real-world problems, such as cryo-EM, computer vision, and, in
particular, wireless communication systems. Using a probabilistic approach to
model MRA, we find a new algorithm that uses relative poses as nuisance
variables to marginalize out -- thereby removing the global symmetries of the
problem and allowing for more direct solutions and improved convergence. The
decentralization of this approach enables significant computational savings by
avoiding the cubic scaling of centralized methods through cycle consistency.
Both proposed algorithms achieve lower reconstruction error across experimental
settings.

</details>


### [147] [Graph Neural AI with Temporal Dynamics for Comprehensive Anomaly Detection in Microservices](https://arxiv.org/abs/2511.03285)
*Qingyuan Zhang,Ning Lyu,Le Liu,Yuxi Wang,Ziyu Cheng,Cancan Hua*

Main category: cs.LG

TL;DR: The paper presents a unified framework combining graph neural networks and temporal modeling for anomaly detection and root cause tracing in microservice architectures, outperforming baselines in key metrics.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of anomaly detection and root cause tracing in microservice architectures by leveraging structural and temporal dependencies.

Method: Microservice call chains are modeled as directed graphs with graph convolution for structural relationships and gated recurrent units for temporal evolution. Anomaly scoring functions at node and path levels are defined.

Result: The framework outperforms baseline methods in AUC, ACC, Recall, and F1-Score, showing high accuracy and stability in dynamic environments.

Conclusion: The research offers a new approach for anomaly detection in microservices and supports intelligent operations in distributed systems.

Abstract: This study addresses the problem of anomaly detection and root cause tracing
in microservice architectures and proposes a unified framework that combines
graph neural networks with temporal modeling. The microservice call chain is
abstracted as a directed graph, where multidimensional features of nodes and
edges are used to construct a service topology representation, and graph
convolution is applied to aggregate features across nodes and model
dependencies, capturing complex structural relationships among services. On
this basis, gated recurrent units are introduced to model the temporal
evolution of call chains, and multi-layer stacking and concatenation operations
are used to jointly obtain structural and temporal representations, improving
the ability to identify anomaly patterns. Furthermore, anomaly scoring
functions at both the node and path levels are defined to achieve unified
modeling from local anomaly detection to global call chain tracing, which
enables the identification of abnormal service nodes and the reconstruction of
potential anomaly propagation paths. Sensitivity experiments are then designed
from multiple dimensions, including hyperparameters, environmental
disturbances, and data distribution, to evaluate the framework, and results
show that it outperforms baseline methods in key metrics such as AUC, ACC,
Recall, and F1-Score, maintaining high accuracy and stability under dynamic
topologies and complex environments. This research not only provides a new
technical path for anomaly detection in microservices but also lays a
methodological foundation for intelligent operations in distributed systems.

</details>


### [148] [Extending Fair Null-Space Projections for Continuous Attributes to Kernel Methods](https://arxiv.org/abs/2511.03304)
*Felix StÃ¶rck,Fabian Hinder,Barbara Hammer*

Main category: cs.LG

TL;DR: The paper addresses fairness in machine learning, particularly focusing on continuous attributes in regression tasks, introducing a kernel-based method for fairness-score agnostic approaches.


<details>
  <summary>Details</summary>
Motivation: Fairness in machine learning is crucial as these systems become more integrated into social life. Current literature lacks focus on continuous attributes, especially in regression, prompting the need for generalized solutions.

Method: The authors propose a kernel-based method for iterative null-space projection, extending its application beyond linear models to handle continuous protected attributes.

Result: The approach, when combined with Support Vector Regression (SVR), shows competitive or improved performance across multiple datasets compared to other methods.

Conclusion: The study generalizes fairness considerations to kernel methods, offering a flexible solution for continuous attributes in regression tasks, enhancing fairness-score agnostic applicability.

Abstract: With the on-going integration of machine learning systems into the everyday
social life of millions the notion of fairness becomes an ever increasing
priority in their development. Fairness notions commonly rely on protected
attributes to assess potential biases. Here, the majority of literature focuses
on discrete setups regarding both target and protected attributes. The
literature on continuous attributes especially in conjunction with regression
-- we refer to this as \emph{continuous fairness} -- is scarce. A common
strategy is iterative null-space projection which as of now has only been
explored for linear models or embeddings such as obtained by a non-linear
encoder. We improve on this by generalizing to kernel methods, significantly
extending the scope. This yields a model and fairness-score agnostic method for
kernel embeddings applicable to continuous protected attributes. We demonstrate
that our novel approach in conjunction with Support Vector Regression (SVR)
provides competitive or improved performance across multiple datasets in
comparisons to other contemporary methods.

</details>


### [149] [SORTeD Rashomon Sets of Sparse Decision Trees: Anytime Enumeration](https://arxiv.org/abs/2511.03344)
*Elif Arslan,Jacobus G. M. van der Linden,Serge Hoogendoorn,Marco Rinaldi,Emir DemiroviÄ‡*

Main category: cs.LG

TL;DR: SORTD is a scalable framework for enumerating high-performing decision trees in the Rashomon set, improving runtime and practical usability.


<details>
  <summary>Details</summary>
Motivation: High-stakes applications benefit from interpretable models like decision trees, but relying on a single 'best' tree limits flexibility. Rashomon sets offer diverse trees for better analysis and user choice.

Method: Introduces SORTD, a framework to efficiently enumerate trees in the Rashomon set, ordered by objective value, supporting anytime behavior and separable objectives.

Result: SORTD achieves up to 100x faster runtime than state-of-the-art methods and works with various objectives.

Conclusion: SORTD makes Rashomon set exploration practical, enabling richer model analysis and decision-making.

Abstract: Sparse decision tree learning provides accurate and interpretable predictive
models that are ideal for high-stakes applications by finding the single most
accurate tree within a (soft) size limit. Rather than relying on a single
"best" tree, Rashomon sets-trees with similar performance but varying
structures-can be used to enhance variable importance analysis, enrich
explanations, and enable users to choose simpler trees or those that satisfy
stakeholder preferences (e.g., fairness) without hard-coding such criteria into
the objective function. However, because finding the optimal tree is NP-hard,
enumerating the Rashomon set is inherently challenging. Therefore, we introduce
SORTD, a novel framework that improves scalability and enumerates trees in the
Rashomon set in order of the objective value, thus offering anytime behavior.
Our experiments show that SORTD reduces runtime by up to two orders of
magnitude compared with the state of the art. Moreover, SORTD can compute
Rashomon sets for any separable and totally ordered objective and supports
post-evaluating the set using other separable (and partially ordered)
objectives. Together, these advances make exploring Rashomon sets more
practical in real-world applications.

</details>


### [150] [A Modular, Data-Free Pipeline for Multi-Label Intention Recognition in Transportation Agentic AI Applications](https://arxiv.org/abs/2511.03363)
*Xiaocai Zhang,Hur Lim,Ke Wang,Zhe Xiao,Jing Wang,Kelvin Lee,Xiuju Fu,Zheng Qin*

Main category: cs.LG

TL;DR: A modular, data-free pipeline (DMTC) for multi-label intent recognition in transportation uses synthetic queries from LLMs, Sentence-T5 embeddings, and a novel OFC loss, achieving high accuracy without costly data collection.


<details>
  <summary>Details</summary>
Motivation: Traditional intent recognition systems rely on large annotated datasets and struggle with fine-grained, multi-label tasks. DMTC aims to eliminate costly data collection while improving accuracy.

Method: The DMTC pipeline involves: 1) generating synthetic queries via prompt engineering with LLMs, 2) encoding queries with Sentence-T5 embeddings, and 3) training a lightweight classifier using an online focal-contrastive (OFC) loss.

Result: DMTC achieves a Hamming loss of 5.35% and an AUC of 95.92%, outperforming state-of-the-art baselines. Sentence-T5 embeddings improve subset accuracy by â‰¥3.29%, and OFC adds a 0.98% gain over standard contrastive loss.

Conclusion: The system autonomously routes user queries to task-specific modules in transportation, enabling intention-aware agents without manual labeling.

Abstract: In this study, a modular, data-free pipeline for multi-label intention
recognition is proposed for agentic AI applications in transportation. Unlike
traditional intent recognition systems that depend on large, annotated corpora
and often struggle with fine-grained, multi-label discrimination, our approach
eliminates the need for costly data collection while enhancing the accuracy of
multi-label intention understanding. Specifically, the overall pipeline, named
DMTC, consists of three steps: 1) using prompt engineering to guide large
language models (LLMs) to generate diverse synthetic queries in different
transport scenarios; 2) encoding each textual query with a Sentence-T5 model to
obtain compact semantic embeddings; 3) training a lightweight classifier using
a novel online focal-contrastive (OFC) loss that emphasizes hard samples and
maximizes inter-class separability. The applicability of the proposed pipeline
is demonstrated in an agentic AI application in the maritime transportation
context. Extensive experiments show that DMTC achieves a Hamming loss of 5.35%
and an AUC of 95.92%, outperforming state-of-the-art multi-label classifiers
and recent end-to-end SOTA LLM-based baselines. Further analysis reveals that
Sentence-T5 embeddings improve subset accuracy by at least 3.29% over
alternative encoders, and integrating the OFC loss yields an additional 0.98%
gain compared to standard contrastive objectives. In conclusion, our system
seamlessly routes user queries to task-specific modules (e.g., ETA information,
traffic risk evaluation, and other typical scenarios in the transportation
domain), laying the groundwork for fully autonomous, intention-aware agents
without costly manual labelling.

</details>


### [151] [TripleWin: Fixed-Point Equilibrium Pricing for Data-Model Coupled Markets](https://arxiv.org/abs/2511.03368)
*Hongrun Ren,Yun Xiong,Lei You,Yingying Wang,Haixu Xiong,Yangyong Zhu*

Main category: cs.LG

TL;DR: A unified data-model coupled market is proposed to integrate dataset and model trading as a single system, ensuring symmetric interactions and fair pricing.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the separation of data and model transactions in ML markets and the lack of a symmetric mechanism for data sellers, model producers, and buyers.

Method: The approach uses a supply-side mapping to convert dataset payments into model quotations and a demand-side mapping to allocate buyer prices back to datasets using Shapley-based allocation.

Result: The joint operator is proven to be a standard interference function (SIF), ensuring equilibrium price existence, uniqueness, and global convergence. Experiments show efficient convergence and improved fairness.

Conclusion: The proposed unified market mechanism successfully integrates data and model trading, offering a fair and efficient solution compared to existing broker-centric and one-sided methods.

Abstract: The rise of the machine learning (ML) model economy has intertwined markets
for training datasets and pre-trained models. However, most pricing approaches
still separate data and model transactions or rely on broker-centric pipelines
that favor one side. Recent studies of data markets with externalities capture
buyer interactions but do not yield a simultaneous and symmetric mechanism
across data sellers, model producers, and model buyers. We propose a unified
data-model coupled market that treats dataset and model trading as a single
system. A supply-side mapping transforms dataset payments into buyer-visible
model quotations, while a demand-side mapping propagates buyer prices back to
datasets through Shapley-based allocation. Together, they form a closed loop
that links four interactions: supply-demand propagation in both directions and
mutual coupling among buyers and among sellers. We prove that the joint
operator is a standard interference function (SIF), guaranteeing existence,
uniqueness, and global convergence of equilibrium prices. Experiments
demonstrate efficient convergence and improved fairness compared with
broker-centric and one-sided baselines. The code is available on
https://github.com/HongrunRen1109/Triple-Win-Pricing.

</details>


### [152] [Adaptable Hindsight Experience Replay for Search-Based Learning](https://arxiv.org/abs/2511.03405)
*Alexandros Vazaios,Jannis Brugger,Cedric Derstroff,Kristian Kersting,Mira Mezini*

Main category: cs.LG

TL;DR: Adaptable HER integrates Hindsight Experience Replay with AlphaZero, enhancing its performance in sparse reward settings by relabeling unsuccessful trajectories from the search tree as supervised learning signals. The framework outperforms pure supervised or reinforcement learning in tasks like equation discovery.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of AlphaZero-like systems in sparse reward settings, particularly in early training stages where neural network guidance is weak. Hindsight Experience Replay (HER) is used to relabel unsuccessful trajectories to improve learning.

Method: The method involves integrating HER with AlphaZero, creating Adaptable HER. This framework allows adjustments to HER properties like relabeled goals, policy targets, and trajectory selection, making it flexible for various tasks.

Result: Experiments, including equation discovery, demonstrate that Adaptable HER improves performance compared to pure supervised or reinforcement learning methods.

Conclusion: Adaptable HER successfully enhances AlphaZero's capabilities in sparse reward settings by leveraging HER, proving its flexibility and effectiveness in improving learning outcomes.

Abstract: AlphaZero-like Monte Carlo Tree Search systems, originally introduced for
two-player games, dynamically balance exploration and exploitation using neural
network guidance. This combination makes them also suitable for classical
search problems. However, the original method of training the network with
simulation results is limited in sparse reward settings, especially in the
early stages, where the network cannot yet give guidance. Hindsight Experience
Replay (HER) addresses this issue by relabeling unsuccessful trajectories from
the search tree as supervised learning signals. We introduce Adaptable HER
(\ours{}), a flexible framework that integrates HER with AlphaZero, allowing
easy adjustments to HER properties such as relabeled goals, policy targets, and
trajectory selection. Our experiments, including equation discovery, show that
the possibility of modifying HER is beneficial and surpasses the performance of
pure supervised or reinforcement learning.

</details>


### [153] [POEMS: Product of Experts for Interpretable Multi-omic Integration using Sparse Decoding](https://arxiv.org/abs/2511.03464)
*Mihriban Kocak Balik,Pekka Marttinen,Negar Safinianaini*

Main category: cs.LG

TL;DR: POEMS is a deep generative model integrating multiomics data, balancing predictive performance and interpretability without linearizing decoders, demonstrated effectively in cancer subtyping.


<details>
  <summary>Details</summary>
Motivation: Current deep generative models for multiomics data integration often sacrifice interpretability for performance or vice versa, creating a need for a solution that combines both.

Method: POEMS employs sparse connections for feature mapping, a product of experts model for cross-omic associations, and a gating network for omic-specific contributions, alongside an efficient sparse decoder.

Result: In cancer subtyping, POEMS achieves competitive clustering and classification performance while providing interpretable biomarker insights.

Conclusion: POEMS successfully bridges the gap between interpretability and predictive accuracy in multiomics integration, proving their coexistence is feasible.

Abstract: Integrating different molecular layers, i.e., multiomics data, is crucial for
unraveling the complexity of diseases; yet, most deep generative models either
prioritize predictive performance at the expense of interpretability or enforce
interpretability by linearizing the decoder, thereby weakening the network's
nonlinear expressiveness. To overcome this tradeoff, we introduce POEMS:
Product Of Experts for Interpretable Multiomics Integration using Sparse
Decoding, an unsupervised probabilistic framework that preserves predictive
performance while providing interpretability. POEMS provides interpretability
without linearizing any part of the network by 1) mapping features to latent
factors using sparse connections, which directly translates to biomarker
discovery, 2) allowing for cross-omic associations through a shared latent
space using product of experts model, and 3) reporting contributions of each
omic by a gating network that adaptively computes their influence in the
representation learning. Additionally, we present an efficient sparse decoder.
In a cancer subtyping case study, POEMS achieves competitive clustering and
classification performance while offering our novel set of interpretations,
demonstrating that biomarker based insight and predictive accuracy can coexist
in multiomics representation learning.

</details>


### [154] [Reinforcement Learning Using known Invariances](https://arxiv.org/abs/2511.03473)
*Alexandru Cioba,Aya Kayal,Laura Toni,Sattar Vakili,Alberto Bernacchia*

Main category: cs.LG

TL;DR: The paper proposes a symmetry-aware RL framework using invariant kernels, improving learning efficiency by exploiting environmental symmetries.


<details>
  <summary>Details</summary>
Motivation: To enhance RL efficiency by leveraging inherent symmetries in environments, addressing the challenge of sample inefficiency in traditional methods.

Method: Develops a symmetry-aware variant of optimistic LSVI using invariant kernels for rewards and transitions, with theoretical analysis on information gain and covering numbers.

Result: Empirical tests on Frozen Lake and 2D placement show symmetry-aware RL outperforms standard kernel methods, validating theoretical gains.

Conclusion: Incorporating structural priors like symmetries significantly improves RL sample efficiency, offering practical benefits for real-world applications.

Abstract: In many real-world reinforcement learning (RL) problems, the environment
exhibits inherent symmetries that can be exploited to improve learning
efficiency. This paper develops a theoretical and algorithmic framework for
incorporating known group symmetries into kernel-based RL. We propose a
symmetry-aware variant of optimistic least-squares value iteration (LSVI),
which leverages invariant kernels to encode invariance in both rewards and
transition dynamics. Our analysis establishes new bounds on the maximum
information gain and covering numbers for invariant RKHSs, explicitly
quantifying the sample efficiency gains from symmetry. Empirical results on a
customized Frozen Lake environment and a 2D placement design problem confirm
the theoretical improvements, demonstrating that symmetry-aware RL achieves
significantly better performance than their standard kernel counterparts. These
findings highlight the value of structural priors in designing more
sample-efficient reinforcement learning algorithms.

</details>


### [155] [RAGBoost: Efficient Retrieval-Augmented Generation with Accuracy-Preserving Context Reuse](https://arxiv.org/abs/2511.03475)
*Yinsicheng Jiang,Yeqi Huang,Liang Cheng,Cheng Deng,Xuan Sun,Luo Mai*

Main category: cs.LG

TL;DR: RAGBoost is an efficient RAG system that maximizes cache reuse and maintains accuracy through context reuse techniques, improving prefill performance by 1.5-3X.


<details>
  <summary>Details</summary>
Motivation: Current RAG systems suffer from degraded prefill performance and trade-offs between cache reuse and reasoning accuracy.

Method: RAGBoost detects overlapping retrieved items, uses context indexing, ordering, and de-duplication for reuse, and maintains fidelity with contextual hints.

Result: Improves prefill performance by 1.5-3X over state-of-the-art methods while preserving reasoning accuracy.

Conclusion: RAGBoost offers a seamless integration with LLM inference engines, enhancing efficiency and accuracy in diverse RAG workloads.

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs)
with retrieved context but often suffers from downgraded prefill performance as
modern applications demand longer and more complex inputs. Existing caching
techniques either preserve accuracy with low cache reuse or improve reuse at
the cost of degraded reasoning quality. We present RAGBoost, an efficient RAG
system that achieves high cache reuse without sacrificing accuracy through
accuracy-preserving context reuse. RAGBoost detects overlapping retrieved items
across concurrent sessions and multi-turn interactions, using efficient context
indexing, ordering, and de-duplication to maximize reuse, while lightweight
contextual hints maintain reasoning fidelity. It integrates seamlessly with
existing LLM inference engines and improves their prefill performance by 1.5-3X
over state-of-the-art methods, while preserving or even enhancing reasoning
accuracy across diverse RAG and agentic AI workloads. Our code is released at:
https://github.com/Edinburgh-AgenticAI/RAGBoost.

</details>


### [156] [NAP: Attention-Based Late Fusion for Automatic Sleep Staging](https://arxiv.org/abs/2511.03488)
*Alvise Dei Rossi,Julia van der Meer,Markus H. Schmidt,Claudio L. A. Bassetti,Luigi Fiorillo,Francesca Faraci*

Main category: cs.LG

TL;DR: NAP (Neural Aggregator of Predictions) is an attention-based model designed to handle polysomnography's heterogeneous signals, outperforming single-channel models and ensembles with superior zero-shot generalization.


<details>
  <summary>Details</summary>
Motivation: Polysomnography signals vary widely in modality, channel availability, and protocols, but existing models often ignore this by using fixed inputs. NAP aims to exploit the full multimodal potential.

Method: NAP uses a tri-axial attention mechanism to combine prediction streams from frozen, pretrained single-channel models, capturing temporal, spatial, and predictor-level dependencies while adapting to varying input dimensions.

Result: NAP consistently outperforms individual predictors and simple ensembles, achieving state-of-the-art zero-shot generalization across multiple datasets.

Conclusion: NAP effectively addresses polysomnography's heterogeneity and has potential for broader multimodal physiological applications beyond sleep staging.

Abstract: Polysomnography signals are highly heterogeneous, varying in modality
composition (e.g., EEG, EOG, ECG), channel availability (e.g., frontal,
occipital EEG), and acquisition protocols across datasets and clinical sites.
Most existing models that process polysomnography data rely on a fixed subset
of modalities or channels and therefore neglect to fully exploit its inherently
multimodal nature. We address this limitation by introducing NAP (Neural
Aggregator of Predictions), an attention-based model which learns to combine
multiple prediction streams using a tri-axial attention mechanism that captures
temporal, spatial, and predictor-level dependencies. NAP is trained to adapt to
different input dimensions. By aggregating outputs from frozen, pretrained
single-channel models, NAP consistently outperforms individual predictors and
simple ensembles, achieving state-of-the-art zero-shot generalization across
multiple datasets. While demonstrated in the context of automated sleep staging
from polysomnography, the proposed approach could be extended to other
multimodal physiological applications.

</details>


### [157] [Efficient Neural Networks with Discrete Cosine Transform Activations](https://arxiv.org/abs/2511.03531)
*Marc Martinez-Gost,Sara Pepe,Ana PÃ©rez-Neira,Miguel Ãngel Lagunas*

Main category: cs.LG

TL;DR: The paper extends ENN (Expressive Neural Network) with adaptive activation functions using DCT, highlighting efficiency, interpretability, and pruning. It proposes a pruning strategy leveraging DCT's decorrelation to remove redundant coefficients without performance loss.


<details>
  <summary>Details</summary>
Motivation: To enhance ENNs by improving efficiency, interpretability, and pruning capabilities while maintaining expressiveness and compactness.

Method: Uses DCT-based parameterization for structured representation, identifies redundant neurons, and prunes unnecessary DCT coefficients efficiently.

Result: ENNs achieve SOTA accuracy with fewer parameters, and up to 40% of activation coefficients can be pruned without performance loss.

Conclusion: The ENN framework integrates signal processing principles into neural network design, balancing expressiveness, compactness, and interpretability.

Abstract: In this paper, we extend our previous work on the Expressive Neural Network
(ENN), a multilayer perceptron with adaptive activation functions parametrized
using the Discrete Cosine Transform (DCT). Building upon previous work that
demonstrated the strong expressiveness of ENNs with compact architectures, we
now emphasize their efficiency, interpretability and pruning capabilities. The
DCT-based parameterization provides a structured and decorrelated
representation that reveals the functional role of each neuron and allows
direct identification of redundant components. Leveraging this property, we
propose an efficient pruning strategy that removes unnecessary DCT coefficients
with negligible or no loss in performance. Experimental results across
classification and implicit neural representation tasks confirm that ENNs
achieve state-of-the-art accuracy while maintaining a low number of parameters.
Furthermore, up to 40% of the activation coefficients can be safely pruned,
thanks to the orthogonality and bounded nature of the DCT basis. Overall, these
findings demonstrate that the ENN framework offers a principled integration of
signal processing concepts into neural network design, achieving a balanced
trade-off between expressiveness, compactness, and interpretability.

</details>


### [158] [Why Less is More (Sometimes): A Theory of Data Curation](https://arxiv.org/abs/2511.03492)
*Elvis Dohmatob,Mohammad Pezeshki,Reyhane Askari-Hemmat*

Main category: cs.LG

TL;DR: The paper addresses the paradox in ML about when using less data is beneficial, introducing a framework to analyze data curation's impact on performance.


<details>
  <summary>Details</summary>
Motivation: To resolve the paradox of when less data outperforms more, challenging classical scaling laws.

Method: Studies data curation via an imperfect oracle, analyzing label-agnostic and label-aware strategies.

Result: Shows conditions where small, curated datasets outperform full ones, validated on ImageNet.

Conclusion: Provides principles for data curation, explaining contradictory strategies in LLM reasoning.

Abstract: This paper introduces a theoretical framework to resolve a central paradox in
modern machine learning: When is it better to use less data? This question has
become critical as classical scaling laws suggesting ``more is more'' (Sun et
al., 2025) are challenged by methods like LIMO (``less is more'') and s1 (Ye et
al., 2025; Muenighoff et al., 2025), which achieve superior performance with
small, aggressively curated datasets. Here, we study data curation strategies
where an imperfect oracle selects the training examples according to their
difficulty and correctness. Our results provide exact scaling law curves for
test error under both label-agnostic and label-aware curation rules, revealing
when and why keeping only a subset of data can improve generalization. In
contrast to classical scaling laws, we show that under certain conditions,
small curated datasets can outperform full datasets, and we provide analytical
conditions for this by deriving precise phase transition curves tied to data
size and quality. We validate these theoretical claims with empirical results
on ImageNet, confirming our predictions about when curation improves accuracy
and can even mitigate model collapse. Furthermore, our framework provides a
principled explanation for the contradictory curation strategies recently
observed in LLM mathematical reasoning.

</details>


### [159] [Learning Without Critics? Revisiting GRPO in Classical Reinforcement Learning Environments](https://arxiv.org/abs/2511.03527)
*Bryan L. M. de Oliveira,Felipe V. Frujeri,Marcos P. C. M. Queiroz,Luana G. B. Martins,Telma W. de L. Soares,Luckeciano C. Melo*

Main category: cs.LG

TL;DR: GRPO simplifies policy-gradient methods by removing learned critics, but its effectiveness depends on task horizon, discount factors, and group sizes.


<details>
  <summary>Details</summary>
Motivation: To study whether learned baselines are necessary in policy-gradient methods by examining GRPO in single-task RL environments.

Method: Controlled ablations isolating baselines, discounting, and group sampling across discrete and continuous control tasks.

Result: 1. Critics are crucial for long-horizon tasks. 2. GRPO works best with high discount factors (except in HalfCheetah). 3. Smaller group sizes perform better.

Conclusion: Critic-free methods like GRPO have limitations in classical control but can work under specific conditions.

Abstract: Group Relative Policy Optimization (GRPO) has emerged as a scalable
alternative to Proximal Policy Optimization (PPO) by eliminating the learned
critic and instead estimating advantages through group-relative comparisons of
trajectories. This simplification raises fundamental questions about the
necessity of learned baselines in policy-gradient methods. We present the first
systematic study of GRPO in classical single-task reinforcement learning
environments, spanning discrete and continuous control tasks. Through
controlled ablations isolating baselines, discounting, and group sampling, we
reveal three key findings: (1) learned critics remain essential for
long-horizon tasks: all critic-free baselines underperform PPO except in
short-horizon environments like CartPole where episodic returns can be
effective; (2) GRPO benefits from high discount factors (gamma = 0.99) except
in HalfCheetah, where lack of early termination favors moderate discounting
(gamma = 0.9); (3) smaller group sizes outperform larger ones, suggesting
limitations in batch-based grouping strategies that mix unrelated episodes.
These results reveal both the limitations of critic-free methods in classical
control and the specific conditions where they remain viable alternatives to
learned value functions.

</details>


### [160] [Byzantine-Robust Federated Learning with Learnable Aggregation Weights](https://arxiv.org/abs/2511.03529)
*Javad Parsa,Amir Hossein Daghestani,AndrÃ© M. H. Teixeira,Mikael Johansson*

Main category: cs.LG

TL;DR: Proposes a novel Byzantine-robust Federated Learning method with adaptive weighting, outperforming existing approaches in heterogeneous data and high malicious client scenarios.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of malicious (Byzantine) clients in Federated Learning, especially with heterogeneous data distributions, by improving robustness.

Method: Introduces an adaptive weighting mechanism in the aggregation process, treating weights as learnable parameters, and develops an alternating minimization algorithm for optimization.

Result: The method consistently outperforms state-of-the-art Byzantine-robust FL approaches, especially in highly heterogeneous and adversarial settings.

Conclusion: The proposed approach effectively enhances the robustness of Federated Learning against malicious clients and heterogeneous data distributions.

Abstract: Federated Learning (FL) enables clients to collaboratively train a global
model without sharing their private data. However, the presence of malicious
(Byzantine) clients poses significant challenges to the robustness of FL,
particularly when data distributions across clients are heterogeneous. In this
paper, we propose a novel Byzantine-robust FL optimization problem that
incorporates adaptive weighting into the aggregation process. Unlike
conventional approaches, our formulation treats aggregation weights as
learnable parameters, jointly optimizing them alongside the global model
parameters. To solve this optimization problem, we develop an alternating
minimization algorithm with strong convergence guarantees under adversarial
attack. We analyze the Byzantine resilience of the proposed objective. We
evaluate the performance of our algorithm against state-of-the-art
Byzantine-robust FL approaches across various datasets and attack scenarios.
Experimental results demonstrate that our method consistently outperforms
existing approaches, particularly in settings with highly heterogeneous data
and a large proportion of malicious clients.

</details>


### [161] [Imitation Learning in the Deep Learning Era: A Novel Taxonomy and Recent Advances](https://arxiv.org/abs/2511.03565)
*Iason Chrysomallis,Georgios Chalkiadakis*

Main category: cs.LG

TL;DR: The paper surveys recent advances in imitation learning (IL), focusing on deep learning-driven scalability, novel methodologies, and practical applications. It introduces a new taxonomy to reflect current trends and critically evaluates strengths, limitations, and future challenges in IL.


<details>
  <summary>Details</summary>
Motivation: The paper aims to review and organize the rapidly evolving field of imitation learning, addressing gaps in existing taxonomies and highlighting emerging methodologies and challenges.

Method: The authors conduct a comprehensive survey of IL literature, propose a novel taxonomy, and critically analyze representative works, their methodologies, and evaluation practices.

Result: The survey showcases advancements in IL, identifies key trends, and outlines unresolved issues like generalization and demonstration quality. The new taxonomy provides a clearer framework for current research.

Conclusion: The paper underscores the dynamic progress in IL but emphasizes the need for further research to tackle challenges and improve methodologies, offering a roadmap for future work.

Abstract: Imitation learning (IL) enables agents to acquire skills by observing and
replicating the behavior of one or multiple experts. In recent years, advances
in deep learning have significantly expanded the capabilities and scalability
of imitation learning across a range of domains, where expert data can range
from full state-action trajectories to partial observations or unlabeled
sequences. Alongside this growth, novel approaches have emerged, with new
methodologies being developed to address longstanding challenges such as
generalization, covariate shift, and demonstration quality. In this survey, we
review the latest advances in imitation learning research, highlighting recent
trends, methodological innovations, and practical applications. We propose a
novel taxonomy that is distinct from existing categorizations to better reflect
the current state of the IL research stratum and its trends. Throughout the
survey, we critically examine the strengths, limitations, and evaluation
practices of representative works, and we outline key challenges and open
directions for future research.

</details>


### [162] [Flat Minima and Generalization: Insights from Stochastic Convex Optimization](https://arxiv.org/abs/2511.03548)
*Matan Schliserman,Shira Vansover-Hager,Tomer Koren*

Main category: cs.LG

TL;DR: The paper explores the relationship between flat minima and generalization in stochastic convex optimization, revealing that flat minima may generalize poorly, contrary to common belief, and analyzes the impact on sharpness-aware algorithms like SA-GD and SAM.


<details>
  <summary>Details</summary>
Motivation: To investigate whether flat minima, often linked to better generalization, truly improve performance in stochastic convex optimization and to evaluate the effectiveness of sharpness-aware algorithms like SA-GD and SAM.

Method: The study uses theoretical analysis in the setting of stochastic convex optimization with a non-negative, Î²-smooth objective, examining the generalization behavior of flat minima and the performance of SA-GD and SAM.

Result: Flat minima can generalize poorly (Î©(1) population risk), while sharp minima generalize optimally. SA-GD and SAM, despite converging to flat minima, may also suffer poor generalization (Î©(1) population risk).

Conclusion: Flat minima are not universally better for generalization, and sharpness-aware algorithms like SA-GD and SAM may fail to ensure good generalization despite their design goals.

Abstract: Understanding the generalization behavior of learning algorithms is a central
goal of learning theory. A recently emerging explanation is that learning
algorithms are successful in practice because they converge to flat minima,
which have been consistently associated with improved generalization
performance. In this work, we study the link between flat minima and
generalization in the canonical setting of stochastic convex optimization with
a non-negative, $\beta$-smooth objective. Our first finding is that, even in
this fundamental and well-studied setting, flat empirical minima may incur
trivial $\Omega(1)$ population risk while sharp minima generalizes optimally.
Then, we show that this poor generalization behavior extends to two natural
''sharpness-aware'' algorithms originally proposed by Foret et al. (2021),
designed to bias optimization toward flat solutions: Sharpness-Aware Gradient
Descent (SA-GD) and Sharpness-Aware Minimization (SAM). For SA-GD, which
performs gradient steps on the maximal loss in a predefined neighborhood, we
prove that while it successfully converges to a flat minimum at a fast rate,
the population risk of the solution can still be as large as $\Omega(1)$,
indicating that even flat minima found algorithmically using a sharpness-aware
gradient method might generalize poorly. For SAM, a computationally efficient
approximation of SA-GD based on normalized ascent steps, we show that although
it minimizes the empirical loss, it may converge to a sharp minimum and also
incur population risk $\Omega(1)$. Finally, we establish population risk upper
bounds for both SA-GD and SAM using algorithmic stability techniques.

</details>


### [163] [Learning Under Laws: A Constraint-Projected Neural PDE Solver that Eliminates Hallucinations](https://arxiv.org/abs/2511.03578)
*Mainak Singha*

Main category: cs.LG

TL;DR: The paper introduces Constraint-Projected Learning (CPL), a framework ensuring neural networks respect physical laws while solving PDEs, combining differentiable projections and stabilization techniques for accurate, lawful results.


<details>
  <summary>Details</summary>
Motivation: Neural networks often violate physics laws they model, causing issues like mass creation or entropy violations. CPL aims to enforce physical admissibility during training.

Method: CPL projects network outputs onto constraint sets (conservation, entropy, etc.) with minimal overhead. It uses total-variation damping and a rollout curriculum for stability.

Result: CPL eliminates violations, ensuring conservation at machine precision and bounded entropy/error. It works well on Burgers and Euler systems without accuracy loss.

Conclusion: CPL makes physics compliance intrinsic to neural solvers, eliminating the need for post-training fixes.

Abstract: Neural networks can approximate solutions to partial differential equations,
but they often break the very laws they are meant to model-creating mass from
nowhere, drifting shocks, or violating conservation and entropy. We address
this by training within the laws of physics rather than beside them. Our
framework, called Constraint-Projected Learning (CPL), keeps every update
physically admissible by projecting network outputs onto the intersection of
constraint sets defined by conservation, Rankine-Hugoniot balance, entropy, and
positivity. The projection is differentiable and adds only about 10%
computational overhead, making it fully compatible with back-propagation. We
further stabilize training with total-variation damping (TVD) to suppress small
oscillations and a rollout curriculum that enforces consistency over long
prediction horizons. Together, these mechanisms eliminate both hard and soft
violations: conservation holds at machine precision, total-variation growth
vanishes, and entropy and error remain bounded. On Burgers and Euler systems,
CPL produces stable, physically lawful solutions without loss of accuracy.
Instead of hoping neural solvers will respect physics, CPL makes that behavior
an intrinsic property of the learning process.

</details>


### [164] [TabGemma: Text-Based Tabular ICL via LLM using Continued Pretraining and Retrieval](https://arxiv.org/abs/2511.03570)
*GÃ¼nther Schindler,Maximilian Schambach,Michael Medek,Sam Thelin*

Main category: cs.LG

TL;DR: TabGemma, a schema-agnostic LLM for tabular prediction, addresses numeric tokenization and context size issues, achieving SOTA in classification and competitive regression results.


<details>
  <summary>Details</summary>
Motivation: To adapt pretrained LLMs for tabular predictions despite challenges like unstable numeric tokenization and limited context size.

Method: Uses canonicalized numbers (signed scientific notation), continues pretraining Gemma 3 with target imputation, and employs n-gram retrieval for exemplar selection.

Result: SOTA in classification across data regimes; competitive regression at small samples. Performance improves with more context rows.

Conclusion: LLMs can excel in semantic tabular tasks with proper numeric handling and context retrieval, but further advances in numeric modeling and long-context scaling are needed.

Abstract: We study LLMs for tabular prediction with mixed text, numeric, and
categorical fields. We introduce TabGemma, a schema-agnostic in-context learner
that treats rows as sequences and tackles two practical hurdles when adapting
pretrained LLMs for tabular predictions: unstable numeric tokenization and
limited context size. We propose to canonicalize numbers via signed scientific
notation and continue pretraining of a 12B Gemma 3 model with a target
imputation objective using a large-scale real world dataset. For inference, we
use a compact n-gram-based retrieval to select informative exemplars that fit
within a 128k-token window.
  On semantically rich benchmarks, TabGemma establishes a new state of the art
on classification across low- and high-data regimes and improves monotonically
with more context rows. For regression, it is competitive at small sample sizes
but trails conventional approaches as data grows. Our results show that LLMs
can be effective tabular in-context learners on highly semantic tasks when
paired with dedicated numeric handling and context retrieval, while motivating
further advances in numeric modeling and long-context scaling.

</details>


### [165] [DQN Performance with Epsilon Greedy Policies and Prioritized Experience Replay](https://arxiv.org/abs/2511.03670)
*Daniel Perkins,Oscar J. Escobar,Luke Green*

Main category: cs.LG

TL;DR: Study of Deep Q-Networks (DQN) in finite environments, focusing on epsilon-greedy exploration and prioritized experience replay, showing their impact on learning efficiency and reward optimization.


<details>
  <summary>Details</summary>
Motivation: To understand how different epsilon decay schedules and prioritized experience replay affect DQN performance in finite environments.

Method: Systematic experimentation with variations in epsilon decay schedules and comparisons of uniform, no replay, and prioritized experience replay strategies.

Result: Prioritized experience replay leads to faster convergence and higher returns, with empirical results highlighting trade-offs between exploration and memory management.

Conclusion: Provides practical recommendations for optimizing DQN training in resource-constrained settings by balancing exploration strategies and memory management.

Abstract: We present a detailed study of Deep Q-Networks in finite environments,
emphasizing the impact of epsilon-greedy exploration schedules and prioritized
experience replay. Through systematic experimentation, we evaluate how
variations in epsilon decay schedules affect learning efficiency, convergence
behavior, and reward optimization. We investigate how prioritized experience
replay leads to faster convergence and higher returns and show empirical
results comparing uniform, no replay, and prioritized strategies across
multiple simulations. Our findings illuminate the trade-offs and interactions
between exploration strategies and memory management in DQN training, offering
practical recommendations for robust reinforcement learning in
resource-constrained settings.

</details>


### [166] [Tensor-Efficient High-Dimensional Q-learning](https://arxiv.org/abs/2511.03595)
*Junyi Wu,Dan Li*

Main category: cs.LG

TL;DR: Proposes Tensor-Efficient Q-Learning (TEQL) for high-dimensional RL, using low-rank tensor decomposition and novel exploration/regularization mechanisms to outperform existing methods in sample efficiency and rewards.


<details>
  <summary>Details</summary>
Motivation: Addresses challenges of high-dimensional RL, such as complex calculations and low sample efficiency, by improving tensor-based methods with better exploration and regularization.

Method: TEQL enhances low-rank tensor decomposition via improved block coordinate descent, introduces exploration based on approximation error and visit counts, and adds frequency-based regularization.

Result: Outperforms conventional matrix-based and deep RL methods in sample efficiency and total rewards on classic control tasks.

Conclusion: TEQL is effective for resource-constrained applications like space and healthcare, offering better performance with lower sampling costs.

Abstract: High-dimensional reinforcement learning faces challenges with complex
calculations and low sample efficiency in large state-action spaces. Q-learning
algorithms struggle particularly with the curse of dimensionality, where the
number of state-action pairs grows exponentially with problem size. While
neural network-based approaches like Deep Q-Networks have shown success, recent
tensor-based methods using low-rank decomposition offer more
parameter-efficient alternatives. Building upon existing tensor-based methods,
we propose Tensor-Efficient Q-Learning (TEQL), which enhances low-rank tensor
decomposition via improved block coordinate descent on discretized state-action
spaces, incorporating novel exploration and regularization mechanisms. The key
innovation is an exploration strategy that combines approximation error with
visit count-based upper confidence bound to prioritize actions with high
uncertainty, avoiding wasteful random exploration. Additionally, we incorporate
a frequency-based penalty term in the objective function to encourage
exploration of less-visited state-action pairs and reduce overfitting to
frequently visited regions. Empirical results on classic control tasks
demonstrate that TEQL outperforms conventional matrix-based methods and deep RL
approaches in both sample efficiency and total rewards, making it suitable for
resource-constrained applications, such as space and healthcare where sampling
costs are high.

</details>


### [167] [Structured Matrix Scaling for Multi-Class Calibration](https://arxiv.org/abs/2511.03685)
*EugÃ¨ne Berta,David HolzmÃ¼ller,Michael I. Jordan,Francis Bach*

Main category: cs.LG

TL;DR: The paper discusses post-hoc recalibration methods for classifiers, emphasizing logistic regression's theoretical basis and advocating for more expressive calibration techniques.


<details>
  <summary>Details</summary>
Motivation: To improve classifier probability estimates by addressing limitations of current recalibration methods, especially in multiclass scenarios with limited data.

Method: Uses logistic regression for recalibration, introduces structured regularization, robust preprocessing, and efficient optimization to manage bias-variance tradeoffs.

Result: Substantial improvements over existing logistic-based calibration techniques, with efficient open-source implementations.

Conclusion: The proposed methods offer a better alternative to common recalibration techniques like temperature, vector, and matrix scaling.

Abstract: Post-hoc recalibration methods are widely used to ensure that classifiers
provide faithful probability estimates. We argue that parametric recalibration
functions based on logistic regression can be motivated from a simple
theoretical setting for both binary and multiclass classification. This insight
motivates the use of more expressive calibration methods beyond standard
temperature scaling. For multi-class calibration however, a key challenge lies
in the increasing number of parameters introduced by more complex models, often
coupled with limited calibration data, which can lead to overfitting. Through
extensive experiments, we demonstrate that the resulting bias-variance tradeoff
can be effectively managed by structured regularization, robust preprocessing
and efficient optimization. The resulting methods lead to substantial gains
over existing logistic-based calibration techniques. We provide efficient and
easy-to-use open-source implementations of our methods, making them an
attractive alternative to common temperature, vector, and matrix scaling
implementations.

</details>


### [168] [Going Beyond Expert Performance via Deep Implicit Imitation Reinforcement Learning](https://arxiv.org/abs/2511.03616)
*Iason Chrysomallis,Georgios Chalkiadakis*

Main category: cs.LG

TL;DR: The paper introduces Deep Implicit Imitation Q-Network (DIIQN) and its variant HA-DIIQN, addressing limitations in imitation learning by combining reinforcement learning with observation-only datasets and handling heterogeneous action sets.


<details>
  <summary>Details</summary>
Motivation: Traditional imitation learning needs complete state-action demonstrations from optimal experts, limiting practicality. Observational data without actions and suboptimal experts are common issues addressed here.

Method: DIIQN uses action inference and dynamic confidence mechanisms for expert-guided learning. HA-DIIQN adds infeasibility detection and bridging for heterogeneous action sets.

Result: DIIQN achieves up to 130% higher returns than DQN, surpassing expert performance. HA-DIIQN learns 64% faster in heterogeneous settings.

Conclusion: The framework effectively leverages expert guidance and adapts to suboptimal or mismatched expert data, enhancing learning efficiency and performance.

Abstract: Imitation learning traditionally requires complete state-action
demonstrations from optimal or near-optimal experts. These requirements
severely limit practical applicability, as many real-world scenarios provide
only state observations without corresponding actions and expert performance is
often suboptimal. In this paper we introduce a deep implicit imitation
reinforcement learning framework that addresses both limitations by combining
deep reinforcement learning with implicit imitation learning from
observation-only datasets. Our main algorithm, Deep Implicit Imitation
Q-Network (DIIQN), employs an action inference mechanism that reconstructs
expert actions through online exploration and integrates a dynamic confidence
mechanism that adaptively balances expert-guided and self-directed learning.
This enables the agent to leverage expert guidance for accelerated training
while maintaining capacity to surpass suboptimal expert performance. We further
extend our framework with a Heterogeneous Actions DIIQN (HA-DIIQN) algorithm to
tackle scenarios where expert and agent possess different action sets, a
challenge previously unaddressed in the implicit imitation learning literature.
HA-DIIQN introduces an infeasibility detection mechanism and a bridging
procedure identifying alternative pathways connecting agent capabilities to
expert guidance when direct action replication is impossible. Our experimental
results demonstrate that DIIQN achieves up to 130% higher episodic returns
compared to standard DQN, while consistently outperforming existing implicit
imitation methods that cannot exceed expert performance. In heterogeneous
action settings, HA-DIIQN learns up to 64% faster than baselines, leveraging
expert datasets unusable by conventional approaches. Extensive parameter
sensitivity analysis reveals the framework's robustness across varying dataset
sizes and hyperparameter configurations.

</details>


### [169] [AnaFlow: Agentic LLM-based Workflow for Reasoning-Driven Explainable and Sample-Efficient Analog Circuit Sizing](https://arxiv.org/abs/2511.03697)
*Mohsen Ahmadzadeh,Kaichang Chen,Georges Gielen*

Main category: cs.LG

TL;DR: The paper introduces AnaFlow, a novel AI framework using multi-agent LLMs for efficient and explainable analog circuit sizing, overcoming traditional bottlenecks like simulation time and lack of transparency.


<details>
  <summary>Details</summary>
Motivation: Address inefficiencies in analog circuit design, such as time-consuming simulations and unclear AI-generated solutions, by leveraging explainable AI and multi-agent collaboration.

Method: AnaFlow employs specialized LLM-based agents to interpret circuit topology, refine design parameters iteratively, and use adaptive simulations for sample efficiency.

Result: Demonstrated success in sizing circuits of varying complexity automatically, improving efficiency and avoiding past mistakes via learning from optimization history.

Conclusion: AnaFlow represents a transformative paradigm in analog EDA, offering transparent AI assistance for design space exploration.

Abstract: Analog/mixed-signal circuits are key for interfacing electronics with the
physical world. Their design, however, remains a largely handcrafted process,
resulting in long and error-prone design cycles. While the recent rise of
AI-based reinforcement learning and generative AI has created new techniques to
automate this task, the need for many time-consuming simulations is a critical
bottleneck hindering the overall efficiency. Furthermore, the lack of
explainability of the resulting design solutions hampers widespread adoption of
the tools. To address these issues, a novel agentic AI framework for
sample-efficient and explainable analog circuit sizing is presented. It employs
a multi-agent workflow where specialized Large Language Model (LLM)-based
agents collaborate to interpret the circuit topology, to understand the design
goals, and to iteratively refine the circuit's design parameters towards the
target goals with human-interpretable reasoning. The adaptive simulation
strategy creates an intelligent control that yields a high sample efficiency.
The AnaFlow framework is demonstrated for two circuits of varying complexity
and is able to complete the sizing task fully automatically, differently from
pure Bayesian optimization and reinforcement learning approaches. The system
learns from its optimization history to avoid past mistakes and to accelerate
convergence. The inherent explainability makes this a powerful tool for analog
design space exploration and a new paradigm in analog EDA, where AI agents
serve as transparent design assistants.

</details>


### [170] [Towards Formalizing Reinforcement Learning Theory](https://arxiv.org/abs/2511.03618)
*Shangtong Zhang*

Main category: cs.LG

TL;DR: Formal verification of almost sure convergence for Q-learning and linear TD learning using Lean 4 and Mathlib library, with potential extensions for convergence rates and other modes.


<details>
  <summary>Details</summary>
Motivation: To address the growing interest in understanding the convergence properties of foundational RL algorithms like Q-learning and linear TD learning.

Method: Utilizes the Lean 4 theorem prover and the Robbins-Siegmund theorem in a unified framework to verify convergence.

Result: Successfully formalizes the almost sure convergence of these algorithms.

Conclusion: The work contributes significantly to formally verifying RL convergence results, with potential for broader applications.

Abstract: In this paper, we formalize the almost sure convergence of $Q$-learning and
linear temporal difference (TD) learning with Markovian samples using the Lean
4 theorem prover based on the Mathlib library. $Q$-learning and linear TD are
among the earliest and most influential reinforcement learning (RL) algorithms.
The investigation of their convergence properties is not only a major research
topic during the early development of the RL field but also receives increasing
attention nowadays. This paper formally verifies their almost sure convergence
in a unified framework based on the Robbins-Siegmund theorem. The framework
developed in this work can be easily extended to convergence rates and other
modes of convergence. This work thus makes an important step towards fully
formalizing convergent RL results. The code is available at
https://github.com/ShangtongZhang/rl-theory-in-lean.

</details>


### [171] [nanoTabPFN: A Lightweight and Educational Reimplementation of TabPFN](https://arxiv.org/abs/2511.03634)
*Alexander Pfefferle,Johannes Hog,Lennart Purucker,Frank Hutter*

Main category: cs.LG

TL;DR: nanoTabPFN simplifies TabPFN v2's architecture and training loop, making tabular foundation models more accessible with faster performance.


<details>
  <summary>Details</summary>
Motivation: Existing tabular foundation models are complex, poorly documented, and hard to adapt, limiting accessibility for beginners and researchers.

Method: nanoTabPFN offers a lightweight implementation of TabPFN v2, using pre-generated training data for efficient pre-training on a single GPU.

Result: It achieves comparable performance to traditional baselines in one minute (160,000x faster than TabPFN v2) and reduces computational resource needs.

Conclusion: nanoTabPFN democratizes access to tabular foundation models, enabling educational use and experimentation with minimal resources.

Abstract: Tabular foundation models such as TabPFN have revolutionized predictive
machine learning for tabular data. At the same time, the driving factors of
this revolution are hard to understand. Existing open-source tabular foundation
models are implemented in complicated pipelines boasting over 10,000 lines of
code, lack architecture documentation or code quality. In short, the
implementations are hard to understand, not beginner-friendly, and complicated
to adapt for new experiments. We introduce nanoTabPFN, a simplified and
lightweight implementation of the TabPFN v2 architecture and a corresponding
training loop that uses pre-generated training data. nanoTabPFN makes tabular
foundation models more accessible to students and researchers alike. For
example, restricted to a small data setting it achieves a performance
comparable to traditional machine learning baselines within one minute of
pre-training on a single GPU (160,000x faster than TabPFN v2 pretraining). This
eliminated requirement of large computational resources makes pre-training
tabular foundation models accessible for educational purposes. Our code is
available at https://github.com/automl/nanoTabPFN.

</details>


### [172] [SHIELD: Securing Healthcare IoT with Efficient Machine Learning Techniques for Anomaly Detection](https://arxiv.org/abs/2511.03661)
*Mahek Desai,Apoorva Rumale,Marjan Asadinia*

Main category: cs.LG

TL;DR: A machine learning framework for detecting cyberattacks and device anomalies in IoT healthcare achieved high accuracy with XGBoost and KNN, while GAN underperformed.


<details>
  <summary>Details</summary>
Motivation: To address security and reliability challenges in IoT-enabled healthcare by detecting cyber threats and operational anomalies.

Method: Evaluated eight ML models (XGBoost, KNN, GAN, VAE, One-Class SVM, Isolation Forest, GNN, LSTM Autoencoders) across supervised, semi-supervised, and unsupervised learning using 200,000 records.

Result: XGBoost achieved 99% accuracy for anomaly detection; KNN excelled in attack detection. GAN and LSTM Autoencoders underperformed.

Conclusion: The framework enhances IoT healthcare security, preventing breaches and ensuring device reliability.

Abstract: The integration of IoT devices in healthcare introduces significant security
and reliability challenges, increasing susceptibility to cyber threats and
operational anomalies. This study proposes a machine learning-driven framework
for (1) detecting malicious cyberattacks and (2) identifying faulty device
anomalies, leveraging a dataset of 200,000 records. Eight machine learning
models are evaluated across three learning approaches: supervised learning
(XGBoost, K-Nearest Neighbors (K- NN)), semi-supervised learning (Generative
Adversarial Networks (GAN), Variational Autoencoders (VAE)), and unsupervised
learning (One-Class Support Vector Machine (SVM), Isolation Forest, Graph
Neural Networks (GNN), and Long Short-Term Memory (LSTM) Autoencoders). The
comprehensive evaluation was conducted across multiple metrics like F1-score,
precision, recall, accuracy, ROC-AUC, computational efficiency. XGBoost
achieved 99\% accuracy with minimal computational overhead (0.04s) for anomaly
detection, while Isolation Forest balanced precision and recall effectively.
LSTM Autoencoders underperformed with lower accuracy and higher latency. For
attack detection, KNN achieved near-perfect precision, recall, and F1-score
with the lowest computational cost (0.05s), followed by VAE at 97% accuracy.
GAN showed the highest computational cost with lowest accuracy and ROC-AUC.
These findings enhance IoT-enabled healthcare security through effective
anomaly detection strategies. By improving early detection of cyber threats and
device failures, this framework has the potential to prevent data breaches,
minimize system downtime, and ensure the continuous and safe operation of
medical devices, ultimately safeguarding patient health and trust in IoT-driven
healthcare solutions.

</details>


### [173] [Behavior-Adaptive Q-Learning: A Unifying Framework for Offline-to-Online RL](https://arxiv.org/abs/2511.03695)
*Lipeng Zu,Hansong Zhou,Xiaonan Zhang*

Main category: cs.LG

TL;DR: BAQ is a framework for smoother offline-to-online RL transitions by using behavior-consistency signals during online fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Offline RL policies struggle in dynamic environments due to distributional shift and unreliable value estimates, necessitating a reliable transition method.

Method: BAQ leverages an implicit behavioral model from offline data, using a dual-objective loss to align policies toward offline behavior or relax constraints based on confidence.

Result: BAQ outperforms prior methods, enabling faster recovery, improved robustness, and higher performance in benchmarks.

Conclusion: Implicit behavior adaptation is effective for reliable real-world policy deployment.

Abstract: Offline reinforcement learning (RL) enables training from fixed data without
online interaction, but policies learned offline often struggle when deployed
in dynamic environments due to distributional shift and unreliable value
estimates on unseen state-action pairs. We introduce Behavior-Adaptive
Q-Learning (BAQ), a framework designed to enable a smooth and reliable
transition from offline to online RL. The key idea is to leverage an implicit
behavioral model derived from offline data to provide a behavior-consistency
signal during online fine-tuning. BAQ incorporates a dual-objective loss that
(i) aligns the online policy toward the offline behavior when uncertainty is
high, and (ii) gradually relaxes this constraint as more confident online
experience is accumulated. This adaptive mechanism reduces error propagation
from out-of-distribution estimates, stabilizes early online updates, and
accelerates adaptation to new scenarios. Across standard benchmarks, BAQ
consistently outperforms prior offline-to-online RL approaches, achieving
faster recovery, improved robustness, and higher overall performance. Our
results demonstrate that implicit behavior adaptation is a principled and
practical solution for reliable real-world policy deployment.

</details>


### [174] [Shrinking the Variance: Shrinkage Baselines for Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2511.03710)
*Guanning Zeng,Zhaoyi Zhou,Daman Arora,Andrea Zanette*

Main category: cs.LG

TL;DR: The paper introduces shrinkage estimators for improving mean reward estimation in RLVR, reducing variance in policy-gradient methods like GRPO and enhancing training stability.


<details>
  <summary>Details</summary>
Motivation: The need for stable and efficient training of large reasoning models (LRMs) using reinforcement learning with verifiable rewards (RLVR) motivates the search for better baselines to reduce variance in policy-gradient estimators.

Method: The authors propose shrinkage estimators that combine per-prompt and across-prompt means, inspired by Stein's paradox, to improve mean reward estimation accuracy, especially in low-generation regimes.

Result: Theoretically, the shrinkage-based baseline reduces variance in policy-gradient estimators. Empirically, it outperforms standard empirical-mean baselines, leading to lower-variance updates and better training stability.

Conclusion: Shrinkage baselines are effective for RLVR, offering a drop-in replacement for existing baselines without extra hyper-parameters or computation.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a
powerful paradigm for post-training large reasoning models (LRMs) using
policy-gradient methods such as GRPO. To stabilize training, these methods
typically center trajectory rewards by subtracting the empirical mean for each
prompt. Statistically, this centering acts as a control variate (or baseline),
reducing the variance of the policy-gradient estimator.
  Typically, the mean reward is estimated using per-prompt empirical averages
for each prompt in a batch. Drawing inspiration from Stein's paradox, we
propose using shrinkage estimators that combine per-prompt and across-prompt
means to improve the overall per-prompt mean estimation accuracy --
particularly in the low-generation regime typical of RLVR. Theoretically, we
construct a shrinkage-based baseline that provably yields lower-variance
policy-gradient estimators across algorithms. Our proposed baseline serves as a
drop-in replacement for existing per-prompt mean baselines, requiring no
additional hyper-parameters or computation. Empirically, shrinkage baselines
consistently outperform standard empirical-mean baselines, leading to
lower-variance gradient updates and improved training stability.

</details>
