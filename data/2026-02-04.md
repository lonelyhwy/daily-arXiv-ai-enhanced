<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 106]
- [cs.CL](#cs.CL) [Total: 75]
- [cs.AI](#cs.AI) [Total: 67]
- [cs.MA](#cs.MA) [Total: 5]
- [cs.LG](#cs.LG) [Total: 243]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [WorldVQA: Measuring Atomic World Knowledge in Multimodal Large Language Models](https://arxiv.org/abs/2602.02537)
*Runjie Zhou,Youbo Shao,Haoyu Lu,Bowei Xing,Tongtong Bai,Yujie Chen,Jie Zhao,Lin Sui,Haotian Yao,Zijia Zhao,Hao Yang,Haoning Wu,Zaida Zhou,Jinguo Zhu,Zhiqi Huang,Yiping Bao,Yangyang Liu,Y. Charles,Xinyu Zhou*

Main category: cs.CV

TL;DR: WorldVQA is a benchmark to evaluate Multimodal Large Language Models' visual knowledge by decoupling memorization from reasoning, focusing on grounding and naming visual entities across a taxonomy.


<details>
  <summary>Details</summary>
Motivation: Current evaluations conflate visual knowledge retrieval with reasoning, limiting accurate measurement of what models memorize. WorldVQA aims to address this gap.

Method: The benchmark assesses atomic capabilities of grounding and naming visual entities across a stratified taxonomy, from common to rare objects.

Result: WorldVQA provides a rigorous test for visual factuality, measuring encyclopedic breadth and hallucination rates in models.

Conclusion: WorldVQA establishes a standard for evaluating visual knowledge in current and future MLLMs, ensuring accurate assessment of memorized content.

Abstract: We introduce WorldVQA, a benchmark designed to evaluate the atomic visual world knowledge of Multimodal Large Language Models (MLLMs). Unlike current evaluations, which often conflate visual knowledge retrieval with reasoning, WorldVQA decouples these capabilities to strictly measure "what the model memorizes." The benchmark assesses the atomic capability of grounding and naming visual entities across a stratified taxonomy, spanning from common head-class objects to long-tail rarities. We expect WorldVQA to serve as a rigorous test for visual factuality, thereby establishing a standard for assessing the encyclopedic breadth and hallucination rates of current and next-generation frontier models.

</details>


### [2] [AdaptMMBench: Benchmarking Adaptive Multimodal Reasoning for Mode Selection and Reasoning Process](https://arxiv.org/abs/2602.02676)
*Xintong Zhang,Xiaowen Zhang,Jongrong Wu,Zhi Gao,Shilin Yan,Zhenxin Diao,Kunpeng Gao,Xuanyan Chen,Yuwei Wu,Yunde Jia,Qing Li*

Main category: cs.CV

TL;DR: AdaptMMBench is proposed as a benchmark for adaptive multimodal reasoning, addressing gaps in evaluations by dynamically assessing task difficulty and mode selection effectiveness across five domains.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations lack dynamic difficulty assessment and fail to distinguish adaptive mode selection from general performance, necessitating a more nuanced benchmark.

Method: AdaptMMBench evaluates multimodal reasoning using MCC for mode selection rationality and multidimensional process analysis (key step coverage, tool effectiveness, computational efficiency).

Result: Adaptive mode selection improves with model capacity but doesn't align with final accuracy. Key step coverage correlates with performance, while tool effectiveness varies across architectures.

Conclusion: AdaptMMBench provides a comprehensive framework for evaluating adaptive multimodal reasoning, highlighting the decoupling of mode selection from accuracy and variability in tool effectiveness.

Abstract: Adaptive multimodal reasoning has emerged as a promising frontier in Vision-Language Models (VLMs), aiming to dynamically modulate between tool-augmented visual reasoning and text reasoning to enhance both effectiveness and efficiency. However, existing evaluations rely on static difficulty labels and simplistic metrics, which fail to capture the dynamic nature of difficulty relative to varying model capacities. Consequently, they obscure the distinction between adaptive mode selection and general performance while neglecting fine-grained process analyses. In this paper, we propose AdaptMMBench, a comprehensive benchmark for adaptive multimodal reasoning across five domains: real-world, OCR, GUI, knowledge, and math, encompassing both direct perception and complex reasoning tasks. AdaptMMBench utilizes a Matthews Correlation Coefficient (MCC) metric to evaluate the selection rationality of different reasoning modes, isolating this meta-cognition ability by dynamically identifying task difficulties based on models' capability boundaries. Moreover, AdaptMMBench facilitates multi-dimensional process evaluation across key step coverage, tool effectiveness, and computational efficiency. Our evaluation reveals that while adaptive mode selection scales with model capacity, it notably decouples from final accuracy. Conversely, key step coverage aligns with performance, though tool effectiveness remains highly inconsistent across model architectures.

</details>


### [3] [End-to-end reconstruction of OCT optical properties and speckle-reduced structural intensity via physics-based learning](https://arxiv.org/abs/2602.02721)
*Jinglun Yu,Yaning Wang,Wenhan Guo,Yuan Gao,Yu Sun,Jin U. Kang*

Main category: cs.CV

TL;DR: A deep learning framework is proposed for joint reconstruction of optical parameter maps and speckle-reduced OCT structural images, leveraging physics-based supervision and Monte Carlo simulations for robust performance.


<details>
  <summary>Details</summary>
Motivation: The inverse scattering problem in OCT is complex due to attenuation, noise, and parameter coupling. The goal is to recover accurate structural images and intrinsic tissue properties for better tissue characterization.

Method: A regularized end-to-end deep learning framework is introduced, incorporating a physics-based OCT forward model. It uses Monte Carlo-simulated ground truth for training, ensuring physics-consistent supervision for parameter recovery and artifact suppression.

Result: Experiments on synthetic corneal OCT data show robust recovery of optical maps under noise, improved resolution, and enhanced structural fidelity.

Conclusion: The approach demonstrates the effectiveness of combining physics-informed modeling with deep learning for quantitative multi-parameter tissue characterization in OCT.

Abstract: Inverse scattering in optical coherence tomography (OCT) seeks to recover both structural images and intrinsic tissue optical properties, including refractive index, scattering coefficient, and anisotropy. This inverse problem is challenging due to attenuation, speckle noise, and strong coupling among parameters. We propose a regularized end-to-end deep learning framework that jointly reconstructs optical parameter maps and speckle-reduced OCT structural intensity for layer visualization. Trained with Monte Carlo-simulated ground truth, our network incorporates a physics-based OCT forward model that generates predicted signals from the estimated parameters, providing physics-consistent supervision for parameter recovery and artifact suppression. Experiments on the synthetic corneal OCT dataset demonstrate robust optical map recovery under noise, improved resolution, and enhanced structural fidelity. This approach enables quantitative multi-parameter tissue characterization and highlights the benefit of combining physics-informed modeling with deep learning for computational OCT.

</details>


### [4] [SVD-ViT: Does SVD Make Vision Transformers Attend More to the Foreground?](https://arxiv.org/abs/2602.02765)
*Haruhiko Murata,Kazuhiro Hotta*

Main category: cs.CV

TL;DR: SVD-ViT uses SVD to enhance ViT by prioritizing foreground features, reducing background noise, and improving classification accuracy.


<details>
  <summary>Details</summary>
Motivation: ViTs lack a mechanism to distinguish foreground from background, leading to degraded performance due to unnecessary background features.

Method: SVD-ViT introduces SPC, SSVA, and ID-RSVD modules to extract and aggregate singular vectors focusing on foreground information.

Result: The method improves classification accuracy and effectively learns foreground representations while reducing background noise.

Conclusion: SVD-ViT enhances ViTs by addressing foreground-background distinction, leading to better performance.

Abstract: Vision Transformers (ViT) have been established as large-scale foundation models. However, because self-attention operates globally, they lack an explicit mechanism to distinguish foreground from background. As a result, ViT may learn unnecessary background features and artifacts, leading to degraded classification performance. To address this issue, we propose SVD-ViT, which leverages singular value decomposition (SVD) to prioritize the learning of foreground features. SVD-ViT consists of three components-\textbf{SPC module}, \textbf{SSVA}, and \textbf{ID-RSVD}-and suppresses task-irrelevant factors such as background noise and artifacts by extracting and aggregating singular vectors that capture object foreground information. Experimental results demonstrate that our method improves classification accuracy and effectively learns informative foreground representations while reducing the impact of background noise.

</details>


### [5] [LmPT: Conditional Point Transformer for Anatomical Landmark Detection on 3D Point Clouds](https://arxiv.org/abs/2602.02808)
*Matteo Bastico,Pierre Onghena,David Ryckelynck,Beatriz Marcotegui,Santiago Velasco-Forero,Laurent Corté,Caroline Robine--Decourcelle,Etienne Decencière*

Main category: cs.CV

TL;DR: The paper introduces Landmark Point Transformer (LmPT), an automatic method for detecting anatomical landmarks on point clouds, addressing limitations of manual and rule-based techniques. It leverages cross-species learning for translational research and demonstrates effectiveness on human and dog femurs.


<details>
  <summary>Details</summary>
Motivation: Existing landmarking methods are time-consuming, inconsistent, or limited in scope. The need for adaptable, accurate, and generalized solutions motivates the proposed LmPT method.

Method: LmPT uses point clouds to represent anatomical surfaces, incorporating a conditioning mechanism for adaptability across species. It focuses on femoral landmarking for evaluation.

Result: The method demonstrates generalization and effectiveness across human and dog femurs, showcasing its potential for translational research.

Conclusion: LmPT offers an innovative, adaptable solution for anatomical landmark detection, overcoming traditional limitations and enabling cross-species applications.

Abstract: Accurate identification of anatomical landmarks is crucial for various medical applications. Traditional manual landmarking is time-consuming and prone to inter-observer variability, while rule-based methods are often tailored to specific geometries or limited sets of landmarks. In recent years, anatomical surfaces have been effectively represented as point clouds, which are lightweight structures composed of spatial coordinates. Following this strategy and to overcome the limitations of existing landmarking techniques, we propose Landmark Point Transformer (LmPT), a method for automatic anatomical landmark detection on point clouds that can leverage homologous bones from different species for translational research. The LmPT model incorporates a conditioning mechanism that enables adaptability to different input types to conduct cross-species learning. We focus the evaluation of our approach on femoral landmarking using both human and newly annotated dog femurs, demonstrating its generalization and effectiveness across species. The code and dog femur dataset will be publicly available at: https://github.com/Pierreoo/LandmarkPointTransformer.

</details>


### [6] [Self-Supervised Uncalibrated Multi-View Video Anonymization in the Operating Room](https://arxiv.org/abs/2602.02850)
*Keqi Chen,Vinkle Srivastav,Armine Vardazaryan,Cindy Rolland,Didier Mutter,Nicolas Padoy*

Main category: cs.CV

TL;DR: A novel self-supervised framework for multi-view video anonymization in ORs eliminates manual annotation and calibration, achieving high recall.


<details>
  <summary>Details</summary>
Motivation: Privacy preservation in OR research requires effective anonymization, but existing methods lack scalability due to manual effort and calibration needs.

Method: Uses self-supervised whole-body detection and pose estimation, retrieving false negatives via tracking and multi-view association for iterative detector tuning.

Result: Achieves over 97% recall on simulated and real surgery datasets and enables real-time detector training with pseudo labels.

Conclusion: The framework demonstrates scalable, practical anonymization without manual intervention, advancing OR research privacy.

Abstract: Privacy preservation is a prerequisite for using video data in Operating Room (OR) research. Effective anonymization relies on the exhaustive localization of every individual; even a single missed detection necessitates extensive manual correction. However, existing approaches face two critical scalability bottlenecks: (1) they usually require manual annotations of each new clinical site for high accuracy; (2) while multi-camera setups have been widely adopted to address single-view ambiguity, camera calibration is typically required whenever cameras are repositioned. To address these problems, we propose a novel self-supervised multi-view video anonymization framework consisting of whole-body person detection and whole-body pose estimation, without annotation or camera calibration. Our core strategy is to enhance the single-view detector by "retrieving" false negatives using temporal and multi-view context, and conducting self-supervised domain adaptation. We first run an off-the-shelf whole-body person detector in each view with a low-score threshold to gather candidate detections. Then, we retrieve the low-score false negatives that exhibit consistency with the high-score detections via tracking and self-supervised uncalibrated multi-view association. These recovered detections serve as pseudo labels to iteratively fine-tune the whole-body detector. Finally, we apply whole-body pose estimation on each detected person, and fine-tune the pose model using its own high-score predictions. Experiments on the 4D-OR dataset of simulated surgeries and our dataset of real surgeries show the effectiveness of our approach achieving over 97% recall. Moreover, we train a real-time whole-body detector using our pseudo labels, achieving comparable performance and highlighting our method's practical applicability. Code is available at https://github.com/CAMMA-public/OR_anonymization.

</details>


### [7] [ViThinker: Active Vision-Language Reasoning via Dynamic Perceptual Querying](https://arxiv.org/abs/2602.02873)
*Weihang You,Qingchan Zhu,David Liu,Yi Pan,Geng Yuan,Hanqi Jiang*

Main category: cs.CV

TL;DR: ViThinker is a framework that enhances vision-language models by enabling active perception through generative mental simulation, outperforming passive methods in reasoning and perceptual grounding.


<details>
  <summary>Details</summary>
Motivation: Chain-of-Thought reasoning in vision-language models lacks active perception and discards continuous visual information, limiting effectiveness.

Method: ViThinker introduces autonomous query token generation to synthesize expert-aligned visual features, trained via a two-stage curriculum with sparsity penalties.

Result: ViThinker improves reasoning accuracy and perceptual grounding across vision-centric benchmarks compared to passive methods.

Conclusion: Active perception through ViThinker outperforms passive approaches, offering enhanced capabilities for vision-language models.

Abstract: Chain-of-Thought (CoT) reasoning excels in language models but struggles in vision-language models due to premature visual-to-text conversion that discards continuous information such as geometry and spatial layout. While recent methods enhance CoT through static enumeration or attention-based selection, they remain passive, i.e., processing pre-computed inputs rather than actively seeking task-relevant details. Inspired by human active perception, we introduce ViThinker, a framework that enables vision-language models to autonomously generate decision (query) tokens triggering the synthesis of expert-aligned visual features on demand. ViThinker internalizes vision-expert capabilities during training, performing generative mental simulation during inference without external tool calls. Through a two-stage curriculum: first distilling frozen experts into model parameters, then learning task-driven querying via sparsity penalties, i.e., ViThinker discovers minimal sufficient perception for each reasoning step. Evaluations across vision-centric benchmarks demonstrate consistent improvements, validating that active query generation outperforms passive approaches in both perceptual grounding and reasoning accuracy.

</details>


### [8] [DoubleTake: Contrastive Reasoning for Faithful Decision-Making in Medical Imaging](https://arxiv.org/abs/2602.02894)
*Daivik Patel,Shrenik Patel*

Main category: cs.CV

TL;DR: The paper introduces a contrastive, document-aware reference selection framework for medical imaging, addressing redundancy in existing nearest neighbor retrieval methods. It proposes Counterfactual-Contrastive Inference for improved accuracy and decision-making.


<details>
  <summary>Details</summary>
Motivation: Existing approaches in medical imaging rely on redundant evidence retrieval, reinforcing single hypotheses. The study aims to improve discrimination by balancing visual relevance, diversity, and provenance.

Method: The framework uses ROCO embeddings and metadata to construct compact evidence sets. It includes a reproducible protocol and curated reference bank for contrastive retrieval. Counterfactual-Contrastive Inference performs structured comparisons and aggregates evidence using margin-based rules.

Result: The approach achieves state-of-the-art performance on the MediConfusion benchmark, improving set-level accuracy by 15% and reducing confusion compared to prior methods.

Conclusion: The proposed framework enhances decision-making in medical imaging by optimizing evidence selection and introducing contrastive reasoning, significantly outperforming existing methods.

Abstract: Accurate decision making in medical imaging requires reasoning over subtle visual differences between confusable conditions, yet most existing approaches rely on nearest neighbor retrieval that returns redundant evidence and reinforces a single hypothesis. We introduce a contrastive, document-aware reference selection framework that constructs compact evidence sets optimized for discrimination rather than similarity by explicitly balancing visual relevance, embedding diversity, and source-level provenance using ROCO embeddings and metadata. While ROCO provides large-scale image-caption pairs, it does not specify how references should be selected for contrastive reasoning, and naive retrieval frequently yields near-duplicate figures from the same document. To address this gap, we release a reproducible reference selection protocol and curated reference bank that enable a systematic study of contrastive retrieval in medical image reasoning. Building on these contrastive evidence sets, we propose Counterfactual-Contrastive Inference, a confidence-aware reasoning framework that performs structured pairwise visual comparisons and aggregates evidence using margin-based decision rules with faithful abstention. On the MediConfusion benchmark, our approach achieves state-of-the-art performance, improving set-level accuracy by nearly 15% relative to prior methods while reducing confusion and improving individual accuracy.

</details>


### [9] [FaceLinkGen: Rethinking Identity Leakage in Privacy-Preserving Face Recognition with Identity Extraction](https://arxiv.org/abs/2602.02914)
*Wenqi Guo,Shan Du*

Main category: cs.CV

TL;DR: Privacy-preserving face recognition (PPFR) systems often fail to protect identity data, as shown by FaceLinkGen, an attack that bypasses pixel-level reconstruction metrics.


<details>
  <summary>Details</summary>
Motivation: Current PPFR evaluations focus on pixel-level reconstruction resistance (measured by PSNR/SSIM), but ignore actual privacy risks like identity extraction.

Method: Developed FaceLinkGen, an attack that links/matches identities and regenerates faces directly from protected templates without pixel recovery.

Result: FaceLinkGen achieved over 98.5% matching accuracy and 96% regeneration success on recent PPFR systems, even exceeding 92%/94% in low-knowledge settings.

Conclusion: Pixel distortion metrics inadequately measure real privacy risks; visual obfuscation leaves identities vulnerable to extraction by attackers and untrusted providers.

Abstract: Transformation-based privacy-preserving face recognition (PPFR) aims to verify identities while hiding facial data from attackers and malicious service providers. Existing evaluations mostly treat privacy as resistance to pixel-level reconstruction, measured by PSNR and SSIM. We show that this reconstruction-centric view fails. We present FaceLinkGen, an identity extraction attack that performs linkage/matching and face regeneration directly from protected templates without recovering original pixels. On three recent PPFR systems, FaceLinkGen reaches over 98.5\% matching accuracy and above 96\% regeneration success, and still exceeds 92\% matching and 94\% regeneration in a near zero knowledge setting. These results expose a structural gap between pixel distortion metrics, which are widely used in PPFR evaluation, and real privacy. We show that visual obfuscation leaves identity information broadly exposed to both external intruders and untrusted service providers.

</details>


### [10] [A Multi-scale Linear-time Encoder for Whole-Slide Image Analysis](https://arxiv.org/abs/2602.02918)
*Jagan Mohan Reddy Dwarampudi,Joshua Wong,Hien Van Nguyen,Tania Banerjee*

Main category: cs.CV

TL;DR: MARBLE is a Mamba-based multi-state MIL framework for WSI analysis, improving efficiency and performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: WSI analysis is complex due to gigapixel resolutions and multi-scale hierarchies, and current MIL methods lack scalability and efficiency.

Method: MARBLE uses parallel multi-scale processing and linear-time state-space modeling to capture cross-scale dependencies with minimal overhead.

Result: On five datasets, MARBLE improves AUC by 6.9%, accuracy by 20.3%, and C-index by 2.3%.

Conclusion: MARBLE offers a scalable, efficient, and generalizable solution for multi-scale WSI analysis, outperforming transformer-based approaches.

Abstract: We introduce Multi-scale Adaptive Recurrent Biomedical Linear-time Encoder (MARBLE), the first \textit{purely Mamba-based} multi-state multiple instance learning (MIL) framework for whole-slide image (WSI) analysis. MARBLE processes multiple magnification levels in parallel and integrates coarse-to-fine reasoning within a linear-time state-space model, efficiently capturing cross-scale dependencies with minimal parameter overhead. WSI analysis remains challenging due to gigapixel resolutions and hierarchical magnifications, while existing MIL methods typically operate at a single scale and transformer-based approaches suffer from quadratic attention costs. By coupling parallel multi-scale processing with linear-time sequence modeling, MARBLE provides a scalable and modular alternative to attention-based architectures. Experiments on five public datasets show improvements of up to \textbf{6.9\%} in AUC, \textbf{20.3\%} in accuracy, and \textbf{2.3\%} in C-index, establishing MARBLE as an efficient and generalizable framework for multi-scale WSI analysis.

</details>


### [11] [SRA-Seg: Synthetic to Real Alignment for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2602.02944)
*OFM Riaz Rahman Aranya,Kevin Desai*

Main category: cs.CV

TL;DR: SRA-Seg aligns synthetic and real medical image features to improve segmentation, outperforming existing methods with minimal real data.


<details>
  <summary>Details</summary>
Motivation: Synthetic data lacks effectiveness due to domain gaps with real medical images, despite visual realism.

Method: SRA-Seg uses similarity-alignment loss, soft edge blending, and pseudo-labeling with an EMA teacher model.

Result: Achieves 89.34% Dice on ACDC and 84.42% on FIVES using only 10% real labeled data.

Conclusion: SRA-Seg bridges the domain gap between synthetic and real data, matching performance of methods using real unlabeled data.

Abstract: Synthetic data, an appealing alternative to extensive expert-annotated data for medical image segmentation, consistently fails to improve segmentation performance despite its visual realism. The reason being that synthetic and real medical images exist in different semantic feature spaces, creating a domain gap that current semi-supervised learning methods cannot bridge. We propose SRA-Seg, a framework explicitly designed to align synthetic and real feature distributions for medical image segmentation. SRA-Seg introduces a similarity-alignment (SA) loss using frozen DINOv2 embeddings to pull synthetic representations toward their nearest real counterparts in semantic space. We employ soft edge blending to create smooth anatomical transitions and continuous labels, eliminating the hard boundaries from traditional copy-paste augmentation. The framework generates pseudo-labels for synthetic images via an EMA teacher model and applies soft-segmentation losses that respect uncertainty in mixed regions. Our experiments demonstrate strong results: using only 10% labeled real data and 90% synthetic unlabeled data, SRA-Seg achieves 89.34% Dice on ACDC and 84.42% on FIVES, significantly outperforming existing semi-supervised methods and matching the performance of methods using real unlabeled data.

</details>


### [12] [Nüwa: Mending the Spatial Integrity Torn by VLM Token Pruning](https://arxiv.org/abs/2602.02951)
*Yihong Huang,Fei Ma,Yihua Shao,Jingcai Guo,Zitong Yu,Laizhong Cui,Qi Tian*

Main category: cs.CV

TL;DR: Vision token pruning accelerates Vision Language Models (VLMs). Existing methods perform well on VQA but poorly on VG tasks due to lost spatial references. Nüwa, a two-stage pruning framework, retains spatial anchors and improves performance on both tasks.


<details>
  <summary>Details</summary>
Motivation: Existing pruning methods degrade performance on visual grounding (VG) tasks due to loss of global spatial references derived from tokens' positional information.

Method: Nüwa employs a two-stage approach: (1) separation, alignment, and aggregation after the vision encoder to retain spatial anchors, and (2) text-guided pruning within the LLM to keep task-relevant tokens.

Result: Nüwa improves VQA performance (94% to 95%) and significantly boosts VG tasks (7% to 47%), achieving SOTA results.

Conclusion: The proposed Nüwa framework effectively maintains spatial integrity and enhances performance across VQA and VG tasks, addressing limitations of existing pruning methods.

Abstract: Vision token pruning has proven to be an effective acceleration technique for the efficient Vision Language Model (VLM). However, existing pruning methods demonstrate excellent performance preservation in visual question answering (VQA) and suffer substantial degradation on visual grounding (VG) tasks. Our analysis of the VLM's processing pipeline reveals that strategies utilizing global semantic similarity and attention scores lose the global spatial reference frame, which is derived from the interactions of tokens' positional information. Motivated by these findings, we propose $\text{Nüwa}$, a two-stage token pruning framework that enables efficient feature aggregation while maintaining spatial integrity. In the first stage, after the vision encoder, we apply three operations, namely separation, alignment, and aggregation, which are inspired by swarm intelligence algorithms to retain information-rich global spatial anchors. In the second stage, within the LLM, we perform text-guided pruning to retain task-relevant visual tokens. Extensive experiments demonstrate that $\text{Nüwa}$ achieves SOTA performance on multiple VQA benchmarks (from 94% to 95%) and yields substantial improvements on visual grounding tasks (from 7% to 47%).

</details>


### [13] [TRACE: Temporal Radiology with Anatomical Change Explanation for Grounded X-ray Report Generation](https://arxiv.org/abs/2602.02963)
*OFM Riaz Rahman Aranya,Kevin Desai*

Main category: cs.CV

TL;DR: TRACE is the first model combining temporal comparison, change classification, and spatial localization for chest X-rays, achieving over 90% grounding accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the lack of methods combining vision-language capabilities for temporal change detection in radiology.

Method: TRACE jointly learns temporal comparison, change classification, and spatial localization, generating natural language descriptions with bounding boxes.

Result: Achieves over 90% grounding accuracy; ablation shows change detection requires joint learning of temporal and spatial tasks.

Conclusion: Grounding provides essential spatial attention for temporal reasoning, establishing TRACE as a foundation for this task.

Abstract: Temporal comparison of chest X-rays is fundamental to clinical radiology, enabling detection of disease progression, treatment response, and new findings. While vision-language models have advanced single-image report generation and visual grounding, no existing method combines these capabilities for temporal change detection. We introduce Temporal Radiology with Anatomical Change Explanation (TRACE), the first model that jointly performs temporal comparison, change classification, and spatial localization. Given a prior and current chest X-ray, TRACE generates natural language descriptions of interval changes (worsened, improved, stable) while grounding each finding with bounding box coordinates. TRACE demonstrates effective spatial localization with over 90% grounding accuracy, establishing a foundation for this challenging new task. Our ablation study uncovers an emergent capability: change detection arises only when temporal comparison and spatial grounding are jointly learned, as neither alone enables meaningful change detection. This finding suggests that grounding provides a spatial attention mechanism essential for temporal reasoning.

</details>


### [14] [Dynamic High-frequency Convolution for Infrared Small Target Detection](https://arxiv.org/abs/2602.02969)
*Ruojing Li,Chao Xiao,Qian Yin,Wei An,Nuo Chen,Xinyi Ying,Miao Li,Yingqian Wang*

Main category: cs.CV

TL;DR: A dynamic high-frequency convolution (DHiF) method is proposed for improved infrared small target detection by discriminatively modeling high-frequency components (HFCs).


<details>
  <summary>Details</summary>
Motivation: Existing methods lack explicit modeling of HFCs, leading to challenges in distinguishing targets from other HFCs like bright corners or clouds.

Method: DHiF generates dynamic local filters adjusted via Fourier properties, combined with standard convolution for adaptive HFC processing.

Result: Experiments show DHiF outperforms other convolution operations in detection performance.

Conclusion: DHiF effectively enhances discriminative representation learning for infrared small target detection and integrates seamlessly into existing networks.

Abstract: Infrared small targets are typically tiny and locally salient, which belong to high-frequency components (HFCs) in images. Single-frame infrared small target (SIRST) detection is challenging, since there are many HFCs along with targets, such as bright corners, broken clouds, and other clutters. Current learning-based methods rely on the powerful capabilities of deep networks, but neglect explicit modeling and discriminative representation learning of various HFCs, which is important to distinguish targets from other HFCs. To address the aforementioned issues, we propose a dynamic high-frequency convolution (DHiF) to translate the discriminative modeling process into the generation of a dynamic local filter bank. Especially, DHiF is sensitive to HFCs, owing to the dynamic parameters of its generated filters being symmetrically adjusted within a zero-centered range according to Fourier transformation properties. Combining with standard convolution operations, DHiF can adaptively and dynamically process different HFC regions and capture their distinctive grayscale variation characteristics for discriminative representation learning. DHiF functions as a drop-in replacement for standard convolution and can be used in arbitrary SIRST detection networks without significant decrease in computational efficiency. To validate the effectiveness of our DHiF, we conducted extensive experiments across different SIRST detection networks on real-scene datasets. Compared to other state-of-the-art convolution operations, DHiF exhibits superior detection performance with promising improvement. Codes are available at https://github.com/TinaLRJ/DHiF.

</details>


### [15] [Fisheye Stereo Vision: Depth and Range Error](https://arxiv.org/abs/2602.02973)
*Leaf Jiang,Matthew Holzel,Bernhard Kaplan,Hsiou-Yuan Liu,Sabyasachi Paul,Karen Rankin,Piotr Swierczynski*

Main category: cs.CV

TL;DR: Analytical expressions for depth and range error in fisheye stereo vision systems are derived, focusing on accuracy at large angles.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy of fisheye stereo vision systems, especially at large angles, by providing precise analytical expressions for depth and range errors.

Method: Derived analytical expressions for depth and range error as a function of object distance, specifically addressing large-angle accuracy.

Result: The study provides exact formulas for depth and range error in fisheye stereo vision systems, enhancing accuracy in large-angle scenarios.

Conclusion: This work offers a theoretical foundation for improving fisheye stereo vision accuracy, particularly for objects at large angles.

Abstract: This study derives analytical expressions for the depth and range error of fisheye stereo vision systems as a function of object distance, specifically accounting for accuracy at large angles.

</details>


### [16] [SceneLinker: Compositional 3D Scene Generation via Semantic Scene Graph from RGB Sequences](https://arxiv.org/abs/2602.02974)
*Seok-Young Kim,Dooyoung Kim,Woojin Cho,Hail Song,Suji Kang,Woontack Woo*

Main category: cs.CV

TL;DR: SceneLinker is a framework for generating 3D scenes from RGB sequences using semantic scene graphs, addressing prior limitations in capturing object relationships.


<details>
  <summary>Details</summary>
Motivation: To enable adaptive Mixed Reality (MR) content by generating 3D scenes that reflect real-world layouts while capturing semantic cues.

Method: Uses a graph network with cross-check feature attention for scene graph prediction and a graph-VAE for joint shape and layout generation.

Result: Outperforms state-of-the-art methods on 3RScan/3DSSG and SG-FRONT datasets in complex indoor environments.

Conclusion: SceneLinker allows users to create consistent 3D spaces from physical environments for spatial MR content.

Abstract: We introduce SceneLinker, a novel framework that generates compositional 3D scenes via semantic scene graph from RGB sequences. To adaptively experience Mixed Reality (MR) content based on each user's space, it is essential to generate a 3D scene that reflects the real-world layout by compactly capturing the semantic cues of the surroundings. Prior works struggled to fully capture the contextual relationship between objects or mainly focused on synthesizing diverse shapes, making it challenging to generate 3D scenes aligned with object arrangements. We address these challenges by designing a graph network with cross-check feature attention for scene graph prediction and constructing a graph-variational autoencoder (graph-VAE), which consists of a joint shape and layout block for 3D scene generation. Experiments on the 3RScan/3DSSG and SG-FRONT datasets demonstrate that our approach outperforms state-of-the-art methods in both quantitative and qualitative evaluations, even in complex indoor environments and under challenging scene graph constraints. Our work enables users to generate consistent 3D spaces from their physical environments via scene graphs, allowing them to create spatial MR content. Project page is https://scenelinker2026.github.io.

</details>


### [17] [Aligning Forest and Trees in Images and Long Captions for Visually Grounded Understanding](https://arxiv.org/abs/2602.02977)
*Byeongju Woo,Zilin Wang,Byeonghyun Pak,Sangwoo Mo,Stella X. Yu*

Main category: cs.CV

TL;DR: CAFT is a hierarchical image-text learning framework aligning global and local semantics without pixel-level supervision, achieving state-of-the-art performance on long-text retrieval.


<details>
  <summary>Details</summary>
Motivation: Existing models like CLIP struggle with long captions due to undifferentiated alignment. Hierarchical semantics are needed for fine-grained vision-language understanding.

Method: CAFT combines a fine-to-coarse visual encoder with a hierarchical text transformer and hierarchical alignment loss to match global and local semantics.

Result: CAFT sets new benchmarks on six long-text retrieval tasks and shows strong scaling behavior.

Conclusion: Hierarchical cross-domain alignment enables fine-grained, visually grounded representations without explicit region-level supervision.

Abstract: Large vision-language models such as CLIP struggle with long captions because they align images and texts as undifferentiated wholes. Fine-grained vision-language understanding requires hierarchical semantics capturing both global context and localized details across visual and textual domains. Yet linguistic hierarchies from syntax or semantics rarely match visual organization, and purely visual hierarchies tend to fragment scenes into appearance-driven parts without semantic focus. We propose CAFT (Cross-domain Alignment of Forests and Trees), a hierarchical image-text representation learning framework that aligns global and local semantics across images and long captions without pixel-level supervision. Coupling a fine-to-coarse visual encoder with a hierarchical text transformer, it uses a hierarchical alignment loss that matches whole images with whole captions while biasing region-sentence correspondences, so that coarse semantics are built from fine-grained evidence rather than from aggregation untethered to part-level grounding. Trained on 30M image-text pairs, CAFT achieves state-of-the-art performance on six long-text retrieval benchmarks and exhibits strong scaling behavior. Experiments show that hierarchical cross-domain alignment enables fine-grained, visually grounded image-text representations to emerge without explicit region-level supervision.

</details>


### [18] [SharpTimeGS: Sharp and Stable Dynamic Gaussian Splatting via Lifespan Modulation](https://arxiv.org/abs/2602.02989)
*Zhanfeng Liao,Jiajun Zhang,Hanzhang Tu,Zhixi Wang,Yunqi Gao,Hongwen Zhang,Yebin Liu*

Main category: cs.CV

TL;DR: SharpTimeGS is a lifespan-aware 4D Gaussian framework for dynamic scene synthesis, balancing static and dynamic regions with adaptive modeling and optimized densification.


<details>
  <summary>Details</summary>
Motivation: To improve the balance between long-term static and short-term dynamic regions in 4D reconstruction, addressing limitations in existing Gaussian-based methods.

Method: Introduces a learnable lifespan parameter for temporal visibility and motion modulation, along with a lifespan-velocity-aware densification strategy.

Result: Achieves state-of-the-art performance with real-time rendering at 4K resolution and 100 FPS on high-end hardware.

Conclusion: SharpTimeGS effectively decouples motion and duration, enhancing stability and fidelity in dynamic scene synthesis.

Abstract: Novel view synthesis of dynamic scenes is fundamental to achieving photorealistic 4D reconstruction and immersive visual experiences. Recent progress in Gaussian-based representations has significantly improved real-time rendering quality, yet existing methods still struggle to maintain a balance between long-term static and short-term dynamic regions in both representation and optimization. To address this, we present SharpTimeGS, a lifespan-aware 4D Gaussian framework that achieves temporally adaptive modeling of both static and dynamic regions under a unified representation. Specifically, we introduce a learnable lifespan parameter that reformulates temporal visibility from a Gaussian-shaped decay into a flat-top profile, allowing primitives to remain consistently active over their intended duration and avoiding redundant densification. In addition, the learned lifespan modulates each primitives' motion, reducing drift in long-lived static points while retaining unrestricted motion for short-lived dynamic ones. This effectively decouples motion magnitude from temporal duration, improving long-term stability without compromising dynamic fidelity. Moreover, we design a lifespan-velocity-aware densification strategy that mitigates optimization imbalance between static and dynamic regions by allocating more capacity to regions with pronounced motion while keeping static areas compact and stable. Extensive experiments on multiple benchmarks demonstrate that our method achieves state-of-the-art performance while supporting real-time rendering up to 4K resolution at 100 FPS on one RTX 4090.

</details>


### [19] [Video-OPD: Efficient Post-Training of Multimodal Large Language Models for Temporal Video Grounding via On-Policy Distillation](https://arxiv.org/abs/2602.02994)
*Jiaze Li,Hao Yin,Haoran Xu,Boshen Xu,Wenhui Tan,Zewen He,Jianzhong Ju,Zhenbo Luo,Jian Luan*

Main category: cs.CV

TL;DR: Video-OPD is an efficient post-training framework for Temporal Video Grounding (TVG) using on-policy distillation with dense supervision, outperforming GRPO in performance, convergence speed, and computational cost.


<details>
  <summary>Details</summary>
Motivation: Existing GRPO-based methods for TVG suffer from sparse reward signals and high computational overhead. Video-OPD aims to address these limitations by leveraging on-policy distillation for dense, fine-grained supervision.

Method: Video-OPD optimizes trajectories sampled from the current policy with dense, token-level supervision via a reverse KL divergence objective. It includes TVDF, a curriculum prioritizing teacher-reliable and informative trajectories to enhance training efficiency.

Result: Video-OPD outperforms GRPO, achieving faster convergence and lower computational cost, demonstrating the effectiveness of on-policy distillation for TVG.

Conclusion: On-policy distillation is a viable alternative to conventional reinforcement learning for TVG, offering improved performance and efficiency through dense supervision and optimized training dynamics.

Abstract: Reinforcement learning has emerged as a principled post-training paradigm for Temporal Video Grounding (TVG) due to its on-policy optimization, yet existing GRPO-based methods remain fundamentally constrained by sparse reward signals and substantial computational overhead. We propose Video-OPD, an efficient post-training framework for TVG inspired by recent advances in on-policy distillation. Video-OPD optimizes trajectories sampled directly from the current policy, thereby preserving alignment between training and inference distributions, while a frontier teacher supplies dense, token-level supervision via a reverse KL divergence objective. This formulation preserves the on-policy property critical for mitigating distributional shift, while converting sparse, episode-level feedback into fine-grained, step-wise learning signals. Building on Video-OPD, we introduce Teacher-Validated Disagreement Focusing (TVDF), a lightweight training curriculum that iteratively prioritizes trajectories that are both teacher-reliable and maximally informative for the student, thereby improving training efficiency. Empirical results demonstrate that Video-OPD consistently outperforms GRPO while achieving substantially faster convergence and lower computational cost, establishing on-policy distillation as an effective alternative to conventional reinforcement learning for TVG.

</details>


### [20] [VOILA: Value-of-Information Guided Fidelity Selection for Cost-Aware Multimodal Question Answering](https://arxiv.org/abs/2602.03007)
*Rahul Atul Bhope,K. R. Jayaram,Vinod Muthusamy,Ritesh Kumar,Vatche Isahagian,Nalini Venkatasubramanian*

Main category: cs.CV

TL;DR: VOILA is a framework for adaptive fidelity selection in VQA, reducing costs by 50-60% while maintaining 90-95% accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing vision-language systems operate at fixed fidelity levels, incurring unnecessary costs for retrieval and processing.

Method: VOILA uses a two-stage pipeline: a regressor predicts correctness likelihood from question features, followed by calibration for decision-making.

Result: Achieves 50-60% cost reduction with 90-95% retained accuracy across diverse datasets and models.

Conclusion: Pre-retrieval fidelity selection is crucial for optimizing multimodal inference under resource constraints.

Abstract: Despite significant costs from retrieving and processing high-fidelity visual inputs, most multimodal vision-language systems operate at fixed fidelity levels. We introduce VOILA, a framework for Value-Of-Information-driven adaptive fidelity selection in Visual Question Answering (VQA) that optimizes what information to retrieve before model execution. Given a query, VOILA uses a two-stage pipeline: a gradient-boosted regressor estimates correctness likelihood at each fidelity from question features alone, then an isotonic calibrator refines these probabilities for reliable decision-making. The system selects the minimum-cost fidelity maximizing expected utility given predicted accuracy and retrieval costs. We evaluate VOILA across three deployment scenarios using five datasets (VQA-v2, GQA, TextVQA, LoCoMo, FloodNet) and six Vision-Language Models (VLMs) with 7B-235B parameters. VOILA consistently achieves 50-60% cost reductions while retaining 90-95% of full-resolution accuracy across diverse query types and model architectures, demonstrating that pre-retrieval fidelity selection is vital to optimize multimodal inference under resource constraints.

</details>


### [21] [Thinking inside the Convolution for Image Inpainting: Reconstructing Texture via Structure under Global and Local Side](https://arxiv.org/abs/2602.03013)
*Haipeng Liu,Yang Wang,Biao Qian,Yong Rui,Meng Wang*

Main category: cs.CV

TL;DR: The paper addresses information loss during convolutional downsampling in image inpainting by proposing a method using statistical normalization and denormalization for structure and texture feature maps, improving results.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook information loss in structure and texture feature maps during convolutional downsampling, leading to suboptimal upsampling outputs.

Method: The authors use statistical normalization and denormalization for reconstruction guidance during convolutional downsampling.

Result: Experimental results show improved performance over state-of-the-art methods, especially when replacing all encoders with their approach.

Conclusion: The proposed method effectively mitigates information loss during downsampling, enhancing image inpainting quality.

Abstract: Image inpainting has earned substantial progress, owing to the encoder-and-decoder pipeline, which is benefited from the Convolutional Neural Networks (CNNs) with convolutional downsampling to inpaint the masked regions semantically from the known regions within the encoder, coupled with an upsampling process from the decoder for final inpainting output. Recent studies intuitively identify the high-frequency structure and low-frequency texture to be extracted by CNNs from the encoder, and subsequently for a desirable upsampling recovery. However, the existing arts inevitably overlook the information loss for both structure and texture feature maps during the convolutional downsampling process, hence suffer from a non-ideal upsampling output. In this paper, we systematically answer whether and how the structure and texture feature map can mutually help to alleviate the information loss during the convolutional downsampling. Given the structure and texture feature maps, we adopt the statistical normalization and denormalization strategy for the reconstruction guidance during the convolutional downsampling process. The extensive experimental results validate its advantages to the state-of-the-arts over the images from low-to-high resolutions including 256*256 and 512*512, especially holds by substituting all the encoders by ours. Our code is available at https://github.com/htyjers/ConvInpaint-TSGL

</details>


### [22] [A Vision-Based Analysis of Congestion Pricing in New York City](https://arxiv.org/abs/2602.03015)
*Mehmet Kerem Turkcan,Jhonatan Tavori,Javad Ghaderi,Gil Zussman,Zoran Kostic,Andrew Smyth*

Main category: cs.CV

TL;DR: Study analyzes NYC's congestion pricing via traffic camera data, tracking changes from Nov 2024 to Jan 2026.


<details>
  <summary>Details</summary>
Motivation: Understand the impact of NYC's congestion pricing program on traffic patterns.

Method: Automated computer vision analysis of 900+ traffic cameras in Manhattan, comparing pre- and post-implementation data.

Result: Identified systematic changes in vehicle density post-implementation.

Conclusion: Congestion pricing led to measurable shifts in NYC traffic patterns.

Abstract: We examine the impact of New York City's congestion pricing program through automated analysis of traffic camera data. Our computer vision pipeline processes footage from over 900 cameras distributed throughout Manhattan and New York, comparing traffic patterns from November 2024 through the program's implementation in January 2025 until January 2026. We establish baseline traffic patterns and identify systematic changes in vehicle density across the monitored region.

</details>


### [23] [MUSE: A Multi-agent Framework for Unconstrained Story Envisioning via Closed-Loop Cognitive Orchestration](https://arxiv.org/abs/2602.03028)
*Wenzhang Sun,Zhenyu Wang,Zhangchi Hu,Chunfeng Wang,Hao Li,Wei Chen*

Main category: cs.CV

TL;DR: MUSE is a multi-agent framework addressing the challenge of generating coherent long-form audio-visual stories by using an iterative plan-execute-verify-revise loop, improving narrative coherence and identity consistency.


<details>
  <summary>Details</summary>
Motivation: The intent-execution gap in generating long-form audio-visual stories from short prompts leads to semantic drift and identity inconsistency, which existing methods fail to address effectively.

Method: MUSE formulates storytelling as a closed-loop constraint enforcement problem, using a multi-agent framework with iterative loops to enforce narrative intent through explicit controls and multimodal feedback.

Result: MUSE significantly improves long-horizon narrative coherence, cross-modal identity consistency, and cinematic quality compared to existing baselines.

Conclusion: The proposed MUSE framework successfully addresses the challenges of long-form story generation by leveraging iterative verification and targeted feedback, validated by MUSEBench, a reference-free evaluation protocol.

Abstract: Generating long-form audio-visual stories from a short user prompt remains challenging due to an intent-execution gap, where high-level narrative intent must be preserved across coherent, shot-level multimodal generation over long horizons. Existing approaches typically rely on feed-forward pipelines or prompt-only refinement, which often leads to semantic drift and identity inconsistency as sequences grow longer. We address this challenge by formulating storytelling as a closed-loop constraint enforcement problem and propose MUSE, a multi-agent framework that coordinates generation through an iterative plan-execute-verify-revise loop. MUSE translates narrative intent into explicit, machine-executable controls over identity, spatial composition, and temporal continuity, and applies targeted multimodal feedback to correct violations during generation. To evaluate open-ended storytelling without ground-truth references, we introduce MUSEBench, a reference-free evaluation protocol validated by human judgments. Experiments demonstrate that MUSE substantially improves long-horizon narrative coherence, cross-modal identity consistency, and cinematic quality compared with representative baselines.

</details>


### [24] [Bongards at the Boundary of Perception and Reasoning: Programs or Language?](https://arxiv.org/abs/2602.03038)
*Cassidy Langenfeld,Claas Beger,Gloria Geng,Wasu Top Piriyakulkij,Keya Hu,Yewen Pu,Kevin Ellis*

Main category: cs.CV

TL;DR: A neurosymbolic approach combines LLMs and Bayesian optimization to solve Bongard problems, achieving success in classification and rule-solving tasks.


<details>
  <summary>Details</summary>
Motivation: Humans excel in visual reasoning for novel situations, such as Bongard problems, which VLMs struggle with despite progress in common visual tasks.

Method: Proposed neurosymbolic method uses LLMs to generate programmatic rules and fits parameters via Bayesian optimization for solving Bongard problems.

Result: The approach effectively classifies Bongard problem images using ground truth rules and solves problems from scratch.

Conclusion: The neurosymbolic method demonstrates potential for advancing VLMs' visual reasoning capabilities in novel scenarios.

Abstract: Vision-Language Models (VLMs) have made great strides in everyday visual tasks, such as captioning a natural image, or answering commonsense questions about such images. But humans possess the puzzling ability to deploy their visual reasoning abilities in radically new situations, a skill rigorously tested by the classic set of visual reasoning challenges known as the Bongard problems. We present a neurosymbolic approach to solving these problems: given a hypothesized solution rule for a Bongard problem, we leverage LLMs to generate parameterized programmatic representations for the rule and perform parameter fitting using Bayesian optimization. We evaluate our method on classifying Bongard problem images given the ground truth rule, as well as on solving the problems from scratch.

</details>


### [25] [HP-GAN: Harnessing pretrained networks for GAN improvement with FakeTwins and discriminator consistency](https://arxiv.org/abs/2602.03039)
*Geonhui Son,Jeong Ryong Lee,Dosik Hwang*

Main category: cs.CV

TL;DR: HP-GAN enhances GAN performance by integrating self-supervised learning (FakeTwins) and discriminator consistency, improving image diversity and quality across multiple datasets.


<details>
  <summary>Details</summary>
Motivation: To leverage pretrained networks more effectively for GAN training and improve image synthesis quality and diversity.

Method: Introduces FakeTwins for self-supervised learning and a discriminator consistency mechanism between CNN and ViT feature networks.

Result: HP-GAN outperforms state-of-the-art methods in FID scores across seventeen datasets, showing improved image diversity and quality.

Conclusion: HP-GAN successfully exploits neural network priors and discriminator consistency to achieve superior GAN performance.

Abstract: Generative Adversarial Networks (GANs) have made significant progress in enhancing the quality of image synthesis. Recent methods frequently leverage pretrained networks to calculate perceptual losses or utilize pretrained feature spaces. In this paper, we extend the capabilities of pretrained networks by incorporating innovative self-supervised learning techniques and enforcing consistency between discriminators during GAN training. Our proposed method, named HP-GAN, effectively exploits neural network priors through two primary strategies: FakeTwins and discriminator consistency. FakeTwins leverages pretrained networks as encoders to compute a self-supervised loss and applies this through the generated images to train the generator, thereby enabling the generation of more diverse and high quality images. Additionally, we introduce a consistency mechanism between discriminators that evaluate feature maps extracted from Convolutional Neural Network (CNN) and Vision Transformer (ViT) feature networks. Discriminator consistency promotes coherent learning among discriminators and enhances training robustness by aligning their assessments of image quality. Our extensive evaluation across seventeen datasets-including scenarios with large, small, and limited data, and covering a variety of image domains-demonstrates that HP-GAN consistently outperforms current state-of-the-art methods in terms of Fréchet Inception Distance (FID), achieving significant improvements in image diversity and quality. Code is available at: https://github.com/higun2/HP-GAN.

</details>


### [26] [IVC-Prune: Revealing the Implicit Visual Coordinates in LVLMs for Vision Token Pruning](https://arxiv.org/abs/2602.03060)
*Zhichao Sun,Yidong Ma,Gang Liu,Yibo Chen,Xu Tang,Yao Hu,Yongchao Xu*

Main category: cs.CV

TL;DR: IVC-Prune is a training-free pruning strategy for Large Vision-Language Models (LVLMs) that retains tokens crucial for spatial reasoning and semantic relevance, reducing tokens by ~50% while preserving ≥99% performance.


<details>
  <summary>Details</summary>
Motivation: Existing token pruning methods for LVLMs often discard tokens essential for spatial reasoning due to focusing only on semantic relevance.

Method: IVC-Prune identifies implicit visual coordinates (IVC tokens) via RoPE analysis and foreground tokens via semantic seed discovery and contextual refinement.

Result: IVC-Prune reduces visual tokens by ~50% while maintaining ≥99% of original performance, sometimes even improving it.

Conclusion: The proposed IVC-Prune effectively balances token reduction and performance retention by leveraging spatial reasoning insights.

Abstract: Large Vision-Language Models (LVLMs) achieve impressive performance across multiple tasks. A significant challenge, however, is their prohibitive inference cost when processing high-resolution visual inputs. While visual token pruning has emerged as a promising solution, existing methods that primarily focus on semantic relevance often discard tokens that are crucial for spatial reasoning. We address this gap through a novel insight into \emph{how LVLMs process spatial reasoning}. Specifically, we reveal that LVLMs implicitly establish visual coordinate systems through Rotary Position Embeddings (RoPE), where specific token positions serve as \textbf{implicit visual coordinates} (IVC tokens) that are essential for spatial reasoning. Based on this insight, we propose \textbf{IVC-Prune}, a training-free, prompt-aware pruning strategy that retains both IVC tokens and semantically relevant foreground tokens. IVC tokens are identified by theoretically analyzing the mathematical properties of RoPE, targeting positions at which its rotation matrices approximate identity matrix or the $90^\circ$ rotation matrix. Foreground tokens are identified through a robust two-stage process: semantic seed discovery followed by contextual refinement via value-vector similarity. Extensive evaluations across four representative LVLMs and twenty diverse benchmarks show that IVC-Prune reduces visual tokens by approximately 50\% while maintaining $\geq$ 99\% of the original performance and even achieving improvements on several benchmarks. Source codes are available at https://github.com/FireRedTeam/IVC-Prune.

</details>


### [27] [Finding Optimal Video Moment without Training: Gaussian Boundary Optimization for Weakly Supervised Video Grounding](https://arxiv.org/abs/2602.03071)
*Sunoh Kim,Kimin Yun,Daeho Um*

Main category: cs.CV

TL;DR: GBO is a novel inference framework for weakly supervised temporal video grounding, improving localization by optimizing Gaussian-based proposals without training.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on heuristic mappings for segment boundaries, leading to suboptimal performance. GBO addresses this by optimizing coverage and compactness.

Method: GBO solves a principled optimization problem to predict boundaries, offering a closed-form solution and compatibility with various proposal architectures.

Result: GBO achieves state-of-the-art performance across benchmarks, demonstrating efficiency and generalizability.

Conclusion: GBO provides a theoretically grounded and practical solution for temporal video grounding, improving localization without additional training.

Abstract: Weakly supervised temporal video grounding aims to localize query-relevant segments in untrimmed videos using only video-sentence pairs, without requiring ground-truth segment annotations that specify exact temporal boundaries. Recent approaches tackle this task by utilizing Gaussian-based temporal proposals to represent query-relevant segments. However, their inference strategies rely on heuristic mappings from Gaussian parameters to segment boundaries, resulting in suboptimal localization performance. To address this issue, we propose Gaussian Boundary Optimization (GBO), a novel inference framework that predicts segment boundaries by solving a principled optimization problem that balances proposal coverage and segment compactness. We derive a closed-form solution for this problem and rigorously analyze the optimality conditions under varying penalty regimes. Beyond its theoretical foundations, GBO offers several practical advantages: it is training-free and compatible with both single-Gaussian and mixture-based proposal architectures. Our experiments show that GBO significantly improves localization, achieving state-of-the-art results across standard benchmarks. Extensive experiments demonstrate the efficiency and generalizability of GBO across various proposal schemes. The code is available at \href{https://github.com/sunoh-kim/gbo}{https://github.com/sunoh-kim/gbo}.

</details>


### [28] [A generalizable large-scale foundation model for musculoskeletal radiographs](https://arxiv.org/abs/2602.03076)
*Shinn Kim,Soobin Lee,Kyoungseob Shin,Han-Soo Kim,Yongsung Kim,Minsu Kim,Juhong Nam,Somang Ko,Daeheon Kwon,Wook Huh,Ilkyu Han,Sunghoon Kwon*

Main category: cs.CV

TL;DR: SKELEX is a scalable, label-efficient foundation model for musculoskeletal radiographs, trained on 1.2M images, outperforming baselines in multiple tasks and enabling zero-shot abnormality localization.


<details>
  <summary>Details</summary>
Motivation: Existing AI models for musculoskeletal diseases are task-specific, annotation-dependent, and lack generalizability. A large-scale foundation model is needed to address these limitations.

Method: SKELEX was trained using self-supervised learning on 1.2M diverse musculoskeletal radiographs and evaluated on 12 downstream diagnostic tasks, including fracture detection and tumor classification.

Result: SKELEX outperformed baselines in multiple tasks and demonstrated zero-shot abnormality localization. It also enabled an interpretable, region-guided model for bone tumor prediction, robust across external datasets.

Conclusion: SKELEX provides a generalizable, scalable AI framework for musculoskeletal imaging, supporting clinical translation and data-efficient research.

Abstract: Artificial intelligence (AI) has shown promise in detecting and characterizing musculoskeletal diseases from radiographs. However, most existing models remain task-specific, annotation-dependent, and limited in generalizability across diseases and anatomical regions. Although a generalizable foundation model trained on large-scale musculoskeletal radiographs is clinically needed, publicly available datasets remain limited in size and lack sufficient diversity to enable training across a wide range of musculoskeletal conditions and anatomical sites. Here, we present SKELEX, a large-scale foundation model for musculoskeletal radiographs, trained using self-supervised learning on 1.2 million diverse, condition-rich images. The model was evaluated on 12 downstream diagnostic tasks and generally outperformed baselines in fracture detection, osteoarthritis grading, and bone tumor classification. Furthermore, SKELEX demonstrated zero-shot abnormality localization, producing error maps that identified pathologic regions without task-specific training. Building on this capability, we developed an interpretable, region-guided model for predicting bone tumors, which maintained robust performance on independent external datasets and was deployed as a publicly accessible web application. Overall, SKELEX provides a scalable, label-efficient, and generalizable AI framework for musculoskeletal imaging, establishing a foundation for both clinical translation and data-efficient research in musculoskeletal radiology.

</details>


### [29] [Gromov Wasserstein Optimal Transport for Semantic Correspondences](https://arxiv.org/abs/2602.03105)
*Francis Snelgar,Stephen Gould,Ming Xu,Liang Zheng,Akshay Asthana*

Main category: cs.CV

TL;DR: The paper proposes replacing Stable Diffusion features with an optimal transport algorithm (Gromov Wasserstein) for semantic correspondence tasks, improving efficiency and performance compared to ensemble methods using DINOv2 and Stable Diffusion.


<details>
  <summary>Details</summary>
Motivation: Current state-of-the-art methods combining DINOv2 and Stable Diffusion features are computationally expensive. The paper aims to enhance efficiency and performance by using a superior matching algorithm instead.

Method: Replaces Stable Diffusion features with an optimal transport algorithm incorporating a Gromov Wasserstein spatial smoothness prior, improving spatial consistency in semantic correspondence.

Result: Significantly boosts DINOv2 baseline performance, matches or surpasses ensemble methods, and achieves 5--10x greater efficiency.

Conclusion: The proposed method offers a computationally efficient and high-performing alternative to ensemble-based semantic correspondence techniques.

Abstract: Establishing correspondences between image pairs is a long studied problem in computer vision. With recent large-scale foundation models showing strong zero-shot performance on downstream tasks including classification and segmentation, there has been interest in using the internal feature maps of these models for the semantic correspondence task. Recent works observe that features from DINOv2 and Stable Diffusion (SD) are complementary, the former producing accurate but sparse correspondences, while the latter produces spatially consistent correspondences. As a result, current state-of-the-art methods for semantic correspondence involve combining features from both models in an ensemble. While the performance of these methods is impressive, they are computationally expensive, requiring evaluating feature maps from large-scale foundation models. In this work we take a different approach, instead replacing SD features with a superior matching algorithm which is imbued with the desirable spatial consistency property. Specifically, we replace the standard nearest neighbours matching with an optimal transport algorithm that includes a Gromov Wasserstein spatial smoothness prior. We show that we can significantly boost the performance of the DINOv2 baseline, and be competitive and sometimes surpassing state-of-the-art methods using Stable Diffusion features, while being 5--10x more efficient. We make code available at https://github.com/fsnelgar/semantic_matching_gwot .

</details>


### [30] [Beyond Cropping and Rotation: Automated Evolution of Powerful Task-Specific Augmentations with Generative Models](https://arxiv.org/abs/2602.03123)
*Judah Goldfeder,Shreyes Kaliyur,Vaibhav Sourirajan,Patrick Minwan Puma,Philippe Martin Wyder,Yuhang Hu,Jiong Lin,Hod Lipson*

Main category: cs.CV

TL;DR: EvoAug is an automated augmentation learning pipeline that uses generative models and an evolutionary algorithm to create task-specific, structured augmentations, improving robustness in fine-grained classification and few-shot learning tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional augmentation methods like cropping or rotation have limitations, while generative models offer greater diversity and realism. However, poorly matched augmentations can degrade performance. EvoAug addresses this by learning optimal augmentations.

Method: EvoAug leverages generative models (e.g., conditional diffusion, NeRFs) and an evolutionary algorithm to learn stochastic augmentation trees. These trees hierarchically compose augmentations for structured and adaptive transformations.

Result: The pipeline achieves strong performance in fine-grained classification and few-shot learning. It also discovers augmentations that align with domain knowledge, even in low-data settings.

Conclusion: EvoAug demonstrates the potential of learned generative augmentations, offering a new approach to robust model training by automating the design of task-specific augmentations.

Abstract: Data augmentation has long been a cornerstone for reducing overfitting in vision models, with methods like AutoAugment automating the design of task-specific augmentations. Recent advances in generative models, such as conditional diffusion and few-shot NeRFs, offer a new paradigm for data augmentation by synthesizing data with significantly greater diversity and realism. However, unlike traditional augmentations like cropping or rotation, these methods introduce substantial changes that enhance robustness but also risk degrading performance if the augmentations are poorly matched to the task. In this work, we present EvoAug, an automated augmentation learning pipeline, which leverages these generative models alongside an efficient evolutionary algorithm to learn optimal task-specific augmentations. Our pipeline introduces a novel approach to image augmentation that learns stochastic augmentation trees that hierarchically compose augmentations, enabling more structured and adaptive transformations. We demonstrate strong performance across fine-grained classification and few-shot learning tasks. Notably, our pipeline discovers augmentations that align with domain knowledge, even in low-data settings. These results highlight the potential of learned generative augmentations, unlocking new possibilities for robust model training.

</details>


### [31] [Feature, Alignment, and Supervision in Category Learning: A Comparative Approach with Children and Neural Networks](https://arxiv.org/abs/2602.03124)
*Fanxiao Wani Qiu,Oscar Leong*

Main category: cs.CV

TL;DR: Children and CNNs are compared in few-shot semi-supervised category learning. Children generalize quickly with biases, while CNNs improve with more supervision.


<details>
  <summary>Details</summary>
Motivation: To understand differences in learning from sparse data between humans (children) and machines (CNNs).

Method: Exposed children and CNNs to novel object categories with varied supervision, target features, and perceptual alignment.

Result: Children generalize rapidly with biases; CNNs improve with supervision, moderated by alignment and feature structure.

Conclusion: Human-model comparisons require considering supervision, feature structure, and alignment, not just accuracy.

Abstract: Understanding how humans and machines learn from sparse data is central to cognitive science and machine learning. Using a species-fair design, we compare children and convolutional neural networks (CNNs) in a few-shot semi-supervised category learning task. Both learners are exposed to novel object categories under identical conditions. Learners receive mixtures of labeled and unlabeled exemplars while we vary supervision (1/3/6 labels), target feature (size, shape, pattern), and perceptual alignment (high/low). We find that children generalize rapidly from minimal labels but show strong feature-specific biases and sensitivity to alignment. CNNs show a different interaction profile: added supervision improves performance, but both alignment and feature structure moderate the impact additional supervision has on learning. These results show that human-model comparisons must be drawn under the right conditions, emphasizing interactions among supervision, feature structure, and alignment rather than overall accuracy.

</details>


### [32] [Flexible Geometric Guidance for Probabilistic Human Pose Estimation with Diffusion Models](https://arxiv.org/abs/2602.03126)
*Francis Snelgar,Ming Xu,Stephen Gould,Liang Zheng,Akshay Asthana*

Main category: cs.CV

TL;DR: The paper proposes a diffusion model framework for 3D human pose estimation from 2D images, addressing ambiguity and generalization issues without requiring paired 2D-3D training data.


<details>
  <summary>Details</summary>
Motivation: Existing methods assume deterministic mappings or require large paired datasets, limiting robustness and generalization.

Method: A diffusion model framework guides unconditional 3D pose samples using gradients from 2D keypoint heatmaps.

Result: State-of-the-art performance on Human 3.6M without paired data, competitive results on MPI-INF-3DHP and 3DPW, and flexibility for tasks like pose generation/completion.

Conclusion: The framework effectively handles ambiguity, generalization, and novel tasks without conditional training, offering a versatile solution.

Abstract: 3D human pose estimation from 2D images is a challenging problem due to depth ambiguity and occlusion. Because of these challenges the task is underdetermined, where there exists multiple -- possibly infinite -- poses that are plausible given the image. Despite this, many prior works assume the existence of a deterministic mapping and estimate a single pose given an image. Furthermore, methods based on machine learning require a large amount of paired 2D-3D data to train and suffer from generalization issues to unseen scenarios. To address both of these issues, we propose a framework for pose estimation using diffusion models, which enables sampling from a probability distribution over plausible poses which are consistent with a 2D image. Our approach falls under the guidance framework for conditional generation, and guides samples from an unconditional diffusion model, trained only on 3D data, using the gradients of the heatmaps from a 2D keypoint detector. We evaluate our method on the Human 3.6M dataset under best-of-$m$ multiple hypothesis evaluation, showing state-of-the-art performance among methods which do not require paired 2D-3D data for training. We additionally evaluate the generalization ability using the MPI-INF-3DHP and 3DPW datasets and demonstrate competitive performance. Finally, we demonstrate the flexibility of our framework by using it for novel tasks including pose generation and pose completion, without the need to train bespoke conditional models. We make code available at https://github.com/fsnelgar/diffusion_pose .

</details>


### [33] [FinMTM: A Multi-Turn Multimodal Benchmark for Financial Reasoning and Agent Evaluation](https://arxiv.org/abs/2602.03130)
*Chenxi Zhang,Ziliang Gan,Liyun Zhu,Youwei Pang,Qing Zhang,Rongjunchen Zhang*

Main category: cs.CV

TL;DR: A new benchmark, FinMTM, addresses gaps in evaluating vision-language models (VLMs) for financial tasks by introducing diverse data and tasks.


<details>
  <summary>Details</summary>
Motivation: Existing financial benchmarks are limited in diversity and scope, failing to evaluate VLMs comprehensively in realistic scenarios.

Method: FinMTM includes 11,133 bilingual QA pairs grounded in financial visuals and covers various question formats, dialogues, and agent tasks, with tailored evaluation protocols.

Result: Evaluation of 22 VLMs highlights their shortcomings in fine-grained perception, long-context reasoning, and complex workflows.

Conclusion: FinMTM provides a robust benchmark for assessing VLMs in financial applications, revealing areas needing improvement.

Abstract: The financial domain poses substantial challenges for vision-language models (VLMs) due to specialized chart formats and knowledge-intensive reasoning requirements. However, existing financial benchmarks are largely single-turn and rely on a narrow set of question formats, limiting comprehensive evaluation in realistic application scenarios. To address this gap, we propose FinMTM, a multi-turn multimodal benchmark that expands diversity along both data and task dimensions. On the data side, we curate and annotate 11{,}133 bilingual (Chinese and English) financial QA pairs grounded in financial visuals, including candlestick charts, statistical plots, and report figures. On the task side, FinMTM covers single- and multiple-choice questions, multi-turn open-ended dialogues, and agent-based tasks. We further design task-specific evaluation protocols, including a set-overlap scoring rule for multiple-choice questions, a weighted combination of turn-level and session-level scores for multi-turn dialogues, and a composite metric that integrates planning quality with final outcomes for agent tasks. Extensive experimental evaluation of 22 VLMs reveal their limitations in fine-grained visual perception, long-context reasoning, and complex agent workflows.

</details>


### [34] [SwiftVLM: Efficient Vision-Language Model Inference via Cross-Layer Token Bypass](https://arxiv.org/abs/2602.03134)
*Chen Qian,Xinran Yu,Danyang Li,Guoxuan Chi,Zheng Yang,Qiang Ma,Xin Miao*

Main category: cs.CV

TL;DR: Visual token pruning reduces computational costs in vision-language models (VLMs), but existing methods lose fine-grained details. SwiftVLM introduces bypass pruning to preserve tokens for re-evaluation, improving accuracy-efficiency trade-offs.


<details>
  <summary>Details</summary>
Motivation: Existing pruning methods degrade performance on tasks needing fine-grained visual details due to irreversible early pruning decisions. The study aims to mitigate this by preserving tokens for later re-evaluation.

Method: SwiftVLM uses a bypass pruning paradigm, forwarding unselected tokens to later stages for re-evaluation. It prunes at model-specific layers without training, allowing independent decisions across layers.

Result: SwiftVLM outperforms existing pruning strategies across benchmarks, achieving better accuracy-efficiency trade-offs and more reliable visual token selection.

Conclusion: SwiftVLM's bypass paradigm addresses critical information loss in early pruning, enabling superior performance and efficiency in VLMs.

Abstract: Visual token pruning is a promising approach for reducing the computational cost of vision-language models (VLMs), and existing methods often rely on early pruning decisions to improve efficiency. While effective on coarse-grained reasoning tasks, they suffer from significant performance degradation on tasks requiring fine-grained visual details. Through layer-wise analysis, we reveal substantial discrepancies in visual token importance across layers, showing that tokens deemed unimportant at shallow layers can later become highly relevant for text-conditioned reasoning. To avoid irreversible critical information loss caused by premature pruning, we introduce a new pruning paradigm, termed bypass, which preserves unselected visual tokens and forwards them to subsequent pruning stages for re-evaluation. Building on this paradigm, we propose SwiftVLM, a simple and training-free method that performs pruning at model-specific layers with strong visual token selection capability, while enabling independent pruning decisions across layers. Experiments across multiple VLMs and benchmarks demonstrate that SwiftVLM consistently outperforms existing pruning strategies, achieving superior accuracy-efficiency trade-offs and more faithful visual token selection behavior.

</details>


### [35] [FSOD-VFM: Few-Shot Object Detection with Vision Foundation Models and Graph Diffusion](https://arxiv.org/abs/2602.03137)
*Chen-Bin Feng,Youyang Sha,Longfei Liu,Yongjun Yu,Chi Man Vong,Xuanlong Yu,Xi Shen*

Main category: cs.CV

TL;DR: FSOD-VFM leverages vision foundation models for few-shot object detection, using a universal proposal network, SAM2, and DINOv2 features, enhanced by graph-based confidence reweighting to improve detection accuracy.


<details>
  <summary>Details</summary>
Motivation: The challenge of few-shot object detection is addressed by integrating vision foundation models, aiming to improve detection accuracy and reduce false positives.

Method: The framework combines a universal proposal network, SAM2 for mask extraction, and DINOv2 features. A graph-based confidence reweighting method refines proposal scores.

Result: FSOD-VFM outperforms existing methods on Pascal-5$^i$, COCO-20$^i$, and CD-FSOD datasets, achieving 31.6 AP on CD-FSOD without additional training.

Conclusion: The proposed method effectively enhances few-shot object detection by refining proposal confidence scores, demonstrating superior performance across diverse datasets.

Abstract: In this paper, we present FSOD-VFM: Few-Shot Object Detectors with Vision Foundation Models, a framework that leverages vision foundation models to tackle the challenge of few-shot object detection. FSOD-VFM integrates three key components: a universal proposal network (UPN) for category-agnostic bounding box generation, SAM2 for accurate mask extraction, and DINOv2 features for efficient adaptation to new object categories. Despite the strong generalization capabilities of foundation models, the bounding boxes generated by UPN often suffer from overfragmentation, covering only partial object regions and leading to numerous small, false-positive proposals rather than accurate, complete object detections. To address this issue, we introduce a novel graph-based confidence reweighting method. In our approach, predicted bounding boxes are modeled as nodes in a directed graph, with graph diffusion operations applied to propagate confidence scores across the network. This reweighting process refines the scores of proposals, assigning higher confidence to whole objects and lower confidence to local, fragmented parts. This strategy improves detection granularity and effectively reduces the occurrence of false-positive bounding box proposals. Through extensive experiments on Pascal-5$^i$, COCO-20$^i$, and CD-FSOD datasets, we demonstrate that our method substantially outperforms existing approaches, achieving superior performance without requiring additional training. Notably, on the challenging CD-FSOD dataset, which spans multiple datasets and domains, our FSOD-VFM achieves 31.6 AP in the 10-shot setting, substantially outperforming previous training-free methods that reach only 21.4 AP. Code is available at: https://intellindust-ai-lab.github.io/projects/FSOD-VFM.

</details>


### [36] [Diversity-Preserved Distribution Matching Distillation for Fast Visual Synthesis](https://arxiv.org/abs/2602.03139)
*Tianhe Wu,Ruibin Li,Lei Zhang,Kede Ma*

Main category: cs.CV

TL;DR: DP-DMD proposes a role-separated distillation framework to prevent mode collapse in DMD by disentangling steps: diversity preservation in the first step and quality refinement in subsequent steps, achieving competitive results without additional computational costs.


<details>
  <summary>Details</summary>
Motivation: Address mode collapse in Distribution Matching Distillation (DMD) caused by its reverse-KL formulation, which inherently encourages mode-seeking behavior. Existing solutions rely on costly and unstable perceptual or adversarial regularization.

Method: Introduce Diversity-Preserved DMD (DP-DMD), a role-separated distillation framework where the first step preserves diversity via target-prediction (e.g., v-prediction), and subsequent steps refine quality under standard DMD loss while blocking DMD gradients at the first step.

Result: DP-DMD maintains sample diversity and visual quality comparable to state-of-the-art methods in text-to-image experiments, without requiring perceptual backbones, discriminators, auxiliary networks, or extra ground-truth images.

Conclusion: DP-DMD offers a simple yet effective solution to mitigate mode collapse in DMD, balancing diversity and quality without additional computational overhead.

Abstract: Distribution matching distillation (DMD) aligns a multi-step generator with its few-step counterpart to enable high-quality generation under low inference cost. However, DMD tends to suffer from mode collapse, as its reverse-KL formulation inherently encourages mode-seeking behavior, for which existing remedies typically rely on perceptual or adversarial regularization, thereby incurring substantial computational overhead and training instability. In this work, we propose a role-separated distillation framework that explicitly disentangles the roles of distilled steps: the first step is dedicated to preserving sample diversity via a target-prediction (e.g., v-prediction) objective, while subsequent steps focus on quality refinement under the standard DMD loss, with gradients from the DMD objective blocked at the first step. We term this approach Diversity-Preserved DMD (DP-DMD), which, despite its simplicity -- no perceptual backbone, no discriminator, no auxiliary networks, and no additional ground-truth images -- preserves sample diversity while maintaining visual quality on par with state-of-the-art methods in extensive text-to-image experiments.

</details>


### [37] [Fully Kolmogorov-Arnold Deep Model in Medical Image Segmentation](https://arxiv.org/abs/2602.03156)
*Xingyu Qiu,Xinghua Ma,Dong Liang,Gongning Luo,Wei Wang,Kuanquan Wang,Shuo Li*

Main category: cs.CV

TL;DR: This paper introduces Share-activation KAN (SaKAN) and Grad-Free Spline to overcome training and memory challenges of deep KANs, proposing the first fully KA-based deep model (ALL U-KAN) with superior performance in medical image segmentation.


<details>
  <summary>Details</summary>
Motivation: Existing deep KAN models face training difficulties and high memory usage, limiting their exploration. This study aims to address these issues and fully utilize KAN layers in deep learning.

Method: (1) SaKAN reformulates Sprecher's variant for easier training. (2) Grad-Free Spline reduces memory and computational overhead. (3) ALL U-KAN replaces traditional layers with KA and KAonv layers.

Result: ALL U-KAN achieves higher segmentation accuracy, reduces parameters by 10x, and cuts memory usage by over 20x compared to deep KAN stacks.

Conclusion: The proposed innovations enable practical deep KAN architectures, outperforming traditional methods in medical image segmentation tasks.

Abstract: Deeply stacked KANs are practically impossible due to high training difficulties and substantial memory requirements. Consequently, existing studies can only incorporate few KAN layers, hindering the comprehensive exploration of KANs. This study overcomes these limitations and introduces the first fully KA-based deep model, demonstrating that KA-based layers can entirely replace traditional architectures in deep learning and achieve superior learning capacity. Specifically, (1) the proposed Share-activation KAN (SaKAN) reformulates Sprecher's variant of Kolmogorov-Arnold representation theorem, which achieves better optimization due to its simplified parameterization and denser training samples, to ease training difficulty, (2) this paper indicates that spline gradients contribute negligibly to training while consuming huge GPU memory, thus proposes the Grad-Free Spline to significantly reduce memory usage and computational overhead. (3) Building on these two innovations, our ALL U-KAN is the first representative implementation of fully KA-based deep model, where the proposed KA and KAonv layers completely replace FC and Conv layers. Extensive evaluations on three medical image segmentation tasks confirm the superiority of the full KA-based architecture compared to partial KA-based and traditional architectures, achieving all higher segmentation accuracy. Compared to directly deeply stacked KAN, ALL U-KAN achieves 10 times reduction in parameter count and reduces memory consumption by more than 20 times, unlocking the new explorations into deep KAN architectures.

</details>


### [38] [Human-in-the-loop Adaptation in Group Activity Feature Learning for Team Sports Video Retrieval](https://arxiv.org/abs/2602.03157)
*Chihiro Nakatani,Hiroaki Kawashima,Norimichi Ukita*

Main category: cs.CV

TL;DR: The paper introduces human-in-the-loop adaptation for GAFL without group activity annotations, improving video retrieval performance via self-supervised pre-training and interactive fine-tuning with contrastive learning.


<details>
  <summary>Details</summary>
Motivation: To enhance group-activity video retrieval performance without relying on pre-defined annotations, leveraging human feedback for better adaptability.

Method: Pre-trains GAF space self-supervised, then fine-tunes interactively using user-labeled videos via contrastive learning. Includes data-efficient video selection for manual labeling.

Result: Significant improvement in retrieval performance on team sports datasets, validated through comprehensive experiments and ablation studies.

Conclusion: Human-in-the-loop adaptation effectively boosts GAFL retrieval performance, with key components contributing to success.

Abstract: This paper proposes human-in-the-loop adaptation for Group Activity Feature Learning (GAFL) without group activity annotations. This human-in-the-loop adaptation is employed in a group-activity video retrieval framework to improve its retrieval performance. Our method initially pre-trains the GAF space based on the similarity of group activities in a self-supervised manner, unlike prior work that classifies videos into pre-defined group activity classes in a supervised learning manner. Our interactive fine-tuning process updates the GAF space to allow a user to better retrieve videos similar to query videos given by the user. In this fine-tuning, our proposed data-efficient video selection process provides several videos, which are selected from a video database, to the user in order to manually label these videos as positive or negative. These labeled videos are used to update (i.e., fine-tune) the GAF space, so that the positive and negative videos move closer to and farther away from the query videos through contrastive learning. Our comprehensive experimental results on two team sports datasets validate that our method significantly improves the retrieval performance. Ablation studies also demonstrate that several components in our human-in-the-loop adaptation contribute to the improvement of the retrieval performance. Code: https://github.com/chihina/GAFL-FINE-CVIU.

</details>


### [39] [BinaryDemoire: Moiré-Aware Binarization for Image Demoiréing](https://arxiv.org/abs/2602.03176)
*Zheng Chen,Zhi Yang,Xiaoyang Liu,Weihang Zhang,Mengfan Wang,Yifan Fu,Linghe Kong,Yulun Zhang*

Main category: cs.CV

TL;DR: BinaryDemoire is a binarized demoiréing framework that leverages frequency-aware techniques and structured shortcuts to remove moiré artifacts efficiently.


<details>
  <summary>Details</summary>
Motivation: Existing deep networks for image demoiréing are computationally expensive. Binarization, while offering extreme compression, performs poorly for this task when naively applied.

Method: Proposes BinaryDemoire with a moiré-aware binary gate (MABG) for frequency-aware processing and a shuffle-grouped residual adapter (SGRA) for structured sparse shortcut alignment.

Result: Extensive experiments show BinaryDemoire outperforms current binarization methods on four benchmarks.

Conclusion: BinaryDemoire effectively balances efficiency and performance in demoiréing tasks, offering a promising solution for deployment.

Abstract: Image demoiréing aims to remove structured moiré artifacts in recaptured imagery, where degradations are highly frequency-dependent and vary across scales and directions. While recent deep networks achieve high-quality restoration, their full-precision designs remain costly for deployment. Binarization offers an extreme compression regime by quantizing both activations and weights to 1-bit. Yet, it has been rarely studied for demoiréing and performs poorly when naively applied. In this work, we propose BinaryDemoire, a binarized demoiréing framework that explicitly accommodates the frequency structure of moiré degradations. First, we introduce a moiré-aware binary gate (MABG) that extracts lightweight frequency descriptors together with activation statistics. It predicts channel-wise gating coefficients to condition the aggregation of binary convolution responses. Second, we design a shuffle-grouped residual adapter (SGRA) that performs structured sparse shortcut alignment. It further integrates interleaved mixing to promote information exchange across different channel partitions. Extensive experiments on four benchmarks demonstrate that the proposed BinaryDemoire surpasses current binarization methods. Code: https://github.com/zhengchen1999/BinaryDemoire.

</details>


### [40] [LSGQuant: Layer-Sensitivity Guided Quantization for One-Step Diffusion Real-World Video Super-Resolution](https://arxiv.org/abs/2602.03182)
*Tianxing Wu,Zheng Chen,Cirou Xu,Bowen Chai,Yong Guo,Yutong Liu,Linghe Kong,Yulun Zhang*

Main category: cs.CV

TL;DR: LSGQuant introduces a layer-sensitivity guided quantization approach for one-step diffusion-based video super-resolution, addressing challenges like high dynamic range and diverse layer behaviors, achieving near-original performance with significantly lower computational cost.


<details>
  <summary>Details</summary>
Motivation: The motivation is to reduce the substantial model size and computational cost of Diffusion Transformers (DiTs) in video super-resolution (VSR) via quantization, while overcoming challenges posed by high dynamic ranges and diverse layer behaviors.

Method: The method includes a Dynamic Range Adaptive Quantizer (DRAQ), Variance-Oriented Layer Training Strategy (VOLTS), and Quantization-Aware Optimization (QAO) to refine quantized and high-precision branches.

Result: Experiments show LSGQuant achieves nearly the same performance as the full-precision model and outperforms existing quantization techniques.

Conclusion: LSGQuant effectively compresses the model for real-world VSR applications without significant performance loss, making it practical for downstream use.

Abstract: One-Step Diffusion Models have demonstrated promising capability and fast inference in video super-resolution (VSR) for real-world. Nevertheless, the substantial model size and high computational cost of Diffusion Transformers (DiTs) limit downstream applications. While low-bit quantization is a common approach for model compression, the effectiveness of quantized models is challenged by the high dynamic range of input latent and diverse layer behaviors. To deal with these challenges, we introduce LSGQuant, a layer-sensitivity guided quantizing approach for one-step diffusion-based real-world VSR. Our method incorporates a Dynamic Range Adaptive Quantizer (DRAQ) to fit video token activations. Furthermore, we estimate layer sensitivity and implement a Variance-Oriented Layer Training Strategy (VOLTS) by analyzing layer-wise statistics in calibration. We also introduce Quantization-Aware Optimization (QAO) to jointly refine the quantized branch and a retained high-precision branch. Extensive experiments demonstrate that our method has nearly performance to origin model with full-precision and significantly exceeds existing quantization techniques. Code is available at: https://github.com/zhengchen1999/LSGQuant.

</details>


### [41] [From Single Scan to Sequential Consistency: A New Paradigm for LIDAR Relocalization](https://arxiv.org/abs/2602.03198)
*Minghang Zhu,Zhijing Wang,Yuxin Guo,Wen Li,Sheng Ao,Cheng Wang*

Main category: cs.CV

TL;DR: TempLoc improves LiDAR relocalization by modeling sequential consistency, outperforming state-of-the-art methods on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing regression-based LiDAR relocalization methods struggle with dynamic or ambiguous scenarios due to reliance on single-frame inference or neglect of spatio-temporal consistency.

Method: TempLoc uses three modules: Global Coordinate Estimation for point-wise predictions, Prior Coordinate Generation for inter-frame correspondences via attention, and Uncertainty-Guided Coordinate Fusion for end-to-end integration.

Result: TempLoc surpasses state-of-the-art methods significantly on NCLT and Oxford Robot-Car benchmarks.

Conclusion: Temporal-aware correspondence modeling enhances LiDAR relocalization robustness and accuracy.

Abstract: LiDAR relocalization aims to estimate the global 6-DoF pose of a sensor in the environment. However, existing regression-based approaches are prone to dynamic or ambiguous scenarios, as they either solely rely on single-frame inference or neglect the spatio-temporal consistency across scans. In this paper, we propose TempLoc, a new LiDAR relocalization framework that enhances the robustness of localization by effectively modeling sequential consistency. Specifically, a Global Coordinate Estimation module is first introduced to predict point-wise global coordinates and associated uncertainties for each LiDAR scan. A Prior Coordinate Generation module is then presented to estimate inter-frame point correspondences by the attention mechanism. Lastly, an Uncertainty-Guided Coordinate Fusion module is deployed to integrate both predictions of point correspondence in an end-to-end fashion, yielding a more temporally consistent and accurate global 6-DoF pose. Experimental results on the NCLT and Oxford Robot-Car benchmarks show that our TempLoc outperforms stateof-the-art methods by a large margin, demonstrating the effectiveness of temporal-aware correspondence modeling in LiDAR relocalization. Our code will be released soon.

</details>


### [42] [Hand3R: Online 4D Hand-Scene Reconstruction in the Wild](https://arxiv.org/abs/2602.03200)
*Wendi Hu,Haonan Zhou,Wenhao Hu,Gaoang Wang*

Main category: cs.CV

TL;DR: Hand3R is an online framework for joint 4D hand-scene reconstruction from monocular video, integrating hand and scene models for simultaneous accuracy.


<details>
  <summary>Details</summary>
Motivation: Most methods focus on isolated hand reconstruction, ignoring the 3D environment, which is crucial for understanding physical interactions in Embodied AI.

Method: Hand3R combines a pre-trained hand expert with a 4D scene foundation model using scene-aware visual prompting, enabling high-fidelity hand priors injection into a persistent scene memory.

Result: The framework achieves competitive performance in hand reconstruction and global positioning without offline optimization.

Conclusion: Hand3R advances joint hand-scene reconstruction by leveraging synergy between hand and scene models in a single forward pass.

Abstract: For Embodied AI, jointly reconstructing dynamic hands and the dense scene context is crucial for understanding physical interaction. However, most existing methods recover isolated hands in local coordinates, overlooking the surrounding 3D environment. To address this, we present Hand3R, the first online framework for joint 4D hand-scene reconstruction from monocular video. Hand3R synergizes a pre-trained hand expert with a 4D scene foundation model via a scene-aware visual prompting mechanism. By injecting high-fidelity hand priors into a persistent scene memory, our approach enables simultaneous reconstruction of accurate hand meshes and dense metric-scale scene geometry in a single forward pass. Experiments demonstrate that Hand3R bypasses the reliance on offline optimization and delivers competitive performance in both local hand reconstruction and global positioning.

</details>


### [43] [VIRAL: Visual In-Context Reasoning via Analogy in Diffusion Transformers](https://arxiv.org/abs/2602.03210)
*Zhiwen Li,Zhongjie Duan,Jinyan Ye,Cen Chen,Daoyuan Chen,Yaliang Li,Yingda Chen*

Main category: cs.CV

TL;DR: VIRAL is a framework that addresses the challenge of replicating In-Context Learning (ICL) in computer vision by leveraging visual analogy and adapting a Diffusion Transformer (DiT) with multi-image conditioning and Mixture-of-Experts LoRA.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome task heterogeneity in visual ICL by proposing a unified framework.

Method: VIRAL formulates ICL as conditional generation via visual analogy and adapts a frozen DiT with role-aware multi-image conditioning and Mixture-of-Experts LoRA.

Result: VIRAL outperforms existing methods and demonstrates effectiveness across diverse visual tasks, including open-domain editing.

Conclusion: The paper validates that VIRAL can handle most visual tasks, providing a scalable and unified approach for visual ICL.

Abstract: Replicating In-Context Learning (ICL) in computer vision remains challenging due to task heterogeneity. We propose \textbf{VIRAL}, a framework that elicits visual reasoning from a pre-trained image editing model by formulating ICL as conditional generation via visual analogy ($x_s : x_t :: x_q : y_q$). We adapt a frozen Diffusion Transformer (DiT) using role-aware multi-image conditioning and introduce a Mixture-of-Experts LoRA to mitigate gradient interference across diverse tasks. Additionally, to bridge the gaps in current visual context datasets, we curate a large-scale dataset spanning perception, restoration, and editing. Experiments demonstrate that VIRAL outperforms existing methods, validating that a unified V-ICL paradigm can handle the majority of visual tasks, including open-domain editing. Our code is available at https://anonymous.4open.science/r/VIRAL-744A

</details>


### [44] [ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask](https://arxiv.org/abs/2602.03213)
*Zhuoran Yang,Yanyong Zhang*

Main category: cs.CV

TL;DR: ConsisDrive is an identity-preserving driving world model that tackles identity drift in generated driving videos by enforcing instance-level temporal consistency, improving video quality and downstream task performance.


<details>
  <summary>Details</summary>
Motivation: Autonomous driving requires high-quality multi-view driving data. Current world models often suffer from identity drift, where objects change appearance or category across frames due to lack of temporal constraints.

Method: ConsisDrive introduces two components: Instance-Masked Attention to ensure visual tokens interact only with corresponding instance features, and Instance-Masked Loss to emphasize foreground regions while reducing background noise.

Result: ConsisDrive achieves state-of-the-art driving video generation quality and shows significant improvements in downstream autonomous driving tasks on the nuScenes dataset.

Conclusion: ConsisDrive effectively addresses identity drift and enhances driving video generation, benefiting autonomous driving applications.

Abstract: Autonomous driving relies on robust models trained on large-scale, high-quality multi-view driving videos. Although world models provide a cost-effective solution for generating realistic driving data, they often suffer from identity drift, where the same object changes its appearance or category across frames due to the absence of instance-level temporal constraints. We introduce ConsisDrive, an identity-preserving driving world model designed to enforce temporal consistency at the instance level. Our framework incorporates two key components: (1) Instance-Masked Attention, which applies instance identity masks and trajectory masks within attention blocks to ensure that visual tokens interact only with their corresponding instance features across spatial and temporal dimensions, thereby preserving object identity consistency; and (2) Instance-Masked Loss, which adaptively emphasizes foreground regions with probabilistic instance masking, reducing background noise while maintaining overall scene fidelity. By integrating these mechanisms, ConsisDrive achieves state-of-the-art driving video generation quality and demonstrates significant improvements in downstream autonomous driving tasks on the nuScenes dataset. Our project page is https://shanpoyang654.github.io/ConsisDrive/page.html.

</details>


### [45] [FARTrack: Fast Autoregressive Visual Tracking with High Performance](https://arxiv.org/abs/2602.03214)
*Guijie Wang,Tong Lin,Yifan Bai,Anjia Cao,Shiyi Liang,Wangbo Zhao,Xing Wei*

Main category: cs.CV

TL;DR: FARTrack is a Fast Auto-Regressive Tracking framework designed to balance high performance and speed for visual tracking, achieving real-time results on GPUs and CPUs.


<details>
  <summary>Details</summary>
Motivation: High-performance trackers are often slow, limiting their use on resource-constrained devices. FARTrack addresses this by combining autoregression for efficiency with innovative techniques.

Method: FARTrack uses Task-Specific Self-Distillation for model compression and Inter-frame Autoregressive Sparsification for template optimization, enhancing speed without sacrificing performance.

Result: FARTrack achieves 70.6% AO on GOT-10k in real-time, with speeds of 343 FPS on GPU and 121 FPS on CPU.

Conclusion: FARTrack offers a practical solution for fast and high-performance visual tracking, suitable for deployment on limited-resource devices.

Abstract: Inference speed and tracking performance are two critical evaluation metrics in the field of visual tracking. However, high-performance trackers often suffer from slow processing speeds, making them impractical for deployment on resource-constrained devices. To alleviate this issue, we propose FARTrack, a Fast Auto-Regressive Tracking framework. Since autoregression emphasizes the temporal nature of the trajectory sequence, it can maintain high performance while achieving efficient execution across various devices. FARTrack introduces Task-Specific Self-Distillation and Inter-frame Autoregressive Sparsification, designed from the perspectives of shallow-yet-accurate distillation and redundant-to-essential token optimization, respectively. Task-Specific Self-Distillation achieves model compression by distilling task-specific tokens layer by layer, enhancing the model's inference speed while avoiding suboptimal manual teacher-student layer pairs assignments. Meanwhile, Inter-frame Autoregressive Sparsification sequentially condenses multiple templates, avoiding additional runtime overhead while learning a temporally-global optimal sparsification strategy. FARTrack demonstrates outstanding speed and competitive performance. It delivers an AO of 70.6% on GOT-10k in real-time. Beyond, our fastest model achieves a speed of 343 FPS on the GPU and 121 FPS on the CPU.

</details>


### [46] [PokeFusion Attention: Enhancing Reference-Free Style-Conditioned Generation](https://arxiv.org/abs/2602.03220)
*Jingbang Tang*

Main category: cs.CV

TL;DR: The paper introduces PokeFusion Attention, a lightweight decoder-level cross-attention mechanism for reference-free style-conditioned character generation in text-to-image diffusion models, improving style fidelity and consistency without increasing complexity.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with style drift, geometric inconsistency, or rely on external references, limiting flexibility and deployment.

Method: Proposes PokeFusion Attention, decoupling text and style conditioning at the attention level, training only decoder cross-attention layers and a style projection module.

Result: Outperforms adapter-based baselines in style fidelity, semantic alignment, and character shape consistency while being parameter-efficient.

Conclusion: PokeFusion Attention offers a plug-and-play solution for stylized character generation without modifying pretrained diffusion backbones.

Abstract: This paper studies reference-free style-conditioned character generation in text-to-image diffusion models, where high-quality synthesis requires both stable character structure and consistent, fine-grained style expression across diverse prompts. Existing approaches primarily rely on text-only prompting, which is often under-specified for visual style and tends to produce noticeable style drift and geometric inconsistency, or introduce reference-based adapters that depend on external images at inference time, increasing architectural complexity and limiting deployment flexibility.We propose PokeFusion Attention, a lightweight decoder-level cross-attention mechanism that fuses textual semantics with learned style embeddings directly inside the diffusion decoder. By decoupling text and style conditioning at the attention level, our method enables effective reference-free stylized generation while keeping the pretrained diffusion backbone fully frozen.PokeFusion Attention trains only decoder cross-attention layers together with a compact style projection module, resulting in a parameter-efficient and plug-and-play control component that can be easily integrated into existing diffusion pipelines and transferred across different backbones.Experiments on a stylized character generation benchmark (Pokemon-style) demonstrate that our method consistently improves style fidelity, semantic alignment, and character shape consistency compared with representative adapter-based baselines, while maintaining low parameter overhead and inference-time simplicity.

</details>


### [47] [Spiral RoPE: Rotate Your Rotary Positional Embeddings in the 2D Plane](https://arxiv.org/abs/2602.03227)
*Haoyu Liu,Sucheng Ren,Tingyu Zhu,Peng Wang,Cihang Xie,Alan Yuille,Zeyu Zheng,Feng Wang*

Main category: cs.CV

TL;DR: Spiral RoPE extends the standard axial 2D RoPE to enable multi-directional positional encoding, improving performance in vision tasks by better capturing oblique spatial relationships in images.


<details>
  <summary>Details</summary>
Motivation: The standard axial formulation of RoPE in vision transformers restricts positional encoding to axis-aligned directions, limiting its ability to model oblique spatial relationships in natural images.

Method: Spiral RoPE partitions embedding channels into groups associated with uniformly distributed directions, rotating each group based on the patch position's projection to encode multi-directional spatial relationships.

Result: Spiral RoPE consistently enhances performance in tasks like classification, segmentation, and generation, with attention maps showing more focused activations on relevant objects and better boundary respect.

Conclusion: Multi-directional positional encoding, as achieved by Spiral RoPE, is crucial for vision transformers to effectively model spatial relationships beyond horizontal and vertical axes.

Abstract: Rotary Position Embedding (RoPE) is the de facto positional encoding in large language models due to its ability to encode relative positions and support length extrapolation. When adapted to vision transformers, the standard axial formulation decomposes two-dimensional spatial positions into horizontal and vertical components, implicitly restricting positional encoding to axis-aligned directions. We identify this directional constraint as a fundamental limitation of the standard axial 2D RoPE, which hinders the modeling of oblique spatial relationships that naturally exist in natural images. To overcome this limitation, we propose Spiral RoPE, a simple yet effective extension that enables multi-directional positional encoding by partitioning embedding channels into multiple groups associated with uniformly distributed directions. Each group is rotated according to the projection of the patch position onto its corresponding direction, allowing spatial relationships to be encoded beyond the horizontal and vertical axes. Across a wide range of vision tasks including classification, segmentation, and generation, Spiral RoPE consistently improves performance. Qualitative analysis of attention maps further show that Spiral RoPE exhibits more concentrated activations on semantically relevant objects and better respects local object boundaries, highlighting the importance of multi-directional positional encoding in vision transformers.

</details>


### [48] [EventFlash: Towards Efficient MLLMs for Event-Based Vision](https://arxiv.org/abs/2602.03230)
*Shaoyu Liu,Jianing Li,Guanghui Zhao,Yunjian Zhang,Wen Jiang,Ming Li,Xiangyang Ji*

Main category: cs.CV

TL;DR: EventFlash is an efficient multimodal large language model (MLLM) that addresses computational inefficiency in event-based vision by leveraging spatiotemporal token sparsification, adaptive temporal sampling, and sparse attention.


<details>
  <summary>Details</summary>
Motivation: Current event-based MLLMs suffer from high computational costs due to dense processing of sparse event streams. EventFlash aims to reduce redundancy and accelerate inference.

Method: EventFlash introduces EventMind dataset for curriculum training, adaptive temporal window aggregation for token compression, and sparse density-guided attention for spatial efficiency.

Result: EventFlash achieves a 12.4x throughput improvement over the baseline (EventFlash-Zero) and supports long-range processing (up to 1,000 bins), surpassing EventGPT's 5-bin limit.

Conclusion: EventFlash is a highly efficient foundation model for event-based vision, balancing performance and computational cost.

Abstract: Event-based multimodal large language models (MLLMs) enable robust perception in high-speed and low-light scenarios, addressing key limitations of frame-based MLLMs. However, current event-based MLLMs often rely on dense image-like processing paradigms, overlooking the spatiotemporal sparsity of event streams and resulting in high computational cost. In this paper, we propose EventFlash, a novel and efficient MLLM to explore spatiotemporal token sparsification for reducing data redundancy and accelerating inference. Technically, we build EventMind, a large-scale and scene-diverse dataset with over 500k instruction sets, providing both short and long event stream sequences to support our curriculum training strategy. We then present an adaptive temporal window aggregation module for efficient temporal sampling, which adaptively compresses temporal tokens while retaining key temporal cues. Finally, a sparse density-guided attention module is designed to improve spatial token efficiency by selecting informative regions and suppressing empty or sparse areas. Experimental results show that EventFlash achieves a $12.4\times$ throughput improvement over the baseline (EventFlash-Zero) while maintaining comparable performance. It supports long-range event stream processing with up to 1,000 bins, significantly outperforming the 5-bin limit of EventGPT. We believe EventFlash serves as an efficient foundation model for event-based vision.

</details>


### [49] [InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation](https://arxiv.org/abs/2602.03242)
*Zhuoran Yang,Xi Guo,Chenjing Ding,Chiyu Wang,Wei Wu,Yanyong Zhang*

Main category: cs.CV

TL;DR: InstaDrive enhances driving video realism with Instance Flow Guider and Spatial Geometric Aligner, improving temporal consistency and spatial fidelity for autonomous driving tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in maintaining instance-level temporal consistency and spatial geometric fidelity in world models for autonomous driving.

Method: Proposes InstaDrive with two key components: Instance Flow Guider for temporal consistency and Spatial Geometric Aligner for spatial accuracy.

Result: Achieves state-of-the-art video generation quality and improves downstream autonomous driving tasks on the nuScenes dataset.

Conclusion: InstaDrive effectively enhances driving video realism and supports rigorous safety evaluation through procedural simulation in CARLA.

Abstract: Autonomous driving relies on robust models trained on high-quality, large-scale multi-view driving videos. While world models offer a cost-effective solution for generating realistic driving videos, they struggle to maintain instance-level temporal consistency and spatial geometric fidelity. To address these challenges, we propose InstaDrive, a novel framework that enhances driving video realism through two key advancements: (1) Instance Flow Guider, which extracts and propagates instance features across frames to enforce temporal consistency, preserving instance identity over time. (2) Spatial Geometric Aligner, which improves spatial reasoning, ensures precise instance positioning, and explicitly models occlusion hierarchies. By incorporating these instance-aware mechanisms, InstaDrive achieves state-of-the-art video generation quality and enhances downstream autonomous driving tasks on the nuScenes dataset. Additionally, we utilize CARLA's autopilot to procedurally and stochastically simulate rare but safety-critical driving scenarios across diverse maps and regions, enabling rigorous safety evaluation for autonomous systems. Our project page is https://shanpoyang654.github.io/InstaDrive/page.html.

</details>


### [50] [LaVPR: Benchmarking Language and Vision for Place Recognition](https://arxiv.org/abs/2602.03253)
*Ofer Idan,Dan Badur,Yosi Keller,Yoli Shavit*

Main category: cs.CV

TL;DR: LaVPR introduces a large-scale benchmark with 650K+ language descriptions for VPR, improving robustness and enabling language-based localization. Smaller models with language match larger vision-only ones, and LoRA-based retrieval outperforms standard methods.


<details>
  <summary>Details</summary>
Motivation: Addresses VPR failures under environmental changes and perceptual aliasing, and enables blind localization from verbal descriptions, crucial for applications like emergency response.

Method: Extends VPR datasets with natural-language descriptions, investigates Multi-Modal Fusion (robustness) and Cross-Modal Retrieval (language-based localization), and uses LoRA with Multi-Similarity loss.

Result: Language descriptions improve performance in degraded conditions, especially for smaller models. Cross-modal retrieval with LoRA beats standard contrastive methods.

Conclusion: LaVPR enables resilient, practical localization systems and advances vision-language integration for real-world stochasticity and resource-constrained deployment.

Abstract: Visual Place Recognition (VPR) often fails under extreme environmental changes and perceptual aliasing. Furthermore, standard systems cannot perform "blind" localization from verbal descriptions alone, a capability needed for applications such as emergency response. To address these challenges, we introduce LaVPR, a large-scale benchmark that extends existing VPR datasets with over 650,000 rich natural-language descriptions. Using LaVPR, we investigate two paradigms: Multi-Modal Fusion for enhanced robustness and Cross-Modal Retrieval for language-based localization. Our results show that language descriptions yield consistent gains in visually degraded conditions, with the most significant impact on smaller backbones. Notably, adding language allows compact models to rival the performance of much larger vision-only architectures. For cross-modal retrieval, we establish a baseline using Low-Rank Adaptation (LoRA) and Multi-Similarity loss, which substantially outperforms standard contrastive methods across vision-language models. Ultimately, LaVPR enables a new class of localization systems that are both resilient to real-world stochasticity and practical for resource-constrained deployment. Our dataset and code are available at https://github.com/oferidan1/LaVPR.

</details>


### [51] [HypCBC: Domain-Invariant Hyperbolic Cross-Branch Consistency for Generalizable Medical Image Analysis](https://arxiv.org/abs/2602.03264)
*Francesco Di Salvo,Sebastian Doerrich,Jonas Alle,Christian Ledig*

Main category: cs.CV

TL;DR: The paper proposes hyperbolic representation learning to address domain generalization challenges in medical image analysis, outperforming Euclidean methods.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks struggle with generalization in medical image analysis due to data scarcity and covariate shifts from diverse sources.

Method: The work leverages hyperbolic manifolds to model complex data structures and introduces an unsupervised domain-invariant hyperbolic cross-branch consistency constraint.

Result: The method achieves statistically significant improvements, outperforming state-of-the-art Euclidean methods by +2.1% AUC on three domain generalization benchmarks.

Conclusion: Hyperbolic representation learning enhances generalization in medical image analysis, validated across diverse datasets and models.

Abstract: Robust generalization beyond training distributions remains a critical challenge for deep neural networks. This is especially pronounced in medical image analysis, where data is often scarce and covariate shifts arise from different hardware devices, imaging protocols, and heterogeneous patient populations. These factors collectively hinder reliable performance and slow down clinical adoption. Despite recent progress, existing learning paradigms primarily rely on the Euclidean manifold, whose flat geometry fails to capture the complex, hierarchical structures present in clinical data. In this work, we exploit the advantages of hyperbolic manifolds to model complex data characteristics. We present the first comprehensive validation of hyperbolic representation learning for medical image analysis and demonstrate statistically significant gains across eleven in-distribution datasets and three ViT models. We further propose an unsupervised, domain-invariant hyperbolic cross-branch consistency constraint. Extensive experiments confirm that our proposed method promotes domain-invariant features and outperforms state-of-the-art Euclidean methods by an average of $+2.1\%$ AUC on three domain generalization benchmarks: Fitzpatrick17k, Camelyon17-WILDS, and a cross-dataset setup for retinal imaging. These datasets span different imaging modalities, data sizes, and label granularities, confirming generalization capabilities across substantially different conditions. The code is available at https://github.com/francescodisalvo05/hyperbolic-cross-branch-consistency .

</details>


### [52] [Global Geometry Is Not Enough for Vision Representations](https://arxiv.org/abs/2602.03282)
*Jiwan Chung,Seon Joo Kim*

Main category: cs.CV

TL;DR: The paper challenges the assumption that globally well-distributed embeddings ensure robust representations, showing that geometric metrics fail to predict compositional binding, while functional sensitivity (via Jacobian) does.


<details>
  <summary>Details</summary>
Motivation: To question the reliance on global geometry as a proxy for representational competence, highlighting its insensitivity to compositional binding.

Method: Tested geometric metrics and functional sensitivity (input-output Jacobian) across 21 vision encoders to predict compositional binding.

Result: Geometric metrics showed near-zero correlation with compositional binding, whereas functional sensitivity reliably tracked it.

Conclusion: Global geometry captures only part of representational competence, and functional sensitivity is a critical complementary axis.

Abstract: A common assumption in representation learning is that globally well-distributed embeddings support robust and generalizable representations. This focus has shaped both training objectives and evaluation protocols, implicitly treating global geometry as a proxy for representational competence. While global geometry effectively encodes which elements are present, it is often insensitive to how they are composed. We investigate this limitation by testing the ability of geometric metrics to predict compositional binding across 21 vision encoders. We find that standard geometry-based statistics exhibit near-zero correlation with compositional binding. In contrast, functional sensitivity, as measured by the input-output Jacobian, reliably tracks this capability. We further provide an analytic account showing that this disparity arises from objective design, as existing losses explicitly constrain embedding geometry but leave the local input-output mapping unconstrained. These results suggest that global embedding geometry captures only a partial view of representational competence and establish functional sensitivity as a critical complementary axis for modeling composite structure.

</details>


### [53] [A3-TTA: Adaptive Anchor Alignment Test-Time Adaptation for Image Segmentation](https://arxiv.org/abs/2602.03292)
*Jianghao Wu,Xiangde Luo,Yubo Zhou,Lianming Wu,Guotai Wang,Shaoting Zhang*

Main category: cs.CV

TL;DR: A3-TTA is a Test-Time Adaptation framework for image segmentation under domain shift, using anchor-guided supervision and semantic consistency to improve pseudo-label reliability and stability.


<details>
  <summary>Details</summary>
Motivation: Existing pseudo-label-based TTA methods rely on unstable perturbation-ensemble heuristics, leading to error accumulation and catastrophic forgetting. A3-TTA addresses these issues with distributionally grounded pseudo-labels.

Method: A3-TTA identifies confident predictions as anchors, uses them for pseudo-label generation, and regularizes with semantic consistency and entropy minimization. A self-adaptive EMA strategy is added to stabilize updates.

Result: A3-TTA improves Dice scores by 10.40–17.68 percentage points over the source model and outperforms state-of-the-art TTA methods in multi-domain medical and natural image segmentation.

Conclusion: A3-TTA effectively mitigates error accumulation and forgetting in TTA, demonstrating robustness across sequential domains and architectures.

Abstract: Test-Time Adaptation (TTA) offers a practical solution for deploying image segmentation models under domain shift without accessing source data or retraining. Among existing TTA strategies, pseudo-label-based methods have shown promising performance. However, they often rely on perturbation-ensemble heuristics (e.g., dropout sampling, test-time augmentation, Gaussian noise), which lack distributional grounding and yield unstable training signals. This can trigger error accumulation and catastrophic forgetting during adaptation. To address this, we propose \textbf{A3-TTA}, a TTA framework that constructs reliable pseudo-labels through anchor-guided supervision. Specifically, we identify well-predicted target domain images using a class compact density metric, under the assumption that confident predictions imply distributional proximity to the source domain. These anchors serve as stable references to guide pseudo-label generation, which is further regularized via semantic consistency and boundary-aware entropy minimization. Additionally, we introduce a self-adaptive exponential moving average strategy to mitigate label noise and stabilize model update during adaptation. Evaluated on both multi-domain medical images (heart structure and prostate segmentation) and natural images, A3-TTA significantly improves average Dice scores by 10.40 to 17.68 percentage points compared to the source model, outperforming several state-of-the-art TTA methods under different segmentation model architectures. A3-TTA also excels in continual TTA, maintaining high performance across sequential target domains with strong anti-forgetting ability. The code will be made publicly available at https://github.com/HiLab-git/A3-TTA.

</details>


### [54] [LEVIO: Lightweight Embedded Visual Inertial Odometry for Resource-Constrained Devices](https://arxiv.org/abs/2602.03294)
*Jonas Kühne,Christian Vogt,Michele Magno,Luca Benini*

Main category: cs.CV

TL;DR: LEVIO is a computationally efficient VIO pipeline for ultra-low-power platforms, enabling real-time 6-DoF motion tracking with minimal energy consumption.


<details>
  <summary>Details</summary>
Motivation: To address the need for accurate, infrastructure-less motion tracking in resource-constrained devices like micro-drones and smart glasses, where existing VIO systems are too demanding.

Method: LEVIO integrates ORB feature tracking and bundle adjustment while optimizing for low power and memory usage. It uses parallelization and hardware-software co-optimization.

Result: Achieves 20 FPS on an ultra-low-power RISC-V SoC with <100 mW consumption, balancing efficiency and accuracy on public VIO datasets.

Conclusion: LEVIO provides a viable solution for real-time motion tracking in power-constrained devices, with open-source availability for wider adoption.

Abstract: Accurate, infrastructure-less sensor systems for motion tracking are essential for mobile robotics and augmented reality (AR) applications. The most popular state-of-the-art visual-inertial odometry (VIO) systems, however, are too computationally demanding for resource-constrained hardware, such as micro-drones and smart glasses. This work presents LEVIO, a fully featured VIO pipeline optimized for ultra-low-power compute platforms, allowing six-degrees-of-freedom (DoF) real-time sensing. LEVIO incorporates established VIO components such as Oriented FAST and Rotated BRIEF (ORB) feature tracking and bundle adjustment, while emphasizing a computationally efficient architecture with parallelization and low memory usage to suit embedded microcontrollers and low-power systems-on-chip (SoCs). The paper proposes and details the algorithmic design choices and the hardware-software co-optimization approach, and presents real-time performance on resource-constrained hardware. LEVIO is validated on a parallel-processing ultra-low-power RISC-V SoC, achieving 20 FPS while consuming less than 100 mW, and benchmarked against public VIO datasets, offering a compelling balance between efficiency and accuracy. To facilitate reproducibility and adoption, the complete implementation is released as open-source.

</details>


### [55] [Full end-to-end diagnostic workflow automation of 3D OCT via foundation model-driven AI for retinal diseases](https://arxiv.org/abs/2602.03302)
*Jinze Zhang,Jian Zhong,Li Lin,Jiaxiong Li,Ke Ma,Naiyang Li,Meng Li,Yuan Pan,Zeyu Meng,Mengyun Zhou,Shang Huang,Shilong Yu,Zhengyu Duan,Sutong Li,Honghui Xia,Juping Liu,Dan Liang,Yantao Wei,Xiaoying Tang,Jin Yuan,Peng Xiao*

Main category: cs.CV

TL;DR: FOCUS is an end-to-end automated framework for 3D OCT retinal disease diagnosis, using a foundation model-driven approach to integrate image quality assessment, abnormality detection, and multi-disease classification.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of multi-stage workflows and single-slice AI models in automating OCT-based retinal disease diagnosis, aiming for clinical utility.

Method: Sequential use of EfficientNetV2-S for image quality assessment, fine-tuned Vision Foundation Model for abnormality detection and classification, and unified adaptive aggregation for 3D diagnosis.

Result: High F1 scores in quality assessment (99.01%), abnormality detection (97.46%), and patient-level diagnosis (94.39%), validated across diverse centers and devices.

Conclusion: FOCUS matches expert performance and improves efficiency, automating diagnosis pipelines to advance unmanned ophthalmology and enhance retinal care accessibility.

Abstract: Optical coherence tomography (OCT) has revolutionized retinal disease diagnosis with its high-resolution and three-dimensional imaging nature, yet its full diagnostic automation in clinical practices remains constrained by multi-stage workflows and conventional single-slice single-task AI models. We present Full-process OCT-based Clinical Utility System (FOCUS), a foundation model-driven framework enabling end-to-end automation of 3D OCT retinal disease diagnosis. FOCUS sequentially performs image quality assessment with EfficientNetV2-S, followed by abnormality detection and multi-disease classification using a fine-tuned Vision Foundation Model. Crucially, FOCUS leverages a unified adaptive aggregation method to intelligently integrate 2D slices-level predictions into comprehensive 3D patient-level diagnosis. Trained and tested on 3,300 patients (40,672 slices), and externally validated on 1,345 patients (18,498 slices) across four different-tier centers and diverse OCT devices, FOCUS achieved high F1 scores for quality assessment (99.01%), abnormally detection (97.46%), and patient-level diagnosis (94.39%). Real-world validation across centers also showed stable performance (F1: 90.22%-95.24%). In human-machine comparisons, FOCUS matched expert performance in abnormality detection (F1: 95.47% vs 90.91%) and multi-disease diagnosis (F1: 93.49% vs 91.35%), while demonstrating better efficiency. FOCUS automates the image-to-diagnosis pipeline, representing a critical advance towards unmanned ophthalmology with a validated blueprint for autonomous screening to enhance population scale retinal care accessibility and efficiency.

</details>


### [56] [PQTNet: Pixel-wise Quantitative Thermography Neural Network for Estimating Defect Depth in Polylactic Acid Parts by Additive Manufacturing](https://arxiv.org/abs/2602.03314)
*Lei Deng,Wenhao Huang,Chao Yang,Haoyuan Zheng,Yinbin Tian,Yue Ma*

Main category: cs.CV

TL;DR: The paper introduces PQT-Net, a neural network for quantifying defect depths in additively manufactured PLA parts using thermography, achieving high precision with innovative data augmentation.


<details>
  <summary>Details</summary>
Motivation: Quantifying defect depths in AM components is challenging for NDT, prompting the need for an accurate solution.

Method: PQT-Net uses a novel data augmentation strategy transforming thermal sequences into stripe images and combines EfficientNetV2-S with a custom RRH for refined outputs.

Result: PQT-Net outperforms other models with an MAE of 0.0094 mm and R exceeding 99%.

Conclusion: PQT-Net shows strong potential for precise defect characterization in AM.

Abstract: Defect depth quantification in additively manufactured (AM) components remains a significant challenge for non-destructive testing (NDT). This study proposes a Pixel-wise Quantitative Thermography Neural Network (PQT-Net) to address this challenge for polylactic acid (PLA) parts. A key innovation is a novel data augmentation strategy that reconstructs thermal sequence data into two-dimensional stripe images, preserving the complete temporal evolution of heat diffusion for each pixel. The PQT-Net architecture incorporates a pre-trained EfficientNetV2-S backbone and a custom Residual Regression Head (RRH) with learnable parameters to refine outputs. Comparative experiments demonstrate the superiority of PQT-Net over other deep learning models, achieving a minimum Mean Absolute Error (MAE) of 0.0094 mm and a coefficient of determination (R) exceeding 99%. The high precision of PQT-Net underscores its potential for robust quantitative defect characterization in AM.

</details>


### [57] [Invisible Clean-Label Backdoor Attacks for Generative Data Augmentation](https://arxiv.org/abs/2602.03316)
*Ting Xiang,Jinhui Zhao,Changjian Chen,Zhuo Tang*

Main category: cs.CV

TL;DR: Summarizes InvLBA, an invisible clean-label backdoor attack method targeting generative data augmentation at the latent feature level.


<details>
  <summary>Details</summary>
Motivation: Generative data augmentation is vulnerable to clean-label backdoor attacks, but existing pixel-level methods yield low success rates.

Method: Proposes InvLBA, leveraging latent feature perturbations for attacks.

Result: Achieves 46.43% higher attack success rates with no clean accuracy loss and robustness against defenses.

Conclusion: InvLBA is effective for latent-level attacks in generative augmentation.

Abstract: With the rapid advancement of image generative models, generative data augmentation has become an effective way to enrich training images, especially when only small-scale datasets are available. At the same time, in practical applications, generative data augmentation can be vulnerable to clean-label backdoor attacks, which aim to bypass human inspection. However, based on theoretical analysis and preliminary experiments, we observe that directly applying existing pixel-level clean-label backdoor attack methods (e.g., COMBAT) to generated images results in low attack success rates. This motivates us to move beyond pixel-level triggers and focus instead on the latent feature level. To this end, we propose InvLBA, an invisible clean-label backdoor attack method for generative data augmentation by latent perturbation. We theoretically prove that the generalization of the clean accuracy and attack success rates of InvLBA can be guaranteed. Experiments on multiple datasets show that our method improves the attack success rate by 46.43% on average, with almost no reduction in clean accuracy and high robustness against SOTA defense methods.

</details>


### [58] [MedSAM-Agent: Empowering Interactive Medical Image Segmentation with Multi-turn Agentic Reinforcement Learning](https://arxiv.org/abs/2602.03320)
*Shengyuan Liu,Liuxin Bao,Qi Yang,Wanting Geng,Boyun Zheng,Chenxin Li,Wenting Chen,Houwen Peng,Yixuan Yuan*

Main category: cs.CV

TL;DR: MedSAM-Agent introduces a multi-step autonomous decision-making framework for medical image segmentation, combining hybrid prompting and a two-stage training pipeline to improve interaction efficiency and achieve state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Task-specific models are transitioning to generalizable frameworks, but existing methods lack dynamic interaction strategies and process-level supervision, limiting their potential.

Method: Proposes MedSAM-Agent with hybrid prompting for trajectory generation and a two-stage training pipeline integrating multi-turn verification and process rewards.

Result: Achieves state-of-the-art performance across 6 medical modalities and 21 datasets.

Conclusion: MedSAM-Agent effectively unifies autonomous reasoning with iterative optimization, advancing interactive segmentation.

Abstract: Medical image segmentation is evolving from task-specific models toward generalizable frameworks. Recent research leverages Multi-modal Large Language Models (MLLMs) as autonomous agents, employing reinforcement learning with verifiable reward (RLVR) to orchestrate specialized tools like the Segment Anything Model (SAM). However, these approaches often rely on single-turn, rigid interaction strategies and lack process-level supervision during training, which hinders their ability to fully exploit the dynamic potential of interactive tools and leads to redundant actions. To bridge this gap, we propose MedSAM-Agent, a framework that reformulates interactive segmentation as a multi-step autonomous decision-making process. First, we introduce a hybrid prompting strategy for expert-curated trajectory generation, enabling the model to internalize human-like decision heuristics and adaptive refinement strategies. Furthermore, we develop a two-stage training pipeline that integrates multi-turn, end-to-end outcome verification with a clinical-fidelity process reward design to promote interaction parsimony and decision efficiency. Extensive experiments across 6 medical modalities and 21 datasets demonstrate that MedSAM-Agent achieves state-of-the-art performance, effectively unifying autonomous medical reasoning with robust, iterative optimization. Code is available \href{https://github.com/CUHK-AIM-Group/MedSAM-Agent}{here}.

</details>


### [59] [PWAVEP: Purifying Imperceptible Adversarial Perturbations in 3D Point Clouds via Spectral Graph Wavelets](https://arxiv.org/abs/2602.03333)
*Haoran Li,Renyang Liu,Hongjia Liu,Chen Wang,Long Yin,Jian Xu*

Main category: cs.CV

TL;DR: The paper proposes PWAVEP, a plug-and-play defense for 3D point clouds, using spectral analysis to purify adversarial perturbations without invasive model changes.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of adversarial attacks on 3D point clouds, which often require cumbersome defenses, the paper aims to introduce a non-invasive, efficient solution.

Method: PWAVEP uses spectral graph wavelet saliency and sparsity scores to hierarchically remove adversarial outliers and apply spectral filtering to attenuate high-frequency noise.

Result: PWAVEP outperforms existing methods in accuracy and robustness for 3D point cloud purification.

Conclusion: The study advances 3D point cloud defense with a practical, non-invasive approach, validated by superior performance.

Abstract: Recent progress in adversarial attacks on 3D point clouds, particularly in achieving spatial imperceptibility and high attack performance, presents significant challenges for defenders. Current defensive approaches remain cumbersome, often requiring invasive model modifications, expensive training procedures or auxiliary data access. To address these threats, in this paper, we propose a plug-and-play and non-invasive defense mechanism in the spectral domain, grounded in a theoretical and empirical analysis of the relationship between imperceptible perturbations and high-frequency spectral components. Building upon these insights, we introduce a novel purification framework, termed PWAVEP, which begins by computing a spectral graph wavelet domain saliency score and local sparsity score for each point. Guided by these values, PWAVEP adopts a hierarchical strategy, it eliminates the most salient points, which are identified as hardly recoverable adversarial outliers. Simultaneously, it applies a spectral filtering process to a broader set of moderately salient points. This process leverages a graph wavelet transform to attenuate high-frequency coefficients associated with the targeted points, thereby effectively suppressing adversarial noise. Extensive evaluations demonstrate that the proposed PWAVEP achieves superior accuracy and robustness compared to existing approaches, advancing the state-of-the-art in 3D point cloud purification. Code and datasets are available at https://github.com/a772316182/pwavep

</details>


### [60] [Composable Visual Tokenizers with Generator-Free Diagnostics of Learnability](https://arxiv.org/abs/2602.03339)
*Bingchen Zhao,Qiushan Guo,Ye Wang,Yixuan Huang,Zhonghua Zhai,Yu Tian*

Main category: cs.CV

TL;DR: CompTok is a training framework for visual tokenizers enhanced for compositionality, using a token-conditioned diffusion decoder and InfoGAN-style objectives.


<details>
  <summary>Details</summary>
Motivation: To improve compositional control in visual tokenizers and ensure tokens are not ignored by the decoder, enabling high-level semantic editing.

Method: Employs a token-conditioned diffusion decoder with an InfoGAN-style objective and trains on tokens formed by swapping subsets between images, using adversarial flow regularization.

Result: Achieves state-of-the-art performance in image generation and demonstrates effective token swapping for semantic editing. Introduces metrics for token space compositionality.

Conclusion: CompTok enhances compositionality and supports advanced generators, improving both metrics and generation quality.

Abstract: We introduce CompTok, a training framework for learning visual tokenizers whose tokens are enhanced for compositionality. CompTok uses a token-conditioned diffusion decoder. By employing an InfoGAN-style objective, where we train a recognition model to predict the tokens used to condition the diffusion decoder using the decoded images, we enforce the decoder to not ignore any of the tokens. To promote compositional control, besides the original images, CompTok also trains on tokens formed by swapping token subsets between images, enabling more compositional control of the token over the decoder. As the swapped tokens between images do not have ground truth image targets, we apply a manifold constraint via an adversarial flow regularizer to keep unpaired swap generations on the natural-image distribution. The resulting tokenizer not only achieves state-of-the-art performance on image class-conditioned generation, but also demonstrates properties such as swapping tokens between images to achieve high level semantic editing of an image. Additionally, we propose two metrics that measures the landscape of the token space that can be useful to describe not only the compositionality of the tokens, but also how easy to learn the landscape is for a generator to be trained on this space. We show in experiments that CompTok can improve on both of the metrics as well as supporting state-of-the-art generators for class conditioned generation.

</details>


### [61] [Tiled Prompts: Overcoming Prompt Underspecification in Image and Video Super-Resolution](https://arxiv.org/abs/2602.03342)
*Bryan Sangwoo Kim,Jonghyun Park,Jong Chul Ye*

Main category: cs.CV

TL;DR: Tiled Prompts improves image and video super-resolution by generating tile-specific prompts, addressing issues like prompt sparsity and misguidance in global-caption approaches.


<details>
  <summary>Details</summary>
Motivation: Global captions in super-resolution pipelines often lack localized details and provide irrelevant guidance, especially when scaled to high resolutions. This leads to issues like prompt sparsity and misguidance.

Method: The proposed Tiled Prompts framework generates unique prompts for each latent tile, enabling locally text-conditioned super-resolution. This approach minimizes overhead while providing high-information guidance.

Result: Experiments demonstrate improved perceptual quality, better text alignment, and reduced hallucinations and tile-level artifacts compared to global-prompt methods.

Conclusion: Tiled Prompts offers a unified and effective solution for high-resolution image and video super-resolution by addressing the limitations of global-caption techniques.

Abstract: Text-conditioned diffusion models have advanced image and video super-resolution by using prompts as semantic priors, but modern super-resolution pipelines typically rely on latent tiling to scale to high resolutions, where a single global caption causes prompt underspecification. A coarse global prompt often misses localized details (prompt sparsity) and provides locally irrelevant guidance (prompt misguidance) that can be amplified by classifier-free guidance. We propose Tiled Prompts, a unified framework for image and video super-resolution that generates a tile-specific prompt for each latent tile and performs super-resolution under locally text-conditioned posteriors, providing high-information guidance that resolves prompt underspecification with minimal overhead. Experiments on high resolution real-world images and videos show consistent gains in perceptual quality and text alignment, while reducing hallucinations and tile-level artifacts relative to global-prompt baselines.

</details>


### [62] [Z3D: Zero-Shot 3D Visual Grounding from Images](https://arxiv.org/abs/2602.03361)
*Nikita Drozdov,Andrey Lemeshko,Nikita Gavrilov,Anton Konushin,Danila Rukhovich,Maksim Kolodiazhnyi*

Main category: cs.CV

TL;DR: Z3D introduces a zero-shot 3D visual grounding pipeline using multi-view images, improving performance via novel segmentation and reasoning techniques.


<details>
  <summary>Details</summary>
Motivation: To address the lack of geometric supervision or object priors in 3DVG, enabling universal grounding from multi-view images alone.

Method: Combines a zero-shot 3D instance segmentation method for bounding box proposals and prompt-based segmentation leveraging modern VLMs.

Result: Achieves state-of-the-art performance on ScanRefer and Nr3D benchmarks without requiring prior training data.

Conclusion: Z3D provides a flexible, high-performing zero-shot solution for 3DVG, advancing the field beyond traditional supervised methods.

Abstract: 3D visual grounding (3DVG) aims to localize objects in a 3D scene based on natural language queries. In this work, we explore zero-shot 3DVG from multi-view images alone, without requiring any geometric supervision or object priors. We introduce Z3D, a universal grounding pipeline that flexibly operates on multi-view images while optionally incorporating camera poses and depth maps. We identify key bottlenecks in prior zero-shot methods causing significant performance degradation and address them with (i) a state-of-the-art zero-shot 3D instance segmentation method to generate high-quality 3D bounding box proposals and (ii) advanced reasoning via prompt-based segmentation, which utilizes full capabilities of modern VLMs. Extensive experiments on the ScanRefer and Nr3D benchmarks demonstrate that our approach achieves state-of-the-art performance among zero-shot methods. Code is available at https://github.com/col14m/z3d .

</details>


### [63] [Symbol-Aware Reasoning with Masked Discrete Diffusion for Handwritten Mathematical Expression Recognition](https://arxiv.org/abs/2602.03370)
*Takaya Kawakatsu,Ryo Ishiyama*

Main category: cs.CV

TL;DR: A diffusion framework for HMER that refines symbols and structures iteratively, outperforming traditional models on benchmarks like MathWriting and CROHME.


<details>
  <summary>Details</summary>
Motivation: Autoregressive models for HMER suffer from exposure bias and syntactic inconsistency, prompting a need for iterative refinement methods.

Method: Discrete diffusion framework with multi-step remasking, symbol-aware tokenization, and Random-Masking Mutual Learning.

Result: Achieves 5.56% CER and 60.42% EM on MathWriting, surpassing Transformer and commercial models.

Conclusion: Discrete diffusion offers a new paradigm for structure-aware visual recognition in HMER.

Abstract: Handwritten Mathematical Expression Recognition (HMER) requires reasoning over diverse symbols and 2D structural layouts, yet autoregressive models struggle with exposure bias and syntactic inconsistency. We present a discrete diffusion framework that reformulates HMER as iterative symbolic refinement instead of sequential generation. Through multi-step remasking, the proposal progressively refines both symbols and structural relations, removing causal dependencies and improving structural consistency. A symbol-aware tokenization and Random-Masking Mutual Learning further enhance syntactic alignment and robustness to handwriting diversity. On the MathWriting benchmark, the proposal achieves 5.56\% CER and 60.42\% EM, outperforming strong Transformer and commercial baselines. Consistent gains on CROHME 2014--2023 demonstrate that discrete diffusion provides a new paradigm for structure-aware visual recognition beyond generative modeling.

</details>


### [64] [Multi-Resolution Alignment for Voxel Sparsity in Camera-Based 3D Semantic Scene Completion](https://arxiv.org/abs/2602.03371)
*Zhiwen Yang,Yuxin Peng*

Main category: cs.CV

TL;DR: The paper proposes a Multi-Resolution Alignment (MRA) approach to address voxel sparsity in camera-based 3D semantic scene completion, enhancing optimization and performance through scene and instance-level alignment.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on voxel labels for optimization, facing challenges due to voxel sparsity, which limits efficiency and performance. The paper aims to mitigate this issue.

Method: The MRA approach includes a Multi-resolution View Transformer module for scene-level alignment, a Cubic Semantic Anisotropy module for instance-level semantic significance, and a Critical Distribution Alignment module for feature consistency.

Result: The proposed method improves voxel sparsity handling and enhances optimization efficiency and model performance.

Conclusion: The MRA approach effectively addresses voxel sparsity in 3D semantic scene completion, providing better scene perception for autonomous driving systems.

Abstract: Camera-based 3D semantic scene completion (SSC) offers a cost-effective solution for assessing the geometric occupancy and semantic labels of each voxel in the surrounding 3D scene with image inputs, providing a voxel-level scene perception foundation for the perception-prediction-planning autonomous driving systems. Although significant progress has been made in existing methods, their optimization rely solely on the supervision from voxel labels and face the challenge of voxel sparsity as a large portion of voxels in autonomous driving scenarios are empty, which limits both optimization efficiency and model performance. To address this issue, we propose a \textit{Multi-Resolution Alignment (MRA)} approach to mitigate voxel sparsity in camera-based 3D semantic scene completion, which exploits the scene and instance level alignment across multi-resolution 3D features as auxiliary supervision. Specifically, we first propose the Multi-resolution View Transformer module, which projects 2D image features into multi-resolution 3D features and aligns them at the scene level through fusing discriminative seed features. Furthermore, we design the Cubic Semantic Anisotropy module to identify the instance-level semantic significance of each voxel, accounting for the semantic differences of a specific voxel against its neighboring voxels within a cubic area. Finally, we devise a Critical Distribution Alignment module, which selects critical voxels as instance-level anchors with the guidance of cubic semantic anisotropy, and applies a circulated loss for auxiliary supervision on the critical feature distribution consistency across different resolutions. The code is available at https://github.com/PKU-ICST-MIPL/MRA_TIP.

</details>


### [65] [SLIM-Diff: Shared Latent Image-Mask Diffusion with Lp loss for Data-Scarce Epilepsy FLAIR MRI](https://arxiv.org/abs/2602.03372)
*Mario Pascual-González,Ariadna Jiménez-Partinen,R. M. Luque-Baena,Fátima Nagib-Raya,Ezequiel López-Rubio*

Main category: cs.CV

TL;DR: SLIM-Diff is a compact joint diffusion model for generating FCD lesions in epilepsy FLAIR MRI, using a shared-bottleneck U-Net and tunable Lp loss.


<details>
  <summary>Details</summary>
Motivation: FCD lesions in epilepsy FLAIR MRI are subtle and scarce, making generative modeling prone to instability and memorization.

Method: SLIM-Diff uses a single shared-bottleneck U-Net for tight coupling between anatomy and lesion geometry, and a tunable Lp loss for optimization.

Result: x0-prediction is the strongest choice for joint synthesis; L1.5 improves image fidelity, while L2 preserves lesion mask morphology better.

Conclusion: SLIM-Diff effectively addresses instability in joint image-mask generative modeling for FCD lesions, with optimized loss-geometry tuning.

Abstract: Focal cortical dysplasia (FCD) lesions in epilepsy FLAIR MRI are subtle and scarce, making joint image--mask generative modeling prone to instability and memorization. We propose SLIM-Diff, a compact joint diffusion model whose main contributions are (i) a single shared-bottleneck U-Net that enforces tight coupling between anatomy and lesion geometry from a 2-channel image+mask representation, and (ii) loss-geometry tuning via a tunable $L_p$ objective. As an internal baseline, we include the canonical DDPM-style objective ($ε$-prediction with $L_2$ loss) and isolate the effect of prediction parameterization and $L_p$ geometry under a matched setup. Experiments show that $x_0$-prediction is consistently the strongest choice for joint synthesis, and that fractional sub-quadratic penalties ($L_{1.5}$) improve image fidelity while $L_2$ better preserves lesion mask morphology. Our code and model weights are available in https://github.com/MarioPasc/slim-diff

</details>


### [66] [Unifying Watermarking via Dimension-Aware Mapping](https://arxiv.org/abs/2602.03373)
*Jiale Meng,Runyi Hu,Jie Zhang,Zheming Lu,Ivor Tsang,Tianwei Zhang*

Main category: cs.CV

TL;DR: DiM is a multi-dimensional watermarking framework that unifies existing methods by modeling watermark information as payloads of varying dimensionalities, with embedding and extraction dimensions determining functionality.


<details>
  <summary>Details</summary>
Motivation: Deep watermarking methods share similar architectures but differ in behaviors. DiM aims to unify these methods by linking dimensional configuration to functional outcomes.

Method: DiM formulates watermarking as dimension-aware mapping, modeling payloads as 1D, 2D, or 3D structures. It explores same-dimensional (preserving structure) and cross-dimensional (enabling localization) mappings.

Result: Experiments show DiM's versatility in video watermarking—spatiotemporal tamper localization, local embedding control, and temporal recovery—without architectural changes.

Conclusion: DiM provides a unified framework for diverse watermarking behaviors by leveraging dimensional configurations of payloads and mappings.

Abstract: Deep watermarking methods often share similar encoder-decoder architectures, yet differ substantially in their functional behaviors. We propose DiM, a new multi-dimensional watermarking framework that formulates watermarking as a dimension-aware mapping problem, thereby unifying existing watermarking methods at the functional level. Under DiM, watermark information is modeled as payloads of different dimensionalities, including one-dimensional binary messages, two-dimensional spatial masks, and three-dimensional spatiotemporal structures. We find that the dimensional configuration of embedding and extraction largely determines the resulting watermarking behavior. Same-dimensional mappings preserve payload structure and support fine-grained control, while cross-dimensional mappings enable spatial or spatiotemporal localization. We instantiate DiM in the video domain, where spatiotemporal representations enable a broader set of dimension mappings. Experiments demonstrate that varying only the embedding and extraction dimensions, without architectural changes, leads to different watermarking capabilities, including spatiotemporal tamper localization, local embedding control, and recovery of temporal order under frame disruptions.

</details>


### [67] [From Vicious to Virtuous Cycles: Synergistic Representation Learning for Unsupervised Video Object-Centric Learning](https://arxiv.org/abs/2602.03390)
*Hyun Seok Seong,WonJun Moon,Jae-Pil Heo*

Main category: cs.CV

TL;DR: Synergistic Representation Learning (SRL) addresses the encoder-decoder discrepancy in unsupervised object-centric learning by mutual refinement, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: The conflict between sharp encoder attention maps and blurry decoder reconstructions creates a vicious cycle, hindering performance.

Method: SRL introduces mutual refinement between encoder and decoder, stabilized by a warm-up phase with slot regularization.

Result: SRL achieves state-of-the-art results on video object-centric learning benchmarks.

Conclusion: SRL successfully bridges the representational gap between encoder and decoder, improving unsupervised object-centric learning.

Abstract: Unsupervised object-centric learning models, particularly slot-based architectures, have shown great promise in decomposing complex scenes. However, their reliance on reconstruction-based training creates a fundamental conflict between the sharp, high-frequency attention maps of the encoder and the spatially consistent but blurry reconstruction maps of the decoder. We identify that this discrepancy gives rise to a vicious cycle: the noisy feature map from the encoder forces the decoder to average over possibilities and produce even blurrier outputs, while the gradient computed from blurry reconstruction maps lacks high-frequency details necessary to supervise encoder features. To break this cycle, we introduce Synergistic Representation Learning (SRL) that establishes a virtuous cycle where the encoder and decoder mutually refine one another. SRL leverages the encoder's sharpness to deblur the semantic boundary within the decoder output, while exploiting the decoder's spatial consistency to denoise the encoder's features. This mutual refinement process is stabilized by a warm-up phase with a slot regularization objective that initially allocates distinct entities per slot. By bridging the representational gap between the encoder and decoder, SRL achieves state-of-the-art results on video object-centric learning benchmarks. Codes are available at https://github.com/hynnsk/SRL.

</details>


### [68] [UnHype: CLIP-Guided Hypernetworks for Dynamic LoRA Unlearning](https://arxiv.org/abs/2602.03410)
*Piotr Wójcik,Maksym Petrenko,Wojciech Gromski,Przemysław Spurek,Maciej Zieba*

Main category: cs.CV

TL;DR: UnHype is a framework integrating hypernetworks into LoRA training for scalable and context-aware machine unlearning in diffusion models, outperforming traditional methods in tasks like object erasure and explicit content removal.


<details>
  <summary>Details</summary>
Motivation: Addressing concerns about misuse of diffusion models by enabling selective removal of harmful or disruptive content without degrading general generative capabilities.

Method: Incorporates hypernetworks into LoRA training to dynamically generate adaptive weights based on CLIP embeddings, ensuring context-aware and scalable unlearning.

Result: Effective in tasks like object erasure, celebrity erasure, and explicit content removal, with stable training behavior and versatile application in models like Stable Diffusion.

Conclusion: UnHype offers a robust solution for machine unlearning, balancing concept removal with generalization and scalability.

Abstract: Recent advances in large-scale diffusion models have intensified concerns about their potential misuse, particularly in generating realistic yet harmful or socially disruptive content. This challenge has spurred growing interest in effective machine unlearning, the process of selectively removing specific knowledge or concepts from a model without compromising its overall generative capabilities. Among various approaches, Low-Rank Adaptation (LoRA) has emerged as an effective and efficient method for fine-tuning models toward targeted unlearning. However, LoRA-based methods often exhibit limited adaptability to concept semantics and struggle to balance removing closely related concepts with maintaining generalization across broader meanings. Moreover, these methods face scalability challenges when multiple concepts must be erased simultaneously. To address these limitations, we introduce UnHype, a framework that incorporates hypernetworks into single- and multi-concept LoRA training. The proposed architecture can be directly plugged into Stable Diffusion as well as modern flow-based text-to-image models, where it demonstrates stable training behavior and effective concept control. During inference, the hypernetwork dynamically generates adaptive LoRA weights based on the CLIP embedding, enabling more context-aware, scalable unlearning. We evaluate UnHype across several challenging tasks, including object erasure, celebrity erasure, and explicit content removal, demonstrating its effectiveness and versatility. Repository: https://github.com/gmum/UnHype.

</details>


### [69] [Socratic-Geo: Synthetic Data Generation and Geometric Reasoning via Multi-Agent Interaction](https://arxiv.org/abs/2602.03414)
*Zhengbo Jiao,Shaobo Wang,Zifan Zhang,Wei Wang,Bing Zhao,Hu Wei,Linfeng Zhang*

Main category: cs.CV

TL;DR: Socratic-Geo autonomously improves geometric reasoning in MLLMs by dynamically coupling data synthesis with model learning, outperforming baselines with minimal seed data.


<details>
  <summary>Details</summary>
Motivation: Addressing the scarcity of high-quality image-text pairs for geometric reasoning in MLLMs, which human annotation can't scale and automated methods lack fidelity.

Method: Uses multi-agent interaction: Teacher generates parameterized scripts with feedback; Solver optimizes reasoning; Generator learns image generation from triplets.

Result: Achieves 49.11 on benchmarks with 1/4 baseline data and 42.4% on GenExam, surpassing Seedream-4.0 and nearing Gemini-2.5-Flash-Image.

Conclusion: Socratic-Geo sets a new state-of-the-art for geometric reasoning and visual generation in MLLMs, demonstrating efficiency and scalability.

Abstract: Multimodal Large Language Models (MLLMs) have significantly advanced vision-language understanding. However, even state-of-the-art models struggle with geometric reasoning, revealing a critical bottleneck: the extreme scarcity of high-quality image-text pairs. Human annotation is prohibitively expensive, while automated methods fail to ensure fidelity and training effectiveness. Existing approaches either passively adapt to available images or employ inefficient random exploration with filtering, decoupling generation from learning needs. We propose Socratic-Geo, a fully autonomous framework that dynamically couples data synthesis with model learning through multi-agent interaction. The Teacher agent generates parameterized Python scripts with reflective feedback (Reflect for solvability, RePI for visual validity), ensuring image-text pair purity. The Solver agent optimizes reasoning through preference learning, with failure paths guiding Teacher's targeted augmentation. Independently, the Generator learns image generation capabilities on accumulated "image-code-instruction" triplets, distilling programmatic drawing intelligence into visual generation. Starting from only 108 seed problems, Socratic-Solver achieves 49.11 on six benchmarks using one-quarter of baseline data, surpassing strong baselines by 2.43 points. Socratic-Generator achieves 42.4% on GenExam, establishing new state-of-the-art for open-source models, surpassing Seedream-4.0 (39.8%) and approaching Gemini-2.5-Flash-Image (43.1%).

</details>


### [70] [ConsistentRFT: Reducing Visual Hallucinations in Flow-based Reinforcement Fine-Tuning](https://arxiv.org/abs/2602.03425)
*Xiaofeng Tan,Jun Liu,Yuanting Fan,Bin-Bin Gao,Xi Jiang,Xiaochen Chen,Jinlong Peng,Chengjie Wang,Hongsong Wang,Feng Zheng*

Main category: cs.CV

TL;DR: The paper explores visual hallucinations in Reinforcement Fine-Tuning (RFT) of flow-based models and proposes ConsistentRFT, a framework to mitigate these issues.


<details>
  <summary>Details</summary>
Motivation: Visual hallucinations like over-optimized details and semantic misalignment arise in RFT, negatively impacting preference alignment. This work aims to understand and reduce these hallucinations.

Method: ConsistentRFT introduces Dynamic Granularity Rollout (DGR) for balanced exploration and Consistent Policy Gradient Optimization (CPGO) to maintain model consistency.

Result: ConsistentRFT reduces hallucinations by 49% (low-level) and 38% (high-level), outperforming baselines with a 5.1% improvement on out-of-domain metrics.

Conclusion: ConsistentRFT effectively addresses visual hallucinations in RFT, demonstrating significant improvements in model performance and consistency.

Abstract: Reinforcement Fine-Tuning (RFT) on flow-based models is crucial for preference alignment. However, they often introduce visual hallucinations like over-optimized details and semantic misalignment. This work preliminarily explores why visual hallucinations arise and how to reduce them. We first investigate RFT methods from a unified perspective, and reveal the core problems stemming from two aspects, exploration and exploitation: (1) limited exploration during stochastic differential equation (SDE) rollouts, leading to an over-emphasis on local details at the expense of global semantics, and (2) trajectory imitation process inherent in policy gradient methods, distorting the model's foundational vector field and its cross-step consistency. Building on this, we propose ConsistentRFT, a general framework to mitigate these hallucinations. Specifically, we design a Dynamic Granularity Rollout (DGR) mechanism to balance exploration between global semantics and local details by dynamically scheduling different noise sources. We then introduce a Consistent Policy Gradient Optimization (CPGO) that preserves the model's consistency by aligning the current policy with a more stable prior. Extensive experiments demonstrate that ConsistentRFT significantly mitigates visual hallucinations, achieving average reductions of 49\% for low-level and 38\% for high-level perceptual hallucinations. Furthermore, ConsistentRFT outperforms other RFT methods on out-of-domain metrics, showing an improvement of 5.1\% (v.s. the baseline's decrease of -0.4\%) over FLUX1.dev. This is \href{https://xiaofeng-tan.github.io/projects/ConsistentRFT}{Project Page}.

</details>


### [71] [Hierarchical Concept-to-Appearance Guidance for Multi-Subject Image Generation](https://arxiv.org/abs/2602.03448)
*Yijia Xu,Zihao Wang,Jinshi Cui*

Main category: cs.CV

TL;DR: The paper introduces Hierarchical Concept-to-Appearance Guidance (CAG) for multi-subject image generation, addressing identity inconsistency and compositional control issues in existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with identity inconsistency and limited compositional control due to implicit associations between text prompts and reference images.

Method: CAG provides explicit supervision from high-level concepts (using VAE dropout training) to fine-grained appearances (via correspondence-aware masked attention in Diffusion Transformer).

Result: The method achieves state-of-the-art performance, improving prompt following and subject consistency.

Conclusion: CAG effectively enhances multi-subject image generation by combining robust semantic signals and precise attribute binding.

Abstract: Multi-subject image generation aims to synthesize images that faithfully preserve the identities of multiple reference subjects while following textual instructions. However, existing methods often suffer from identity inconsistency and limited compositional control, as they rely on diffusion models to implicitly associate text prompts with reference images. In this work, we propose Hierarchical Concept-to-Appearance Guidance (CAG), a framework that provides explicit, structured supervision from high-level concepts to fine-grained appearances. At the conceptual level, we introduce a VAE dropout training strategy that randomly omits reference VAE features, encouraging the model to rely more on robust semantic signals from a Visual Language Model (VLM) and thereby promoting consistent concept-level generation in the absence of complete appearance cues. At the appearance level, we integrate the VLM-derived correspondences into a correspondence-aware masked attention module within the Diffusion Transformer (DiT). This module restricts each text token to attend only to its matched reference regions, ensuring precise attribute binding and reliable multi-subject composition. Extensive experiments demonstrate that our method achieves state-of-the-art performance on the multi-subject image generation, substantially improving prompt following and subject consistency.

</details>


### [72] [Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars](https://arxiv.org/abs/2602.01538)
*Youliang Zhang,Zhengguang Zhou,Zhentao Yu,Ziyao Huang,Teng Hu,Sen Liang,Guozhen Zhang,Ziqiao Peng,Shunkai Li,Yi Chen,Zixiang Zhou,Yuan Zhou,Qinglin Lu,Xiu Li*

Main category: cs.CV

TL;DR: InteractAvatar is a dual-stream framework for generating talking avatars with grounded human-object interactions, addressing perception and control-quality challenges.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack capability for grounded human-object interactions (GHOI) in talking avatars, requiring environmental perception and overcoming control-quality issues.

Method: Proposes InteractAvatar, featuring Perception and Interaction Module (PIM) for text-aligned motions and Audio-Interaction Aware Generation Module (AIM) for video synthesis, coupled with a motion-to-video aligner.

Result: The method successfully generates plausible GHOI videos for talking avatars, demonstrated via experiments and comparisons on the GroundedInter benchmark.

Conclusion: InteractAvatar effectively addresses GHOI challenges in talking avatar generation, introducing a novel framework and benchmark.

Abstract: Generating talking avatars is a fundamental task in video generation. Although existing methods can generate full-body talking avatars with simple human motion, extending this task to grounded human-object interaction (GHOI) remains an open challenge, requiring the avatar to perform text-aligned interactions with surrounding objects. This challenge stems from the need for environmental perception and the control-quality dilemma in GHOI generation. To address this, we propose a novel dual-stream framework, InteractAvatar, which decouples perception and planning from video synthesis for grounded human-object interaction. Leveraging detection to enhance environmental perception, we introduce a Perception and Interaction Module (PIM) to generate text-aligned interaction motions. Additionally, an Audio-Interaction Aware Generation Module (AIM) is proposed to synthesize vivid talking avatars performing object interactions. With a specially designed motion-to-video aligner, PIM and AIM share a similar network structure and enable parallel co-generation of motions and plausible videos, effectively mitigating the control-quality dilemma. Finally, we establish a benchmark, GroundedInter, for evaluating GHOI video generation. Extensive experiments and comparisons demonstrate the effectiveness of our method in generating grounded human-object interactions for talking avatars. Project page: https://interactavatar.github.io

</details>


### [73] [Contextualized Visual Personalization in Vision-Language Models](https://arxiv.org/abs/2602.03454)
*Yeongtak Oh,Sangwon Yu,Junsung Park,Han Cheol Moon,Jisoo Mok,Sungroh Yoon*

Main category: cs.CV

TL;DR: CoViP addresses personalized responses in VLMs by improving contextualized visual personalization through reinforcement learning and caption-augmented generation.


<details>
  <summary>Details</summary>
Motivation: Existing VLMs struggle to generate personalized responses due to their inability to associate visual inputs with users' accumulated visual-textual context.

Method: Proposes CoViP, a framework using reinforcement-learning-based post-training and caption-augmented generation for personalized image captioning.

Result: CoViP improves personalized image captioning and downstream tasks, outperforming existing VLMs.

Conclusion: CoViP advances robust and generalizable contextualized visual personalization in VLMs.

Abstract: Despite recent progress in vision-language models (VLMs), existing approaches often fail to generate personalized responses based on the user's specific experiences, as they lack the ability to associate visual inputs with a user's accumulated visual-textual context. We newly formalize this challenge as contextualized visual personalization, which requires the visual recognition and textual retrieval of personalized visual experiences by VLMs when interpreting new images. To address this issue, we propose CoViP, a unified framework that treats personalized image captioning as a core task for contextualized visual personalization and improves this capability through reinforcement-learning-based post-training and caption-augmented generation. We further introduce diagnostic evaluations that explicitly rule out textual shortcut solutions and verify whether VLMs truly leverage visual context. Extensive experiments demonstrate that existing open-source and proprietary VLMs exhibit substantial limitations, while CoViP not only improves personalized image captioning but also yields holistic gains across downstream personalization tasks. These results highlight CoViP as a crucial stage for enabling robust and generalizable contextualized visual personalization.

</details>


### [74] [Inlier-Centric Post-Training Quantization for Object Detection Models](https://arxiv.org/abs/2602.03472)
*Minsu Kim,Dongyeun Lee,Jaemyung Yu,Jiwan Hur,Giseop Kim,Junmo Kim*

Main category: cs.CV

TL;DR: InlierQ is a label-free, efficient quantization method for object detection that separates anomalies from informative inliers to reduce quantization error.


<details>
  <summary>Details</summary>
Motivation: Object detection's computational demands slow deployment and increase power usage. Task-irrelevant anomalies complicate quantization by skewing activation distributions.

Method: InlierQ uses gradient-aware volume saliency scores, classifies volumes as inliers or anomalies, and fits a posterior distribution via EM algorithm to suppress anomalies.

Result: InlierQ reduces quantization error significantly on COCO and nuScenes benchmarks for 2D/3D camera-based and LiDAR-based object detection.

Conclusion: InlierQ effectively preserves informative features while suppressing anomalies, offering a practical and efficient quantization solution for object detection.

Abstract: Object detection is pivotal in computer vision, yet its immense computational demands make deployment slow and power-hungry, motivating quantization. However, task-irrelevant morphologies such as background clutter and sensor noise induce redundant activations (or anomalies). These anomalies expand activation ranges and skew activation distributions toward task-irrelevant responses, complicating bit allocation and weakening the preservation of informative features. Without a clear criterion to distinguish anomalies, suppressing them can inadvertently discard useful information. To address this, we present InlierQ, an inlier-centric post-training quantization approach that separates anomalies from informative inliers. InlierQ computes gradient-aware volume saliency scores, classifies each volume as an inlier or anomaly, and fits a posterior distribution over these scores using the Expectation-Maximization (EM) algorithm. This design suppresses anomalies while preserving informative features. InlierQ is label-free, drop-in, and requires only 64 calibration samples. Experiments on the COCO and nuScenes benchmarks show consistent reductions in quantization error for camera-based (2D and 3D) and LiDAR-based (3D) object detection.

</details>


### [75] [Decoupling Skeleton and Flesh: Efficient Multimodal Table Reasoning with Disentangled Alignment and Structure-aware Guidance](https://arxiv.org/abs/2602.03491)
*Yingjie Zhu,Xuefeng Bai,Kehai Chen,Yang Xiang,Youcheng Pan,Xiaoqiang Zhou,Min Zhang*

Main category: cs.CV

TL;DR: The paper introduces DiSCo and Table-GLS frameworks to enhance LVLMs' table reasoning without expensive training or external tools, improving generalization.


<details>
  <summary>Details</summary>
Motivation: Existing methods for table reasoning rely on costly supervised training or external tools, limiting efficiency and scalability. This work aims to adapt LVLMs to tables with minimal annotation and no external tools.

Method: Introduces DiSCo for disentangled structure-content alignment and Table-GLS for global-to-local structure-guided reasoning.

Result: The frameworks improve LVLMs' table understanding and reasoning, especially generalizing to unseen structures.

Conclusion: DiSCo and Table-GLS offer efficient, scalable solutions for LVLMs to reason over tables without expensive resources.

Abstract: Reasoning over table images remains challenging for Large Vision-Language Models (LVLMs) due to complex layouts and tightly coupled structure-content information. Existing solutions often depend on expensive supervised training, reinforcement learning, or external tools, limiting efficiency and scalability. This work addresses a key question: how to adapt LVLMs to table reasoning with minimal annotation and no external tools? Specifically, we first introduce DiSCo, a Disentangled Structure-Content alignment framework that explicitly separates structural abstraction from semantic grounding during multimodal alignment, efficiently adapting LVLMs to tables structures. Building on DiSCo, we further present Table-GLS, a Global-to-Local Structure-guided reasoning framework that performs table reasoning via structured exploration and evidence-grounded inference. Extensive experiments across diverse benchmarks demonstrate that our framework efficiently enhances LVLM's table understanding and reasoning capabilities, particularly generalizing to unseen table structures.

</details>


### [76] [Semantic Routing: Exploring Multi-Layer LLM Feature Weighting for Diffusion Transformers](https://arxiv.org/abs/2602.03510)
*Bozhou Li,Yushuo Guan,Haolin Li,Bohan Zeng,Yiyan Ji,Yue Ding,Pengfei Wan,Kun Gai,Yuanxing Zhang,Wentao Zhang*

Main category: cs.CV

TL;DR: The paper introduces a normalized convex fusion framework with lightweight gates to better align multi-layer LLM hidden states with DiT generation dynamics, improving text-image alignment and compositional generation.


<details>
  <summary>Details</summary>
Motivation: To address the static and single-layer use of LLMs in DiT-based text-to-image models, despite the semantic hierarchy across LLM layers and dynamic denoising processes.

Method: Proposes a unified normalized convex fusion framework with lightweight gates for systematic organization of multi-layer LLM states via time-wise, depth-wise, and joint fusion.

Result: Depth-wise Semantic Routing outperforms other strategies, improving alignment (e.g., +9.97 on GenAI-Bench Counting), while time-wise fusion can degrade fidelity due to train-inference mismatch.

Conclusion: Depth-wise routing is an effective baseline, emphasizing the need for trajectory-aware signals for robust time-dependent conditioning.

Abstract: Recent DiT-based text-to-image models increasingly adopt LLMs as text encoders, yet text conditioning remains largely static and often utilizes only a single LLM layer, despite pronounced semantic hierarchy across LLM layers and non-stationary denoising dynamics over both diffusion time and network depth. To better match the dynamic process of DiT generation and thereby enhance the diffusion model's generative capability, we introduce a unified normalized convex fusion framework equipped with lightweight gates to systematically organize multi-layer LLM hidden states via time-wise, depth-wise, and joint fusion. Experiments establish Depth-wise Semantic Routing as the superior conditioning strategy, consistently improving text-image alignment and compositional generation (e.g., +9.97 on the GenAI-Bench Counting task). Conversely, we find that purely time-wise fusion can paradoxically degrade visual generation fidelity. We attribute this to a train-inference trajectory mismatch: under classifier-free guidance, nominal timesteps fail to track the effective SNR, causing semantically mistimed feature injection during inference. Overall, our results position depth-wise routing as a strong and effective baseline and highlight the critical need for trajectory-aware signals to enable robust time-dependent conditioning.

</details>


### [77] [Interpretable Logical Anomaly Classification via Constraint Decomposition and Instruction Fine-Tuning](https://arxiv.org/abs/2602.03530)
*Xufei Zhang,Xinjiao Zhou,Ziling Deng,Dongdong Geng,Jianxiong Wang*

Main category: cs.CV

TL;DR: LogiCls framework unifies anomaly detection and logical violation classification for industrial images


<details>
  <summary>Details</summary>
Motivation: Prior anomaly detection methods lack fine-grained violation classification, limiting their practical value for quality assurance.

Method: Proposes LogiCls, a vision-language framework that decomposes logical constraints into verifiable subqueries. Uses data-centric instruction synthesis for chain-of-thought supervision and augmentations to adapt vision-language models.

Result: LogiCls provides robust, interpretable, and accurate classification of logical anomalies, including violation categories and evidence trails.

Conclusion: The approach effectively bridges the gap between anomaly detection and logical rule violation classification, enhancing industrial quality assurance.

Abstract: Logical anomalies are violations of predefined constraints on object quantity, spatial layout, and compositional relationships in industrial images. While prior work largely treats anomaly detection as a binary decision, such formulations cannot indicate which logical rule is broken and therefore offer limited value for quality assurance. We introduce Logical Anomaly Classification (LAC), a task that unifies anomaly detection and fine-grained violation classification in a single inference step. To tackle LAC, we propose LogiCls, a vision-language framework that decomposes complex logical constraints into a sequence of verifiable subqueries. We further present a data-centric instruction synthesis pipeline that generates chain-of-thought (CoT) supervision for these subqueries, coupling precise grounding annotations with diverse image-text augmentations to adapt vision language models (VLMs) to logic-sensitive reasoning. Training is stabilized by a difficulty-aware resampling strategy that emphasizes challenging subqueries and long tail constraint types. Extensive experiments demonstrate that LogiCls delivers robust, interpretable, and accurate industrial logical anomaly classification, providing both the predicted violation categories and their evidence trails.

</details>


### [78] [PnP-U3D: Plug-and-Play 3D Framework Bridging Autoregression and Diffusion for Unified Understanding and Generation](https://arxiv.org/abs/2602.03533)
*Yongwei Chen,Tianyi Wei,Yushi Lan,Zhaoyang Lyu,Shangchen Zhou,Xudong Xu,Xingang Pan*

Main category: cs.CV

TL;DR: The paper introduces a unified framework combining autoregression and diffusion for 3D understanding and generation, addressing limitations of existing methods.


<details>
  <summary>Details</summary>
Motivation: To extend unified frameworks from 2D to 3D while avoiding performance degradation and high training costs of existing autoregressive approaches.

Method: Combines autoregressive next-token prediction for 3D understanding with continuous diffusion for 3D generation, linked by a lightweight transformer.

Result: Achieves state-of-the-art performance across 3D understanding, generation, and editing benchmarks.

Conclusion: The AR+diffusion model is a promising direction for general-purpose 3D intelligence.

Abstract: The rapid progress of large multimodal models has inspired efforts toward unified frameworks that couple understanding and generation. While such paradigms have shown remarkable success in 2D, extending them to 3D remains largely underexplored. Existing attempts to unify 3D tasks under a single autoregressive (AR) paradigm lead to significant performance degradation due to forced signal quantization and prohibitive training cost. Our key insight is that the essential challenge lies not in enforcing a unified autoregressive paradigm, but in enabling effective information interaction between generation and understanding while minimally compromising their inherent capabilities and leveraging pretrained models to reduce training cost. Guided by this perspective, we present the first unified framework for 3D understanding and generation that combines autoregression with diffusion. Specifically, we adopt an autoregressive next-token prediction paradigm for 3D understanding, and a continuous diffusion paradigm for 3D generation. A lightweight transformer bridges the feature space of large language models and the conditional space of 3D diffusion models, enabling effective cross-modal information exchange while preserving the priors learned by standalone models. Extensive experiments demonstrate that our framework achieves state-of-the-art performance across diverse 3D understanding and generation benchmarks, while also excelling in 3D editing tasks. These results highlight the potential of unified AR+diffusion models as a promising direction for building more general-purpose 3D intelligence.

</details>


### [79] [Constrained Dynamic Gaussian Splatting](https://arxiv.org/abs/2602.03538)
*Zihan Zheng,Zhenglong Wu,Xuanxuan Wang,Houqiang Zhong,Xiaoyun Zhang,Qiang Hu,Guangtao Zhai,Wenjun Zhang*

Main category: cs.CV

TL;DR: CDGS introduces a budget-constrained optimization framework for dynamic Gaussian Splatting, using a differentiable controller and adaptive allocation to balance memory and rendering quality.


<details>
  <summary>Details</summary>
Motivation: To address the trade-off between memory consumption and rendering quality in dynamic Gaussian Splatting, especially for edge devices.

Method: Uses a differentiable budget controller with multi-modal importance scores, adaptive allocation for static/dynamic elements, and a three-phase training strategy.

Result: Achieves strict adherence to hardware constraints (<2% error) and 3x better compression than state-of-the-art methods.

Conclusion: CDGS effectively optimizes dynamic scene reconstruction under budget constraints, advancing rate-distortion performance.

Abstract: While Dynamic Gaussian Splatting enables high-fidelity 4D reconstruction, its deployment is severely hindered by a fundamental dilemma: unconstrained densification leads to excessive memory consumption incompatible with edge devices, whereas heuristic pruning fails to achieve optimal rendering quality under preset Gaussian budgets. In this work, we propose Constrained Dynamic Gaussian Splatting (CDGS), a novel framework that formulates dynamic scene reconstruction as a budget-constrained optimization problem to enforce a strict, user-defined Gaussian budget during training. Our key insight is to introduce a differentiable budget controller as the core optimization driver. Guided by a multi-modal unified importance score, this controller fuses geometric, motion, and perceptual cues for precise capacity regulation. To maximize the utility of this fixed budget, we further decouple the optimization of static and dynamic elements, employing an adaptive allocation mechanism that dynamically distributes capacity based on motion complexity. Furthermore, we implement a three-phase training strategy to seamlessly integrate these constraints, ensuring precise adherence to the target count. Coupled with a dual-mode hybrid compression scheme, CDGS not only strictly adheres to hardware constraints (error < 2%}) but also pushes the Pareto frontier of rate-distortion performance. Extensive experiments demonstrate that CDGS delivers optimal rendering quality under varying capacity limits, achieving over 3x compression compared to state-of-the-art methods.

</details>


### [80] [Cut to the Mix: Simple Data Augmentation Outperforms Elaborate Ones in Limited Organ Segmentation Datasets](https://arxiv.org/abs/2602.03555)
*Chang Liu,Fuxin Fan,Annette Schwarz,Andreas Maier*

Main category: cs.CV

TL;DR: The paper explores inter-image data augmentation (DA) strategies to improve multi-organ segmentation with limited data, finding CutMix, CarveMix, and AnatoMix effective in boosting performance.


<details>
  <summary>Details</summary>
Motivation: Limited annotated clinical data hinders deep learning model training for multi-organ segmentation, and traditional DA methods are insufficient. Exploring inter-image DA can help bridge this gap.

Method: Four inter-image DA strategies (CutMix, CarveMix, ObjectAug, AnatoMix) were tested on two organ segmentation datasets, comparing their performance with nnUNet without DA.

Result: CutMix, CarveMix, and AnatoMix improved average dice scores by 4.9, 2.0, and 1.9, respectively, outperforming the baseline. Adding traditional DA further enhanced results.

Conclusion: CutMix is a robust and simple DA strategy for multi-organ segmentation, even when producing 'wrong' images, significantly improving model performance.

Abstract: Multi-organ segmentation is a widely applied clinical routine and automated organ segmentation tools dramatically improve the pipeline of the radiologists. Recently, deep learning (DL) based segmentation models have shown the capacity to accomplish such a task. However, the training of the segmentation networks requires large amount of data with manual annotations, which is a major concern due to the data scarcity from clinic. Working with limited data is still common for researches on novel imaging modalities. To enhance the effectiveness of DL models trained with limited data, data augmentation (DA) is a crucial regularization technique. Traditional DA (TDA) strategies focus on basic intra-image operations, i.e. generating images with different orientations and intensity distributions. In contrast, the interimage and object-level DA operations are able to create new images from separate individuals. However, such DA strategies are not well explored on the task of multi-organ segmentation. In this paper, we investigated four possible inter-image DA strategies: CutMix, CarveMix, ObjectAug and AnatoMix, on two organ segmentation datasets. The result shows that CutMix, CarveMix and AnatoMix can improve the average dice score by 4.9, 2.0 and 1.9, compared with the state-of-the-art nnUNet without DA strategies. These results can be further improved by adding TDA strategies. It is revealed in our experiments that Cut-Mix is a robust but simple DA strategy to drive up the segmentation performance for multi-organ segmentation, even when CutMix produces intuitively 'wrong' images. Our implementation is publicly available for future benchmarks.

</details>


### [81] [ELIQ: A Label-Free Framework for Quality Assessment of Evolving AI-Generated Images](https://arxiv.org/abs/2602.03558)
*Xinyue Li,Zhiming Xu,Zhichao Zhang,Zhaolin Cai,Sijing Wu,Xiongkuo Min,Yitong Chen,Guangtao Zhai*

Main category: cs.CV

TL;DR: ELIQ is a label-free framework for assessing quality and prompt-image alignment in evolving AI-generated images, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To address the unreliability of older labels for newer AI-generated images due to rapid advancements in generative models.

Method: Automatically constructs positive and negative pairs for supervision, adapts a pre-trained multimodal model via instruction tuning, and predicts quality using gated fusion and a Quality Query Transformer.

Result: Consistently outperforms existing label-free methods and generalizes to user-generated content without modification.

Conclusion: ELIQ enables scalable, label-free quality assessment for evolving generative models and will release code upon publication.

Abstract: Generative text-to-image models are advancing at an unprecedented pace, continuously shifting the perceptual quality ceiling and rendering previously collected labels unreliable for newer generations. To address this, we present ELIQ, a Label-free Framework for Quality Assessment of Evolving AI-generated Images. Specifically, ELIQ focuses on visual quality and prompt-image alignment, automatically constructs positive and aspect-specific negative pairs to cover both conventional distortions and AIGC-specific distortion modes, enabling transferable supervision without human annotations. Building on these pairs, ELIQ adapts a pre-trained multimodal model into a quality-aware critic via instruction tuning and predicts two-dimensional quality using lightweight gated fusion and a Quality Query Transformer. Experiments across multiple benchmarks demonstrate that ELIQ consistently outperforms existing label-free methods, generalizes from AI-generated content (AIGC) to user-generated content (UGC) scenarios without modification, and paves the way for scalable and label-free quality assessment under continuously evolving generative models. The code will be released upon publication.

</details>


### [82] [SlowFocus: Enhancing Fine-grained Temporal Understanding in Video LLM](https://arxiv.org/abs/2602.03589)
*Ming Nie,Dan Ding,Chunwei Wang,Yuanfan Guo,Jianhua Han,Hang Xu,Li Zhang*

Main category: cs.CV

TL;DR: The paper introduces SlowFocus, a mechanism enhancing video LLMs' ability to retain both frame-level semantics and video-level temporal information for fine-grained video understanding, validated by a new benchmark, FineAction-CGR.


<details>
  <summary>Details</summary>
Motivation: Current video LLMs struggle to balance high-quality frame-level semantics and comprehensive temporal information, limiting fine-grained video understanding.

Method: SlowFocus identifies query-related temporal segments, densely samples them for local features, and uses multi-frequency mixing attention to combine these with global contexts. Training strategies and the FineAction-CGR benchmark are introduced.

Result: SlowFocus outperforms on existing benchmarks and FineAction-CGR, showing improved temporal comprehension.

Conclusion: SlowFocus effectively addresses the limitations of current Vid-LLMs, enabling better fine-grained video understanding.

Abstract: Large language models (LLMs) have demonstrated exceptional capabilities in text understanding, which has paved the way for their expansion into video LLMs (Vid-LLMs) to analyze video data. However, current Vid-LLMs struggle to simultaneously retain high-quality frame-level semantic information (i.e., a sufficient number of tokens per frame) and comprehensive video-level temporal information (i.e., an adequate number of sampled frames per video). This limitation hinders the advancement of Vid-LLMs towards fine-grained video understanding. To address this issue, we introduce the SlowFocus mechanism, which significantly enhances the equivalent sampling frequency without compromising the quality of frame-level visual tokens. SlowFocus begins by identifying the query-related temporal segment based on the posed question, then performs dense sampling on this segment to extract local high-frequency features. A multi-frequency mixing attention module is further leveraged to aggregate these local high-frequency details with global low-frequency contexts for enhanced temporal comprehension. Additionally, to tailor Vid-LLMs to this innovative mechanism, we introduce a set of training strategies aimed at bolstering both temporal grounding and detailed temporal reasoning capabilities. Furthermore, we establish FineAction-CGR, a benchmark specifically devised to assess the ability of Vid-LLMs to process fine-grained temporal understanding tasks. Comprehensive experiments demonstrate the superiority of our mechanism across both existing public video understanding benchmarks and our proposed FineAction-CGR.

</details>


### [83] [High-Resolution Underwater Camouflaged Object Detection: GBU-UCOD Dataset and Topology-Aware and Frequency-Decoupled Networks](https://arxiv.org/abs/2602.03591)
*Wenji Wu,Shuo Ye,Yiyu Liu,Jiguang He,Zhuo Wang,Zitong Yu*

Main category: cs.CV

TL;DR: DeepTopo-Net integrates topology-aware modeling and frequency-decoupled perception for underwater camouflaged object detection, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges like topological fragmentation of slender creatures and subtle feature extraction of transparent organisms in underwater environments.

Method: Proposes DeepTopo-Net with Water-Conditioned Adaptive Perceptor (WCAP) for dynamic convolutional sampling and Abyssal-Topology Refinement Module (ATRM) for structural connectivity.

Result: Achieves state-of-the-art performance on MAS3K, RMAS, and GBU-UCOD datasets, preserving morphological integrity of underwater patterns.

Conclusion: DeepTopo-Net is effective for UCOD, with datasets and code made publicly available.

Abstract: Underwater Camouflaged Object Detection (UCOD) is a challenging task due to the extreme visual similarity between targets and backgrounds across varying marine depths. Existing methods often struggle with topological fragmentation of slender creatures in the deep sea and the subtle feature extraction of transparent organisms. In this paper, we propose DeepTopo-Net, a novel framework that integrates topology-aware modeling with frequency-decoupled perception. To address physical degradation, we design the Water-Conditioned Adaptive Perceptor (WCAP), which employs Riemannian metric tensors to dynamically deform convolutional sampling fields. Furthermore, the Abyssal-Topology Refinement Module (ATRM) is developed to maintain the structural connectivity of spindly targets through skeletal priors. Specifically, we first introduce GBU-UCOD, the first high-resolution (2K) benchmark tailored for marine vertical zonation, filling the data gap for hadal and abyssal zones. Extensive experiments on MAS3K, RMAS, and our proposed GBU-UCOD datasets demonstrate that DeepTopo-Net achieves state-of-the-art performance, particularly in preserving the morphological integrity of complex underwater patterns. The datasets and codes will be released at https://github.com/Wuwenji18/GBU-UCOD.

</details>


### [84] [TIPS Over Tricks: Simple Prompts for Effective Zero-shot Anomaly Detection](https://arxiv.org/abs/2602.03594)
*Alireza Salehi,Ehsan Karami,Sepehr Noey,Sahand Noey,Makoto Yamada,Reshad Hosseini,Mohammad Sabokrou*

Main category: cs.CV

TL;DR: Zero-shot anomaly detection (ZSAD) using vision-language models (VLMs) is improved by TIPS, a spatially aware VLM, addressing CLIP's limitations with decoupled prompts and local feature integration.


<details>
  <summary>Details</summary>
Motivation: Current ZSAD methods using CLIP suffer from spatial misalignment and weak sensitivity to fine-grained anomalies, prompting exploration of alternative VLMs.

Method: Uses TIPS (a VLM with spatially aware training) and introduces decoupled prompts—fixed for detection, learnable for localization—while injecting local features globally.

Result: Improves image-level performance by 1.1-3.9% and pixel-level by 1.5-6.9% across seven datasets, demonstrating strong generalization.

Conclusion: TIPS-based pipeline outperforms CLIP-specific methods, offering a lean yet effective solution for ZSAD.

Abstract: Anomaly detection identifies departures from expected behavior in safety-critical settings. When target-domain normal data are unavailable, zero-shot anomaly detection (ZSAD) leverages vision-language models (VLMs). However, CLIP's coarse image-text alignment limits both localization and detection due to (i) spatial misalignment and (ii) weak sensitivity to fine-grained anomalies; prior work compensates with complex auxiliary modules yet largely overlooks the choice of backbone. We revisit the backbone and use TIPS-a VLM trained with spatially aware objectives. While TIPS alleviates CLIP's issues, it exposes a distributional gap between global and local features. We address this with decoupled prompts-fixed for image-level detection and learnable for pixel-level localization-and by injecting local evidence into the global score. Without CLIP-specific tricks, our TIPS-based pipeline improves image-level performance by 1.1-3.9% and pixel-level by 1.5-6.9% across seven industrial datasets, delivering strong generalization with a lean architecture. Code is available at github.com/AlirezaSalehy/Tipsomaly.

</details>


### [85] [Refer-Agent: A Collaborative Multi-Agent System with Reasoning and Reflection for Referring Video Object Segmentation](https://arxiv.org/abs/2602.03595)
*Haichao Jiang,Tianming Liang,Wei-Shi Zheng,Jian-Fang Hu*

Main category: cs.CV

TL;DR: Refer-Agent is a multi-agent system for RVOS that improves performance through step-by-step reasoning and reflection mechanisms, outperforming existing methods without requiring fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Current RVOS methods depend heavily on supervised fine-tuning or underperform in zero-shot settings, necessitating a more flexible and scalable solution.

Method: Refer-Agent uses alternating reasoning-reflection mechanisms, a Coarse-to-Fine frame selection strategy, Dynamic Focus Layout, and Chain-of-Reflection for self-verification.

Result: Refer-Agent outperforms state-of-the-art methods on five benchmarks and integrates new MLLMs without fine-tuning.

Conclusion: Refer-Agent offers a scalable and high-performing alternative to traditional RVOS methods, eliminating the need for fine-tuning.

Abstract: Referring Video Object Segmentation (RVOS) aims to segment objects in videos based on textual queries. Current methods mainly rely on large-scale supervised fine-tuning (SFT) of Multi-modal Large Language Models (MLLMs). However, this paradigm suffers from heavy data dependence and limited scalability against the rapid evolution of MLLMs. Although recent zero-shot approaches offer a flexible alternative, their performance remains significantly behind SFT-based methods, due to the straightforward workflow designs. To address these limitations, we propose \textbf{Refer-Agent}, a collaborative multi-agent system with alternating reasoning-reflection mechanisms. This system decomposes RVOS into step-by-step reasoning process. During reasoning, we introduce a Coarse-to-Fine frame selection strategy to ensure the frame diversity and textual relevance, along with a Dynamic Focus Layout that adaptively adjusts the agent's visual focus. Furthermore, we propose a Chain-of-Reflection mechanism, which employs a Questioner-Responder pair to generate a self-reflection chain, enabling the system to verify intermediate results and generates feedback for next-round reasoning refinement. Extensive experiments on five challenging benchmarks demonstrate that Refer-Agent significantly outperforms state-of-the-art methods, including both SFT-based models and zero-shot approaches. Moreover, Refer-Agent is flexible and enables fast integration of new MLLMs without any additional fine-tuning costs. Code will be released.

</details>


### [86] [A Lightweight Library for Energy-Based Joint-Embedding Predictive Architectures](https://arxiv.org/abs/2602.03604)
*Basile Terver,Randall Balestriero,Megi Dervishi,David Fan,Quentin Garrido,Tushar Nagarajan,Koustuv Sinha,Wancong Zhang,Mike Rabbat,Yann LeCun,Amir Bar*

Main category: cs.CV

TL;DR: EB-JEPA is an open-source library for learning representations and world models using Joint-Embedding Predictive Architectures (JEPAs), focusing on modular implementations suitable for image, video, and action-conditioned tasks.


<details>
  <summary>Details</summary>
Motivation: The goal is to simplify energy-based self-supervised learning by providing modular, efficient implementations that avoid pitfalls of generative modeling while capturing semantically meaningful features.

Method: The library implements JEPAs, which predict in representation space rather than pixel space, and includes examples for image-level (CIFAR-10), video (Moving MNIST), and action-conditioned (Two Rooms) tasks. Training is optimized for single-GPU setups.

Result: EB-JEPA achieves 91% accuracy on CIFAR-10 probing, demonstrates temporal modeling on Moving MNIST, and reaches a 97% planning success rate on the Two Rooms task. Ablations highlight key regularization components.

Conclusion: EB-JEPA successfully transfers representation learning techniques across tasks, proving JEPAs effective for scalable self-supervised learning while maintaining simplicity and accessibility.

Abstract: We present EB-JEPA, an open-source library for learning representations and world models using Joint-Embedding Predictive Architectures (JEPAs). JEPAs learn to predict in representation space rather than pixel space, avoiding the pitfalls of generative modeling while capturing semantically meaningful features suitable for downstream tasks. Our library provides modular, self-contained implementations that illustrate how representation learning techniques developed for image-level self-supervised learning can transfer to video, where temporal dynamics add complexity, and ultimately to action-conditioned world models, where the model must additionally learn to predict the effects of control inputs. Each example is designed for single-GPU training within a few hours, making energy-based self-supervised learning accessible for research and education. We provide ablations of JEA components on CIFAR-10. Probing these representations yields 91% accuracy, indicating that the model learns useful features. Extending to video, we include a multi-step prediction example on Moving MNIST that demonstrates how the same principles scale to temporal modeling. Finally, we show how these representations can drive action-conditioned world models, achieving a 97% planning success rate on the Two Rooms navigation task. Comprehensive ablations reveal the critical importance of each regularization component for preventing representation collapse. Code is available at https://github.com/facebookresearch/eb_jepa.

</details>


### [87] [KTV: Keyframes and Key Tokens Selection for Efficient Training-Free Video LLMs](https://arxiv.org/abs/2602.03615)
*Baiyang Song,Jun Peng,Yuxin Zhang,Guangyao Chen,Feidiao Yang,Jianyuan Guo*

Main category: cs.CV

TL;DR: KTV is a two-stage framework for training-free video understanding that reduces redundancy and computational overhead by selecting keyframes and pruning visual tokens, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current training-free video understanding methods suffer from visual redundancy, high computational costs, and biases in keyframe selection, leading to suboptimal performance.

Method: KTV uses a two-stage approach: (1) question-agnostic keyframe selection via clustering frame-level features, and (2) key visual token selection to prune redundant tokens.

Result: KTV achieves 44.8% accuracy on MLVU-Test with only 504 visual tokens for a 60-minute video, outperforming state-of-the-art training-free methods and some training-based approaches.

Conclusion: KTV effectively addresses redundancy and computational challenges in training-free video understanding, demonstrating superior performance and efficiency.

Abstract: Training-free video understanding leverages the strong image comprehension capabilities of pre-trained vision language models (VLMs) by treating a video as a sequence of static frames, thus obviating the need for costly video-specific training. However, this paradigm often suffers from severe visual redundancy and high computational overhead, especially when processing long videos. Crucially, existing keyframe selection strategies, especially those based on CLIP similarity, are prone to biases and may inadvertently overlook critical frames, resulting in suboptimal video comprehension. To address these significant challenges, we propose \textbf{KTV}, a novel two-stage framework for efficient and effective training-free video understanding. In the first stage, KTV performs question-agnostic keyframe selection by clustering frame-level visual features, yielding a compact, diverse, and representative subset of frames that mitigates temporal redundancy. In the second stage, KTV applies key visual token selection, pruning redundant or less informative tokens from each selected keyframe based on token importance and redundancy, which significantly reduces the number of tokens fed into the LLM. Extensive experiments on the Multiple-Choice VideoQA task demonstrate that KTV outperforms state-of-the-art training-free baselines while using significantly fewer visual tokens, \emph{e.g.}, only 504 visual tokens for a 60-min video with 10800 frames, achieving $44.8\%$ accuracy on the MLVU-Test benchmark. In particular, KTV also exceeds several training-based approaches on certain benchmarks.

</details>


### [88] [Quasi-multimodal-based pathophysiological feature learning for retinal disease diagnosis](https://arxiv.org/abs/2602.03622)
*Lu Zhang,Huizhen Yu,Zuowei Wang,Fu Gui,Yatu Guo,Wei Zhang,Mengyu Jia*

Main category: cs.CV

TL;DR: A unified framework synthesizes and fuses multimodal retinal data (FFA, MSI, saliency maps) for disease classification/grading, outperforming state-of-the-art methods in accuracy and scalability.


<details>
  <summary>Details</summary>
Motivation: Multimodal retinal diagnosis faces challenges like data heterogeneity and invasiveness, necessitating a unified approach for effective disease identification.

Method: Modality-specific representations are learned via parallel models, adaptively calibrated and fused for downstream tasks, supported by visualizations.

Result: Superior performance in multi-label classification (F1:0.683, AUC:0.953) and diabetic retinopathy grading (Accuracy:0.842, Kappa:0.861).

Conclusion: The framework improves retinal disease screening accuracy/efficiency and offers scalable data augmentation for medical imaging.

Abstract: Retinal diseases spanning a broad spectrum can be effectively identified and diagnosed using complementary signals from multimodal data. However, multimodal diagnosis in ophthalmic practice is typically challenged in terms of data heterogeneity, potential invasiveness, registration complexity, and so on. As such, a unified framework that integrates multimodal data synthesis and fusion is proposed for retinal disease classification and grading. Specifically, the synthesized multimodal data incorporates fundus fluorescein angiography (FFA), multispectral imaging (MSI), and saliency maps that emphasize latent lesions as well as optic disc/cup regions. Parallel models are independently trained to learn modality-specific representations that capture cross-pathophysiological signatures. These features are then adaptively calibrated within and across modalities to perform information pruning and flexible integration according to downstream tasks. The proposed learning system is thoroughly interpreted through visualizations in both image and feature spaces. Extensive experiments on two public datasets demonstrated the superiority of our approach over state-of-the-art ones in the tasks of multi-label classification (F1-score: 0.683, AUC: 0.953) and diabetic retinopathy grading (Accuracy:0.842, Kappa: 0.861). This work not only enhances the accuracy and efficiency of retinal disease screening but also offers a scalable framework for data augmentation across various medical imaging modalities.

</details>


### [89] [Multi-Objective Optimization for Synthetic-to-Real Style Transfer](https://arxiv.org/abs/2602.03625)
*Estelle Chigot,Thomas Oberlin,Manon Huguenin,Dennis Wilson*

Main category: cs.CV

TL;DR: The paper addresses the domain gap between synthetic and real images in semantic segmentation by optimizing style transfer pipelines using multi-objective genetic algorithms. It focuses on balancing structural coherence and style similarity efficiently.


<details>
  <summary>Details</summary>
Motivation: Semantic segmentation models trained on synthetic images often underperform on real images due to domain differences. Style transfer can bridge this gap, but selecting effective transformations is challenging due to combinatorial complexity.

Method: The authors use multi-objective genetic algorithms to optimize style transfer pipelines, employing paired-image metrics for rapid evaluation during evolution. The optimized pipelines are then evaluated using distributional metrics and segmentation performance.

Result: The approach successfully generates diverse augmentation pipelines tailored to different objectives, improving adaptation from synthetic datasets (GTA5) to real ones (Cityscapes, ACDC), especially under adverse conditions.

Conclusion: Evolutionary optimization proves effective for formulating style transfer as a sequencing problem, with efficient metrics enabling practical search in this space. The source code is publicly available for further research.

Abstract: Semantic segmentation networks require large amounts of pixel-level annotated data, which are costly to obtain for real-world images. Computer graphics engines can generate synthetic images alongside their ground-truth annotations. However, models trained on such images can perform poorly on real images due to the domain gap between real and synthetic images. Style transfer methods can reduce this difference by applying a realistic style to synthetic images. Choosing effective data transformations and their sequence is difficult due to the large combinatorial search space of style transfer operators. Using multi-objective genetic algorithms, we optimize pipelines to balance structural coherence and style similarity to target domains. We study the use of paired-image metrics on individual image samples during evolution to enable rapid pipeline evaluation, as opposed to standard distributional metrics that require the generation of many images. After optimization, we evaluate the resulting Pareto front using distributional metrics and segmentation performance. We apply this approach to standard datasets in synthetic-to-real domain adaptation: from the video game GTA5 to real image datasets Cityscapes and ACDC, focusing on adverse conditions. Results demonstrate that evolutionary algorithms can propose diverse augmentation pipelines adapted to different objectives. The contribution of this work is the formulation of style transfer as a sequencing problem suitable for evolutionary optimization and the study of efficient metrics that enable feasible search in this space. The source code is available at: https://github.com/echigot/MOOSS.

</details>


### [90] [SPWOOD: Sparse Partial Weakly-Supervised Oriented Object Detection](https://arxiv.org/abs/2602.03634)
*Wei Zhang,Xiang Liu,Ningjing Liu,Mingxin Liu,Wei Liao,Chunyan Xu,Xue Yang*

Main category: cs.CV

TL;DR: The paper introduces a Sparse Partial Weakly-Supervised Oriented Object Detection framework to reduce labeling costs while maintaining performance in remote sensing, incorporating innovations like SOS-Student and Multi-level Pseudo-label Filtering.


<details>
  <summary>Details</summary>
Motivation: High labeling costs in remote sensing due to dense object distribution and diverse categories drive the need for efficient annotation methods.

Method: The proposed framework uses sparse weakly-labeled and unlabeled data, featuring SOS-Student for orientation/scale learning, Multi-level Pseudo-label Filtering, and sparse partitioning for category fairness.

Result: The framework outperforms traditional methods on DOTA and DIOR datasets, offering a cost-effective solution.

Conclusion: The approach effectively reduces annotation costs while maintaining detection performance, advancing oriented object detection in resource-constrained settings.

Abstract: A consistent trend throughout the research of oriented object detection has been the pursuit of maintaining comparable performance with fewer and weaker annotations. This is particularly crucial in the remote sensing domain, where the dense object distribution and a wide variety of categories contribute to prohibitively high costs. Based on the supervision level, existing oriented object detection algorithms can be broadly grouped into fully supervised, semi-supervised, and weakly supervised methods. Within the scope of this work, we further categorize them to include sparsely supervised and partially weakly-supervised methods. To address the challenges of large-scale labeling, we introduce the first Sparse Partial Weakly-Supervised Oriented Object Detection framework, designed to efficiently leverage only a few sparse weakly-labeled data and plenty of unlabeled data. Our framework incorporates three key innovations: (1) We design a Sparse-annotation-Orientation-and-Scale-aware Student (SOS-Student) model to separate unlabeled objects from the background in a sparsely-labeled setting, and learn orientation and scale information from orientation-agnostic or scale-agnostic weak annotations. (2) We construct a novel Multi-level Pseudo-label Filtering strategy that leverages the distribution of model predictions, which is informed by the model's multi-layer predictions. (3) We propose a unique sparse partitioning approach, ensuring equal treatment for each category. Extensive experiments on the DOTA and DIOR datasets show that our framework achieves a significant performance gain over traditional oriented object detection methods mentioned above, offering a highly cost-effective solution. Our code is publicly available at https://github.com/VisionXLab/SPWOOD.

</details>


### [91] [MM-SCALE: Grounded Multimodal Moral Reasoning via Scalar Judgment and Listwise Alignment](https://arxiv.org/abs/2602.03665)
*Eunkyu Park,Wesley Hanwen Deng,Cheyon Jin,Matheus Kunzler Maldaner,Jordan Wheeler,Jason I. Hong,Hong Shen,Adam Perer,Ken Holstein,Motahhare Eslami,Gunhee Kim*

Main category: cs.CV

TL;DR: MM-SCALE is a dataset designed to improve Vision-Language Models' moral judgments using scalar ratings and modality grounding, outperforming binary supervision methods.


<details>
  <summary>Details</summary>
Motivation: Current VLMs struggle with morally salient judgments in ambiguous contexts due to limited supervision methods.

Method: Introduces MM-SCALE, a dataset with 5-point scalar ratings and modality grounding for fine-tuning VLMs.

Result: VLMs trained on MM-SCALE show improved ranking fidelity and safety calibration.

Conclusion: Scalar supervision provides richer alignment signals for better moral reasoning in VLMs.

Abstract: Vision-Language Models (VLMs) continue to struggle to make morally salient judgments in multimodal and socially ambiguous contexts. Prior works typically rely on binary or pairwise supervision, which often fail to capture the continuous and pluralistic nature of human moral reasoning. We present MM-SCALE (Multimodal Moral Scale), a large-scale dataset for aligning VLMs with human moral preferences through 5-point scalar ratings and explicit modality grounding. Each image-scenario pair is annotated with moral acceptability scores and grounded reasoning labels by humans using an interface we tailored for data collection, enabling listwise preference optimization over ranked scenario sets. By moving from discrete to scalar supervision, our framework provides richer alignment signals and finer calibration of multimodal moral reasoning. Experiments show that VLMs fine-tuned on MM-SCALE achieve higher ranking fidelity and more stable safety calibration than those trained with binary signals.

</details>


### [92] [Efficient Sequential Neural Network with Spatial-Temporal Attention and Linear LSTM for Robust Lane Detection Using Multi-Frame Images](https://arxiv.org/abs/2602.03669)
*Sandeep Patil,Yongqi Dong,Haneen Farah,Hans Hellendoorn*

Main category: cs.CV

TL;DR: A novel spatial-temporal attention-based sequential neural network model for lane detection outperforms existing methods in accuracy, robustness, and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Current lane detection methods lack versatility and struggle in challenging scenarios like occlusion and lighting issues, prompting the need for a more robust solution.

Method: The study introduces a sequential neural network with a spatial-temporal attention mechanism, built on an encoder-decoder structure, and trained on large-scale datasets.

Result: The model demonstrates superior performance, fewer parameters, and reduced computational operations compared to baseline methods.

Conclusion: The proposed model offers an efficient and robust solution for lane detection, suitable for automated vehicles in mixed-traffic environments.

Abstract: Lane detection is a crucial perception task for all levels of automated vehicles (AVs) and Advanced Driver Assistance Systems, particularly in mixed-traffic environments where AVs must interact with human-driven vehicles (HDVs) and challenging traffic scenarios. Current methods lack versatility in delivering accurate, robust, and real-time compatible lane detection, especially vision-based methods often neglect critical regions of the image and their spatial-temporal (ST) salience, leading to poor performance in difficult circumstances such as serious occlusion and dazzle lighting. This study introduces a novel sequential neural network model with a spatial-temporal attention mechanism to focus on key features of lane lines and exploit salient ST correlations among continuous image frames. The proposed model, built on a standard encoder-decoder structure and common neural network backbones, is trained and evaluated on three large-scale open-source datasets. Extensive experiments demonstrate the strength and robustness of the proposed model, outperforming state-of-the-art methods in various testing scenarios. Furthermore, with the ST attention mechanism, the developed sequential neural network models exhibit fewer parameters and reduced Multiply-Accumulate Operations (MACs) compared to baseline sequential models, highlighting their computational efficiency. Relevant data, code, and models are released at https://doi.org/10.4121/4619cab6-ae4a-40d5-af77-582a77f3d821.

</details>


### [93] [Referring Industrial Anomaly Segmentation](https://arxiv.org/abs/2602.03673)
*Pengfei Yue,Xiaokang Jiang,Yilin Lu,Jianghang Lin,Shengchuan Zhang,Liujuan Cao*

Main category: cs.CV

TL;DR: RIAS introduces a language-guided approach for industrial anomaly detection, eliminating manual thresholds and enabling single-model detection of diverse anomalies with precise masks.


<details>
  <summary>Details</summary>
Motivation: Traditional IAD methods struggle with manual thresholds, data imbalance, and the 'One Anomaly Class, One Model' limitation, prompting the need for a more flexible and efficient solution.

Method: RIAS leverages text descriptions for anomaly detection without manual thresholds, using universal prompts and the DQFormer benchmark with Dual Query Tokens and LMA for multi-scale segmentation.

Result: RIAS achieves precise anomaly localization and segmentation, demonstrated by experiments on the MVTec-Ref dataset, which includes diverse referring expressions and small anomalies.

Conclusion: RIAS advances IAD by introducing language-guided detection, offering open-set capabilities and efficient anomaly segmentation with a single model.

Abstract: Industrial Anomaly Detection (IAD) is vital for manufacturing, yet traditional methods face significant challenges: unsupervised approaches yield rough localizations requiring manual thresholds, while supervised methods overfit due to scarce, imbalanced data. Both suffer from the "One Anomaly Class, One Model" limitation. To address this, we propose Referring Industrial Anomaly Segmentation (RIAS), a paradigm leveraging language to guide detection. RIAS generates precise masks from text descriptions without manual thresholds and uses universal prompts to detect diverse anomalies with a single model. We introduce the MVTec-Ref dataset to support this, designed with diverse referring expressions and focusing on anomaly patterns, notably with 95% small anomalies. We also propose the Dual Query Token with Mask Group Transformer (DQFormer) benchmark, enhanced by Language-Gated Multi-Level Aggregation (LMA) to improve multi-scale segmentation. Unlike traditional methods using redundant queries, DQFormer employs only "Anomaly" and "Background" tokens for efficient visual-textual integration. Experiments demonstrate RIAS's effectiveness in advancing IAD toward open-set capabilities. Code: https://github.com/swagger-coder/RIAS-MVTec-Ref.

</details>


### [94] [RegionReasoner: Region-Grounded Multi-Round Visual Reasoning](https://arxiv.org/abs/2602.03733)
*Wenfang Sun,Hao Chen,Yingjun Du,Yefeng Zheng,Cees G. M. Snoek*

Main category: cs.CV

TL;DR: A multi-round visual reasoning benchmark and reinforcement learning framework, RegionReasoner, are introduced to improve iterative reasoning accuracy and grounding precision in vision-language models.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitation of single-step or text-only reasoning in vision-language models by enabling iterative refinement across multiple visual contexts.

Method: Propose RegionReasoner, a reinforcement learning framework with grounding fidelity and global-local semantic alignment rewards, tested on detection and segmentation tasks.

Result: RegionReasoner-7B with RegionDial-Bench significantly enhances multi-round reasoning accuracy, spatial grounding precision, and global-local consistency.

Conclusion: The framework establishes a strong baseline for multi-round visual reasoning, improving both grounding and coherence in vision-language tasks.

Abstract: Large vision-language models have achieved remarkable progress in visual reasoning, yet most existing systems rely on single-step or text-only reasoning, limiting their ability to iteratively refine understanding across multiple visual contexts. To address this limitation, we introduce a new multi-round visual reasoning benchmark with training and test sets spanning both detection and segmentation tasks, enabling systematic evaluation under iterative reasoning scenarios. We further propose RegionReasoner, a reinforcement learning framework that enforces grounded reasoning by requiring each reasoning trace to explicitly cite the corresponding reference bounding boxes, while maintaining semantic coherence via a global-local consistency reward. This reward extracts key objects and nouns from both global scene captions and region-level captions, aligning them with the reasoning trace to ensure consistency across reasoning steps. RegionReasoner is optimized with structured rewards combining grounding fidelity and global-local semantic alignment. Experiments on detection and segmentation tasks show that RegionReasoner-7B, together with our newly introduced benchmark RegionDial-Bench, considerably improves multi-round reasoning accuracy, spatial grounding precision, and global-local consistency, establishing a strong baseline for this emerging research direction.

</details>


### [95] [Edge-Optimized Vision-Language Models for Underground Infrastructure Assessment](https://arxiv.org/abs/2602.03742)
*Johny J. Lopez,Md Meftahul Ferdaus,Mahdi Abdelguerfi*

Main category: cs.CV

TL;DR: A two-stage pipeline combines lightweight segmentation (RAPID-SCAN) and a fine-tuned Vision-Language Model (VLM) to generate human-readable summaries of underground infrastructure defects on edge devices.


<details>
  <summary>Details</summary>
Motivation: Automated, human-readable defect summaries are crucial for public safety and urban sustainability but challenging on resource-constrained edge devices.

Method: 1) RAPID-SCAN for efficient defect segmentation (0.64M parameters, 0.834 F1-score). 2) Fine-tuned Phi-3.5 VLM for natural language summaries. Dataset curation and hardware optimization enable real-time performance.

Result: Pipeline achieves efficient segmentation and high-quality summaries on edge devices, validated in real-world robotic inspections.

Conclusion: Edge-deployable AI systems bridge automated detection and actionable insights, enhancing scalable and autonomous infrastructure inspection.

Abstract: Autonomous inspection of underground infrastructure, such as sewer and culvert systems, is critical to public safety and urban sustainability. Although robotic platforms equipped with visual sensors can efficiently detect structural deficiencies, the automated generation of human-readable summaries from these detections remains a significant challenge, especially on resource-constrained edge devices. This paper presents a novel two-stage pipeline for end-to-end summarization of underground deficiencies, combining our lightweight RAPID-SCAN segmentation model with a fine-tuned Vision-Language Model (VLM) deployed on an edge computing platform. The first stage employs RAPID-SCAN (Resource-Aware Pipeline Inspection and Defect Segmentation using Compact Adaptive Network), achieving 0.834 F1-score with only 0.64M parameters for efficient defect segmentation. The second stage utilizes a fine-tuned Phi-3.5 VLM that generates concise, domain-specific summaries in natural language from the segmentation outputs. We introduce a curated dataset of inspection images with manually verified descriptions for VLM fine-tuning and evaluation. To enable real-time performance, we employ post-training quantization with hardware-specific optimization, achieving significant reductions in model size and inference latency without compromising summarization quality. We deploy and evaluate our complete pipeline on a mobile robotic platform, demonstrating its effectiveness in real-world inspection scenarios. Our results show the potential of edge-deployable integrated AI systems to bridge the gap between automated defect detection and actionable insights for infrastructure maintenance, paving the way for more scalable and autonomous inspection solutions.

</details>


### [96] [LIVE: Long-horizon Interactive Video World Modeling](https://arxiv.org/abs/2602.03747)
*Junchao Huang,Ziyang Ye,Xinting Hu,Tianyu He,Guiyu Zhang,Shaoshuai Shi,Jiang Bian,Li Jiang*

Main category: cs.CV

TL;DR: LIVE, a Long-horizon Interactive Video world modEl, uses cycle-consistency to limit error accumulation in autoregressive video prediction, outperforming prior methods without teacher models.


<details>
  <summary>Details</summary>
Motivation: Autoregressive models struggle with long-horizon video generation due to error accumulation. Prior solutions like teacher models add computational cost and fail beyond training horizons.

Method: LIVE employs cycle-consistency via forward rollout and reverse generation to reconstruct initial states, computing diffusion loss on reconstructed terminal states. It also uses progressive training curriculum.

Result: LIVE achieves state-of-the-art performance on long-horizon benchmarks, generating high-quality videos beyond training lengths.

Conclusion: LIVE effectively addresses long-horizon error propagation without teacher models, offering stable and high-quality video generation.

Abstract: Autoregressive video world models predict future visual observations conditioned on actions. While effective over short horizons, these models often struggle with long-horizon generation, as small prediction errors accumulate over time. Prior methods alleviate this by introducing pre-trained teacher models and sequence-level distribution matching, which incur additional computational cost and fail to prevent error propagation beyond the training horizon. In this work, we propose LIVE, a Long-horizon Interactive Video world modEl that enforces bounded error accumulation via a novel cycle-consistency objective, thereby eliminating the need for teacher-based distillation. Specifically, LIVE first performs a forward rollout from ground-truth frames and then applies a reverse generation process to reconstruct the initial state. The diffusion loss is subsequently computed on the reconstructed terminal state, providing an explicit constraint on long-horizon error propagation. Moreover, we provide an unified view that encompasses different approaches and introduce progressive training curriculum to stabilize training. Experiments demonstrate that LIVE achieves state-of-the-art performance on long-horizon benchmarks, generating stable, high-quality videos far beyond training rollout lengths.

</details>


### [97] [See-through: Single-image Layer Decomposition for Anime Characters](https://arxiv.org/abs/2602.03749)
*Jian Lin,Chengze Li,Haoyun Qin,Kwun Wang Chan,Yanghua Jin,Hanyuan Liu,Stephen Chun Wang Choy,Xueting Liu*

Main category: cs.CV

TL;DR: A framework transforms static anime images into 2.5D models using automated segmentation, inpainted layers, and inferred drawing orders, leveraging diffusion-based coherence and pseudo-depth inference for professional animation.


<details>
  <summary>Details</summary>
Motivation: Current workflows for animating static anime illustrations involve manual segmentation and artistic guesswork for occluded regions, which is time-consuming and labor-intensive.

Method: The approach decomposes images into semantically distinct layers, uses a scalable engine for training data from Live2D models, and employs a diffusion-based Body Part Consistency Module with pseudo-depth inference.

Result: The method produces high-fidelity, manipulatable 2.5D models suitable for real-time animation, overcoming manual process limitations.

Conclusion: This framework efficiently automates the transformation of static anime images into dynamic animation-ready models, addressing data scarcity and geometric complexity.

Abstract: We introduce a framework that automates the transformation of static anime illustrations into manipulatable 2.5D models. Current professional workflows require tedious manual segmentation and the artistic ``hallucination'' of occluded regions to enable motion. Our approach overcomes this by decomposing a single image into fully inpainted, semantically distinct layers with inferred drawing orders. To address the scarcity of training data, we introduce a scalable engine that bootstraps high-quality supervision from commercial Live2D models, capturing pixel-perfect semantics and hidden geometry. Our methodology couples a diffusion-based Body Part Consistency Module, which enforces global geometric coherence, with a pixel-level pseudo-depth inference mechanism. This combination resolves the intricate stratification of anime characters, e.g., interleaving hair strands, allowing for dynamic layer reconstruction. We demonstrate that our approach yields high-fidelity, manipulatable models suitable for professional, real-time animation applications.

</details>


### [98] [Test-Time Conditioning with Representation-Aligned Visual Features](https://arxiv.org/abs/2602.03753)
*Nicolas Sereyjol-Garros,Ellington Kirby,Victor Letzelter,Victor Besnier,Nermin Samet*

Main category: cs.CV

TL;DR: The paper introduces REPA-G, a framework using aligned representations for test-time conditioning in diffusion models, enabling versatile control and multi-concept composition.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of representation alignment for enhancing inference-time conditioning in diffusion models, addressing limitations of ambiguous text prompts or coarse class labels.

Method: REPA-G leverages aligned representations to steer the denoising process during inference by optimizing a similarity objective, enabling control at multiple scales.

Result: Demonstrates high-quality, diverse generations on ImageNet and COCO, with versatile control from fine-grained texture matching to broad semantic guidance.

Conclusion: REPA-G offers a flexible and precise alternative for inference-time conditioning in diffusion models, validated by theoretical and empirical results.

Abstract: While representation alignment with self-supervised models has been shown to improve diffusion model training, its potential for enhancing inference-time conditioning remains largely unexplored. We introduce Representation-Aligned Guidance (REPA-G), a framework that leverages these aligned representations, with rich semantic properties, to enable test-time conditioning from features in generation. By optimizing a similarity objective (the potential) at inference, we steer the denoising process toward a conditioned representation extracted from a pre-trained feature extractor. Our method provides versatile control at multiple scales, ranging from fine-grained texture matching via single patches to broad semantic guidance using global image feature tokens. We further extend this to multi-concept composition, allowing for the faithful combination of distinct concepts. REPA-G operates entirely at inference time, offering a flexible and precise alternative to often ambiguous text prompts or coarse class labels. We theoretically justify how this guidance enables sampling from the potential-induced tilted distribution. Quantitative results on ImageNet and COCO demonstrate that our approach achieves high-quality, diverse generations. Code is available at https://github.com/valeoai/REPA-G.

</details>


### [99] [RAWDet-7: A Multi-Scenario Benchmark for Object Detection and Description on Quantized RAW Images](https://arxiv.org/abs/2602.03760)
*Mishal Fatima,Shashank Agnihotri,Kanchana Vaishnavi Gandikota,Michael Moeller,Margret Keuper*

Main category: cs.CV

TL;DR: RAWDet-7 is a dataset of RAW images for object detection and description, emphasizing the preservation of sensor-level information often lost in processed RGB images.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitation of vision models trained on RGB images processed for human perception, which discard useful sensor-level data for machine reasoning.

Method: The authors introduce RAWDet-7, a dataset of ~25k training and 7.6k test RAW images annotated for seven object categories, with additional object-level descriptions from high-resolution sRGB images.

Result: The dataset enables evaluation under varying quantization levels (4-bit, 6-bit, 8-bit) and supports research on detection performance, description quality, and generalization in RAW image processing.

Conclusion: RAWDet-7 provides a benchmark for studying the benefits of RAW images in machine vision tasks, offering richer cues compared to processed RGB images.

Abstract: Most vision models are trained on RGB images processed through ISP pipelines optimized for human perception, which can discard sensor-level information useful for machine reasoning. RAW images preserve unprocessed scene data, enabling models to leverage richer cues for both object detection and object description, capturing fine-grained details, spatial relationships, and contextual information often lost in processed images. To support research in this domain, we introduce RAWDet-7, a large-scale dataset of ~25k training and 7.6k test RAW images collected across diverse cameras, lighting conditions, and environments, densely annotated for seven object categories following MS-COCO and LVIS conventions. In addition, we provide object-level descriptions derived from the corresponding high-resolution sRGB images, facilitating the study of object-level information preservation under RAW image processing and low-bit quantization. The dataset allows evaluation under simulated 4-bit, 6-bit, and 8-bit quantization, reflecting realistic sensor constraints, and provides a benchmark for studying detection performance, description quality & detail, and generalization in low-bit RAW image processing. Dataset & code upon acceptance.

</details>


### [100] [FOVI: A biologically-inspired foveated interface for deep vision models](https://arxiv.org/abs/2602.03766)
*Nicholas M. Blauch,George A. Alvarez,Talia Konkle*

Main category: cs.CV

TL;DR: The paper proposes a foveated vision interface (FOVI) mimicking human vision to process high-resolution images efficiently, reducing computational costs while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Human vision efficiently processes variable-resolution inputs, unlike uniform-resolution computer vision systems, which face challenges with high-resolution images.

Method: FOVI reformats variable-resolution sensor data into a uniform manifold, using k-nearest-neighborhood (kNN) convolution and a novel kernel mapping technique. Two use cases are demonstrated: an end-to-end kNN-convolutional model and a foveated DINOv3 ViT model with LoRA.

Result: The models achieve competitive performance at significantly lower computational costs compared to non-foveated baselines.

Conclusion: FOVI enables efficient and scalable active sensing for high-resolution egocentric vision, with potential applications in computer vision.

Abstract: Human vision is foveated, with variable resolution peaking at the center of a large field of view; this reflects an efficient trade-off for active sensing, allowing eye-movements to bring different parts of the world into focus with other parts of the world in context. In contrast, most computer vision systems encode the visual world at a uniform resolution, raising challenges for processing full-field high-resolution images efficiently. We propose a foveated vision interface (FOVI) based on the human retina and primary visual cortex, that reformats a variable-resolution retina-like sensor array into a uniformly dense, V1-like sensor manifold. Receptive fields are defined as k-nearest-neighborhoods (kNNs) on the sensor manifold, enabling kNN-convolution via a novel kernel mapping technique. We demonstrate two use cases: (1) an end-to-end kNN-convolutional architecture, and (2) a foveated adaptation of the foundational DINOv3 ViT model, leveraging low-rank adaptation (LoRA). These models provide competitive performance at a fraction of the computational cost of non-foveated baselines, opening pathways for efficient and scalable active sensing for high-resolution egocentric vision. Code and pre-trained models are available at https://github.com/nblauch/fovi and https://huggingface.co/fovi-pytorch.

</details>


### [101] [QVLA: Not All Channels Are Equal in Vision-Language-Action Model's Quantization](https://arxiv.org/abs/2602.03782)
*Yuhao Xu,Yantai Yang,Zhenyang Fan,Yufan Liu,Yuming Li,Bing Li,Zhipeng Zhang*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The advent of Vision-Language-Action (VLA) models represents a significant leap for embodied intelligence, yet their immense computational demands critically hinder deployment on resource-constrained robotic platforms. Intuitively, low-bit quantization is a prevalent and preferred technique for large-scale model compression. However, we find that a systematic analysis of VLA model's quantization is fundamentally lacking. We argue that naively applying uniform-bit quantization from Large Language Models (LLMs) to robotics is flawed, as these methods prioritize passive data fidelity while ignoring how minor action deviations compound into catastrophic task failures. To bridge this gap, we introduce QVLA, the first action-centric quantization framework specifically designed for embodied control. In a sharp departure from the rigid, uniform-bit quantization of LLM-based methods, QVLA introduces a highly granular, channel-wise bit allocation strategy. Its core mechanism is to directly measure the final action-space sensitivity when quantizing each individual channel to various bit-widths. This process yields a precise, per-channel importance metric that guides a global optimization, which elegantly unifies quantization and pruning (0-bit) into a single, cohesive framework. Extensive evaluations on different baselines demonstrate the superiority of our approach. In the LIBERO, the quantization version of OpenVLA-OFT with our method requires only 29.2% of the original model's VRAM while maintaining 98.9% of its original performance and achieving a 1.49x speedup. This translates to a 22.6% performance improvement over the LLM-derived method SmoothQuant. Our work establishes a new, principled foundation for compressing VLA models in robotics, paving the way for deploying powerful, large-scale models on real-world hardware. Code will be released.

</details>


### [102] [From Pre- to Intra-operative MRI: Predicting Brain Shift in Temporal Lobe Resection for Epilepsy Surgery](https://arxiv.org/abs/2602.03785)
*Jingjing Peng,Giorgio Fiore,Yang Liu,Ksenia Ellum,Debayan Daspupta,Keyoumars Ashkan,Andrew McEvoy,Anna Miserocchi,Sebastien Ourselin,John Duncan,Alejandro Granados*

Main category: cs.CV

TL;DR: NeuralShift predicts brain shift from pre-op MRI using U-Net, achieving high accuracy (DICE 0.97, TRE 1.12mm) for temporal lobe resection.


<details>
  <summary>Details</summary>
Motivation: Brain shift invalidates pre-op MRI during neurosurgery, requiring intraoperative updates for precision.

Method: U-Net-based model predicts brain shift from pre-op MRI, evaluated via TREs and DICE scores.

Result: Accurate prediction of global deformation (DICE 0.97) and local displacements (TRE 1.12mm).

Conclusion: NeuralShift enhances neurosurgery safety/efficiency by predicting brain shift using pre-op images alone.

Abstract: Introduction: In neurosurgery, image-guided Neurosurgery Systems (IGNS) highly rely on preoperative brain magnetic resonance images (MRI) to assist surgeons in locating surgical targets and determining surgical paths. However, brain shift invalidates the preoperative MRI after dural opening. Updated intraoperative brain MRI with brain shift compensation is crucial for enhancing the precision of neuronavigation systems and ensuring the optimal outcome of surgical interventions. Methodology: We propose NeuralShift, a U-Net-based model that predicts brain shift entirely from pre-operative MRI for patients undergoing temporal lobe resection. We evaluated our results using Target Registration Errors (TREs) computed on anatomical landmarks located on the resection side and along the midline, and DICE scores comparing predicted intraoperative masks with masks derived from intraoperative MRI. Results: Our experimental results show that our model can predict the global deformation of the brain (DICE of 0.97) with accurate local displacements (achieve landmark TRE as low as 1.12 mm), compensating for large brain shifts during temporal lobe removal neurosurgery. Conclusion: Our proposed model is capable of predicting the global deformation of the brain during temporal lobe resection using only preoperative images, providing potential opportunities to the surgical team to increase safety and efficiency of neurosurgery and better outcomes to patients. Our contributions will be publicly available after acceptance in https://github.com/SurgicalDataScienceKCL/NeuralShift.

</details>


### [103] [Progressive Checkerboards for Autoregressive Multiscale Image Generation](https://arxiv.org/abs/2602.03811)
*David Eigen*

Main category: cs.CV

TL;DR: The paper proposes a fixed ordering method using progressive checkerboards for efficient multiscale autoregressive image generation, achieving competitive results with fewer steps.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of efficiently sampling independent locations in parallel while modeling mutual dependencies in autoregressive image generation.

Method: Uses progressive checkerboards for multiscale ordering, enabling parallel sampling from evenly spaced regions at each scale.

Result: Achieves competitive performance on class-conditional ImageNet with fewer sampling steps compared to state-of-the-art systems.

Conclusion: The proposed balanced ordering method is effective for multiscale autoregressive generation, with scale-up factors having minor impact if total steps remain constant.

Abstract: A key challenge in autoregressive image generation is to efficiently sample independent locations in parallel, while still modeling mutual dependencies with serial conditioning. Some recent works have addressed this by conditioning between scales in a multiscale pyramid. Others have looked at parallelizing samples in a single image using regular partitions or randomized orders. In this work we examine a flexible, fixed ordering based on progressive checkerboards for multiscale autoregressive image generation. Our ordering draws samples in parallel from evenly spaced regions at each scale, maintaining full balance in all levels of a quadtree subdivision at each step. This enables effective conditioning both between and within scales. Intriguingly, we find evidence that in our balanced setting, a wide range of scale-up factors lead to similar results, so long as the total number of serial steps is constant. On class-conditional ImageNet, our method achieves competitive performance compared to recent state-of-the-art autoregressive systems with like model capacity, using fewer sampling steps.

</details>


### [104] [Fast-Slow Efficient Training for Multimodal Large Language Models via Visual Token Pruning](https://arxiv.org/abs/2602.03815)
*Dingkun Zhang,Shuhan Qi,Yulin Wu,Xinyu Xiao,Xuan Wang,Long Chen*

Main category: cs.CV

TL;DR: DualSpeed addresses training inefficiency in Multimodal Large Language Models (MLLMs) by reducing visual tokens without degrading performance, using a fast-slow framework.


<details>
  <summary>Details</summary>
Motivation: MLLMs struggle with training inefficiency due to large model sizes and visual tokens. Existing methods focus on model reduction, leaving visual token reduction unexplored.

Method: DualSpeed employs a fast-mode (token pruning) and slow-mode (full sequence training) with a mode isolator and self-distillation to ensure performance.

Result: DualSpeed speeds up LLaVA-1.5 training by 2.1× and LLaVA-NeXT by 4.0×, maintaining over 99% performance.

Conclusion: DualSpeed successfully balances training efficiency and performance retention in MLLMs.

Abstract: Multimodal Large Language Models (MLLMs) suffer from severe training inefficiency issue, which is associated with their massive model sizes and visual token numbers. Existing efforts in efficient training focus on reducing model sizes or trainable parameters. Inspired by the success of Visual Token Pruning (VTP) in improving inference efficiency, we are exploring another substantial research direction for efficient training by reducing visual tokens. However, applying VTP at the training stage results in a training-inference mismatch: pruning-trained models perform poorly when inferring on non-pruned full visual token sequences. To close this gap, we propose DualSpeed, a fast-slow framework for efficient training of MLLMs. The fast-mode is the primary mode, which incorporates existing VTP methods as plugins to reduce visual tokens, along with a mode isolator to isolate the model's behaviors. The slow-mode is the auxiliary mode, where the model is trained on full visual sequences to retain training-inference consistency. To boost its training, it further leverages self-distillation to learn from the sufficiently trained fast-mode. Together, DualSpeed can achieve both training efficiency and non-degraded performance. Experiments show DualSpeed accelerates the training of LLaVA-1.5 by 2.1$\times$ and LLaVA-NeXT by 4.0$\times$, retaining over 99% performance. Code: https://github.com/dingkun-zhang/DualSpeed

</details>


### [105] [Continuous Control of Editing Models via Adaptive-Origin Guidance](https://arxiv.org/abs/2602.03826)
*Alon Wolf,Chen Katzir,Kfir Aberman,Or Patashnik*

Main category: cs.CV

TL;DR: The paper introduces Adaptive-Origin Guidance (AdaOr) for smoother control of text-guided edits in diffusion-based models, addressing limitations of Classifier-Free Guidance (CFG).


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based editing models lack smooth intensity control for text-guided edits, prompting the need for a better method.

Method: Proposes AdaOr, which adjusts the guidance origin with an identity-conditioned adaptive origin and interpolates between identity and unconditional predictions based on edit strength.

Result: AdaOr provides smoother, more consistent control in image and video editing tasks compared to slider-based approaches.

Conclusion: AdaOr enables fine-grained control without specialized datasets or per-edit procedures, improving text-guided editing.

Abstract: Diffusion-based editing models have emerged as a powerful tool for semantic image and video manipulation. However, existing models lack a mechanism for smoothly controlling the intensity of text-guided edits. In standard text-conditioned generation, Classifier-Free Guidance (CFG) impacts prompt adherence, suggesting it as a potential control for edit intensity in editing models. However, we show that scaling CFG in these models does not produce a smooth transition between the input and the edited result. We attribute this behavior to the unconditional prediction, which serves as the guidance origin and dominates the generation at low guidance scales, while representing an arbitrary manipulation of the input content. To enable continuous control, we introduce Adaptive-Origin Guidance (AdaOr), a method that adjusts this standard guidance origin with an identity-conditioned adaptive origin, using an identity instruction corresponding to the identity manipulation. By interpolating this identity prediction with the standard unconditional prediction according to the edit strength, we ensure a continuous transition from the input to the edited result. We evaluate our method on image and video editing tasks, demonstrating that it provides smoother and more consistent control compared to current slider-based editing approaches. Our method incorporates an identity instruction into the standard training framework, enabling fine-grained control at inference time without per-edit procedure or reliance on specialized datasets.

</details>


### [106] [EventNeuS: 3D Mesh Reconstruction from a Single Event Camera](https://arxiv.org/abs/2602.03847)
*Shreyas Sachan,Viktor Rudnev,Mohamed Elgharib,Christian Theobalt,Vladislav Golyanik*

Main category: cs.CV

TL;DR: EventNeuS is a self-supervised neural model for accurate 3D mesh reconstruction from monocular color event streams, combining SDF and density field learning with event-based supervision.


<details>
  <summary>Details</summary>
Motivation: Event cameras provide an alternative to RGB cameras, but existing event-based techniques struggle with accurate 3D reconstruction.

Method: EventNeuS integrates 3D signed distance functions and density field learning with event-based supervision, and uses spherical harmonics for view-dependent effects.

Result: EventNeuS achieves 34% lower Chamfer distance and 31% lower mean absolute error compared to previous methods.

Conclusion: EventNeuS significantly improves 3D reconstruction accuracy from event streams, setting a new benchmark.

Abstract: Event cameras offer a considerable alternative to RGB cameras in many scenarios. While there are recent works on event-based novel-view synthesis, dense 3D mesh reconstruction remains scarcely explored and existing event-based techniques are severely limited in their 3D reconstruction accuracy. To address this limitation, we present EventNeuS, a self-supervised neural model for learning 3D representations from monocular colour event streams. Our approach, for the first time, combines 3D signed distance function and density field learning with event-based supervision. Furthermore, we introduce spherical harmonics encodings into our model for enhanced handling of view-dependent effects. EventNeuS outperforms existing approaches by a significant margin, achieving 34% lower Chamfer distance and 31% lower mean absolute error on average compared to the best previous method.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [107] [The Hypocrisy Gap: Quantifying Divergence Between Internal Belief and Chain-of-Thought Explanation via Sparse Autoencoders](https://arxiv.org/abs/2602.02496)
*Shikhar Shiromani,Archie Chaudhury,Sri Pranav Kunda*

Main category: cs.CL

TL;DR: The paper introduces the Hypocrisy Gap, a metric using Sparse Autoencoders to detect unfaithful behavior in LLMs by comparing internal reasoning to final outputs.


<details>
  <summary>Details</summary>
Motivation: To address the issue of LLMs producing answers that diverge from their internal reasoning to please users, ensuring more reliable and faithful outputs.

Method: Uses Sparse Autoencoders (SAEs) and sparse linear probes to quantify the divergence between internal truth beliefs and final generated outputs in latent space.

Result: Achieves AUROC scores of 0.55-0.73 for detecting sycophantic behavior and 0.55-0.74 for hypocritical cases, outperforming baseline methods.

Conclusion: The Hypocrisy Gap effectively detects and quantifies unfaithful behavior in LLMs, providing a tool for improving model reliability.

Abstract: Large Language Models (LLMs) frequently exhibit unfaithful behavior, producing a final answer that differs significantly from their internal chain of thought (CoT) reasoning in order to appease the user they are conversing with. In order to better detect this behavior, we introduce the Hypocrisy Gap, a mechanistic metric utilizing Sparse Autoencoders (SAEs) to quantify the divergence between a model's internal reasoning and its final generation. By mathematically comparing an internal truth belief, derived via sparse linear probes, to the final generated trajectory in latent space, we quantify and detect a model's tendency to engage in unfaithful behavior. Experiments on Gemma, Llama, and Qwen models using Anthropic's Sycophancy benchmark show that our method achieves an AUROC of 0.55-0.73 for detecting sycophantic runs and 0.55-0.74 for hypocritical cases where the model internally "knows" the user is wrong, consistently outperforming a decision-aligned log-probability baseline (0.41-0.50 AUROC).

</details>


### [108] [STEMVerse: A Dual-Axis Diagnostic Framework for STEM Reasoning in Large Language Models](https://arxiv.org/abs/2602.02497)
*Xuzhao Li,Xuchen Li,Jian Zhao,Shiyu Hu*

Main category: cs.CL

TL;DR: STEMVerse is a diagnostic framework evaluating LLMs' STEM reasoning by analyzing performance across academic disciplines and cognitive complexity, revealing structural failure patterns.


<details>
  <summary>Details</summary>
Motivation: Current evaluation paradigms offer limited diagnostic value by neglecting academic specialization and cognitive depth, making it hard to differentiate domain knowledge gaps from cognitive deficiencies.

Method: STEMVerse re-aggregates 20,000+ STEM problems into a unified "Discipline × Cognition" space, evaluating LLMs systematically across parameter scales and training paradigms.

Result: Empirical results uncover structural failure patterns in LLMs' STEM reasoning, highlighting limitations in domain-specific and cognitive dimensions.

Conclusion: STEMVerse provides actionable insights into LLMs' scientific reasoning by integrating multi-disciplinary coverage and cognitive stratification.

Abstract: As Large Language Models (LLMs) achieve significant breakthroughs in complex reasoning tasks, evaluating their proficiency in science, technology, engineering, and mathematics (STEM) has become a primary method for measuring machine intelligence. However, current evaluation paradigms often treat benchmarks as isolated "silos," offering only monolithic aggregate scores that neglect the intricacies of both academic specialization and cognitive depth. This result-oriented approach fails to distinguish whether model errors stem from insufficient domain knowledge or deficiencies in cognitive capacity, thereby limiting the diagnostic value. To address this, we propose STEMVerse, a diagnostic framework designed to systematically analyze the STEM reasoning capabilities of LLMs. This framework characterizes model performance across academic specialization and cognitive complexity to map the capability required for reasoning. We re-aggregate over 20,000 STEM problems from mainstream benchmarks into a unified "Discipline $\times$ Cognition" capability space, assigning dual-axis labels to every instance. Utilizing this unified diagnostic framework, we systematically evaluate representative LLM families across varying parameter scales and training paradigms. Our empirical results reveal structural failure patterns in STEM reasoning. By integrating multi-disciplinary coverage and fine-grained cognitive stratification into a unified framework, STEMVerse provides a clear and actionable perspective for understanding the scientific reasoning characteristics of LLMs.

</details>


### [109] [Test-Time Detoxification without Training or Learning Anything](https://arxiv.org/abs/2602.02498)
*Baturay Saglam,Dionysis Kalogerias*

Main category: cs.CL

TL;DR: A test-time procedure using zeroth-order optimization reduces toxicity in large language model outputs without retraining or gradients.


<details>
  <summary>Details</summary>
Motivation: To address the risks of toxic or inappropriate text generation by large language models without sacrificing quality or requiring costly retraining.

Method: Approximates toxicity gradients with input embeddings and uses zeroth-order optimization to steer generation toward less toxic continuations.

Result: Achieves robust toxicity reductions across models and prompts, offering the best toxicity-quality trade-off in most settings.

Conclusion: Demonstrates that word embeddings can effectively control and guide autoregressive models for safer text generation without training or access to intermediate computations.

Abstract: Large language models can produce toxic or inappropriate text even for benign inputs, creating risks when deployed at scale. Detoxification is therefore important for safety and user trust, particularly when we want to reduce harmful content without sacrificing the model's generation quality. Many existing approaches rely on model retraining, gradients, or learned auxiliary components, which can be costly and may not transfer across model families or to truly black-box settings. We introduce a test-time procedure that approximates the gradient of completion toxicity with respect to the input embeddings and uses a small number of descent steps to steer generation toward less toxic continuations. This is achieved with zeroth-order optimization that requires only access to input embeddings, a toxicity scoring function, and forward evaluations of the model. Empirically, the approach delivers robust toxicity reductions across models and prompts and, in most settings, achieves the best overall toxicity-quality trade-off. More broadly, our work positions word embeddings as effective control variables and encourages wider use of black-box optimization to guide autoregressive language models toward scalable, safer text generation, without requiring any training or access to intermediate computations.

</details>


### [110] [ROSA-Tuning: Enhancing Long-Context Modeling via Suffix Matching](https://arxiv.org/abs/2602.02499)
*Yunao Zheng,Xiaojie Wang,Lei Ren,Wei Chen*

Main category: cs.CL

TL;DR: ROSA-Tuning enhances long-context modeling in pretrained models via a retrieval-and-recall mechanism, combining efficient attention with a CPU-based retrieval module for improved performance while maintaining computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing the limited coverage and computational inefficiency of existing attention methods in handling long-context modeling for large language models.

Method: Introduces ROSA-Tuning with a CPU-based ROSA retrieval module, trainable information injection, and range-restricted attention. Includes binary discretization, counterfactual gradient algorithm, and CPU-GPU pipeline optimization.

Result: ROSA-Tuning restores long-context modeling ability in windowed-attention models, matching global attention performance on benchmarks like LongBench while maintaining efficiency.

Conclusion: ROSA-Tuning offers a viable technical solution for efficient long-context processing, balancing performance and computational cost.

Abstract: Long-context capability and computational efficiency are among the central challenges facing today's large language models. Existing efficient attention methods reduce computational complexity, but they typically suffer from a limited coverage of the model state. This paper proposes ROSA-Tuning, a retrieval-and-recall mechanism for enhancing the long-context modeling ability of pretrained models. Beyond the standard attention mechanism, ROSA-Tuning introduces in parallel a CPU-based ROSA (RWKV Online Suffix Automaton) retrieval module, which efficiently locates historical positions in long contexts that are relevant to the current query, and injects the retrieved information into the model state in a trainable manner; subsequent weighted fusion can then be handled by range-restricted attention. To enable end-to-end training, we design a binary discretization strategy and a counterfactual gradient algorithm, and further optimize overall execution efficiency via an asynchronous CPU-GPU pipeline. Systematic evaluations on Qwen3-Base-1.7B show that ROSA-Tuning substantially restores the long-context modeling ability of windowed-attention models, achieving performance close to and in some cases matching global attention on benchmarks such as LongBench, while maintaining computational efficiency and GPU memory usage that are nearly comparable to windowed-attention methods, offering a new technical path for efficient long-context processing. The example code can be found at https://github.com/zyaaa-ux/ROSA-Tuning.

</details>


### [111] [Graph-Augmented Reasoning with Large Language Models for Tobacco Pest and Disease Management](https://arxiv.org/abs/2602.02635)
*Siyu Li,Chenwei Song,Qi Zhou,Wan Zhou,Xinyi Liu*

Main category: cs.CL

TL;DR: A graph-augmented reasoning framework integrates domain-specific knowledge graphs with LLMs to enhance pest and disease management in tobacco farming, improving accuracy and reducing hallucinations.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of text-only similarity in LLMs for domain-specific tasks by incorporating structured knowledge to provide relational evidence.

Method: Develop a knowledge graph for tobacco pests/diseases, use GraphRAG for query-relevant subgraph retrieval, and fine-tune ChatGLM with LoRA. A GNN learns node representations to model dependencies.

Result: The framework outperforms text-only baselines, especially in multi-hop and comparative reasoning tasks.

Conclusion: Graph-augmented reasoning improves LLM performance in domain-specific tasks by leveraging structured knowledge and relational evidence.

Abstract: This paper proposes a graph-augmented reasoning framework for tobacco pest and disease management that integrates structured domain knowledge into large language models. Building on GraphRAG, we construct a domain-specific knowledge graph and retrieve query-relevant subgraphs to provide relational evidence during answer generation. The framework adopts ChatGLM as the Transformer backbone with LoRA-based parameter-efficient fine-tuning, and employs a graph neural network to learn node representations that capture symptom-disease-treatment dependencies. By explicitly modeling diseases, symptoms, pesticides, and control measures as linked entities, the system supports evidence-aware retrieval beyond surface-level text similarity. Retrieved graph evidence is incorporated into the LLM input to guide generation toward domain-consistent recommendations and to mitigate hallucinated or inappropriate treatments. Experimental results show consistent improvements over text-only baselines, with the largest gains observed on multi-hop and comparative reasoning questions that require chaining multiple relations.

</details>


### [112] [WideSeek: Advancing Wide Research via Multi-Agent Scaling](https://arxiv.org/abs/2602.02636)
*Ziyang Huang,Haolin Ren,Xiaowei Yuan,Jiawei Wang,Zhongtao Jiang,Kun Xu,Shizhu He,Jun Zhao,Kang Liu*

Main category: cs.CL

TL;DR: The paper introduces WideSeekBench and WideSeek to address the lack of benchmarks and optimization methodologies for Wide Research, focusing on data diversity and multi-agent architecture with RL training.


<details>
  <summary>Details</summary>
Motivation: To advance Wide Research by addressing gaps in benchmarks and optimization methods for search breadth.

Method: Develops WideSeekBench via a multi-phase data pipeline and WideSeek, a dynamic hierarchical multi-agent architecture trained with end-to-end RL.

Result: Demonstrates effectiveness of WideSeek and multi-agent RL, showing promise in scaling agents for Wide Research.

Conclusion: Scaling the number of agents and employing RL are effective strategies for progressing Wide Research.

Abstract: Search intelligence is evolving from Deep Research to Wide Research, a paradigm essential for retrieving and synthesizing comprehensive information under complex constraints in parallel. However, progress in this field is impeded by the lack of dedicated benchmarks and optimization methodologies for search breadth. To address these challenges, we take a deep dive into Wide Research from two perspectives: Data Pipeline and Agent Optimization. First, we produce WideSeekBench, a General Broad Information Seeking (GBIS) benchmark constructed via a rigorous multi-phase data pipeline to ensure diversity across the target information volume, logical constraints, and domains. Second, we introduce WideSeek, a dynamic hierarchical multi-agent architecture that can autonomously fork parallel sub-agents based on task requirements. Furthermore, we design a unified training framework that linearizes multi-agent trajectories and optimizes the system using end-to-end RL. Experimental results demonstrate the effectiveness of WideSeek and multi-agent RL, highlighting that scaling the number of agents is a promising direction for advancing the Wide Research paradigm.

</details>


### [113] [Monotonicity as an Architectural Bias for Robust Language Models](https://arxiv.org/abs/2602.02686)
*Patrick Cooper,Alireza Nadali,Ashutosh Trivedi,Alvaro Velasquez*

Main category: cs.CL

TL;DR: The paper explores monotonicity as an architectural bias to enhance the robustness of Transformer-based LLMs against adversarial attacks, maintaining performance while reducing attack success rates.


<details>
  <summary>Details</summary>
Motivation: LLMs are fragile under adversarial prompts, highlighting the need for robustness improvements without compromising expressivity.

Method: Enforce monotonicity selectively in feed-forward sublayers of Transformers, keeping attention mechanisms flexible.

Result: Adversarial attack success rates drop from ~69% to 19%, with minimal impact on standard summarization performance.

Conclusion: Monotonicity improves robustness significantly while preserving model expressivity and performance.

Abstract: Large language models (LLMs) are known to exhibit brittle behavior under adversarial prompts and jailbreak attacks, even after extensive alignment and fine-tuning. This fragility reflects a broader challenge of modern neural language models: small, carefully structured perturbations in high-dimensional input spaces can induce large and unpredictable changes in internal semantic representations and output.
  We investigate monotonicity as an architectural inductive bias for improving the robustness of Transformer-based language models. Monotonicity constrains semantic transformations so that strengthening information, evidence, or constraints cannot lead to regressions in the corresponding internal representations. Such order-preserving behavior has long been exploited in control and safety-critical systems to simplify reasoning and improve robustness, but has traditionally been viewed as incompatible with the expressivity required by neural language models.
  We show that this trade-off is not inherent. By enforcing monotonicity selectively in the feed-forward sublayers of sequence-to-sequence Transformers -- while leaving attention mechanisms unconstrained -- we obtain monotone language models that preserve the performance of their pretrained counterparts. This architectural separation allows negation, contradiction, and contextual interactions to be introduced explicitly through attention, while ensuring that subsequent semantic refinement is order-preserving. Empirically, monotonicity substantially improves robustness: adversarial attack success rates drop from approximately 69% to 19%, while standard summarization performance degrades only marginally.

</details>


### [114] [InfMem: Learning System-2 Memory Control for Long-Context Agent](https://arxiv.org/abs/2602.02704)
*Xinyu Wang,Mingze Li,Peng Lu,Xiao-Wen Chang,Lifeng Shang,Jinping Li,Fei Mi,Prasanna Parthasarathi,Yufei Cui*

Main category: cs.CL

TL;DR: InfMem, a control-centric agent, improves reasoning over ultra-long documents by actively monitoring evidence, performing targeted retrieval, and applying evidence-aware compression, outperforming MemAgent with significant accuracy gains and reduced inference time.


<details>
  <summary>Details</summary>
Motivation: Handling ultra-long documents requires preserving sparse, bridging evidence for multi-hop reasoning under memory constraints. Current streaming agents' passive memory updates often fail to retain low-salience evidence.

Method: InfMem uses a PreThink-Retrieve-Write protocol for System-2-style control, actively monitoring evidence sufficiency, performing in-document retrieval, and applying joint compression to update memory. Training involves SFT-to-RL alignment.

Result: InfMem outperforms MemAgent on benchmarks up to 1M tokens, improving accuracy by +10.17 to +11.84 points on different models and reducing inference time by up to 5.1×.

Conclusion: InfMem demonstrates superior performance in reasoning over ultra-long documents through active evidence monitoring and targeted retrieval, achieving significant accuracy and efficiency gains.

Abstract: Reasoning over ultra-long documents requires synthesizing sparse evidence scattered across distant segments under strict memory constraints. While streaming agents enable scalable processing, their passive memory update strategy often fails to preserve low-salience bridging evidence required for multi-hop reasoning. We propose InfMem, a control-centric agent that instantiates System-2-style control via a PreThink-Retrieve-Write protocol. InfMem actively monitors evidence sufficiency, performs targeted in-document retrieval, and applies evidence-aware joint compression to update a bounded memory. To ensure reliable control, we introduce a practical SFT-to-RL training recipe that aligns retrieval, writing, and stopping decisions with end-task correctness. On ultra-long QA benchmarks from 32k to 1M tokens, InfMem consistently outperforms MemAgent across backbones. Specifically, InfMem improves average absolute accuracy by +10.17, +11.84, and +8.23 points on Qwen3-1.7B, Qwen3-4B, and Qwen2.5-7B, respectively, while reducing inference time by $3.9\times$ on average (up to $5.1\times$) via adaptive early stopping.

</details>


### [115] [Predicting first-episode homelessness among US Veterans using longitudinal EHR data: time-varying models and social risk factors](https://arxiv.org/abs/2602.02731)
*Rohan Pandey,Haijuan Yan,Hong Yu,Jack Tsai*

Main category: cs.CL

TL;DR: The study uses EHR data to predict homelessness in veterans, showing improved accuracy by including social and behavioral factors, with LLMs performing worse than encoder-based models but with less racial disparity.


<details>
  <summary>Details</summary>
Motivation: Address homelessness among US veterans by leveraging EHR data for proactive intervention.

Method: Analyzed EHR data from 4,276,403 veterans, comparing classical ML, transformer-based models, and fine-tuned LLMs with static and time-varying representations.

Result: Incorporating social/behavioral factors improved PR-AUC by 15-30%. LLMs underperformed in discrimination but reduced racial disparities.

Conclusion: Longitudinal, socially informed EHR modeling enables targeted homelessness prevention strategies for veterans.

Abstract: Homelessness among US veterans remains a critical public health challenge, yet risk prediction offers a pathway for proactive intervention. In this retrospective prognostic study, we analyzed electronic health record (EHR) data from 4,276,403 Veterans Affairs patients during a 2016 observation period to predict first-episode homelessness occurring 3-12 months later in 2017 (prevalence: 0.32-1.19%). We constructed static and time-varying EHR representations, utilizing clinician-informed logic to model the persistence of clinical conditions and social risks over time. We then compared the performance of classical machine learning, transformer-based masked language models, and fine-tuned large language models (LLMs). We demonstrate that incorporating social and behavioral factors into longitudinal models improved precision-recall area under the curve (PR-AUC) by 15-30%. In the top 1% risk tier, models yielded positive predictive values ranging from 3.93-4.72% at 3 months, 7.39-8.30% at 6 months, 9.84-11.41% at 9 months, and 11.65-13.80% at 12 months across model architectures. Large language models underperformed encoder-based models on discrimination but showed smaller performance disparities across racial groups. These results demonstrate that longitudinal, socially informed EHR modeling concentrates homelessness risk into actionable strata, enabling targeted and data-informed prevention strategies for at-risk veterans.

</details>


### [116] [Time-Critical Multimodal Medical Transportation: Organs, Patients, and Medical Supplies](https://arxiv.org/abs/2602.02736)
*Elaheh Sabziyan Varnousfaderani,Syed A. M. Shihab,Mohammad Taghizadeh*

Main category: cs.CL

TL;DR: The paper proposes a multimodal transportation system combining ground and air vehicles for efficient medical transport, using a greedy heuristic algorithm to optimize fleet configurations and minimize costs and time.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of traditional ground ambulances (traffic) and air vehicles (cost, weather) in medical transport by integrating multimodal solutions.

Method: A constructive greedy heuristic algorithm tests four fleet configurations, considering payload consolidation, traffic, and weather constraints.

Result: Comparative evaluation of fleet types identifies the most effective configurations balancing efficiency, cost, and transportation time.

Conclusion: Integrated multimodal fleets outperform single-mode solutions, offering optimized medical transportation under diverse conditions.

Abstract: Timely transportation of organs, patients, and medical supplies is critical to modern healthcare, particularly in emergencies and transplant scenarios where even short delays can severely impact outcomes. Traditional ground-based vehicles such as ambulances are often hindered by traffic congestion; while air vehicles such as helicopters are faster but costly. Emerging air vehicles -- Unmanned Aerial Vehicles and electric vertical take-off and landing aircraft -- have lower operating costs, but remain limited by range and susceptibility to weather conditions. A multimodal transportation system that integrates both air and ground vehicles can leverage the strengths of each to enhance overall transportation efficiency. This study introduces a constructive greedy heuristic algorithm for multimodal vehicle dispatching for medical transportation. Four different fleet configurations were tested: (i) ambulances only, (ii) ambulances with Unmanned Aerial Vehicles, (iii) ambulances with electric vertical take-off and landing aircraft, and (iv) a fully integrated fleet of ambulances, Unmanned Aerial Vehicles, and electric vertical take-off and landing aircraft. The algorithm incorporates payload consolidation across compatible routes, accounts for traffic congestion in ground operations and weather conditions in aerial operations, while enabling rapid vehicle dispatching compared to computationally intensive optimization models. Using a common set of conditions, we evaluate all four fleet types to identify the most effective configurations for fulfilling medical transportation needs while minimizing operating costs, recharging/fuel costs, and total transportation time.

</details>


### [117] [From Task Solving to Robust Real-World Adaptation in LLM Agents](https://arxiv.org/abs/2602.02760)
*Pouya Pezeshkpour,Estevam Hruschka*

Main category: cs.CL

TL;DR: The paper evaluates the robustness of large language model (LLM) agents in real-world deployment scenarios, focusing on challenges like partial observability, noise, dynamic environments, and agent state changes. It uses a grid-based game to test adaptability and finds significant gaps between task-solving ability and deployment-ready robustness.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the gap between ideal, clean-interface evaluations of LLM agents and the messy, uncertain conditions they face in real-world deployments. The paper aims to stress-test agents under realistic operational challenges.

Method: The method involves benchmarking five state-of-the-art LLM agents in a grid-based game with long-horizon execution. The game violates clean-interface assumptions, requiring agents to adapt to partial observability, dynamic environments, noisy signals, and changing agent states.

Result: Results show large gaps between nominal task-solving and deployment-like robustness. Performance degrades with larger grids and longer horizons, and model rankings vary unpredictably. Agents partially infer objectives and exhibit trade-offs between completion, efficiency, and penalty avoidance.

Conclusion: The paper concludes that current LLM agents struggle with real-world robustness due to sensitivity to uncertainty and changing conditions. It calls for future work on verification, safe action selection, and objective inference under partial observability and noise.

Abstract: Large language models are increasingly deployed as specialized agents that plan, call tools, and take actions over extended horizons. Yet many existing evaluations assume a "clean interface" where dynamics are specified and stable, tools and sensors are reliable, and success is captured by a single explicit objective-often overestimating real-world readiness. In practice, agents face underspecified rules, unreliable signals, shifting environments, and implicit, multi-stakeholder goals. The challenge is therefore not just solving tasks, but adapting while solving: deciding what to trust, what is wanted, when to verify, and when to fall back or escalate. We stress-test deployment-relevant robustness under four operational circumstances: partial observability, dynamic environments, noisy signals, and dynamic agent state. We benchmark agentic LLMs in a grid-based game with a simple goal but long-horizon execution. Episodes violate clean-interface assumptions yet remain solvable, forcing agents to infer rules, pay for information, adapt to environmental and internal shifts, and act cautiously under noise. Across five state-of-the-art LLM agents, we find large gaps between nominal task-solving and deployment-like robustness. Performance generally degrades as grid size and horizon increase, but rankings are unstable: weaker models can beat stronger ones when strategy matches the uncertainty regime. Despite no explicit instruction, agents trade off completion, efficiency, and penalty avoidance, suggesting partial objective inference. Ablations and feature analyses reveal model-specific sensitivities and failure drivers, motivating work on verification, safe action selection, and objective inference under partial observability, noise, and non-stationarity.

</details>


### [118] [AmharicStoryQA: A Multicultural Story Question Answering Benchmark in Amharic](https://arxiv.org/abs/2602.02774)
*Israel Abebe Azime,Abenezer Kebede Angamo,Hana Mekonen Tamiru,Dagnachew Mekonnen Marilign,Philipp Slusallek,Seid Muhie Yimam,Dietrich Klakow*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: With the growing emphasis on multilingual and cultural evaluation benchmarks for large language models, language and culture are often treated as synonymous, and performance is commonly used as a proxy for a models understanding of a given language. In this work, we argue that such evaluations overlook meaningful cultural variation that exists within a single language. We address this gap by focusing on narratives from different regions of Ethiopia and demonstrate that, despite shared linguistic characteristics, region-specific and domain-specific content substantially influences language evaluation outcomes. To this end, we introduce \textbf{\textit{AmharicStoryQA}}, a long-sequence story question answering benchmark grounded in culturally diverse narratives from Amharic-speaking regions. Using this benchmark, we reveal a significant narrative understanding gap in existing LLMs, highlight pronounced regional differences in evaluation results, and show that supervised fine-tuning yields uneven improvements across regions and evaluation settings. Our findings emphasize the need for culturally grounded benchmarks that go beyond language-level evaluation to more accurately assess and improve narrative understanding in low-resource languages.

</details>


### [119] [LatentMem: Customizing Latent Memory for Multi-Agent Systems](https://arxiv.org/abs/2602.03036)
*Muxin Fu,Guibin Zhang,Xiangyuan Xue,Yafu Li,Zefeng He,Siyuan Huang,Xiaoye Qu,Yu Cheng,Yang Yang*

Main category: cs.CL

TL;DR: LatentMem introduces a learnable memory framework for LLM-powered multi-agent systems, addressing bottlenecks in memory homogenization and information overload. It includes an experience bank and memory composer, optimized via LMPO, achieving significant performance gains.


<details>
  <summary>Details</summary>
Motivation: Current multi-agent memory designs suffer from memory homogenization due to lack of role-aware customization and information overload from overly detailed entries.

Method: Proposed LatentMem framework consists of an experience bank for lightweight storage and a memory composer for compact latent memories. LMPO optimizes memory representations.

Result: LatentMem achieves up to 19.36% performance improvement over vanilla settings and outperforms existing memory architectures across benchmarks.

Conclusion: LatentMem effectively enhances collective intelligence in MAS by addressing memory bottlenecks, demonstrating broad applicability without framework modifications.

Abstract: Large language model (LLM)-powered multi-agent systems (MAS) demonstrate remarkable collective intelligence, wherein multi-agent memory serves as a pivotal mechanism for continual adaptation. However, existing multi-agent memory designs remain constrained by two fundamental bottlenecks: (i) memory homogenization arising from the absence of role-aware customization, and (ii) information overload induced by excessively fine-grained memory entries. To address these limitations, we propose LatentMem, a learnable multi-agent memory framework designed to customize agent-specific memories in a token-efficient manner. Specifically, LatentMem comprises an experience bank that stores raw interaction trajectories in a lightweight form, and a memory composer that synthesizes compact latent memories conditioned on retrieved experience and agent-specific contexts. Further, we introduce Latent Memory Policy Optimization (LMPO), which propagates task-level optimization signals through latent memories to the composer, encouraging it to produce compact and high-utility representations. Extensive experiments across diverse benchmarks and mainstream MAS frameworks show that LatentMem achieves a performance gain of up to $19.36$% over vanilla settings and consistently outperforms existing memory architectures, without requiring any modifications to the underlying frameworks.

</details>


### [120] [When Efficient Communication Explains Convexity](https://arxiv.org/abs/2602.02821)
*Ashvin Ranjan,Shane Steinert-Threlkeld*

Main category: cs.CL

TL;DR: The paper explores efficient communication in languages, linking optimality in the Information Bottleneck framework to convexity and identifying key factors driving this relationship.


<details>
  <summary>Details</summary>
Motivation: To understand why efficient communication explains language variation, especially focusing on semantic typology and the balance between simplicity and informativeness.

Method: Uses the Information Bottleneck (IB) approach to analyze the trade-off, conducts experiments correlating IB optimality with convexity, and manipulates modeling parameters to identify driving factors.

Result: Finds that the convexity of communicative need distribution is crucial for explaining the correlation between convexity and optimality.

Conclusion: The study advances understanding of efficient communication in languages by pinpointing specific underlying factors responsible for observed patterns.

Abstract: Much recent work has argued that the variation in the languages of the world can be explained from the perspective of efficient communication; in particular, languages can be seen as optimally balancing competing pressures to be simple and to be informative. Focusing on the expression of meaning -- semantic typology -- the present paper asks what factors are responsible for successful explanations in terms of efficient communication. Using the Information Bottleneck (IB) approach to formalizing this trade-off, we first demonstrate and analyze a correlation between optimality in the IB sense and a novel generalization of convexity to this setting. In a second experiment, we manipulate various modeling parameters in the IB framework to determine which factors drive the correlation between convexity and optimality. We find that the convexity of the communicative need distribution plays an especially important role. These results move beyond showing that efficient communication can explain aspects of semantic typology into explanations for why that is the case by identifying which underlying factors are responsible.

</details>


### [121] [R2-Router: A New Paradigm for LLM Routing with Reasoning](https://arxiv.org/abs/2602.02823)
*Jiaqi Xue,Qian Lou,Jiarong Xing,Heng Huang*

Main category: cs.CL

TL;DR: R2-Router introduces a method to jointly select the best LLM and output length budget, improving quality and reducing costs by 4-5x compared to existing routers.


<details>
  <summary>Details</summary>
Motivation: Existing LLM routers assume fixed quality and cost per query, ignoring output length's impact, which leads to suboptimal routing decisions.

Method: R2-Router treats output length as a controllable variable, jointly selecting the best LLM and length budget via length-constrained instructions. It also introduces R2-Bench, a dataset for LLM behavior across output lengths.

Result: R2-Router outperforms existing routers, achieving state-of-the-art performance at 4-5x lower cost.

Conclusion: This work redefines routing as reasoning, enabling routers to deliberate over LLM selection and cost budgets, opening new research directions.

Abstract: As LLMs proliferate with diverse capabilities and costs, LLM routing has emerged by learning to predict each LLM's quality and cost for a given query, then selecting the one with high quality and low cost. However, existing routers implicitly assume a single fixed quality and cost per LLM for each query, ignoring that the same LLM's quality varies with its output length. This causes routers to exclude powerful LLMs when their estimated cost exceeds the budget, missing the opportunity that these LLMs could still deliver high quality at reduced cost with shorter outputs. To address this, we introduce R2-Router, which treats output length budget as a controllable variable and jointly selects the best LLM and length budget, enforcing the budget via length-constrained instructions. This enables R2-Router to discover that a powerful LLM with constrained output can outperform a weaker LLM at comparable cost-efficient configurations invisible to prior methods. Together with the router framework, we construct R2-Bench, the first routing dataset capturing LLM behavior across diverse output length budgets. Experiments show that R2-Router achieves state-of-the-art performance at 4-5x lower cost compared with existing routers. This work opens a new direction: routing as reasoning, where routers evolve from reactive selectors to deliberate reasoners that explore which LLM to use and at what cost budget.

</details>


### [122] [CATNIP: LLM Unlearning via Calibrated and Tokenized Negative Preference Alignment](https://arxiv.org/abs/2602.02824)
*Zhengbang Yang,Yisheng Zhong,Junyuan Hong,Zhuangdi Zhu*

Main category: cs.CL

TL;DR: CATNIP introduces a calibrated, token-level method for effective LLM unlearning, improving precision and robustness without needing retention data or contrastive pairs.


<details>
  <summary>Details</summary>
Motivation: Address safety and privacy concerns by selectively removing undesirable knowledge from LLMs while minimizing degradation of general domain knowledge.

Method: CATNIP uses token-level confidence to rescale unlearning effects, enabling precise gradient updates and handling data scarcity and length variation.

Result: Outperforms state-of-the-art methods on MUSE and WMDP benchmarks, achieving better forgetting and knowledge preservation without extra data requirements.

Conclusion: CATNIP provides a robust and efficient solution for LLM unlearning, addressing key limitations of existing approaches.

Abstract: Pretrained knowledge memorized in LLMs raises critical concerns over safety and privacy, which has motivated LLM Unlearning as a technique for selectively removing the influences of undesirable knowledge. Existing approaches, rooted in Gradient Ascent (GA), often degrade general domain knowledge while relying on retention data or curated contrastive pairs, which can be either impractical or data and computationally prohibitive. Negative Preference Alignment has been explored for unlearning to tackle the limitations of GA, which, however, remains confined by its choice of reference model and shows undermined performance in realistic data settings. These limitations raise two key questions: i) Can we achieve effective unlearning that quantifies model confidence in undesirable knowledge and uses it to calibrate gradient updates more precisely, thus reducing catastrophic forgetting? ii) Can we make unlearning robust to data scarcity and length variation? We answer both questions affirmatively with CATNIP (Calibrated and Tokenized Negative Preference Alignment), a principled method that rescales unlearning effects in proportion to the model's token-level confidence, thus ensuring fine-grained control over forgetting. Extensive evaluations on MUSE and WMDP benchmarks demonstrated that our work enables effective unlearning without requiring retention data or contrastive unlearning response pairs, with stronger knowledge forgetting and preservation tradeoffs than state-of-the-art methods.

</details>


### [123] [Act or Clarify? Modeling Sensitivity to Uncertainty and Cost in Communication](https://arxiv.org/abs/2602.02843)
*Polina Tsvilodub,Karl Mulligan,Todd Snider,Robert D. Hawkins,Michael Franke*

Main category: cs.CL

TL;DR: The paper explores how agents decide to ask clarification questions (CQs) under uncertainty, influenced by context and action costs, formalized in an expected regret model. Experiments confirm humans balance seeking clarification with potential loss risk.


<details>
  <summary>Details</summary>
Motivation: To understand how agents balance uncertainty reduction (via CQs) with action costs in communicative settings, predicting context and cost interactions influence decisions.

Method: Develops a computational model based on expected regret and tests it through two experiments: one on linguistic responses and another on choices between clarification and non-linguistic actions.

Result: Experimental results align with the model, showing humans seek clarification proportionally to the risk of substantial loss when acting under uncertainty.

Conclusion: The study highlights a rational tradeoff in human behavior: clarification-seeking increases with higher potential loss risks due to uncertainty.

Abstract: When deciding how to act under uncertainty, agents may choose to act to reduce uncertainty or they may act despite that uncertainty.In communicative settings, an important way of reducing uncertainty is by asking clarification questions (CQs). We predict that the decision to ask a CQ depends on both contextual uncertainty and the cost of alternative actions, and that these factors interact: uncertainty should matter most when acting incorrectly is costly. We formalize this interaction in a computational model based on expected regret: how much an agent stands to lose by acting now rather than with full information. We test these predictions in two experiments, one examining purely linguistic responses to questions and another extending to choices between clarification and non-linguistic action. Taken together, our results suggest a rational tradeoff: humans tend to seek clarification proportional to the risk of substantial loss when acting under uncertainty.

</details>


### [124] [Which course? Discourse! Teaching Discourse and Generation in the Era of LLMs](https://arxiv.org/abs/2602.02878)
*Junyi Jessy Li,Yang Janet Liu,Kanishka Misra,Valentina Pyatkin,William Sheffield*

Main category: cs.CL

TL;DR: The paper introduces a new undergraduate course, 'Computational Discourse and Natural Language Generation,' designed to bridge NLP sub-disciplines by integrating discourse processing theory with practical applications.


<details>
  <summary>Details</summary>
Motivation: To address gaps in NLP education by connecting discourse processing with text generation, areas often overlooked in curricula.

Method: Collaboratively designed an interdisciplinary course combining Linguistics and Computer Science, integrating theory and practice.

Result: The course was successfully offered, fostering exploratory learning, as reflected in student surveys.

Conclusion: The course model shows promise for future interdisciplinary NLP education, with room for expansion and adaptation.

Abstract: The field of NLP has undergone vast, continuous transformations over the past few years, sparking debates going beyond discipline boundaries. This begs important questions in education: how do we design courses that bridge sub-disciplines in this shifting landscape? This paper explores this question from the angle of discourse processing, an area with rich linguistic insights and computational models for the intentional, attentional, and coherence structure of language. Discourse is highly relevant for open-ended or long-form text generation, yet this connection is under-explored in existing undergraduate curricula. We present a new course, "Computational Discourse and Natural Language Generation". The course is collaboratively designed by a team with complementary expertise and was offered for the first time in Fall 2025 as an upper-level undergraduate course, cross-listed between Linguistics and Computer Science. Our philosophy is to deeply integrate the theoretical and empirical aspects, and create an exploratory mindset inside the classroom and in the assignments. This paper describes the course in detail and concludes with takeaways from an independent survey as well as our vision for future directions.

</details>


### [125] [HALT: Hallucination Assessment via Log-probs as Time series](https://arxiv.org/abs/2602.02888)
*Ahmad Shapiro,Karan Taneja,Ashok Goel*

Main category: cs.CL

TL;DR: HALT is a lightweight hallucination detector for LLMs that uses token log-probabilities and entropy-based features, outperforming larger models like Lettuce while being faster and more efficient. HUB benchmarks its performance across diverse tasks.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in LLMs hinder reliability, especially in safety-critical domains, prompting the need for efficient detection methods without accessing internal model states.

Method: HALT utilizes top-20 token log-probabilities as a time series, employing a gated recurrent unit model with entropy-based features to detect model calibration bias.

Result: HALT is 30x smaller and 60x faster than Lettuce, outperforming it on the HUB benchmark covering ten LLM capabilities.

Conclusion: HALT and HUB provide an efficient, domain-general framework for hallucination detection in LLMs without requiring access to internal weights.

Abstract: Hallucinations remain a major obstacle for large language models (LLMs), especially in safety-critical domains. We present HALT (Hallucination Assessment via Log-probs as Time series), a lightweight hallucination detector that leverages only the top-20 token log-probabilities from LLM generations as a time series. HALT uses a gated recurrent unit model combined with entropy-based features to learn model calibration bias, providing an extremely efficient alternative to large encoders. Unlike white-box approaches, HALT does not require access to hidden states or attention maps, relying only on output log-probabilities. Unlike black-box approaches, it operates on log-probs rather than surface-form text, which enables stronger domain generalization and compatibility with proprietary LLMs without requiring access to internal weights. To benchmark performance, we introduce HUB (Hallucination detection Unified Benchmark), which consolidates prior datasets into ten capabilities covering both reasoning tasks (Algorithmic, Commonsense, Mathematical, Symbolic, Code Generation) and general purpose skills (Chat, Data-to-Text, Question Answering, Summarization, World Knowledge). While being 30x smaller, HALT outperforms Lettuce, a fine-tuned modernBERT-base encoder, achieving a 60x speedup gain on HUB. HALT and HUB together establish an effective framework for hallucination detection across diverse LLM capabilities.

</details>


### [126] [Equal Access, Unequal Interaction: A Counterfactual Audit of LLM Fairness](https://arxiv.org/abs/2602.02932)
*Alireza Amiri-Margavi,Arshia Gharagozlou,Amin Gholami Davodi,Seyed Pouyan Mousavi Davoudi,Hamidreza Hasani Balyani*

Main category: cs.CL

TL;DR: Fairness audits reveal LLMs show disparities in tone and framing across demographics despite equal access, highlighting the need for broader evaluation beyond refusals.


<details>
  <summary>Details</summary>
Motivation: To assess whether equitable access to LLMs ensures equitable interaction quality across different demographic identities.

Method: A controlled fairness audit using counterfactual prompts evaluated GPT-4 and LLaMA-3.1-70B on career advice tasks, varying age, gender, and nationality. Access fairness was measured via refusal rates, and interaction quality via linguistic metrics (sentiment, politeness, hedging).

Result: Both models had zero refusal rates but showed systematic disparities: GPT-4 hedged more with younger males, while LLaMA exhibited broader sentiment variation across identities.

Conclusion: Fairness disparities persist at interaction levels even with equal access, urging evaluations beyond refusal-based audits.

Abstract: Prior work on fairness in large language models (LLMs) has primarily focused on access-level behaviors such as refusals and safety filtering. However, equitable access does not ensure equitable interaction quality once a response is provided. In this paper, we conduct a controlled fairness audit examining how LLMs differ in tone, uncertainty, and linguistic framing across demographic identities after access is granted. Using a counterfactual prompt design, we evaluate GPT-4 and LLaMA-3.1-70B on career advice tasks while varying identity attributes along age, gender, and nationality. We assess access fairness through refusal analysis and measure interaction quality using automated linguistic metrics, including sentiment, politeness, and hedging. Identity-conditioned differences are evaluated using paired statistical tests. Both models exhibit zero refusal rates across all identities, indicating uniform access. Nevertheless, we observe systematic, model-specific disparities in interaction quality: GPT-4 expresses significantly higher hedging toward younger male users, while LLaMA exhibits broader sentiment variation across identity groups. These results show that fairness disparities can persist at the interaction level even when access is equal, motivating evaluation beyond refusal-based audits.

</details>


### [127] [Where Norms and References Collide: Evaluating LLMs on Normative Reasoning](https://arxiv.org/abs/2602.02975)
*Mitchell Abrams,Kaveh Eskandari Miandoab,Felix Gervits,Vasanth Sarathy,Matthias Scheutz*

Main category: cs.CL

TL;DR: The paper introduces SNIC, a testbed to evaluate how well LLMs handle norm-based reference resolution (NBRR) in socially situated environments, finding that current models struggle with implicit or conflicting norms.


<details>
  <summary>Details</summary>
Motivation: To assess whether LLMs can effectively reason over social norms for tasks requiring NBRR, crucial for embodied agents like robots operating in human environments.

Method: Developed SNIC, a human-validated diagnostic testbed focusing on everyday norms (e.g., cleaning, serving) to evaluate LLMs' ability to identify and apply social norms in NBRR.

Result: State-of-the-art LLMs perform inconsistently in recognizing and applying social norms, especially when norms are implicit, underspecified, or conflicting.

Conclusion: Current LLMs have a blind spot in social norm reasoning, posing a challenge for deploying language-based systems in socially situated, embodied contexts.

Abstract: Embodied agents, such as robots, will need to interact in situated environments where successful communication often depends on reasoning over social norms: shared expectations that constrain what actions are appropriate in context. A key capability in such settings is norm-based reference resolution (NBRR), where interpreting referential expressions requires inferring implicit normative expectations grounded in physical and social context. Yet it remains unclear whether Large Language Models (LLMs) can support this kind of reasoning. In this work, we introduce SNIC (Situated Norms in Context), a human-validated diagnostic testbed designed to probe how well state-of-the-art LLMs can extract and utilize normative principles relevant to NBRR. SNIC emphasizes physically grounded norms that arise in everyday tasks such as cleaning, tidying, and serving. Across a range of controlled evaluations, we find that even the strongest LLMs struggle to consistently identify and apply social norms, particularly when norms are implicit, underspecified, or in conflict. These findings reveal a blind spot in current LLMs and highlight a key challenge for deploying language-based systems in socially situated, embodied settings.

</details>


### [128] [CPMobius: Iterative Coach-Player Reasoning for Data-Free Reinforcement Learning](https://arxiv.org/abs/2602.02979)
*Ran Li,Zeyuan Liu,Yinghao chen,Bingxiang He,Jiarui Yuan,Zixuan Fu,Weize Chen,Jinyi Hu,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: CPMöbius introduces a data-free reinforcement learning method for LLMs, using a Coach-Player collaboration to enhance reasoning without external training data, outperforming existing unsupervised approaches.


<details>
  <summary>Details</summary>
Motivation: Current LLM training relies heavily on human-curated data, which is unsustainable and limits scalability. CPMöbius aims to overcome this by enabling unsupervised reasoning improvement.

Method: CPMöbius uses a Coach-Player paradigm where the Coach generates tasks and rewards the Player for solving them, fostering a cooperative loop to enhance reasoning.

Result: CPMöbius outperforms existing methods, improving accuracy by +4.9 overall and +5.4 out-of-distribution, surpassing RENT and R-zero.

Conclusion: CPMöbius demonstrates a scalable, data-free approach to enhance LLM reasoning, reducing reliance on human-curated data.

Abstract: Large Language Models (LLMs) have demonstrated strong potential in complex reasoning, yet their progress remains fundamentally constrained by reliance on massive high-quality human-curated tasks and labels, either through supervised fine-tuning (SFT) or reinforcement learning (RL) on reasoning-specific data. This dependence renders supervision-heavy training paradigms increasingly unsustainable, with signs of diminishing scalability already evident in practice. To overcome this limitation, we introduce CPMöbius (CPMobius), a collaborative Coach-Player paradigm for data-free reinforcement learning of reasoning models. Unlike traditional adversarial self-play, CPMöbius, inspired by real world human sports collaboration and multi-agent collaboration, treats the Coach and Player as independent but cooperative roles. The Coach proposes instructions targeted at the Player's capability and receives rewards based on changes in the Player's performance, while the Player is rewarded for solving the increasingly instructive tasks generated by the Coach. This cooperative optimization loop is designed to directly enhance the Player's mathematical reasoning ability. Remarkably, CPMöbius achieves substantial improvement without relying on any external training data, outperforming existing unsupervised approaches. For example, on Qwen2.5-Math-7B-Instruct, our method improves accuracy by an overall average of +4.9 and an out-of-distribution average of +5.4, exceeding RENT by +1.5 on overall accuracy and R-zero by +4.2 on OOD accuracy.

</details>


### [129] [SAES-SVD: Self-Adaptive Suppression of Accumulated and Local Errors for SVD-based LLM Compression](https://arxiv.org/abs/2602.03051)
*Xing Hu,Dawei Yang,Yuan Cheng,Zhixuan Chen,Zukang Xu*

Main category: cs.CL

TL;DR: SAES-SVD is a novel LLM compression framework that jointly optimizes intra-layer reconstruction and inter-layer error compensation, outperforming existing methods by addressing error propagation and accumulation.


<details>
  <summary>Details</summary>
Motivation: Existing LLM compression techniques independently compress layers, ignoring error propagation, leading to amplified global deviations from full-precision models.

Method: SAES-SVD introduces Cumulative Error-Aware Layer Compression (CEALC) and Adaptive Collaborative Error Suppression (ACES) to optimize reconstruction and error compensation jointly.

Result: SAES-SVD consistently improves post-compression performance across various LLM architectures and tasks without fine-tuning or mixed-rank strategies.

Conclusion: SAES-SVD effectively addresses error propagation in LLM compression, offering superior performance through joint optimization of reconstruction and error compensation.

Abstract: The rapid growth in the parameter scale of large language models (LLMs) has created a high demand for efficient compression techniques. As a hardware-agnostic and highly compatible technique, low-rank compression has been widely adopted. However, existing methods typically compress each layer independently by minimizing per-layer reconstruction error, overlooking a critical limitation: the reconstruction error propagates and accumulates through the network, which leads to amplified global deviations from the full-precision baseline. To address this, we propose Self-Adaptive Error Suppression SVD (SAES-SVD), a LLMs compression framework that jointly optimizes intra-layer reconstruction and inter-layer error compensation. SAES-SVD is composed of two novel components: (1) Cumulative Error-Aware Layer Compression (CEALC), which formulates the compression objective as a combination of local reconstruction and weighted cumulative error compensation. Based on it, we derive a closed-form low-rank solution relied on second-order activation statistics, which explicitly aligns each layer's output with its full-precision counterpart to compensate for accumulated errors. (2) Adaptive Collaborative Error Suppression (ACES), which automatically adjusts the weighting coefficient to enhance the low-rank structure of the compression objective in CEALC. Specifically, the coefficient is optimized to maximize the ratio between the Frobenius norm of the compressed layer's output and that of the compression objective under a fixed rank, thus ensuring that the rank budget is utilized effectively. Extensive experiments across multiple LLM architectures and tasks show that, without fine-tuning or mixed-rank strategies, SAES-SVD consistently improves post-compression performance.

</details>


### [130] [ReMiT: RL-Guided Mid-Training for Iterative LLM Evolution](https://arxiv.org/abs/2602.03075)
*Junjie Huang,Jiarui Qin,Di Yin,Weiwen Liu,Yong Yu,Xing Sun,Weinan Zhang*

Main category: cs.CL

TL;DR: ReMiT introduces a bidirectional training process for LLMs, using RL insights to enhance pre-training, achieving sustained improvements in reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Current LLM training pipelines are unidirectional; exploring bidirectional feedback could improve model performance iteratively.

Method: ReMiT analyzes mid-training dynamics and uses RL-tuned models to reweight tokens in the mid-training phase, focusing on reasoning-critical tokens.

Result: ReMiT improves performance by 3% on pre-training benchmarks and sustains gains by 2% post-training.

Conclusion: The study validates a self-reinforcing feedback loop for continuous LLM evolution.

Abstract: Standard training pipelines for large language models (LLMs) are typically unidirectional, progressing from pre-training to post-training. However, the potential for a bidirectional process--where insights from post-training retroactively improve the pre-trained foundation--remains unexplored. We aim to establish a self-reinforcing flywheel: a cycle in which reinforcement learning (RL)-tuned model strengthens the base model, which in turn enhances subsequent post-training performance, requiring no specially trained teacher or reference model. To realize this, we analyze training dynamics and identify the mid-training (annealing) phase as a critical turning point for model capabilities. This phase typically occurs at the end of pre-training, utilizing high-quality corpora under a rapidly decaying learning rate. Building upon this insight, we introduce ReMiT (Reinforcement Learning-Guided Mid-Training). Specifically, ReMiT leverages the reasoning priors of RL-tuned models to dynamically reweight tokens during the mid-training phase, prioritizing those pivotal for reasoning. Empirically, ReMiT achieves an average improvement of 3\% on 10 pre-training benchmarks, spanning math, code, and general reasoning, and sustains these gains by over 2\% throughout the post-training pipeline. These results validate an iterative feedback loop, enabling continuous and self-reinforcing evolution of LLMs.

</details>


### [131] [AERO: Autonomous Evolutionary Reasoning Optimization via Endogenous Dual-Loop Feedback](https://arxiv.org/abs/2602.03084)
*Zhitao Gao,Jie Ma,Xuhong Li,Pengyu Li,Ning Qu,Yaqiang Wu,Hui Liu,Jun Liu*

Main category: cs.CL

TL;DR: The paper introduces AERO, an unsupervised framework for autonomous reasoning evolution in LLMs, leveraging self-questioning, answering, and criticism to overcome reliance on expert data and avoid hallucinations.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of LLMs, which rely on expert-annotated data and external verifiers, and to mitigate risks like collective hallucinations and incorrect priors.

Method: Proposes AERO, a dual-loop system with self-questioning, answering, and criticism, using entropy-based positioning and Independent Counterfactual Correction. Includes a Staggered Training Strategy.

Result: AERO outperforms baselines with average improvements of 4.57% on Qwen3-4B-Base and 5.10% on Qwen3-8B-Base across nine benchmarks.

Conclusion: AERO effectively enhances LLM reasoning autonomously, avoiding reliance on external inputs and mitigating flaws in self-evolution paradigms.

Abstract: Large Language Models (LLMs) have achieved significant success in complex reasoning but remain bottlenecked by reliance on expert-annotated data and external verifiers. While existing self-evolution paradigms aim to bypass these constraints, they often fail to identify the optimal learning zone and risk reinforcing collective hallucinations and incorrect priors through flawed internal feedback. To address these challenges, we propose \underline{A}utonomous \underline{E}volutionary \underline{R}easoning \underline{O}ptimization (AERO), an unsupervised framework that achieves autonomous reasoning evolution by internalizing self-questioning, answering, and criticism within a synergistic dual-loop system. Inspired by the \textit{Zone of Proximal Development (ZPD)} theory, AERO utilizes entropy-based positioning to target the ``solvability gap'' and employs Independent Counterfactual Correction for robust verification. Furthermore, we introduce a Staggered Training Strategy to synchronize capability growth across functional roles and prevent curriculum collapse. Extensive evaluations across nine benchmarks spanning three domains demonstrate that AERO achieves average performance improvements of 4.57\% on Qwen3-4B-Base and 5.10\% on Qwen3-8B-Base, outperforming competitive baselines. Code is available at https://github.com/mira-ai-lab/AERO.

</details>


### [132] [Test-time Recursive Thinking: Self-Improvement without External Feedback](https://arxiv.org/abs/2602.03094)
*Yufan Zhuang,Chandan Singh,Liyuan Liu,Yelong Shen,Dinghuai Zhang,Jingbo Shang,Jianfeng Gao,Weizhu Chen*

Main category: cs.CL

TL;DR: TRT enables LLMs to self-improve without additional training by leveraging iterative generation and self-verification, achieving significant accuracy gains on challenging benchmarks.


<details>
  <summary>Details</summary>
Motivation: The paper explores whether LLMs can enhance their reasoning capabilities autonomously, addressing the challenges of diverse solution generation and reliable answer selection without ground-truth supervision.

Method: Proposes Test-time Recursive Thinking (TRT), an iterative framework combining rollout-specific strategies, accumulated knowledge, and self-generated verification signals.

Result: Open-source models achieve 100% accuracy on AIME-25/24, and closed-source models improve by 10.4-14.8 percentage points on LiveCodeBench's hardest problems.

Conclusion: TRT demonstrates effective self-improvement in LLMs without external feedback, showcasing potential for autonomous reasoning enhancement.

Abstract: Modern Large Language Models (LLMs) have shown rapid improvements in reasoning capabilities, driven largely by reinforcement learning (RL) with verifiable rewards. Here, we ask whether these LLMs can self-improve without the need for additional training. We identify two core challenges for such systems: (i) efficiently generating diverse, high-quality candidate solutions, and (ii) reliably selecting correct answers in the absence of ground-truth supervision. To address these challenges, we propose Test-time Recursive Thinking (TRT), an iterative self-improvement framework that conditions generation on rollout-specific strategies, accumulated knowledge, and self-generated verification signals. Using TRT, open-source models reach 100% accuracy on AIME-25/24, and on LiveCodeBench's most difficult problems, closed-source models improve by 10.4-14.8 percentage points without external feedback.

</details>


### [133] [Task--Specificity Score: Measuring How Much Instructions Really Matter for Supervision](https://arxiv.org/abs/2602.03103)
*Pritam Kadasi,Abhishek Upperwal,Mayank Singh*

Main category: cs.CL

TL;DR: The paper introduces Task-Specificity Score (TSS) and TSS++ to measure how uniquely an instruction determines its output, showing improved downstream performance when selecting task-specific examples.


<details>
  <summary>Details</summary>
Motivation: To address the issue of weakly specified instruction--input--output pairs in instruction tuning, where multiple instructions can yield the same output.

Method: Proposes TSS to quantify instruction uniqueness by comparing true instructions with plausible alternatives, and TSS++ which uses hard alternatives and a quality term to mitigate easy negatives.

Result: Demonstrates that selecting task-specific examples improves performance under tight token budgets and complements quality-based filters across three datasets and three LLMs.

Conclusion: TSS and TSS++ effectively quantify instruction specificity and enhance downstream performance in instruction tuning.

Abstract: Instruction tuning is now the default way to train and adapt large language models, but many instruction--input--output pairs are only weakly specified: for a given input, the same output can remain plausible under several alternative instructions. This raises a simple question: \emph{does the instruction uniquely determine the target output?}
  We propose the \textbf{Task--Specificity Score (TSS)} to quantify how much an instruction matters for predicting its output, by contrasting the true instruction against plausible alternatives for the same input. We further introduce \textbf{TSS++}, which uses hard alternatives and a small quality term to mitigate easy-negative effects. Across three instruction datasets (\textsc{Alpaca}, \textsc{Dolly-15k}, \textsc{NI-20}) and three open LLMs (Gemma, Llama, Qwen), we show that selecting task-specific examples improves downstream performance under tight token budgets and complements quality-based filters such as perplexity and IFD.

</details>


### [134] [The Mask of Civility: Benchmarking Chinese Mock Politeness Comprehension in Large Language Models](https://arxiv.org/abs/2602.03107)
*Yitong Zhang,Yuhan Xiang,Mingxuan Liu*

Main category: cs.CL

TL;DR: The study evaluates six large language models' ability to recognize politeness phenomena in Chinese using pragmatic frameworks and diverse prompting strategies.


<details>
  <summary>Details</summary>
Motivation: Addressing gaps in pragmatic comprehension and exploring the coexistence of technology and humanities.

Method: Uses Rapport Management Theory and Mock Politeness Model to create a dataset; tests six models under four prompting conditions.

Result: Not explicitly stated in the abstract.

Conclusion: A novel interdisciplinary approach bridging linguistic technology and humanistic reflection.

Abstract: From a pragmatic perspective, this study systematically evaluates the differences in performance among representative large language models (LLMs) in recognizing politeness, impoliteness, and mock politeness phenomena in Chinese. Addressing the existing gaps in pragmatic comprehension, the research adopts the frameworks of Rapport Management Theory and the Model of Mock Politeness to construct a three-category dataset combining authentic and simulated Chinese discourse. Six representative models, including GPT-5.1 and DeepSeek, were selected as test subjects and evaluated under four prompting conditions: zero-shot, few-shot, knowledge-enhanced, and hybrid strategies. This study serves as a meaningful attempt within the paradigm of ``Great Linguistics,'' offering a novel approach to applying pragmatic theory in the age of technological transformation. It also responds to the contemporary question of how technology and the humanities may coexist, representing an interdisciplinary endeavor that bridges linguistic technology and humanistic reflection.

</details>


### [135] [ChemPro: A Progressive Chemistry Benchmark for Large Language Models](https://arxiv.org/abs/2602.03108)
*Aaditya Baranwal,Shruti Vyas*

Main category: cs.CL

TL;DR: ChemPro is a new benchmark with 4,100 chemistry questions across 4 difficulty levels to evaluate LLMs' proficiency in general chemistry. While LLMs perform well on basic questions, their accuracy drops with complexity, revealing limitations in scientific reasoning.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' ability to handle a broad spectrum of chemistry topics, from basic to advanced, and identify their limitations in scientific understanding.

Method: ChemPro includes Multiple Choice and Numerical Questions across 4 difficulty levels, covering Bio-Chemistry, Inorganic-Chemistry, Organic-Chemistry, and Physical-Chemistry. It evaluates 45+7 LLMs on their proficiency.

Result: LLMs perform well on basic chemistry questions but struggle with complex reasoning and nuanced articulation, highlighting limitations in general scientific understanding.

Conclusion: ChemPro exposes critical gaps in LLMs' scientific reasoning, emphasizing the need for more robust methodologies to enhance their performance.

Abstract: We introduce ChemPro, a progressive benchmark with 4100 natural language question-answer pairs in Chemistry, across 4 coherent sections of difficulty designed to assess the proficiency of Large Language Models (LLMs) in a broad spectrum of general chemistry topics. We include Multiple Choice Questions and Numerical Questions spread across fine-grained information recall, long-horizon reasoning, multi-concept questions, problem-solving with nuanced articulation, and straightforward questions in a balanced ratio, effectively covering Bio-Chemistry, Inorganic-Chemistry, Organic-Chemistry and Physical-Chemistry. ChemPro is carefully designed analogous to a student's academic evaluation for basic to high-school chemistry. A gradual increase in the question difficulty rigorously tests the ability of LLMs to progress from solving basic problems to solving more sophisticated challenges.
  We evaluate 45+7 state-of-the-art LLMs, spanning both open-source and proprietary variants, and our analysis reveals that while LLMs perform well on basic chemistry questions, their accuracy declines with different types and levels of complexity. These findings highlight the critical limitations of LLMs in general scientific reasoning and understanding and point towards understudied dimensions of difficulty, emphasizing the need for more robust methodologies to improve LLMs.

</details>


### [136] [One Model, All Roles: Multi-Turn, Multi-Agent Self-Play Reinforcement Learning for Conversational Social Intelligence](https://arxiv.org/abs/2602.03109)
*Bowen Jiang,Taiwei Shi,Ryo Kamoi,Yuan Yuan,Camillo J. Taylor,Longqi Yang,Pei Zhou,Sihao Chen*

Main category: cs.CL

TL;DR: OMAR: One Model, All Roles is a reinforcement learning framework enabling AI to develop social intelligence through multi-agent conversational self-play, achieving complex social norms without human supervision.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome limitations of traditional static, single-turn optimization paradigms by enabling AI to role-play all participants in dynamic conversations, fostering emergent social intelligence.

Method: The framework employs hierarchical advantage estimation (turn-level and token-level) for training stability across long dialogues.

Result: Evaluations in SOTOPIA and Werewolf games show fine-grained social intelligence (e.g., empathy, persuasion, compromise) emerges, even in competitive scenarios, though reward hacking challenges persist.

Conclusion: OMAR demonstrates the potential of unsupervised learning for AI social intelligence in group conversations, encouraging further research.

Abstract: This paper introduces OMAR: One Model, All Roles, a reinforcement learning framework that enables AI to develop social intelligence through multi-turn, multi-agent conversational self-play. Unlike traditional paradigms that rely on static, single-turn optimizations, OMAR allows a single model to role-play all participants in a conversation simultaneously, learning to achieve long-term goals and complex social norms directly from dynamic social interaction. To ensure training stability across long dialogues, we implement a hierarchical advantage estimation that calculates turn-level and token-level advantages. Evaluations in the SOTOPIA social environment and Werewolf strategy games show that our trained models develop fine-grained, emergent social intelligence, such as empathy, persuasion, and compromise seeking, demonstrating the effectiveness of learning collaboration even under competitive scenarios. While we identify practical challenges like reward hacking, our results show that rich social intelligence can emerge without human supervision. We hope this work incentivizes further research on AI social intelligence in group conversations.

</details>


### [137] [FASA: Frequency-aware Sparse Attention](https://arxiv.org/abs/2602.03152)
*Yifei Wang,Yueqi Wang,Zhenrui Yue,Huimin Zeng,Yong Wang,Ismini Lourentzou,Zhengzhong Tu,Xiangxiang Chu,Julian McAuley*

Main category: cs.CL

TL;DR: FASA is a novel framework that dynamically predicts token importance to reduce memory footprint in LLMs by leveraging functional sparsity in RoPE's frequency-chunk level.


<details>
  <summary>Details</summary>
Motivation: Handling lengthy inputs in LLMs is hindered by the high memory footprint of the KV cache. Existing token pruning methods are either static (risking information loss) or dynamic (using inadequate heuristics).

Method: FASA identifies dominant frequency-chunks (FCs) in RoPE as proxies for token importance, enabling dynamic token eviction and focused attention computation on a pruned subset.

Result: FASA outperforms baselines in long-context tasks, achieving near-oracle accuracy. On LongBench-V1, it matches full-KV performance with only 256 tokens and provides a 2.56× speedup using 18.9% of the cache.

Conclusion: FASA offers a robust, efficient solution for reducing memory and computational costs in LLMs while maintaining performance.

Abstract: The deployment of Large Language Models (LLMs) faces a critical bottleneck when handling lengthy inputs: the prohibitive memory footprint of the Key Value (KV) cache. To address this bottleneck, the token pruning paradigm leverages attention sparsity to selectively retain a small, critical subset of tokens. However, existing approaches fall short, with static methods risking irreversible information loss and dynamic strategies employing heuristics that insufficiently capture the query-dependent nature of token importance. We propose FASA, a novel framework that achieves query-aware token eviction by dynamically predicting token importance. FASA stems from a novel insight into RoPE: the discovery of functional sparsity at the frequency-chunk (FC) level. Our key finding is that a small, identifiable subset of "dominant" FCs consistently exhibits high contextual agreement with the full attention head. This provides a robust and computationally free proxy for identifying salient tokens. %making them a powerful and efficient proxy for token importance. Building on this insight, FASA first identifies a critical set of tokens using dominant FCs, and then performs focused attention computation solely on this pruned subset. % Since accessing only a small fraction of the KV cache, FASA drastically lowers memory bandwidth requirements and computational cost. Across a spectrum of long-context tasks, from sequence modeling to complex CoT reasoning, FASA consistently outperforms all token-eviction baselines and achieves near-oracle accuracy, demonstrating remarkable robustness even under constraint budgets. Notably, on LongBench-V1, FASA reaches nearly 100\% of full-KV performance when only keeping 256 tokens, and achieves 2.56$\times$ speedup using just 18.9\% of the cache on AIME24.

</details>


### [138] [Privasis: Synthesizing the Largest "Public" Private Dataset from Scratch](https://arxiv.org/abs/2602.03183)
*Hyunwoo Kim,Niloofar Mireshghallah,Michael Duan,Rui Xin,Shuyue Stella Li,Jaehun Jung,David Acuna,Qi Pang,Hanshen Xiao,G. Edward Suh,Sewoong Oh,Yulia Tsvetkov,Pang Wei Koh,Yejin Choi*

Main category: cs.CL

TL;DR: Privasis is a million-scale synthetic dataset addressing privacy-sensitive data scarcity in AI research, outperforming larger models in sanitization tasks.


<details>
  <summary>Details</summary>
Motivation: Privacy-sensitive research suffers from data scarcity, while AI agents increasingly handle sensitive personal data, necessitating scalable solutions.

Method: Created Privasis, a synthetic dataset with 1.4M records and 55.1M annotated attributes, and developed a pipeline for text sanitization.

Result: Compact sanitization models (<=4B) trained on Privasis outperformed large models like GPT-5 and Qwen-3 235B.

Conclusion: Privasis enables scalable research in privacy-sensitive domains, with plans to release data and tools for broader impact.

Abstract: Research involving privacy-sensitive data has always been constrained by data scarcity, standing in sharp contrast to other areas that have benefited from data scaling. This challenge is becoming increasingly urgent as modern AI agents--such as OpenClaw and Gemini Agent--are granted persistent access to highly sensitive personal information. To tackle this longstanding bottleneck and the rising risks, we present Privasis (i.e., privacy oasis), the first million-scale fully synthetic dataset entirely built from scratch--an expansive reservoir of texts with rich and diverse private information--designed to broaden and accelerate research in areas where processing sensitive social data is inevitable. Compared to existing datasets, Privasis, comprising 1.4 million records, offers orders-of-magnitude larger scale with quality, and far greater diversity across various document types, including medical history, legal documents, financial records, calendars, and text messages with a total of 55.1 million annotated attributes such as ethnicity, date of birth, workplace, etc. We leverage Privasis to construct a parallel corpus for text sanitization with our pipeline that decomposes texts and applies targeted sanitization. Our compact sanitization models (<=4B) trained on this dataset outperform state-of-the-art large language models, such as GPT-5 and Qwen-3 235B. We plan to release data, models, and code to accelerate future research on privacy-sensitive domains and agents.

</details>


### [139] [ForesightKV: Optimizing KV Cache Eviction for Reasoning Models by Learning Long-Term Contribution](https://arxiv.org/abs/2602.03203)
*Zican Dong,Peiyu Liu,Junyi Li,Zhipeng Chen,Han Peng,Shuo Wang,Wayne Xin Zhao*

Main category: cs.CL

TL;DR: ForesightKV is a training-based KV cache eviction framework that balances efficiency and performance by predicting and evicting less important KV pairs during long-text generation, outperforming prior methods under limited cache budgets.


<details>
  <summary>Details</summary>
Motivation: The expansion of KV cache in large language models (LLMs) incurs high memory and computation costs during long-text generation, and existing eviction methods often fail to capture complex KV dependencies, leading to performance degradation.

Method: ForesightKV uses the Golden Eviction algorithm to identify optimal eviction KV pairs based on future attention scores, distilled via supervised training with Pairwise Ranking Loss. Cache eviction is also formulated as a Markov Decision Process using the GRPO algorithm to mitigate language modeling loss on low-entropy tokens.

Result: Experiments on AIME2024 and AIME2025 benchmarks show ForesightKV outperforms prior methods under half the cache budget, leveraging both supervised and reinforcement learning effectively.

Conclusion: ForesightKV provides an efficient and effective solution for KV cache eviction in LLMs, combining supervised training and reinforcement learning to improve performance and reduce resource usage.

Abstract: Recently, large language models (LLMs) have shown remarkable reasoning abilities by producing long reasoning traces. However, as the sequence length grows, the key-value (KV) cache expands linearly, incurring significant memory and computation costs. Existing KV cache eviction methods mitigate this issue by discarding less important KV pairs, but often fail to capture complex KV dependencies, resulting in performance degradation. To better balance efficiency and performance, we introduce ForesightKV, a training-based KV cache eviction framework that learns to predict which KV pairs to evict during long-text generations. We first design the Golden Eviction algorithm, which identifies the optimal eviction KV pairs at each step using future attention scores. These traces and the scores at each step are then distilled via supervised training with a Pairwise Ranking Loss. Furthermore, we formulate cache eviction as a Markov Decision Process and apply the GRPO algorithm to mitigate the significant language modeling loss increase on low-entropy tokens. Experiments on AIME2024 and AIME2025 benchmarks of three reasoning models demonstrate that ForesightKV consistently outperforms prior methods under only half the cache budget, while benefiting synergistically from both supervised and reinforcement learning approaches.

</details>


### [140] [Token Sparse Attention: Efficient Long-Context Inference with Interleaved Token Selection](https://arxiv.org/abs/2602.03216)
*Dongwon Jo,Beomseok Kang,Jiwon Song,Jae-Joon Kim*

Main category: cs.CL

TL;DR: Token Sparse Attention dynamically compresses and decompresses tokens, improving speed and accuracy trade-offs for long-context inference.


<details>
  <summary>Details</summary>
Motivation: Quadratic attention complexity limits long-context inference; existing methods either retain irrelevant tokens or make irreversible decisions.

Method: Proposes Token Sparse Attention, which dynamically compresses/decompresses token sets per-head during attention, compatible with dense/sparse attention kernels.

Result: Achieves up to 3.23x speedup at 128K context with <1% accuracy loss, proving dynamic token-level sparsification effective.

Conclusion: Dynamic token-level sparsification is a scalable and complementary strategy for long-context inference.

Abstract: The quadratic complexity of attention remains the central bottleneck in long-context inference for large language models. Prior acceleration methods either sparsify the attention map with structured patterns or permanently evict tokens at specific layers, which can retain irrelevant tokens or rely on irreversible early decisions despite the layer-/head-wise dynamics of token importance. In this paper, we propose Token Sparse Attention, a lightweight and dynamic token-level sparsification mechanism that compresses per-head $Q$, $K$, $V$ to a reduced token set during attention and then decompresses the output back to the original sequence, enabling token information to be reconsidered in subsequent layers. Furthermore, Token Sparse Attention exposes a new design point at the intersection of token selection and sparse attention. Our approach is fully compatible with dense attention implementations, including Flash Attention, and can be seamlessly composed with existing sparse attention kernels. Experimental results show that Token Sparse Attention consistently improves accuracy-latency trade-off, achieving up to $\times$3.23 attention speedup at 128K context with less than 1% accuracy degradation. These results demonstrate that dynamic and interleaved token-level sparsification is a complementary and effective strategy for scalable long-context inference.

</details>


### [141] [POP: Prefill-Only Pruning for Efficient Large Model Inference](https://arxiv.org/abs/2602.03295)
*Junhui He,Zhihui Fu,Jun Wang,Qingan Li*

Main category: cs.CL

TL;DR: Proposes Prefill-Only Pruning (POP), a stage-aware inference strategy for LLMs/VLMs that prunes deep layers during prefill but retains them for decode, achieving speedup with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Addresses the computational cost of LLMs/VLMs and accuracy degradation in existing pruning methods by highlighting asymmetric roles of prefill and decode stages.

Method: Introduces POP using virtual gates to prune deep layers in prefill stages, maintains cache integrity with KV projections, and ensures accuracy with boundary handling.

Result: Achieves up to 1.37× speedup in prefill latency with minimal performance loss across models like Llama-3.1, Qwen3-VL, and Gemma-3.

Conclusion: POP overcomes trade-offs in structured pruning, offering efficiency gains without significant accuracy degradation.

Abstract: Large Language Models (LLMs) and Vision-Language Models (VLMs) have demonstrated remarkable capabilities. However, their deployment is hindered by significant computational costs. Existing structured pruning methods, while hardware-efficient, often suffer from significant accuracy degradation. In this paper, we argue that this failure stems from a stage-agnostic pruning approach that overlooks the asymmetric roles between the prefill and decode stages. By introducing a virtual gate mechanism, our importance analysis reveals that deep layers are critical for next-token prediction (decode) but largely redundant for context encoding (prefill). Leveraging this insight, we propose Prefill-Only Pruning (POP), a stage-aware inference strategy that safely omits deep layers during the computationally intensive prefill stage while retaining the full model for the sensitive decode stage. To enable the transition between stages, we introduce independent Key-Value (KV) projections to maintain cache integrity, and a boundary handling strategy to ensure the accuracy of the first generated token. Extensive experiments on Llama-3.1, Qwen3-VL, and Gemma-3 across diverse modalities demonstrate that POP achieves up to 1.37$\times$ speedup in prefill latency with minimal performance loss, effectively overcoming the accuracy-efficiency trade-off limitations of existing structured pruning methods.

</details>


### [142] [MIRROR: A Multi-Agent Framework with Iterative Adaptive Revision and Hierarchical Retrieval for Optimization Modeling in Operations Research](https://arxiv.org/abs/2602.03318)
*Yifan Shi,Jialong Shi,Jiayi Wang,Ye Fan,Jianyong Sun*

Main category: cs.CL

TL;DR: MIRROR is a fine-tuning-free, multi-agent framework that translates natural language into optimization models and solver code, outperforming existing methods with its adaptive revision and hierarchical retrieval mechanisms.


<details>
  <summary>Details</summary>
Motivation: The goal is to address the limitations of expert-driven modeling in Operations Research (OR) and unreliable outputs from existing large language model (LLM) approaches.

Method: MIRROR uses execution-driven iterative adaptive revision for error correction and hierarchical retrieval for fetching relevant exemplars from a curated library.

Result: MIRROR outperforms existing methods on standard OR benchmarks and performs well on complex industrial datasets like IndustryOR and Mamo-ComplexLP.

Conclusion: MIRROR provides an efficient and reliable OR modeling solution for non-expert users, overcoming limitations of general-purpose LLMs.

Abstract: Operations Research (OR) relies on expert-driven modeling-a slow and fragile process ill-suited to novel scenarios. While large language models (LLMs) can automatically translate natural language into optimization models, existing approaches either rely on costly post-training or employ multi-agent frameworks, yet most still lack reliable collaborative error correction and task-specific retrieval, often leading to incorrect outputs. We propose MIRROR, a fine-tuning-free, end-to-end multi-agent framework that directly translates natural language optimization problems into mathematical models and solver code. MIRROR integrates two core mechanisms: (1) execution-driven iterative adaptive revision for automatic error correction, and (2) hierarchical retrieval to fetch relevant modeling and coding exemplars from a carefully curated exemplar library. Experiments show that MIRROR outperforms existing methods on standard OR benchmarks, with notable results on complex industrial datasets such as IndustryOR and Mamo-ComplexLP. By combining precise external knowledge infusion with systematic error correction, MIRROR provides non-expert users with an efficient and reliable OR modeling solution, overcoming the fundamental limitations of general-purpose LLMs in expert optimization tasks.

</details>


### [143] [Accurate Failure Prediction in Agents Does Not Imply Effective Failure Prevention](https://arxiv.org/abs/2602.03338)
*Rakshith Vasudev,Melisa Russak,Dan Bikel,Waseem Alshikh*

Main category: cs.CL

TL;DR: LLM critic models, despite high offline accuracy, can cause significant performance degradation. A pre-deployment test with 50 tasks helps predict intervention impact, identifying when to avoid intervention.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand the real-world effects of proactive interventions by LLM critic models, which are assumed to improve reliability but may unpredictably degrade performance.

Method: The authors analyze the effects of a binary LLM critic and propose a pre-deployment test using a small pilot of 50 tasks to estimate intervention outcomes without full deployment.

Result: Intervention can severely degrade performance (up to -26 pp) on high-success tasks but improves high-failure benchmarks modestly (+2.8 pp). The test accurately predicts these outcomes.

Conclusion: The framework's value lies in identifying when not to intervene, preventing severe regressions before deployment. LLM critic accuracy alone is insufficient to ensure safe intervention.

Abstract: Proactive interventions by LLM critic models are often assumed to improve reliability, yet their effects at deployment time are poorly understood. We show that a binary LLM critic with strong offline accuracy (AUROC 0.94) can nevertheless cause severe performance degradation, inducing a 26 percentage point (pp) collapse on one model while affecting another by near zero pp. This variability demonstrates that LLM critic accuracy alone is insufficient to determine whether intervention is safe.
  We identify a disruption-recovery tradeoff: interventions may recover failing trajectories but also disrupt trajectories that would have succeeded. Based on this insight, we propose a pre-deployment test that uses a small pilot of 50 tasks to estimate whether intervention is likely to help or harm, without requiring full deployment. Across benchmarks, the test correctly anticipates outcomes: intervention degrades performance on high-success tasks (0 to -26 pp), while yielding a modest improvement on the high-failure ALFWorld benchmark (+2.8 pp, p=0.014). The primary value of our framework is therefore identifying when not to intervene, preventing severe regressions before deployment.

</details>


### [144] [PEGRL: Improving Machine Translation by Post-Editing Guided Reinforcement Learning](https://arxiv.org/abs/2602.03352)
*Yunzhi Shen,Hao Zhou,Xin Huang,Xue Han,Junlan Feng,Shujian Huang*

Main category: cs.CL

TL;DR: PEGRL is a two-stage RL framework for LLM-based machine translation that uses post-editing to stabilize training and balance global exploration with fine-grained optimization, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Existing RL methods for LLM-based translation face challenges like noisy learning signals and large trajectory spaces, hindering fine-grained optimization.

Method: PEGRL introduces post-editing as an auxiliary task, sampling translations to stabilize return estimation and balance exploration with optimization. A weighting scheme balances objectives.

Result: Experiments show PEGRL outperforms RL baselines across multiple language pairs, achieving comparable performance to advanced systems like DeepSeek-V3.2.

Conclusion: PEGRL's two-stage approach effectively addresses RL challenges in translation, improving sample efficiency and performance.

Abstract: Reinforcement learning (RL) has shown strong promise for LLM-based machine translation, with recent methods such as GRPO demonstrating notable gains; nevertheless, translation-oriented RL remains challenged by noisy learning signals arising from Monte Carlo return estimation, as well as a large trajectory space that favors global exploration over fine-grained local optimization. We introduce \textbf{PEGRL}, a \textit{two-stage} RL framework that uses post-editing as an auxiliary task to stabilize training and guide overall optimization. At each iteration, translation outputs are sampled to construct post-editing inputs, allowing return estimation in the post-editing stage to benefit from conditioning on the current translation behavior, while jointly supporting both global exploration and fine-grained local optimization. A task-specific weighting scheme further balances the contributions of translation and post-editing objectives, yielding a biased yet more sample-efficient estimator. Experiments on English$\to$Finnish, English$\to$Turkish, and English$\leftrightarrow$Chinese show consistent gains over RL baselines, and for English$\to$Turkish, performance on COMET-KIWI is comparable to advanced LLM-based systems (DeepSeek-V3.2).

</details>


### [145] [Pursuing Best Industrial Practices for Retrieval-Augmented Generation in the Medical Domain](https://arxiv.org/abs/2602.03368)
*Wei Zhu*

Main category: cs.CL

TL;DR: This paper analyzes Retrieval-Augmented Generation (RAG) systems for industrial LLM applications, proposing practical alternatives for components and evaluating best practices for performance-efficiency trade-offs.


<details>
  <summary>Details</summary>
Motivation: The lack of consensus on best practices for building RAG systems in industrial applications, especially in the medical domain, drives the need for systematic analysis and practical guidance.

Method: The authors analyze RAG components, propose practical alternatives, and conduct systematic evaluations on three task types.

Result: The study reveals best practices for improving RAG systems and highlights how LLM-based RAG systems balance performance and efficiency.

Conclusion: The work provides actionable insights for optimizing RAG systems in industrial settings, particularly in specialized domains like medicine.

Abstract: While retrieval augmented generation (RAG) has been swiftly adopted in industrial applications based on large language models (LLMs), there is no consensus on what are the best practices for building a RAG system in terms of what are the components, how to organize these components and how to implement each component for the industrial applications, especially in the medical domain. In this work, we first carefully analyze each component of the RAG system and propose practical alternatives for each component. Then, we conduct systematic evaluations on three types of tasks, revealing the best practices for improving the RAG system and how LLM-based RAG systems make trade-offs between performance and efficiency.

</details>


### [146] [Towards Distillation-Resistant Large Language Models: An Information-Theoretic Perspective](https://arxiv.org/abs/2602.03396)
*Hao Fang,Tianyi Zhang,Tianqu Zhuang,Jiawei Kong,Kuofeng Gao,Bin Chen,Leqi Liang,Shu-Tao Xia,Ke Xu*

Main category: cs.CL

TL;DR: The paper proposes an information-theoretic defense against logit-based distillation of proprietary LLMs by minimizing conditional mutual information and preserving output utility.


<details>
  <summary>Details</summary>
Motivation: Existing defenses focus only on text-based distillation, leaving logit-based distillation vulnerable, which adversaries exploit to extract knowledge from black-box LLM APIs.

Method: The authors characterize distillation-relevant information using conditional mutual information (CMI), propose a transformation matrix to purify outputs, and optimize it with an anti-distillation objective.

Result: Experiments show the method significantly reduces distillation effectiveness while maintaining task accuracy across multiple LLMs and distillation algorithms.

Conclusion: The proposed solution effectively protects LLMs' intellectual property by minimizing distillation-relevant information without compromising utility.

Abstract: Proprietary large language models (LLMs) embody substantial economic value and are generally exposed only as black-box APIs, yet adversaries can still exploit their outputs to extract knowledge via distillation. Existing defenses focus exclusively on text-based distillation, leaving the important logit-based distillation largely unexplored. In this work, we analyze this problem and present an effective solution from an information-theoretic perspective. We characterize distillation-relevant information in teacher outputs using the conditional mutual information (CMI) between teacher logits and input queries conditioned on ground-truth labels. This quantity captures contextual information beneficial for model extraction, motivating us to defend distillation via CMI minimization. Guided by our theoretical analysis, we propose learning a transformation matrix that purifies the original outputs to enhance distillation resistance. We further derive a CMI-inspired anti-distillation objective to optimize this transformation, which effectively removes distillation-relevant information while preserving output utility. Extensive experiments across multiple LLMs and strong distillation algorithms demonstrate that the proposed method significantly degrades distillation performance while preserving task accuracy, effectively protecting models' intellectual property.

</details>


### [147] [Verified Critical Step Optimization for LLM Agents](https://arxiv.org/abs/2602.03412)
*Mukai Li,Qingcheng Zeng,Tianqing Fang,Zhenwen Liang,Linfeng Song,Qi Liu,Haitao Mi,Dong Yu*

Main category: cs.CL

TL;DR: CSO focuses preference learning on verified critical steps to improve post-training of language model agents, outperforming baselines with selective supervision.


<details>
  <summary>Details</summary>
Motivation: Prior methods face challenges like noisy step rewards and high computational costs, prompting the need for a more precise and efficient approach.

Method: CSO identifies critical steps from failed trajectories, proposes alternatives using expert models, and verifies successful executions for DPO training.

Result: CSO achieves 37% and 26% relative improvements over baselines on GAIA-Text-103 and XBench-DeepSearch with supervision at only 16% of steps.

Conclusion: Selective verification-based learning, as in CSO, effectively enhances agent post-training by targeting critical decision points.

Abstract: As large language model agents tackle increasingly complex long-horizon tasks, effective post-training becomes critical. Prior work faces fundamental challenges: outcome-only rewards fail to precisely attribute credit to intermediate steps, estimated step-level rewards introduce systematic noise, and Monte Carlo sampling approaches for step reward estimation incur prohibitive computational cost. Inspired by findings that only a small fraction of high-entropy tokens drive effective RL for reasoning, we propose Critical Step Optimization (CSO), which focuses preference learning on verified critical steps, decision points where alternate actions demonstrably flip task outcomes from failure to success. Crucially, our method starts from failed policy trajectories rather than expert demonstrations, directly targeting the policy model's weaknesses. We use a process reward model (PRM) to identify candidate critical steps, leverage expert models to propose high-quality alternatives, then continue execution from these alternatives using the policy model itself until task completion. Only alternatives that the policy successfully executes to correct outcomes are verified and used as DPO training data, ensuring both quality and policy reachability. This yields fine-grained, verifiable supervision at critical decisions while avoiding trajectory-level coarseness and step-level noise. Experiments on GAIA-Text-103 and XBench-DeepSearch show that CSO achieves 37% and 26% relative improvement over the SFT baseline and substantially outperforms other post-training methods, while requiring supervision at only 16% of trajectory steps. This demonstrates the effectiveness of selective verification-based learning for agent post-training.

</details>


### [148] [FactNet: A Billion-Scale Knowledge Graph for Multilingual Factual Grounding](https://arxiv.org/abs/2602.03417)
*Yingli Shen,Wen Lai,Jie Zhou,Xueren Zhang,Yudong Wang,Kangyang Luo,Shuo Wang,Ge Gao,Alexander Fraser,Maosong Sun*

Main category: cs.CL

TL;DR: FactNet is a large-scale, open-source resource combining 1.7 billion assertions with 3.01 billion auditable evidence pointers from 316 Wikipedia editions, ensuring high precision and recoverability for multilingual systems.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of LLMs regarding factual hallucinations and lack of traceable provenance by providing a unified resource for structured knowledge and grounded text.

Method: Develop FactNet using a deterministic construction pipeline linking atomic assertions with auditable evidence from Wikipedia, ensuring byte-level precision.

Result: Achieves 92.1% grounding precision, including in long-tail languages, and includes FactNet-Bench for evaluating tasks like Knowledge Graph Completion.

Conclusion: FactNet offers a reproducible, trustworthy foundation for training and evaluating verifiable multilingual systems.

Abstract: While LLMs exhibit remarkable fluency, their utility is often compromised by factual hallucinations and a lack of traceable provenance. Existing resources for grounding mitigate this but typically enforce a dichotomy: they offer either structured knowledge without textual context (e.g., knowledge bases) or grounded text with limited scale and linguistic coverage. To bridge this gap, we introduce FactNet, a massive, open-source resource designed to unify 1.7 billion atomic assertions with 3.01 billion auditable evidence pointers derived exclusively from 316 Wikipedia editions. Unlike recent synthetic approaches, FactNet employs a strictly deterministic construction pipeline, ensuring that every evidence unit is recoverable with byte-level precision. Extensive auditing confirms a high grounding precision of 92.1%, even in long-tail languages. Furthermore, we establish FactNet-Bench, a comprehensive evaluation suite for Knowledge Graph Completion, Question Answering, and Fact Checking. FactNet provides the community with a foundational, reproducible resource for training and evaluating trustworthy, verifiable multilingual systems.

</details>


### [149] [A-RAG: Scaling Agentic Retrieval-Augmented Generation via Hierarchical Retrieval Interfaces](https://arxiv.org/abs/2602.03442)
*Mingxuan Du,Benfeng Xu,Chiwei Zhu,Shaohan Wang,Pengyu Wang,Xiaorui Wang,Zhendong Mao*

Main category: cs.CL

TL;DR: A-RAG introduces an Agentic RAG framework that allows models to participate in retrieval decisions, outperforming existing RAG systems.


<details>
  <summary>Details</summary>
Motivation: Existing RAG systems fail to leverage frontier language models' reasoning and tool-use capabilities, limiting their efficiency and scalability.

Method: A-RAG provides hierarchical retrieval tools (keyword search, semantic search, chunk read) enabling adaptive information retrieval across granularities.

Result: A-RAG consistently outperforms existing approaches on open-domain QA benchmarks with comparable or lower retrieved tokens.

Conclusion: A-RAG effectively leverages model capabilities and adapts dynamically to RAG tasks, scaling well with model size and compute. Code is released for future research.

Abstract: Frontier language models have demonstrated strong reasoning and long-horizon tool-use capabilities. However, existing RAG systems fail to leverage these capabilities. They still rely on two paradigms: (1) designing an algorithm that retrieves passages in a single shot and concatenates them into the model's input, or (2) predefining a workflow and prompting the model to execute it step-by-step. Neither paradigm allows the model to participate in retrieval decisions, preventing efficient scaling with model improvements. In this paper, we introduce A-RAG, an Agentic RAG framework that exposes hierarchical retrieval interfaces directly to the model. A-RAG provides three retrieval tools: keyword search, semantic search, and chunk read, enabling the agent to adaptively search and retrieve information across multiple granularities. Experiments on multiple open-domain QA benchmarks show that A-RAG consistently outperforms existing approaches with comparable or lower retrieved tokens, demonstrating that A-RAG effectively leverages model capabilities and dynamically adapts to different RAG tasks. We further systematically study how A-RAG scales with model size and test-time compute. We will release our code and evaluation suite to facilitate future research. Code and evaluation suite are available at https://github.com/Ayanami0730/arag.

</details>


### [150] [Preferences for Idiomatic Language are Acquired Slowly -- and Forgotten Quickly: A Case Study on Swedish](https://arxiv.org/abs/2602.03484)
*Jenny Kunz*

Main category: cs.CL

TL;DR: Study on how language models develop preferences for idiomatic vs. linguistically acceptable Swedish during pretraining and adaptation. Findings show idiomatic competence emerges slower, and instruction tuning from English-translated data reduces idiomatic preference.


<details>
  <summary>Details</summary>
Motivation: To understand how language models develop preferences for idiomatic vs. linguistically acceptable Swedish, and how adaptation from English affects these preferences.

Method: Train models from scratch on Swedish or fine-tune English-pretrained models, probing preferences with minimal pairs. Adapt linguistic benchmarks and introduce new datasets for idiomaticity.

Result: Idiomatic competence emerges slower than other linguistic abilities, improves with longer training, but declines with instruction tuning from English-translated data.

Conclusion: Idiomatic learning is slower and sensitive to training methods, especially instruction tuning from non-native data.

Abstract: In this study, we investigate how language models develop preferences for \textit{idiomatic} as compared to \textit{linguistically acceptable} Swedish, both during pretraining and when adapting a model from English to Swedish. To do so, we train models on Swedish from scratch and by fine-tuning English-pretrained models, probing their preferences at various checkpoints using minimal pairs that differ in linguistic acceptability or idiomaticity. For linguistic acceptability, we adapt existing benchmarks into a minimal-pair format. To assess idiomaticity, we introduce two novel datasets: one contrasting conventionalized idioms with plausible variants, and another contrasting idiomatic Swedish with Translationese. Our findings suggest that idiomatic competence emerges more slowly than other linguistic abilities, including grammatical and lexical correctness. While longer training yields diminishing returns for most tasks, idiom-related performance continues to improve, particularly in the largest model tested (8B). However, instruction tuning on data machine-translated from English -- the common approach for languages with little or no native instruction data -- causes models to rapidly lose their preference for idiomatic language.

</details>


### [151] [Self-Verification Dilemma: Experience-Driven Suppression of Overused Checking in LLM Reasoning](https://arxiv.org/abs/2602.03485)
*Quanyu Long,Kai Jie Jiang,Jianda Chen,Xu Guo,Leilei Gan,Wenya Wang*

Main category: cs.CL

TL;DR: LRMs often overuse self-verification steps, which are rarely corrective. A novel framework reduces unnecessary verification by consulting past experiences, saving tokens without losing accuracy.


<details>
  <summary>Details</summary>
Motivation: The study identifies frequent but often unnecessary self-verification steps in LRMs, prompting a need to optimize verification usage.

Method: A test-time framework detects recheck behavior, uses historical verification outcomes to estimate necessity, and suppresses unnecessary steps.

Result: Reduces token usage by up to 20.3% while maintaining or improving accuracy across benchmarks.

Conclusion: Optimizing self-verification steps improves efficiency in LRMs without compromising performance.

Abstract: Large Reasoning Models (LRMs) achieve strong performance by generating long reasoning traces with reflection. Through a large-scale empirical analysis, we find that a substantial fraction of reflective steps consist of self-verification (recheck) that repeatedly confirm intermediate results. These rechecks occur frequently across models and benchmarks, yet the vast majority are confirmatory rather than corrective, rarely identifying errors and altering reasoning outcomes. This reveals a mismatch between how often self-verification is activated and how often it is actually useful. Motivated by this, we propose a novel, experience-driven test-time framework that reduces the overused verification. Our method detects the activation of recheck behavior, consults an offline experience pool of past verification outcomes, and estimates whether a recheck is likely unnecessary via efficient retrieval. When historical experience suggests unnecessary, a suppression signal redirects the model to proceed. Across multiple model and benchmarks, our approach reduces token usage up to 20.3% while maintaining the accuracy, and in some datasets even yields accuracy improvements.

</details>


### [152] [Learning to Reason Faithfully through Step-Level Faithfulness Maximization](https://arxiv.org/abs/2602.03507)
*Runquan Gui,Yafu Li,Xiaoye Qu,Ziyan Liu,Yeqiu Cheng,Yu Cheng*

Main category: cs.CL

TL;DR: FaithRL is a reinforcement learning framework designed to optimize reasoning faithfulness in LLMs, reducing hallucinations while maintaining correctness.


<details>
  <summary>Details</summary>
Motivation: Current RLVR pipelines rely on sparse rewards, leading to over-confidence and spurious reasoning, which increases hallucinations.

Method: FaithRL introduces a geometric reward design and faithfulness-aware advantage modulation to penalize unsupported steps and preserve valid ones.

Result: FaithRL reduces hallucination rates across diverse benchmarks while improving answer correctness.

Conclusion: FaithRL enhances step-wise reasoning faithfulness and generalizes robustly, offering a promising solution to RLVR limitations.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has markedly improved the performance of Large Language Models (LLMs) on tasks requiring multi-step reasoning. However, most RLVR pipelines rely on sparse outcome-based rewards, providing little supervision over intermediate steps and thus encouraging over-confidence and spurious reasoning, which in turn increases hallucinations. To address this, we propose FaithRL, a general reinforcement learning framework that directly optimizes reasoning faithfulness. We formalize a faithfulness-maximization objective and theoretically show that optimizing it mitigates over-confidence. To instantiate this objective, we introduce a geometric reward design and a faithfulness-aware advantage modulation mechanism that assigns step-level credit by penalizing unsupported steps while preserving valid partial derivations. Across diverse backbones and benchmarks, FaithRL consistently reduces hallucination rates while maintaining (and often improving) answer correctness. Further analysis confirms that FaithRL increases step-wise reasoning faithfulness and generalizes robustly. Our code is available at https://github.com/aintdoin/FaithRL.

</details>


### [153] [Can Large Language Models Generalize Procedures Across Representations?](https://arxiv.org/abs/2602.03542)
*Fangru Lin,Valentin Hofmann,Xingchen Wan,Weixing Wang,Zifeng Ding,Anthony G. Cohn,Janet B. Pierrehumbert*

Main category: cs.CL

TL;DR: The paper explores how LLMs generalize across symbolic (code, graphs) and natural language representations, finding limitations in current training methods. It proposes a two-stage curriculum (symbolic then natural language) to improve performance, showing significant gains compared to GPT-4o.


<details>
  <summary>Details</summary>
Motivation: The study addresses the gap between LLMs' training on symbolic data (code, graphs) and their real-world application to natural language tasks, questioning their ability to generalize across these representations.

Method: The authors test LLMs on isomorphic tasks in code, graphs, and natural language. They propose a two-stage data curriculum: first training on symbolic data (code/graphs) followed by natural language data.

Result: Training solely on symbolic or natural language data leads to unreliable or inefficient generalization. The proposed curriculum improves performance across tasks and models, with a 1.5B Qwen model matching GPT-4o's zero-shot performance.

Conclusion: Cross-representation generalization resembles generative analogy, and the two-stage curriculum effectively bridges the gap between symbolic and natural language tasks.

Abstract: Large language models (LLMs) are trained and tested extensively on symbolic representations such as code and graphs, yet real-world user tasks are often specified in natural language. To what extent can LLMs generalize across these representations? Here, we approach this question by studying isomorphic tasks involving procedures represented in code, graphs, and natural language (e.g., scheduling steps in planning). We find that training LLMs with popular post-training methods on graphs or code data alone does not reliably generalize to corresponding natural language tasks, while training solely on natural language can lead to inefficient performance gains. To address this gap, we propose a two-stage data curriculum that first trains on symbolic, then natural language data. The curriculum substantially improves model performance across model families and tasks. Remarkably, a 1.5B Qwen model trained by our method can closely match zero-shot GPT-4o in naturalistic planning. Finally, our analysis suggests that successful cross-representation generalization can be interpreted as a form of generative analogy, which our curriculum effectively encourages.

</details>


### [154] [SEAD: Self-Evolving Agent for Multi-Turn Service Dialogue](https://arxiv.org/abs/2602.03548)
*Yuqin Dai,Ning Gao,Wei Zhang,Jie Wang,Zichen Luo,Jinpeng Wang,Yujie Wang,Ruiyuan Wu,Chaozheng Wang*

Main category: cs.CL

TL;DR: SEAD is a framework for improving service dialogues by decoupling user modeling into Profile Controllers and User Role-play Models, outperforming existing models in task completion and efficiency.


<details>
  <summary>Details</summary>
Motivation: Current methods for service dialogues perform poorly due to reliance on noisy, low-quality data and challenges in simulating authentic user behaviors.

Method: SEAD introduces a framework with two components: Profile Controller for diverse user states and User Role-play Model for realistic role-playing.

Result: SEAD outperforms Open-source and Closed-source models, improving task completion by 17.6% and dialogue efficiency by 11.1%.

Conclusion: SEAD effectively enhances service dialogue performance without needing large-scale human annotations.

Abstract: Large Language Models have demonstrated remarkable capabilities in open-domain dialogues. However, current methods exhibit suboptimal performance in service dialogues, as they rely on noisy, low-quality human conversation data. This limitation arises from data scarcity and the difficulty of simulating authentic, goal-oriented user behaviors. To address these issues, we propose SEAD (Self-Evolving Agent for Service Dialogue), a framework that enables agents to learn effective strategies without large-scale human annotations. SEAD decouples user modeling into two components: a Profile Controller that generates diverse user states to manage training curriculum, and a User Role-play Model that focuses on realistic role-playing. This design ensures the environment provides adaptive training scenarios rather than acting as an unfair adversary. Experiments demonstrate that SEAD significantly outperforms Open-source Foundation Models and Closed-source Commercial Models, improving task completion rate by 17.6% and dialogue efficiency by 11.1%. Code is available at: https://github.com/Da1yuqin/SEAD.

</details>


### [155] [Assessing the Impact of Typological Features on Multilingual Machine Translation in the Age of Large Language Models](https://arxiv.org/abs/2602.03551)
*Vitalii Hirak,Jaap Jumelet,Arianna Bisazza*

Main category: cs.CL

TL;DR: The study explores how target language typology affects translation quality in multilingual models, NLLB-200 and Tower+, beyond factors like data resources and writing scripts.


<details>
  <summary>Details</summary>
Motivation: To understand why quality disparities persist across languages in multilingual models, focusing on typological properties.

Method: Analysis of two large pre-trained multilingual models, NLLB-200 (encoder-decoder) and Tower+ (decoder-only), using FLORES+ MT benchmark.

Result: Target language typology significantly impacts translation quality; certain typologies benefit from broader output space search.

Conclusion: Typological properties play a key role in translation quality, suggesting alternative decoding strategies for certain languages.

Abstract: Despite major advances in multilingual modeling, large quality disparities persist across languages. Besides the obvious impact of uneven training resources, typological properties have also been proposed to determine the intrinsic difficulty of modeling a language. The existing evidence, however, is mostly based on small monolingual language models or bilingual translation models trained from scratch. We expand on this line of work by analyzing two large pre-trained multilingual translation models, NLLB-200 and Tower+, which are state-of-the-art representatives of encoder-decoder and decoder-only machine translation, respectively. Based on a broad set of languages, we find that target language typology drives translation quality of both models, even after controlling for more trivial factors, such as data resourcedness and writing script. Additionally, languages with certain typological properties benefit more from a wider search of the output space, suggesting that such languages could profit from alternative decoding strategies beyond the standard left-to-right beam search. To facilitate further research in this area, we release a set of fine-grained typological properties for 212 languages of the FLORES+ MT evaluation benchmark.

</details>


### [156] [HySparse: A Hybrid Sparse Attention Architecture with Oracle Token Selection and KV Cache Sharing](https://arxiv.org/abs/2602.03560)
*Yizhao Gao,Jianyu Wei,Qihao Zhang,Yu Cheng,Shimao Chen,Zhengju Tang,Zihan Jiang,Yifan Song,Hailin Zhang,Liang Zhao,Bo Yang,Gang Wang,Shijie Cao,Fuli Luo*

Main category: cs.CL

TL;DR: HySparse interleaves full and sparse attention layers, using full attention as an oracle for sparse layer token selection and KV cache reuse, reducing computation and memory while improving performance.


<details>
  <summary>Details</summary>
Motivation: Addresses limitations of prior sparse attention methods by eliminating reliance on proxies for token importance prediction and reducing computation without sacrificing KV cache efficiency.

Method: Hybrid Sparse Attention (HySparse) architecture interleaves full attention layers with sparse attention layers, deriving sparse layer token selection and KV caches directly from preceding full attention layers.

Result: HySparse outperforms full attention and hybrid SWA baselines across 7B dense and 80B MoE models, reducing KV cache storage by nearly 10x while maintaining performance.

Conclusion: HySparse improves efficiency and performance in large models by strategically leveraging full attention layers to guide sparse attention, reducing computational and memory overhead.

Abstract: This work introduces Hybrid Sparse Attention (HySparse), a new architecture that interleaves each full attention layer with several sparse attention layers. While conceptually simple, HySparse strategically derives each sparse layer's token selection and KV caches directly from the preceding full attention layer. This architecture resolves two fundamental limitations of prior sparse attention methods. First, conventional approaches typically rely on additional proxies to predict token importance, introducing extra complexity and potentially suboptimal performance. In contrast, HySparse uses the full attention layer as a precise oracle to identify important tokens. Second, existing sparse attention designs often reduce computation without saving KV cache. HySparse enables sparse attention layers to reuse the full attention KV cache, thereby reducing both computation and memory. We evaluate HySparse on both 7B dense and 80B MoE models. Across all settings, HySparse consistently outperforms both full attention and hybrid SWA baselines. Notably, in the 80B MoE model with 49 total layers, only 5 layers employ full attention, yet HySparse achieves substantial performance gains while reducing KV cache storage by nearly 10x.

</details>


### [157] [ACL: Aligned Contrastive Learning Improves BERT and Multi-exit BERT Fine-tuning](https://arxiv.org/abs/2602.03563)
*Wei Zhu*

Main category: cs.CL

TL;DR: The paper introduces Aligned Contrastive Learning (ACL) to resolve conflicts between cross-entropy and contrastive learning objectives in supervised settings, improving performance on GLUE tasks and multi-exit BERT fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Contrastive learning is understudied in supervised settings, and existing methods conflict with cross-entropy loss, limiting its application. This work aims to address this issue.

Method: Proposes ACL-Embed (aligning label embeddings with samples) and ACL-Grad (resolving conflicts between objectives). Extends ACL-CL for multi-exit BERT optimization.

Result: ACL outperforms or matches CE and CE+SCL on GLUE tasks and enhances multi-exit BERT fine-tuning, offering better speed-quality tradeoffs.

Conclusion: ACL effectively integrates contrastive learning in supervised settings, improving model performance and efficiency, particularly for low-latency applications.

Abstract: Despite its success in self-supervised learning, contrastive learning is less studied in the supervised setting. In this work, we first use a set of pilot experiments to show that in the supervised setting, the cross-entropy loss objective (CE) and the contrastive learning objective often conflict with each other, thus hindering the applications of CL in supervised settings. To resolve this problem, we introduce a novel \underline{A}ligned \underline{C}ontrastive \underline{L}earning (ACL) framework. First, ACL-Embed regards label embeddings as extra augmented samples with different labels and employs contrastive learning to align the label embeddings with its samples' representations. Second, to facilitate the optimization of ACL-Embed objective combined with the CE loss, we propose ACL-Grad, which will discard the ACL-Embed term if the two objectives are in conflict. To further enhance the performances of intermediate exits of multi-exit BERT, we further propose cross-layer ACL (ACL-CL), which is to ask the teacher exit to guide the optimization of student shallow exits. Extensive experiments on the GLUE benchmark results in the following takeaways: (a) ACL-BRT outperforms or performs comparably with CE and CE+SCL on the GLUE tasks; (b) ACL, especially CL-ACL, significantly surpasses the baseline methods on the fine-tuning of multi-exit BERT, thus providing better quality-speed tradeoffs for low-latency applications.

</details>


### [158] [Use Graph When It Needs: Efficiently and Adaptively Integrating Retrieval-Augmented Generation with Graphs](https://arxiv.org/abs/2602.03578)
*Su Dong,Qinggang Zhang,Yilin Xiao,Shengyuan Chen,Chuang Zhou,Xiao Huang*

Main category: cs.CL

TL;DR: EA-GraphRAG dynamically integrates RAG and GraphRAG for better handling of mixed query complexities, improving accuracy and reducing latency.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with knowledge-intensive tasks due to hallucinations and outdated data. Existing solutions like RAG and GraphRAG have limitations—RAG lacks structure and GraphRAG underperforms in real-world scenarios.

Method: EA-GraphRAG introduces syntax-aware complexity analysis, featuring a syntactic feature constructor, lightweight complexity scorer, and score-driven routing policy.

Result: EA-GraphRAG improves accuracy, reduces latency, and achieves state-of-the-art performance on mixed query scenarios.

Conclusion: The adaptive EA-GraphRAG framework effectively balances RAG and GraphRAG paradigms, addressing their individual shortcomings.

Abstract: Large language models (LLMs) often struggle with knowledge-intensive tasks due to hallucinations and outdated parametric knowledge. While Retrieval-Augmented Generation (RAG) addresses this by integrating external corpora, its effectiveness is limited by fragmented information in unstructured domain documents. Graph-augmented RAG (GraphRAG) emerged to enhance contextual reasoning through structured knowledge graphs, yet paradoxically underperforms vanilla RAG in real-world scenarios, exhibiting significant accuracy drops and prohibitive latency despite gains on complex queries. We identify the rigid application of GraphRAG to all queries, regardless of complexity, as the root cause. To resolve this, we propose an efficient and adaptive GraphRAG framework called EA-GraphRAG that dynamically integrates RAG and GraphRAG paradigms through syntax-aware complexity analysis. Our approach introduces: (i) a syntactic feature constructor that parses each query and extracts a set of structural features; (ii) a lightweight complexity scorer that maps these features to a continuous complexity score; and (iii) a score-driven routing policy that selects dense RAG for low-score queries, invokes graph-based retrieval for high-score queries, and applies complexity-aware reciprocal rank fusion to handle borderline cases. Extensive experiments on a comprehensive benchmark, consisting of two single-hop and two multi-hop QA benchmarks, demonstrate that our EA-GraphRAG significantly improves accuracy, reduces latency, and achieves state-of-the-art performance in handling mixed scenarios involving both simple and complex queries.

</details>


### [159] [$V_0$: A Generalist Value Model for Any Policy at State Zero](https://arxiv.org/abs/2602.03584)
*Yi-Kai Zhang,Zhiyuan Yao,Hongyan Hao,Yueqing Sun,Qi Gu,Hui Su,Xunliang Cai,De-Chuan Zhan,Han-Jia Ye*

Main category: cs.CL

TL;DR: Proposes $V_0$, a Generalist Value Model to estimate LLM performance without updates, improving efficiency in GRPO training and deployment.


<details>
  <summary>Details</summary>
Motivation: Avoid the overhead of training synchronous value models in Actor-Critic methods by introducing a scalable, dynamic alternative.

Method: Treats policy capability as context input, using historical data for dynamic profiling instead of parameter fitting.

Result: $V_0$ outperforms heuristic methods, enabling efficient sampling and optimal model routing.

Conclusion: $V_0$ offers a Pareto-optimal solution for performance-cost trade-offs in LLM tasks.

Abstract: Policy gradient methods rely on a baseline to measure the relative advantage of an action, ensuring the model reinforces behaviors that outperform its current average capability. In the training of Large Language Models (LLMs) using Actor-Critic methods (e.g., PPO), this baseline is typically estimated by a Value Model (Critic) often as large as the policy model itself. However, as the policy continuously evolves, the value model requires expensive, synchronous incremental training to accurately track the shifting capabilities of the policy. To avoid this overhead, Group Relative Policy Optimization (GRPO) eliminates the coupled value model by using the average reward of a group of rollouts as the baseline; yet, this approach necessitates extensive sampling to maintain estimation stability. In this paper, we propose $V_0$, a Generalist Value Model capable of estimating the expected performance of any model on unseen prompts without requiring parameter updates. We reframe value estimation by treating the policy's dynamic capability as an explicit context input; specifically, we leverage a history of instruction-performance pairs to dynamically profile the model, departing from the traditional paradigm that relies on parameter fitting to perceive capability shifts. Focusing on value estimation at State Zero (i.e., the initial prompt, hence $V_0$), our model serves as a critical resource scheduler. During GRPO training, $V_0$ predicts success rates prior to rollout, allowing for efficient sampling budget allocation; during deployment, it functions as a router, dispatching instructions to the most cost-effective and suitable model. Empirical results demonstrate that $V_0$ significantly outperforms heuristic budget allocation and achieves a Pareto-optimal trade-off between performance and cost in LLM routing tasks.

</details>


### [160] [CL-bench: A Benchmark for Context Learning](https://arxiv.org/abs/2602.03587)
*Shihan Dou,Ming Zhang,Zhangyue Yin,Chenhao Huang,Yujiong Shen,Junzhe Wang,Jiayi Chen,Yuchen Ni,Junjie Ye,Cheng Zhang,Huaibing Xie,Jianglu Hu,Shaolei Wang,Weichao Wang,Yanling Xiao,Yiting Liu,Zenan Xu,Zhen Guo,Pluto Zhou,Tao Gui,Zuxuan Wu,Xipeng Qiu,Qi Zhang,Xuanjing Huang,Yu-Gang Jiang,Di Wang,Shunyu Yao*

Main category: cs.CL

TL;DR: CL-bench is introduced as a benchmark to test LMs' ability to learn from complex contexts, revealing current models' limitations in context learning.


<details>
  <summary>Details</summary>
Motivation: To address the gap in LMs' ability to learn from task-specific contexts and new knowledge beyond pre-training, a capability termed context learning.

Method: CL-bench, a real-world benchmark with 500 complex contexts, 1,899 tasks, and 31,607 rubrics, designed by experts to test context learning.

Result: Evaluations show LMs solve only 17.2% of tasks on average, with GPT-5.1 performing best at 23.7%, highlighting limitations in context learning.

Conclusion: CL-bench highlights current LMs' weaknesses in context learning and aims to advance their real-world applicability.

Abstract: Current language models (LMs) excel at reasoning over prompts using pre-trained knowledge. However, real-world tasks are far more complex and context-dependent: models must learn from task-specific context and leverage new knowledge beyond what is learned during pre-training to reason and resolve tasks. We term this capability context learning, a crucial ability that humans naturally possess but has been largely overlooked. To this end, we introduce CL-bench, a real-world benchmark consisting of 500 complex contexts, 1,899 tasks, and 31,607 verification rubrics, all crafted by experienced domain experts. Each task is designed such that the new content required to resolve it is contained within the corresponding context. Resolving tasks in CL-bench requires models to learn from the context, ranging from new domain-specific knowledge, rule systems, and complex procedures to laws derived from empirical data, all of which are absent from pre-training. This goes far beyond long-context tasks that primarily test retrieval or reading comprehension, and in-context learning tasks, where models learn simple task patterns via instructions and demonstrations. Our evaluations of ten frontier LMs find that models solve only 17.2% of tasks on average. Even the best-performing model, GPT-5.1, solves only 23.7%, revealing that LMs have yet to achieve effective context learning, which poses a critical bottleneck for tackling real-world, complex context-dependent tasks. CL-bench represents a step towards building LMs with this fundamental capability, making them more intelligent and advancing their deployment in real-world scenarios.

</details>


### [161] [Efficient Algorithms for Partial Constraint Satisfaction Problems over Control-flow Graphs](https://arxiv.org/abs/2602.03588)
*Xuran Cai,Amir Goharshady*

Main category: cs.CL

TL;DR: The paper introduces a linear-time algorithm for solving Partial Constraint Satisfaction Problems (PCSPs) over Series-Parallel-Loop (SPL) decomposed control-flow graphs (CFGs), generalizing previous approaches and improving runtime for tasks like Optimal Bank Selection.


<details>
  <summary>Details</summary>
Motivation: PCSPs generalize Constraint Satisfaction Problems (CSPs) by allowing constraint violations at a cost. Many compiler optimization tasks, such as register allocation and bank selection, can be framed as PCSPs over CFGs, which are sparse and decomposable. The goal is to develop efficient solutions leveraging SPL decompositions.

Method: The authors propose a general algorithm for PCSPs over SPL-decomposed CFGs, achieving a time complexity of O(|G|·|D|^6). This linear-time solution for fixed domains generalizes prior SPL-based methods for tasks like register allocation and LOSPRE. Experimental validation includes Optimal Bank Selection.

Result: The algorithm reduces runtime significantly, achieving speeds four times faster than the state of the art for Optimal Bank Selection. It also unifies and extends previous SPL-based approaches for compiler optimization tasks.

Conclusion: The work presents an efficient, generalized solution for PCSPs over SPL-decomposed CFGs, validated by improved performance in practical applications like Optimal Bank Selection. It advances compiler optimization by unifying prior approaches.

Abstract: In this work, we focus on the Partial Constraint Satisfaction Problem (PCSP) over control-flow graphs (CFGs) of programs. PCSP serves as a generalization of the well-known Constraint Satisfaction Problem (CSP). In the CSP framework, we define a set of variables, a set of constraints, and a finite domain $D$ that encompasses all possible values for each variable. The objective is to assign a value to each variable in such a way that all constraints are satisfied. In the graph variant of CSP, an underlying graph is considered and we have one variable corresponding to each vertex of the graph and one or several constraints corresponding to each edge. In PCSPs, we allow for certain constraints to be violated at a specified cost, aiming to find a solution that minimizes the total cost. Numerous classical compiler optimization tasks can be framed as PCSPs over control-flow graphs. Examples include Register Allocation, Lifetime-optimal Speculative Partial Redundancy Elimination (LOSPRE), and Optimal Placement of Bank Selection Instructions. On the other hand, it is well-known that control-flow graphs of structured programs are sparse and decomposable in a variety of ways. In this work, we rely on the Series-Parallel-Loop (SPL) decompositions as introduced by~\cite{RegisterAllocation}. Our main contribution is a general algorithm for PCSPs over SPL graphs with a time complexity of \(O(|G| \cdot |D|^6)\), where \(|G|\) represents the size of the control-flow graph. Note that for any fixed domain $D,$ this yields a linear-time solution. Our algorithm can be seen as a generalization and unification of previous SPL-based approaches for register allocation and LOSPRE. In addition, we provide experimental results over another classical PCSP task, i.e. Optimal Bank Selection, achieving runtimes four times better than the previous state of the art.

</details>


### [162] [Controlling Output Rankings in Generative Engines for LLM-based Search](https://arxiv.org/abs/2602.03608)
*Haibo Jin,Ruoxi Chen,Peiyan Zhang,Yifeng Luo,Huimin Zeng,Man Luo,Haohan Wang*

Main category: cs.CL

TL;DR: CORE optimizes product rankings in LLM-based search by strategically modifying retrieved content, achieving high promotion success rates.


<details>
  <summary>Details</summary>
Motivation: LLM-based search disadvantages small businesses by favoring initial retrieval rankings, prompting the need for a fairer ranking method.

Method: CORE appends optimization content (string-based, reasoning-based, review-based) to retrieved search results to influence rankings.

Result: CORE achieves 91.4% promotion success @Top-5, 86.6% @Top-3, and 80.3% @Top-1 across 15 categories.

Conclusion: CORE effectively manipulates rankings fairly and preserves content fluency, outperforming existing methods.

Abstract: The way customers search for and choose products is changing with the rise of large language models (LLMs). LLM-based search, or generative engines, provides direct product recommendations to users, rather than traditional online search results that require users to explore options themselves. However, these recommendations are strongly influenced by the initial retrieval order of LLMs, which disadvantages small businesses and independent creators by limiting their visibility.
  In this work, we propose CORE, an optimization method that \textbf{C}ontrols \textbf{O}utput \textbf{R}ankings in g\textbf{E}nerative Engines for LLM-based search. Since the LLM's interactions with the search engine are black-box, CORE targets the content returned by search engines as the primary means of influencing output rankings. Specifically, CORE optimizes retrieved content by appending strategically designed optimization content to steer the ranking of outputs. We introduce three types of optimization content: string-based, reasoning-based, and review-based, demonstrating their effectiveness in shaping output rankings. To evaluate CORE in realistic settings, we introduce ProductBench, a large-scale benchmark with 15 product categories and 200 products per category, where each product is associated with its top-10 recommendations collected from Amazon's search interface.
  Extensive experiments on four LLMs with search capabilities (GPT-4o, Gemini-2.5, Claude-4, and Grok-3) demonstrate that CORE achieves an average Promotion Success Rate of \textbf{91.4\% @Top-5}, \textbf{86.6\% @Top-3}, and \textbf{80.3\% @Top-1}, across 15 product categories, outperforming existing ranking manipulation methods while preserving the fluency of optimized content.

</details>


### [163] [Learning Query-Specific Rubrics from Human Preferences for DeepResearch Report Generation](https://arxiv.org/abs/2602.03619)
*Changze Lv,Jie Zhou,Wentao Zhao,Jingwen Xu,Zisu Huang,Muzhao Tian,Shihan Dou,Tao Gui,Le Tian,Xiao Zhou,Xiaoqing Zheng,Xuanjing Huang,Jie Zhou*

Main category: cs.CL

TL;DR: A pipeline trains query-specific rubric generators for DeepResearch reports, combining human preferences and LLM evaluation. Introduces MaMs workflow for long-horizon reasoning, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of granularity and scalability in existing rubric-based evaluation methods for DeepResearch reports.

Method: Constructs a human-preference-annotated dataset, trains rubric generators via RL with hybrid rewards, and introduces MaMs workflow for report generation.

Result: Rubric generators deliver better human-aligned supervision; MaMs-integrated systems outperform baselines, matching closed-source models.

Conclusion: Proposed method improves rubric quality and report generation performance, bridging gaps in DeepResearch evaluation.

Abstract: Nowadays, training and evaluating DeepResearch-generated reports remain challenging due to the lack of verifiable reward signals. Accordingly, rubric-based evaluation has become a common practice. However, existing approaches either rely on coarse, pre-defined rubrics that lack sufficient granularity, or depend on manually constructed query-specific rubrics that are costly and difficult to scale. In this paper, we propose a pipeline to train human-preference-aligned query-specific rubric generators tailored for DeepResearch report generation. We first construct a dataset of DeepResearch-style queries annotated with human preferences over paired reports, and train rubric generators via reinforcement learning with a hybrid reward combining human preference supervision and LLM-based rubric evaluation. To better handle long-horizon reasoning, we further introduce a Multi-agent Markov-state (MaMs) workflow for report generation. We empirically show that our proposed rubric generators deliver more discriminative and better human-aligned supervision than existing rubric design strategies. Moreover, when integrated into the MaMs training framework, DeepResearch systems equipped with our rubric generators consistently outperform all open-source baselines on the DeepResearch Bench and achieve performance comparable to that of leading closed-source models.

</details>


### [164] [BIRDTurk: Adaptation of the BIRD Text-to-SQL Dataset to Turkish](https://arxiv.org/abs/2602.03633)
*Burak Aktaş,Mehmet Can Baytekin,Süha Kağan Köse,Ömer İlbilgi,Elif Özge Yılmaz,Çağrı Toraman,Bilge Kaan Görür*

Main category: cs.CL

TL;DR: The paper introduces BIRDTurk, a Turkish adaptation of the BIRD benchmark for Text-to-SQL systems, revealing performance degradation due to linguistic divergence and low-resource challenges, with agentic reasoning showing robustness.


<details>
  <summary>Details</summary>
Motivation: To explore Text-to-SQL performance in morphologically rich, low-resource languages like Turkish, addressing gaps in existing benchmarks.

Method: Controlled translation pipeline for BIRDTurk, validated for accuracy. Evaluated inference-based prompting, agentic reasoning, and supervised fine-tuning.

Result: Turkish introduces performance degradation; agentic reasoning is robust. Supervised fine-tuning scales with modern models.

Conclusion: BIRDTurk serves as a cross-lingual evaluation testbed, highlighting challenges and opportunities for low-resource languages.

Abstract: Text-to-SQL systems have achieved strong performance on English benchmarks, yet their behavior in morphologically rich, low-resource languages remains largely unexplored. We introduce BIRDTurk, the first Turkish adaptation of the BIRD benchmark, constructed through a controlled translation pipeline that adapts schema identifiers to Turkish while strictly preserving the logical structure and execution semantics of SQL queries and databases. Translation quality is validated on a sample size determined by the Central Limit Theorem to ensure 95% confidence, achieving 98.15% accuracy on human-evaluated samples. Using BIRDTurk, we evaluate inference-based prompting, agentic multi-stage reasoning, and supervised fine-tuning. Our results reveal that Turkish introduces consistent performance degradation, driven by both structural linguistic divergence and underrepresentation in LLM pretraining, while agentic reasoning demonstrates stronger cross-lingual robustness. Supervised fine-tuning remains challenging for standard multilingual baselines but scales effectively with modern instruction-tuned models. BIRDTurk provides a controlled testbed for cross-lingual Text-to-SQL evaluation under realistic database conditions. We release the training and development splits to support future research.

</details>


### [165] [TRE: Encouraging Exploration in the Trust Region](https://arxiv.org/abs/2602.03635)
*Chao Huang,Yujing Lu,Quangang Li,Shenghe Wang,Yan Wang,Yueyang Zhang,Long Xia,Jiashu Zhao,Zhiyuan Sun,Daiting Shi,Tingwen Liu*

Main category: cs.CL

TL;DR: Entropy regularization in RL often fails with LLMs due to tail risk; proposed Trust Region Entropy (TRE) method improves exploration and performance.


<details>
  <summary>Details</summary>
Motivation: Standard entropy regularization in RL is ineffective or harmful for LLMs due to their large vocabularies and long generation horizons, which disrupt coherent reasoning.

Method: Proposed Trust Region Entropy (TRE) restricts exploration to the model's trust region, avoiding dilution of probability mass into invalid tokens.

Result: TRE outperforms vanilla PPO, standard entropy regularization, and other baselines in tasks like mathematical reasoning, combinatorial search, and preference alignment.

Conclusion: TRE effectively addresses the limitations of entropy regularization in LLMs, enhancing exploration and performance.

Abstract: Entropy regularization is a standard technique in reinforcement learning (RL) to enhance exploration, yet it yields negligible effects or even degrades performance in Large Language Models (LLMs). We attribute this failure to the cumulative tail risk inherent to LLMs with massive vocabularies and long generation horizons. In such environments, standard global entropy maximization indiscriminately dilutes probability mass into the vast tail of invalid tokens rather than focusing on plausible candidates, thereby disrupting coherent reasoning. To address this, we propose Trust Region Entropy (TRE), a method that encourages exploration strictly within the model's trust region. Extensive experiments across mathematical reasoning (MATH), combinatorial search (Countdown), and preference alignment (HH) tasks demonstrate that TRE consistently outperforms vanilla PPO, standard entropy regularization, and other exploration baselines. Our code is available at https://github.com/WhyChaos/TRE-Encouraging-Exploration-in-the-Trust-Region.

</details>


### [166] [RAGTurk: Best Practices for Retrieval Augmented Generation in Turkish](https://arxiv.org/abs/2602.03652)
*Süha Kağan Köse,Mehmet Can Baytekin,Burak Aktaş,Bilge Kaan Görür,Evren Ayberk Munis,Deniz Yılmaz,Muhammed Yusuf Kartal,Çağrı Toraman*

Main category: cs.CL

TL;DR: The paper explores Turkish RAG pipeline optimization, highlighting the effectiveness of complex methods like HyDE and Pareto-optimal configurations for cost and accuracy balance, while cautioning against over-stacking generative modules.


<details>
  <summary>Details</summary>
Motivation: Existing RAG design guidance is English-centric, lacking insights for morphologically rich languages like Turkish. The study aims to fill this gap.

Method: Constructs a Turkish RAG dataset from Turkish Wikipedia and CulturaX, benchmarks seven RAG pipeline stages (e.g., query transformation, reranking) without task-specific fine-tuning.

Result: HyDE achieves highest accuracy (85%), significantly outperforming baseline (78.70%). Pareto-optimal configuration balances cost and performance (84.60%). Over-stacking generative modules harms performance.

Conclusion: Complex methods improve Turkish RAG accuracy, but simple query clarification with robust reranking is effective. Over-stacking generative modules should be avoided.

Abstract: Retrieval-Augmented Generation (RAG) enhances LLM factuality, yet design guidance remains English-centric, limiting insights for morphologically rich languages like Turkish. We address this by constructing a comprehensive Turkish RAG dataset derived from Turkish Wikipedia and CulturaX, comprising question-answer pairs and relevant passage chunks. We benchmark seven stages of the RAG pipeline, from query transformation and reranking to answer refinement, without task-specific fine-tuning. Our results show that complex methods like HyDE maximize accuracy (85%) that is considerably higher than the baseline (78.70%). Also a Pareto-optimal configuration using Cross-encoder Reranking and Context Augmentation achieves comparable performance (84.60%) with much lower cost. We further demonstrate that over-stacking generative modules can degrade performance by distorting morphological cues, whereas simple query clarification with robust reranking offers an effective solution.

</details>


### [167] [Instruction Anchors: Dissecting the Causal Dynamics of Modality Arbitration](https://arxiv.org/abs/2602.03677)
*Yu Zhang,Mufan Xu,Xuefeng Bai,Kehai chen,Pengfei Zhang,Yang Xiang,Min Zhang*

Main category: cs.CL

TL;DR: The paper investigates how MLLMs decide which multimodal contexts to use, highlighting the role of instruction tokens and attention layers in this process, and shows how manipulating key attention heads can significantly impact modality-following behavior.


<details>
  <summary>Details</summary>
Motivation: Understanding how multimodal large language models (MLLMs) selectively use contexts based on instructions is crucial for safety and reliability, but the mechanisms are unclear.

Method: The study uses an information flow lens to analyze the decision-making process, revealing the roles of instruction tokens, attention layers, and specialized attention heads.

Result: Instruction tokens act as anchors, shallow layers transfer multimodal cues non-selectively, deep layers resolve competition guided by intent, and MLP layers provide semantic inertia. Manipulating 5% of critical attention heads can change modality-following behavior by 60%.

Conclusion: The research advances model transparency and offers a framework for managing multimodal information in MLLMs, demonstrating the impact of targeted interventions on modality-following.

Abstract: Modality following serves as the capacity of multimodal large language models (MLLMs) to selectively utilize multimodal contexts based on user instructions. It is fundamental to ensuring safety and reliability in real-world deployments. However, the underlying mechanisms governing this decision-making process remain poorly understood. In this paper, we investigate its working mechanism through an information flow lens. Our findings reveal that instruction tokens function as structural anchors for modality arbitration: Shallow attention layers perform non-selective information transfer, routing multimodal cues to these anchors as a latent buffer; Modality competition is resolved within deep attention layers guided by the instruction intent, while MLP layers exhibit semantic inertia, acting as an adversarial force. Furthermore, we identify a sparse set of specialized attention heads that drive this arbitration. Causal interventions demonstrate that manipulating a mere $5\%$ of these critical heads can decrease the modality-following ratio by $60\%$ through blocking, or increase it by $60\%$ through targeted amplification of failed samples. Our work provides a substantial step toward model transparency and offers a principled framework for the orchestration of multimodal information in MLLMs.

</details>


### [168] [Neural Attention Search Linear: Towards Adaptive Token-Level Hybrid Attention Models](https://arxiv.org/abs/2602.03681)
*Difan Deng,Andreas Bentzen Winje,Lukas Fehring,Marius Lindauer*

Main category: cs.CL

TL;DR: NAtS-L is a hybrid framework combining linear and softmax attention per token for efficiency and expressivity in transformers.


<details>
  <summary>Details</summary>
Motivation: Addressing the quadratic complexity bottleneck of softmax transformers while maintaining expressivity in long-context scenarios.

Method: NAtS-L dynamically applies linear or softmax attention per token, using gating to optimize their combination for efficiency.

Result: Demonstrates a strong yet efficient token-level hybrid architecture.

Conclusion: NAtS-L effectively balances complexity reduction and expressivity by intelligently blending attention mechanisms.

Abstract: The quadratic computational complexity of softmax transformers has become a bottleneck in long-context scenarios. In contrast, linear attention model families provide a promising direction towards a more efficient sequential model. These linear attention models compress past KV values into a single hidden state, thereby efficiently reducing complexity during both training and inference. However, their expressivity remains limited by the size of their hidden state. Previous work proposed interleaving softmax and linear attention layers to reduce computational complexity while preserving expressivity. Nevertheless, the efficiency of these models remains bottlenecked by their softmax attention layers. In this paper, we propose Neural Attention Search Linear (NAtS-L), a framework that applies both linear attention and softmax attention operations within the same layer on different tokens. NAtS-L automatically determines whether a token can be handled by a linear attention model, i.e., tokens that have only short-term impact and can be encoded into fixed-size hidden states, or require softmax attention, i.e., tokens that contain information related to long-term retrieval and need to be preserved for future queries. By searching for optimal Gated DeltaNet and softmax attention combinations across tokens, we show that NAtS-L provides a strong yet efficient token-level hybrid architecture.

</details>


### [169] [Rethinking the Reranker: Boundary-Aware Evidence Selection for Robust Retrieval-Augmented Generation](https://arxiv.org/abs/2602.03689)
*Jiashuo Sun,Pengcheng Jiang,Saizhuo Wang,Jiajun Fan,Heng Wang,Siru Ouyang,Ming Zhong,Yizhu Jiao,Chengsong Huang,Xueqiang Xu,Pengrui Han,Peiran Li,Jiaxin Huang,Ge Liu,Heng Ji,Jiawei Han*

Main category: cs.CL

TL;DR: BAR-RAG improves RAG systems by selecting evidence in a 'Goldilocks Zone'—challenging yet sufficient for generators—leading to better performance under noisy retrieval.


<details>
  <summary>Details</summary>
Motivation: Current RAG systems fail under retrieval noise because retrievers and rerankers optimize only for relevance, often selecting unsuitable evidence for generators.

Method: BAR-RAG reframes reranking as boundary-aware evidence selection, trains the selector with reinforcement learning using generator feedback, and fine-tunes the generator under the induced evidence distribution.

Result: BAR-RAG improves robustness and achieves a 10.3% average gain over baselines on knowledge-intensive QA benchmarks.

Conclusion: BAR-RAG effectively addresses evidence suitability for generators, enhancing RAG performance under noisy conditions.

Abstract: Retrieval-Augmented Generation (RAG) systems remain brittle under realistic retrieval noise, even when the required evidence appears in the top-K results. A key reason is that retrievers and rerankers optimize solely for relevance, often selecting either trivial, answer-revealing passages or evidence that lacks the critical information required to answer the question, without considering whether the evidence is suitable for the generator. We propose BAR-RAG, which reframes the reranker as a boundary-aware evidence selector that targets the generator's Goldilocks Zone -- evidence that is neither trivially easy nor fundamentally unanswerable for the generator, but is challenging yet sufficient for inference and thus provides the strongest learning signal. BAR-RAG trains the selector with reinforcement learning using generator feedback, and adopts a two-stage pipeline that fine-tunes the generator under the induced evidence distribution to mitigate the distribution mismatch between training and inference. Experiments on knowledge-intensive question answering benchmarks show that BAR-RAG consistently improves end-to-end performance under noisy retrieval, achieving an average gain of 10.3 percent over strong RAG and reranking baselines while substantially improving robustness. Code is publicly avaliable at https://github.com/GasolSun36/BAR-RAG.

</details>


### [170] [OCRTurk: A Comprehensive OCR Benchmark for Turkish](https://arxiv.org/abs/2602.03693)
*Deniz Yılmaz,Evren Ayberk Munis,Çağrı Toraman,Süha Kağan Köse,Burak Aktaş,Mehmet Can Baytekin,Bilge Kaan Görür*

Main category: cs.CL

TL;DR: OCRTurk is a new benchmark for Turkish document parsing, addressing gaps in existing benchmarks by covering diverse documents at three difficulty levels.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack coverage for low-resource languages like Turkish and standardized real-world scenarios.

Method: OCRTurk includes 180 Turkish documents from various categories (academic, non-academic, slides) evaluated with seven OCR models.

Result: PaddleOCR performs best overall but varies by document type; slideshows are the most challenging.

Conclusion: OCRTurk fills a critical gap in Turkish document parsing benchmarks and highlights model performance variations.

Abstract: Document parsing is now widely used in applications, such as large-scale document digitization, retrieval-augmented generation, and domain-specific pipelines in healthcare and education. Benchmarking these models is crucial for assessing their reliability and practical robustness. Existing benchmarks mostly target high-resource languages and provide limited coverage for low-resource settings, such as Turkish. Moreover, existing studies on Turkish document parsing lack a standardized benchmark that reflects real-world scenarios and document diversity. To address this gap, we introduce OCRTurk, a Turkish document parsing benchmark covering multiple layout elements and document categories at three difficulty levels. OCRTurk consists of 180 Turkish documents drawn from academic articles, theses, slide decks, and non-academic articles. We evaluate seven OCR models on OCRTurk using element-wise metrics. Across difficulty levels, PaddleOCR achieves the strongest overall results, leading most element-wise metrics except figures and attaining high Normalized Edit Distance scores in easy, medium, and hard subsets. We also observe performance variation by document type. Models perform well on non-academic documents, while slideshows become the most challenging.

</details>


### [171] [Cognitively Diverse Multiple-Choice Question Generation: A Hybrid Multi-Agent Framework with Large Language Models](https://arxiv.org/abs/2602.03704)
*Yu Tian,Linh Huynh,Katerina Christhilf,Shubham Chakraborty,Micah Watanabe,Tracy Arner,Danielle McNamara*

Main category: cs.CL

TL;DR: ReQUESTA is a hybrid framework combining LLMs and rule-based methods to generate cognitively diverse MCQs, outperforming single-pass GPT-5 in difficulty, discrimination, and quality.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of producing MCQs with controlled cognitive demands using LLMs.

Method: Uses a multi-agent framework to decompose MCQ generation into subtasks, integrating LLM-powered agents and rule-based components for planning, generation, evaluation, and post-processing.

Result: ReQUESTA-generated MCQs are more challenging, discriminative, and aligned with comprehension performance, with better distractor quality and topic relevance.

Conclusion: Hybrid agentic orchestration enhances LLM-based MCQ generation, emphasizing workflow design for reliability and controllability.

Abstract: Recent advances in large language models (LLMs) have made automated multiple-choice question (MCQ) generation increasingly feasible; however, reliably producing items that satisfy controlled cognitive demands remains a challenge. To address this gap, we introduce ReQUESTA, a hybrid, multi-agent framework for generating cognitively diverse MCQs that systematically target text-based, inferential, and main idea comprehension. ReQUESTA decomposes MCQ authoring into specialized subtasks and coordinates LLM-powered agents with rule-based components to support planning, controlled generation, iterative evaluation, and post-processing. We evaluated the framework in a large-scale reading comprehension study using academic expository texts, comparing ReQUESTA-generated MCQs with those produced by a single-pass GPT-5 zero-shot baseline. Psychometric analyses of learner responses assessed item difficulty and discrimination, while expert raters evaluated question quality across multiple dimensions, including topic relevance and distractor quality. Results showed that ReQUESTA-generated items were consistently more challenging, more discriminative, and more strongly aligned with overall reading comprehension performance. Expert evaluations further indicated stronger alignment with central concepts and superior distractor linguistic consistency and semantic plausibility, particularly for inferential questions. These findings demonstrate that hybrid, agentic orchestration can systematically improve the reliability and controllability of LLM-based generation, highlighting workflow design as a key lever for structured artifact generation beyond single-pass prompting.

</details>


### [172] [OmniRAG-Agent: Agentic Omnimodal Reasoning for Low-Resource Long Audio-Video Question Answering](https://arxiv.org/abs/2602.03707)
*Yifan Zhu,Xinyu Mu,Tao Feng,Zhonghong Ou,Yuning Gong,Haoran Luo*

Main category: cs.CL

TL;DR: OmniRAG-Agent improves long audio-video QA by combining retrieval-augmented generation with agentic planning and joint optimization, outperforming prior methods in low-resource settings.


<details>
  <summary>Details</summary>
Motivation: Addressing costly dense encoding, weak retrieval, limited planning, and lack of end-to-end optimization in low-resource long audio-video QA.

Method: Proposes OmniRAG-Agent, featuring an image-audio retrieval-augmented generation module, an agent loop for planning and tool use, and group relative policy optimization.

Result: Outperforms prior methods on OmniVideoBench, WorldSense, and Daily-Omni, with ablations confirming component effectiveness.

Conclusion: OmniRAG-Agent is a robust solution for budgeted long audio-video QA, validated by experiments and component analysis.

Abstract: Long-horizon omnimodal question answering answers questions by reasoning over text, images, audio, and video. Despite recent progress on OmniLLMs, low-resource long audio-video QA still suffers from costly dense encoding, weak fine-grained retrieval, limited proactive planning, and no clear end-to-end optimization.To address these issues, we propose OmniRAG-Agent, an agentic omnimodal QA method for budgeted long audio-video reasoning. It builds an image-audio retrieval-augmented generation module that lets an OmniLLM fetch short, relevant frames and audio snippets from external banks. Moreover, it uses an agent loop that plans, calls tools across turns, and merges retrieved evidence to answer complex queries. Furthermore, we apply group relative policy optimization to jointly improve tool use and answer quality over time. Experiments on OmniVideoBench, WorldSense, and Daily-Omni show that OmniRAG-Agent consistently outperforms prior methods under low-resource settings and achieves strong results, with ablations validating each component.

</details>


### [173] [Beyond Tokens: Semantic-Aware Speculative Decoding for Efficient Inference by Probing Internal States](https://arxiv.org/abs/2602.03708)
*Ximing Dong,Shaowei Wang,Dayi Lin,Boyuan Chen,Ahmed E. Hassan*

Main category: cs.CL

TL;DR: TL;DR: SemanticSpec speeds up LLM inference by verifying semantic sequences instead of tokens, achieving up to 2.7x faster performance.


<details>
  <summary>Details</summary>
Motivation: LLMs suffer from high inference latency due to autoregressive decoding, especially in LRMs generating lengthy chains of thought.

Method: SemanticSpec uses semantic-aware speculative decoding, verifying entire semantic sequences via a semantic probability estimation mechanism.

Result: Achieves up to 2.7x speedup on DeepSeekR1-32B and 2.1x on QwQ-32B, outperforming token-level and sequence-level baselines.

Conclusion: SemanticSpec improves efficiency and effectiveness in LLM inference by leveraging semantic equivalence.

Abstract: Large Language Models (LLMs) achieve strong performance across many tasks but suffer from high inference latency due to autoregressive decoding. The issue is exacerbated in Large Reasoning Models (LRMs), which generate lengthy chains of thought. While speculative decoding accelerates inference by drafting and verifying multiple tokens in parallel, existing methods operate at the token level and ignore semantic equivalence (i.e., different token sequences expressing the same meaning), leading to inefficient rejections. We propose SemanticSpec, a semantic-aware speculative decoding framework that verifies entire semantic sequences instead of tokens. SemanticSpec introduces a semantic probability estimation mechanism that probes the model's internal hidden states to assess the likelihood of generating sequences with specific meanings.Experiments on four benchmarks show that SemanticSpec achieves up to 2.7x speedup on DeepSeekR1-32B and 2.1x on QwQ-32B, consistently outperforming token-level and sequence-level baselines in both efficiency and effectiveness.

</details>


### [174] [No Shortcuts to Culture: Indonesian Multi-hop Question Answering for Complex Cultural Understanding](https://arxiv.org/abs/2602.03709)
*Vynska Amalia Permadi,Xingwei Tan,Nafise Sadat Moosavi,Nikos Aletras*

Main category: cs.CL

TL;DR: ID-MoCQA introduces the first large-scale multi-hop QA dataset for assessing cultural understanding in LLMs, focusing on Indonesian traditions.


<details>
  <summary>Details</summary>
Motivation: Current QA benchmarks lack multi-hop reasoning for cultural understanding, often allowing models to exploit shallow cues.

Method: Introduces ID-MoCQA, a dataset transforming single-hop cultural questions into multi-hop reasoning chains with six clue types. Uses expert review and LLM filtering for validation.

Result: Reveals significant gaps in cultural reasoning among state-of-the-art models, especially for nuanced inference tasks.

Conclusion: ID-MoCQA is a crucial benchmark for improving LLMs' cultural competency.

Abstract: Understanding culture requires reasoning across context, tradition, and implicit social knowledge, far beyond recalling isolated facts. Yet most culturally focused question answering (QA) benchmarks rely on single-hop questions, which may allow models to exploit shallow cues rather than demonstrate genuine cultural reasoning. In this work, we introduce ID-MoCQA, the first large-scale multi-hop QA dataset for assessing the cultural understanding of large language models (LLMs), grounded in Indonesian traditions and available in both English and Indonesian. We present a new framework that systematically transforms single-hop cultural questions into multi-hop reasoning chains spanning six clue types (e.g., commonsense, temporal, geographical). Our multi-stage validation pipeline, combining expert review and LLM-as-a-judge filtering, ensures high-quality question-answer pairs. Our evaluation across state-of-the-art models reveals substantial gaps in cultural reasoning, particularly in tasks requiring nuanced inference. ID-MoCQA provides a challenging and essential benchmark for advancing the cultural competency of LLMs.

</details>


### [175] [Training Multi-Turn Search Agent via Contrastive Dynamic Branch Sampling](https://arxiv.org/abs/2602.03719)
*Yubao Zhao,Weiquan Huang,Sudong Wang,Ruochen Zhao,Chen Chen,Yao Shu,Chengwei Qin*

Main category: cs.CL

TL;DR: BranPO introduces step-level contrastive supervision for agentic reinforcement learning, improving long-horizon planning by truncating trajectories and resampling alternatives, outperforming baselines without extra training cost.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in long-horizon reinforcement learning, like sparse rewards and computational inefficiency, BranPO leverages tail-focused decision analysis to enhance performance.

Method: Proposes Branching Relative Policy Optimization (BranPO), which truncates trajectories near the tail and resamples continuations for contrastive supervision, with difficulty-aware branch sampling and redundant step masking for efficiency.

Result: BranPO outperforms baselines on question answering benchmarks, achieving significant accuracy gains in long-horizon tasks without additional training budget.

Conclusion: BranPO effectively mitigates credit ambiguity and computational inefficiency in long-horizon reinforcement learning, demonstrating superior performance and efficiency.

Abstract: Agentic reinforcement learning has enabled large language models to perform complex multi-turn planning and tool use. However, learning in long-horizon settings remains challenging due to sparse, trajectory-level outcome rewards. While prior tree-based methods attempt to mitigate this issue, they often suffer from high variance and computational inefficiency. Through empirical analysis of search agents, We identify a common pattern: performance diverges mainly due to decisions near the tail. Motivated by this observation, we propose Branching Relative Policy Optimization (BranPO), a value-free method that provides step-level contrastive supervision without dense rewards. BranPO truncates trajectories near the tail and resamples alternative continuations to construct contrastive suffixes over shared prefixes, reducing credit ambiguity in long-horizon rollouts. To further boost efficiency and stabilize training, we introduce difficulty-aware branch sampling to adapt branching frequency across tasks, and redundant step masking to suppress uninformative actions. Extensive experiments on various question answering benchmarks demonstrate that BranPO consistently outperforms strong baselines, achieving significant accuracy gains on long-horizon tasks without increasing the overall training budget. Our code is available at \href{https://github.com/YubaoZhao/BranPO}{code}.

</details>


### [176] [CUBO: Self-Contained Retrieval-Augmented Generation on Consumer Laptops 10 GB Corpora, 16 GB RAM, Single-Device Deployment](https://arxiv.org/abs/2602.03731)
*Paolo Astrino*

Main category: cs.CL

TL;DR: CUBO is a lightweight RAG platform for consumer laptops, enabling efficient document retrieval within GDPR compliance using minimal RAM.


<details>
  <summary>Details</summary>
Motivation: Address the tension between cloud-based AI (GDPR risks) and local systems (high RAM requirements) for sensitive documents.

Method: Integrates streaming ingestion, tiered hybrid retrieval, and hardware-aware orchestration to optimize performance within 16 GB RAM.

Result: Achieves competitive Recall@10 (0.48-0.97) and low latency (185 ms p50) on consumer laptops, staying under 15.5 GB RAM.

Conclusion: CUBO is deployable for small-to-medium archives, balancing performance and GDPR compliance locally.

Abstract: Organizations handling sensitive documents face a tension: cloud-based AI risks GDPR violations, while local systems typically require 18-32 GB RAM. This paper presents CUBO, a systems-oriented RAG platform for consumer laptops with 16 GB shared memory. CUBO's novelty lies in engineering integration of streaming ingestion (O(1) buffer overhead), tiered hybrid retrieval, and hardware-aware orchestration that enables competitive Recall@10 (0.48-0.97 across BEIR domains) within a hard 15.5 GB RAM ceiling. The 37,000-line codebase achieves retrieval latencies of 185 ms (p50) on C1,300 laptops while maintaining data minimization through local-only processing aligned with GDPR Art. 5(1)(c). Evaluation on BEIR benchmarks validates practical deployability for small-to-medium professional archives. The codebase is publicly available at https://github.com/PaoloAstrino/CUBO.

</details>


### [177] [Context Compression via Explicit Information Transmission](https://arxiv.org/abs/2602.03784)
*Jiangnan Ye,Hanqi Yan,Zhenyi Shen,Heng Chang,Ye Mao,Yulan He*

Main category: cs.CL

TL;DR: ComprExIT introduces a lightweight framework for soft context compression in LLMs, addressing structural limitations of existing methods by explicitly transmitting information over frozen hidden states, improving performance with minimal added parameters.


<details>
  <summary>Details</summary>
Motivation: Long-context inference in LLMs is expensive due to quadratic attention and growing key-value caches, necessitating efficient context compression methods.

Method: ComprExIT decouples compression from self-attention dynamics, using depth-wise and width-wise transmission to selectively aggregate information into token anchors and globally optimized slots, respectively.

Result: ComprExIT outperforms state-of-the-art methods on six QA benchmarks, with only ~1% additional parameters.

Conclusion: Explicit and coordinated information transmission enhances long-context compression effectiveness and robustness.

Abstract: Long-context inference with Large Language Models (LLMs) is costly due to quadratic attention and growing key-value caches, motivating context compression. In this work, we study soft context compression, where a long context is condensed into a small set of continuous representations. Existing methods typically re-purpose the LLM itself as a trainable compressor, relying on layer-by-layer self-attention to iteratively aggregate information. We argue that this paradigm suffers from two structural limitations: (i) progressive representation overwriting across layers (ii) uncoordinated allocation of compression capacity across tokens. We propose ComprExIT (Context Compression via Explicit Information Transmission), a lightweight framework that formulates soft compression into a new paradigm: explicit information transmission over frozen LLM hidden states. This decouples compression from the model's internal self-attention dynamics. ComprExIT performs (i) depth-wise transmission to selectively transmit multi-layer information into token anchors, mitigating progressive overwriting, and (ii) width-wise transmission to aggregate anchors into a small number of slots via a globally optimized transmission plan, ensuring coordinated allocation of information. Across six question-answering benchmarks, ComprExIT consistently outperforms state-of-the-art context compression methods while introducing only ~1% additional parameters, demonstrating that explicit and coordinated information transmission enables more effective and robust long-context compression.

</details>


### [178] [Kimi K2.5: Visual Agentic Intelligence](https://arxiv.org/abs/2602.02276)
*Kimi Team,Tongtong Bai,Yifan Bai,Yiping Bao,S. H. Cai,Yuan Cao,Y. Charles,H. S. Che,Cheng Chen,Guanduo Chen,Huarong Chen,Jia Chen,Jiahao Chen,Jianlong Chen,Jun Chen,Kefan Chen,Liang Chen,Ruijue Chen,Xinhao Chen,Yanru Chen,Yanxu Chen,Yicun Chen,Yimin Chen,Yingjiang Chen,Yuankun Chen,Yujie Chen,Yutian Chen,Zhirong Chen,Ziwei Chen,Dazhi Cheng,Minghan Chu,Jialei Cui,Jiaqi Deng,Muxi Diao,Hao Ding,Mengfan Dong,Mengnan Dong,Yuxin Dong,Yuhao Dong,Angang Du,Chenzhuang Du,Dikang Du,Lingxiao Du,Yulun Du,Yu Fan,Shengjun Fang,Qiulin Feng,Yichen Feng,Garimugai Fu,Kelin Fu,Hongcheng Gao,Tong Gao,Yuyao Ge,Shangyi Geng,Chengyang Gong,Xiaochen Gong,Zhuoma Gongque,Qizheng Gu,Xinran Gu,Yicheng Gu,Longyu Guan,Yuanying Guo,Xiaoru Hao,Weiran He,Wenyang He,Yunjia He,Chao Hong,Hao Hu,Jiaxi Hu,Yangyang Hu,Zhenxing Hu,Ke Huang,Ruiyuan Huang,Weixiao Huang,Zhiqi Huang,Tao Jiang,Zhejun Jiang,Xinyi Jin,Yu Jing,Guokun Lai,Aidi Li,C. Li,Cheng Li,Fang Li,Guanghe Li,Guanyu Li,Haitao Li,Haoyang Li,Jia Li,Jingwei Li,Junxiong Li,Lincan Li,Mo Li,Weihong Li,Wentao Li,Xinhang Li,Xinhao Li,Yang Li,Yanhao Li,Yiwei Li,Yuxiao Li,Zhaowei Li,Zheming Li,Weilong Liao,Jiawei Lin,Xiaohan Lin,Zhishan Lin,Zichao Lin,Cheng Liu,Chenyu Liu,Hongzhang Liu,Liang Liu,Shaowei Liu,Shudong Liu,Shuran Liu,Tianwei Liu,Tianyu Liu,Weizhou Liu,Xiangyan Liu,Yangyang Liu,Yanming Liu,Yibo Liu,Yuanxin Liu,Yue Liu,Zhengying Liu,Zhongnuo Liu,Enzhe Lu,Haoyu Lu,Zhiyuan Lu,Junyu Luo,Tongxu Luo,Yashuo Luo,Long Ma,Yingwei Ma,Shaoguang Mao,Yuan Mei,Xin Men,Fanqing Meng,Zhiyong Meng,Yibo Miao,Minqing Ni,Kun Ouyang,Siyuan Pan,Bo Pang,Yuchao Qian,Ruoyu Qin,Zeyu Qin,Jiezhong Qiu,Bowen Qu,Zeyu Shang,Youbo Shao,Tianxiao Shen,Zhennan Shen,Juanfeng Shi,Lidong Shi,Shengyuan Shi,Feifan Song,Pengwei Song,Tianhui Song,Xiaoxi Song,Hongjin Su,Jianlin Su,Zhaochen Su,Lin Sui,Jinsong Sun,Junyao Sun,Tongyu Sun,Flood Sung,Yunpeng Tai,Chuning Tang,Heyi Tang,Xiaojuan Tang,Zhengyang Tang,Jiawen Tao,Shiyuan Teng,Chaoran Tian,Pengfei Tian,Ao Wang,Bowen Wang,Chensi Wang,Chuang Wang,Congcong Wang,Dingkun Wang,Dinglu Wang,Dongliang Wang,Feng Wang,Hailong Wang,Haiming Wang,Hengzhi Wang,Huaqing Wang,Hui Wang,Jiahao Wang,Jinhong Wang,Jiuzheng Wang,Kaixin Wang,Linian Wang,Qibin Wang,Shengjie Wang,Shuyi Wang,Si Wang,Wei Wang,Xiaochen Wang,Xinyuan Wang,Yao Wang,Yejie Wang,Yipu Wang,Yiqin Wang,Yucheng Wang,Yuzhi Wang,Zhaoji Wang,Zhaowei Wang,Zhengtao Wang,Zhexu Wang,Zihan Wang,Zizhe Wang,Chu Wei,Ming Wei,Chuan Wen,Zichen Wen,Chengjie Wu,Haoning Wu,Junyan Wu,Rucong Wu,Wenhao Wu,Yuefeng Wu,Yuhao Wu,Yuxin Wu,Zijian Wu,Chenjun Xiao,Jin Xie,Xiaotong Xie,Yuchong Xie,Yifei Xin,Bowei Xing,Boyu Xu,Jianfan Xu,Jing Xu,Jinjing Xu,L. H. Xu,Lin Xu,Suting Xu,Weixin Xu,Xinbo Xu,Xinran Xu,Yangchuan Xu,Yichang Xu,Yuemeng Xu,Zelai Xu,Ziyao Xu,Junjie Yan,Yuzi Yan,Guangyao Yang,Hao Yang,Junwei Yang,Kai Yang,Ningyuan Yang,Ruihan Yang,Xiaofei Yang,Xinlong Yang,Ying Yang,Yi Yang,Yi Yang,Zhen Yang,Zhilin Yang,Zonghan Yang,Haotian Yao,Dan Ye,Wenjie Ye,Zhuorui Ye,Bohong Yin,Chengzhen Yu,Longhui Yu,Tao Yu,Tianxiang Yu,Enming Yuan,Mengjie Yuan,Xiaokun Yuan,Yang Yue,Weihao Zeng,Dunyuan Zha,Haobing Zhan,Dehao Zhang,Hao Zhang,Jin Zhang,Puqi Zhang,Qiao Zhang,Rui Zhang,Xiaobin Zhang,Y. Zhang,Yadong Zhang,Yangkun Zhang,Yichi Zhang,Yizhi Zhang,Yongting Zhang,Yu Zhang,Yushun Zhang,Yutao Zhang,Yutong Zhang,Zheng Zhang,Chenguang Zhao,Feifan Zhao,Jinxiang Zhao,Shuai Zhao,Xiangyu Zhao,Yikai Zhao,Zijia Zhao,Huabin Zheng,Ruihan Zheng,Shaojie Zheng,Tengyang Zheng,Junfeng Zhong,Longguang Zhong,Weiming Zhong,M. Zhou,Runjie Zhou,Xinyu Zhou,Zaida Zhou,Jinguo Zhu,Liya Zhu,Xinhao Zhu,Yuxuan Zhu,Zhen Zhu,Jingze Zhuang,Weiyu Zhuang,Ying Zou,Xinxing Zu*

Main category: cs.CL

TL;DR: Kimi K2.5 is an open-source multimodal agentic model focusing on text-vision joint optimization, featuring Agent Swarm for parallel task execution, achieving state-of-the-art performance and reduced latency.


<details>
  <summary>Details</summary>
Motivation: To advance general agentic intelligence by integrating text and vision modalities for mutual enhancement and improving task execution efficiency.

Method: Utilizes joint text-vision pre-training, zero-vision SFT, and reinforcement learning, alongside Agent Swarm for dynamic task decomposition and parallel execution.

Result: Achieves state-of-the-art results in coding, vision, reasoning, and agentic tasks, with Agent Swarm reducing latency by up to 4.5×.

Conclusion: Kimi K2.5's multimodal and parallel execution approach significantly advances agentic intelligence, with released checkpoints for further research.

Abstract: We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhance each other. This includes a series of techniques such as joint text-vision pre-training, zero-vision SFT, and joint text-vision reinforcement learning. Building on this multimodal foundation, K2.5 introduces Agent Swarm, a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. Extensive evaluations show that Kimi K2.5 achieves state-of-the-art results across various domains including coding, vision, reasoning, and agentic tasks. Agent Swarm also reduces latency by up to $4.5\times$ over single-agent baselines. We release the post-trained Kimi K2.5 model checkpoint to facilitate future research and real-world applications of agentic intelligence.

</details>


### [179] [They Said Memes Were Harmless-We Found the Ones That Hurt: Decoding Jokes, Symbols, and Cultural References](https://arxiv.org/abs/2602.03822)
*Sahil Tripathi,Gautam Siddharth Kashyap,Mehwish Nasim,Jian Yang,Jiechao Gao,Usman Naseem*

Main category: cs.CL

TL;DR: CROSS-ALIGN+ improves meme-based social abuse detection by addressing cultural blindness, boundary ambiguity, and lack of interpretability, outperforming state-of-the-art methods with up to 17% F1 improvement.


<details>
  <summary>Details</summary>
Motivation: Harmful intent in memes relies on cultural symbolism and subtle cross-modal incongruence, which prior methods fail to capture fully due to cultural blindness, boundary ambiguity, and opacity.

Method: A three-stage framework: Stage I enriches multimodal representations with structured knowledge; Stage II uses LoRA adapters to sharpen decision boundaries; Stage III generates cascaded explanations for interpretability.

Result: Outperforms state-of-the-art methods on five benchmarks with up to 17% relative F1 improvement and provides interpretable justifications.

Conclusion: CROSS-ALIGN+ effectively addresses key limitations in meme-based abuse detection, offering both performance gains and transparency.

Abstract: Meme-based social abuse detection is challenging because harmful intent often relies on implicit cultural symbolism and subtle cross-modal incongruence. Prior approaches, from fusion-based methods to in-context learning with Large Vision-Language Models (LVLMs), have made progress but remain limited by three factors: i) cultural blindness (missing symbolic context), ii) boundary ambiguity (satire vs. abuse confusion), and iii) lack of interpretability (opaque model reasoning). We introduce CROSS-ALIGN+, a three-stage framework that systematically addresses these limitations: (1) Stage I mitigates cultural blindness by enriching multimodal representations with structured knowledge from ConceptNet, Wikidata, and Hatebase; (2) Stage II reduces boundary ambiguity through parameter-efficient LoRA adapters that sharpen decision boundaries; and (3) Stage III enhances interpretability by generating cascaded explanations. Extensive experiments on five benchmarks and eight LVLMs demonstrate that CROSS-ALIGN+ consistently outperforms state-of-the-art methods, achieving up to 17% relative F1 improvement while providing interpretable justifications for each decision.

</details>


### [180] [Accelerating Scientific Research with Gemini: Case Studies and Common Techniques](https://arxiv.org/abs/2602.03837)
*David P. Woodruff,Vincent Cohen-Addad,Lalit Jain,Jieming Mao,Song Zuo,MohammadHossein Bateni,Simina Branzei,Michael P. Brenner,Lin Chen,Ying Feng,Lance Fortnow,Gang Fu,Ziyi Guan,Zahra Hadizadeh,Mohammad T. Hajiaghayi,Mahdi JafariRaviz,Adel Javanmard,Karthik C. S.,Ken-ichi Kawarabayashi,Ravi Kumar,Silvio Lattanzi,Euiwoong Lee,Yi Li,Ioannis Panageas,Dimitris Paparas,Benjamin Przybocki,Bernardo Subercaseaux,Ola Svensson,Shayan Taherijam,Xuan Wu,Eylon Yogev,Morteza Zadimoghaddam,Samson Zhou,Vahab Mirrokni*

Main category: cs.CL

TL;DR: The paper explores how advanced AI models, like Google's Gemini variants, assist in expert-level mathematical discovery through human-AI collaboration, showcasing success in solving open problems and generating proofs across multiple disciplines.


<details>
  <summary>Details</summary>
Motivation: To understand and demonstrate the potential of LLMs as genuine partners in scientific research, particularly in novel, expert-level mathematical and theoretical discovery.

Method: Case studies of human-AI collaboration, involving techniques like iterative refinement, problem decomposition, and cross-disciplinary knowledge transfer, with some instances using adversarial review and neuro-symbolic loops.

Result: Successful collaboration with AI models led to solving open problems, refuting conjectures, and generating new proofs in theoretical computer science, economics, optimization, and physics.

Conclusion: AI can serve as a versatile partner in scientific discovery, not just for automation but also as a creative collaborator, with effective techniques for human-AI interaction.

Abstract: Recent advances in large language models (LLMs) have opened new avenues for accelerating scientific research. While models are increasingly capable of assisting with routine tasks, their ability to contribute to novel, expert-level mathematical discovery is less understood. We present a collection of case studies demonstrating how researchers have successfully collaborated with advanced AI models, specifically Google's Gemini-based models (in particular Gemini Deep Think and its advanced variants), to solve open problems, refute conjectures, and generate new proofs across diverse areas in theoretical computer science, as well as other areas such as economics, optimization, and physics. Based on these experiences, we extract common techniques for effective human-AI collaboration in theoretical research, such as iterative refinement, problem decomposition, and cross-disciplinary knowledge transfer. While the majority of our results stem from this interactive, conversational methodology, we also highlight specific instances that push beyond standard chat interfaces. These include deploying the model as a rigorous adversarial reviewer to detect subtle flaws in existing proofs, and embedding it within a "neuro-symbolic" loop that autonomously writes and executes code to verify complex derivations. Together, these examples highlight the potential of AI not just as a tool for automation, but as a versatile, genuine partner in the creative process of scientific discovery.

</details>


### [181] [Parallel-Probe: Towards Efficient Parallel Thinking via 2D Probing](https://arxiv.org/abs/2602.03845)
*Tong Zheng,Chengsong Huang,Runpeng Dai,Yun He,Rui Liu,Xin Ni,Huiwen Bao,Kaishen Wang,Hongtu Zhu,Jiaxin Huang,Furong Huang,Heng Huang*

Main category: cs.CL

TL;DR: Parallel-Probe optimizes parallel thinking by dynamically adjusting reasoning depth and branch width, reducing token costs significantly while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Parallel thinking is computationally intensive, and existing methods lack mechanisms to exploit global dynamics across branches.

Method: Introduces 2D probing to expose width-depth dynamics and Parallel-Probe, a controller for early stopping and branch pruning.

Result: Reduces sequential tokens by 35.8% and total token cost by 25.8% while maintaining competitive accuracy.

Conclusion: Parallel-Probe offers an efficient solution for optimizing parallel thinking, improving scalability and reducing computational costs.

Abstract: Parallel thinking has emerged as a promising paradigm for reasoning, yet it imposes significant computational burdens. Existing efficiency methods primarily rely on local, per-trajectory signals and lack principled mechanisms to exploit global dynamics across parallel branches. We introduce 2D probing, an interface that exposes the width-depth dynamics of parallel thinking by periodically eliciting intermediate answers from all branches. Our analysis reveals three key insights: non-monotonic scaling across width-depth allocations, heterogeneous reasoning branch lengths, and early stabilization of global consensus. Guided by these insights, we introduce $\textbf{Parallel-Probe}$, a training-free controller designed to optimize online parallel thinking. Parallel-Probe employs consensus-based early stopping to regulate reasoning depth and deviation-based branch pruning to dynamically adjust width. Extensive experiments across three benchmarks and multiple models demonstrate that Parallel-Probe establishes a superior Pareto frontier for test-time scaling. Compared to standard majority voting, it reduces sequential tokens by up to $\textbf{35.8}$% and total token cost by over $\textbf{25.8}$% while maintaining competitive accuracy.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [182] [CreditAudit: 2D Auditing for LLM Evaluation and Selection](https://arxiv.org/abs/2602.02515)
*Yiliang Song,Hongjun An,Jiangong Xiao,Haofei Zhao,Jiawei Shao,Xuelong Li*

Main category: cs.AI

TL;DR: CreditAudit evaluates language models' performance stability across diverse scenarios, providing mean scores and volatility grades (AAA-BBB) to aid deployment decisions.


<details>
  <summary>Details</summary>
Motivation: Benchmark scores often misrepresent real-world usability due to evolving system prompts and interaction protocols, leaving practitioners unsure which model to deploy.

Method: CreditAudit assesses models using semantically aligned prompt templates, measuring mean performance and scenario-induced fluctuation (sigma), and mapping volatility into credit grades.

Result: Models with similar mean scores show varying stability risks, which can significantly impact deployment prioritization in critical or agentic settings.

Conclusion: CreditAudit offers a 2D evaluation framework and credit grades for regime-specific model selection, promoting more objective deployment decisions.

Abstract: Leaderboard scores on public benchmarks have been steadily rising and converging, with many frontier language models now separated by only marginal differences. However, these scores often fail to match users' day to day experience, because system prompts, output protocols, and interaction modes evolve under routine iteration, and in agentic multi step pipelines small protocol shifts can trigger disproportionate failures, leaving practitioners uncertain about which model to deploy. We propose CreditAudit, a deployment oriented credit audit framework that evaluates models under a family of semantically aligned and non adversarial system prompt templates across multiple benchmarks, reporting mean ability as average performance across scenarios and scenario induced fluctuation sigma as a stability risk signal, and further mapping volatility into interpretable credit grades from AAA to BBB via cross model quantiles with diagnostics that mitigate template difficulty drift. Controlled experiments on GPQA, TruthfulQA, and MMLU Pro show that models with similar mean ability can exhibit substantially different fluctuation, and stability risk can overturn prioritization decisions in agentic or high failure cost regimes. By providing a 2D and grade based language for regime specific selection, CreditAudit supports tiered deployment and more disciplined allocation of testing and monitoring effort, enabling more objective and trustworthy model evaluation for real world use.

</details>


### [183] [Experience-Driven Multi-Agent Systems Are Training-free Context-aware Earth Observers](https://arxiv.org/abs/2602.02559)
*Pengyu Dai,Weihao Xuan,Junjue Wang,Hongruixuan Chen,Jian Song,Yafei Ou,Naoto Yokoya*

Main category: cs.AI

TL;DR: GeoEvolver is a self-evolving multi-agent system that helps LLM agents learn fine-grained tool expertise for Earth Observation tasks, improving task success by 12% on average.


<details>
  <summary>Details</summary>
Motivation: Existing LLM agents struggle in specialized domains like Earth Observation due to lack of tool-level expertise, leading to execution errors and pipeline failures.

Method: GeoEvolver decomposes queries into sub-goals, explores tool configurations, and distills successful patterns and failures into an evolving memory bank for future use.

Result: Experiments on three EO benchmarks show GeoEvolver improves task success by 12% across multiple LLM backbones.

Conclusion: GeoEvolver demonstrates that EO expertise can emerge from fine-grained interactions, enhancing LLM agent effectiveness in complex workflows.

Abstract: Recent advances have enabled large language model (LLM) agents to solve complex tasks by orchestrating external tools. However, these agents often struggle in specialized, tool-intensive domains that demand long-horizon execution, tight coordination across modalities, and strict adherence to implicit tool constraints. Earth Observation (EO) tasks exemplify this challenge due to the multi-modal and multi-temporal data inputs, as well as the requirements of geo-knowledge constraints (spectrum library, spatial reasoning, etc): many high-level plans can be derailed by subtle execution errors that propagate through a pipeline and invalidate final results. A core difficulty is that existing agents lack a mechanism to learn fine-grained, tool-level expertise from interaction. Without such expertise, they cannot reliably configure tool parameters or recover from mid-execution failures, limiting their effectiveness in complex EO workflows. To address this, we introduce \textbf{GeoEvolver}, a self-evolving multi-agent system~(MAS) that enables LLM agents to acquire EO expertise through structured interaction without any parameter updates. GeoEvolver decomposes each query into independent sub-goals via a retrieval-augmented multi-agent orchestrator, then explores diverse tool-parameter configurations at the sub-goal level. Successful patterns and root-cause attribution from failures are then distilled in an evolving memory bank that provides in-context demonstrations for future queries. Experiments on three tool-integrated EO benchmarks show that GeoEvolver consistently improves end-to-end task success, with an average gain of 12\% across multiple LLM backbones, demonstrating that EO expertise can emerge progressively from efficient, fine-grained interactions with the environment.

</details>


### [184] [Uncertainty and Fairness Awareness in LLM-Based Recommendation Systems](https://arxiv.org/abs/2602.02582)
*Chandan Kumar Sah,Xiaoli Lian,Li Zhang,Tony Xu,Syed Shazaib Shah*

Main category: cs.AI

TL;DR: The paper investigates uncertainty and fairness issues in LLM-generated recommendations, introducing new evaluation metrics, datasets, and methodologies to improve reliability and equity.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of predictive uncertainty and embedded biases in LLM recommendations, ensuring they are accurate, consistent, and fair across diverse demographic attributes.

Method: Introduces a benchmark with curated metrics, a dataset annotated for demographic attributes, and evaluates uncertainty (via entropy) and fairness (e.g., SNSR, SNSV) using case studies.

Result: Identified systematic unfairness in Google DeepMind's Gemini 1.5 Flash for certain attributes. Demonstrated persistent disparities under prompt perturbations and explored personality-linked biases.

Conclusion: Proposes uncertainty-aware evaluation and personality-informed fairness benchmarks to enhance LLM recommendations, paving the way for safer and more interpretable systems.

Abstract: Large language models (LLMs) enable powerful zero-shot recommendations by leveraging broad contextual knowledge, yet predictive uncertainty and embedded biases threaten reliability and fairness. This paper studies how uncertainty and fairness evaluations affect the accuracy, consistency, and trustworthiness of LLM-generated recommendations. We introduce a benchmark of curated metrics and a dataset annotated for eight demographic attributes (31 categorical values) across two domains: movies and music. Through in-depth case studies, we quantify predictive uncertainty (via entropy) and demonstrate that Google DeepMind's Gemini 1.5 Flash exhibits systematic unfairness for certain sensitive attributes; measured similarity-based gaps are SNSR at 0.1363 and SNSV at 0.0507. These disparities persist under prompt perturbations such as typographical errors and multilingual inputs. We further integrate personality-aware fairness into the RecLLM evaluation pipeline to reveal personality-linked bias patterns and expose trade-offs between personalization and group fairness. We propose a novel uncertainty-aware evaluation methodology for RecLLMs, present empirical insights from deep uncertainty case studies, and introduce a personality profile-informed fairness benchmark that advances explainability and equity in LLM recommendations. Together, these contributions establish a foundation for safer, more interpretable RecLLMs and motivate future work on multi-model benchmarks and adaptive calibration for trustworthy deployment.

</details>


### [185] [A Positive Case for Faithfulness: LLM Self-Explanations Help Predict Model Behavior](https://arxiv.org/abs/2602.02639)
*Harry Mayne,Justin Singh Kang,Dewi Gould,Kannan Ramchandran,Adam Mahdi,Noah Y. Siegel*

Main category: cs.AI

TL;DR: The paper introduces NSG, a metric to evaluate the faithfulness of LLM self-explanations, showing they improve behavior prediction but sometimes mislead.


<details>
  <summary>Details</summary>
Motivation: To address the lack of understanding about the faithfulness of LLM self-explanations to their true reasoning processes.

Method: Introduces Normalized Simulatability Gain (NSG) to measure how well explanations help predict model behavior, tested on 18 models and 7,000 counterfactuals.

Result: Self-explanations improve behavior prediction (11-37% NSG) and outperform external explanations, but 5-15% are misleading.

Conclusion: Self-explanations provide valuable predictive information but require caution due to occasional unfaithfulness.

Abstract: LLM self-explanations are often presented as a promising tool for AI oversight, yet their faithfulness to the model's true reasoning process is poorly understood. Existing faithfulness metrics have critical limitations, typically relying on identifying unfaithfulness via adversarial prompting or detecting reasoning errors. These methods overlook the predictive value of explanations. We introduce Normalized Simulatability Gain (NSG), a general and scalable metric based on the idea that a faithful explanation should allow an observer to learn a model's decision-making criteria, and thus better predict its behavior on related inputs. We evaluate 18 frontier proprietary and open-weight models, e.g., Gemini 3, GPT-5.2, and Claude 4.5, on 7,000 counterfactuals from popular datasets covering health, business, and ethics. We find self-explanations substantially improve prediction of model behavior (11-37% NSG). Self-explanations also provide more predictive information than explanations generated by external models, even when those models are stronger. This implies an advantage from self-knowledge that external explanation methods cannot replicate. Our approach also reveals that, across models, 5-15% of self-explanations are egregiously misleading. Despite their imperfections, we show a positive case for self-explanations: they encode information that helps predict model behavior.

</details>


### [186] [MARS: Modular Agent with Reflective Search for Automated AI Research](https://arxiv.org/abs/2602.02660)
*Jiefeng Chen,Bhavana Dalvi Mishra,Jaehyun Nam,Rui Meng,Tomas Pfister,Jinsung Yoon*

Main category: cs.AI

TL;DR: MARS (Modular Agent with Reflective Search) is a framework for autonomous AI research, addressing challenges like high computational costs and opaque performance attribution. It uses budget-aware planning, modular construction, and reflective memory to achieve state-of-the-art performance and generalize insights effectively.


<details>
  <summary>Details</summary>
Motivation: Automating AI research requires handling expensive evaluations and unclear performance attribution, which current LLM-based agents struggle with. MARS aims to overcome these limitations.

Method: MARS employs three key components: Budget-Aware Planning (cost-constrained MCTS), Modular Construction (Design-Decompose-Implement pipeline), and Comparative Reflective Memory (analyzing solution differences to assign credit).

Result: MARS achieves state-of-the-art performance on MLE-Bench, competes with top global methods, and shows effective generalization with 63% of insights from cross-branch transfer.

Conclusion: MARS successfully addresses the challenges of autonomous AI research by balancing performance and cost, modularizing complexity, and improving insight generalization.

Abstract: Automating AI research differs from general software engineering due to computationally expensive evaluation (e.g., model training) and opaque performance attribution. Current LLM-based agents struggle here, often generating monolithic scripts that ignore execution costs and causal factors. We introduce MARS (Modular Agent with Reflective Search), a framework optimized for autonomous AI research. MARS relies on three pillars: (1) Budget-Aware Planning via cost-constrained Monte Carlo Tree Search (MCTS) to explicitly balance performance with execution expense; (2) Modular Construction, employing a "Design-Decompose-Implement" pipeline to manage complex research repositories; and (3) Comparative Reflective Memory, which addresses credit assignment by analyzing solution differences to distill high-signal insights. MARS achieves state-of-the-art performance among open-source frameworks on MLE-Bench under comparable settings, maintaining competitiveness with the global leaderboard's top methods. Furthermore, the system exhibits qualitative "Aha!" moments, where 63% of all utilized lessons originate from cross-branch transfer, demonstrating that the agent effectively generalizes insights across search paths.

</details>


### [187] [ATLAS : Adaptive Self-Evolutionary Research Agent with Task-Distributed Multi-LLM Supporters](https://arxiv.org/abs/2602.02709)
*Ujin Jeon,Jiyong Kwon,Madison Ann Sullivan,Caleb Eunho Lee,Guang Lin*

Main category: cs.AI

TL;DR: ATLAS introduces adaptive task-distributed learning for multi-LLM agent systems, outperforming static baselines in stability and performance for long-horizon tasks.


<details>
  <summary>Details</summary>
Motivation: Existing multi-LLM agent systems often freeze solvers post-fine-tuning or use static optimization loops, limiting adaptability for long-horizon tasks. ATLAS aims to overcome these limitations.

Method: ATLAS employs Evolving Direct Preference Optimization (EvoDPO) to adaptively update reference policies and delegates roles to specialized supporter agents for tasks like exploration and hyperparameter tuning.

Result: Experiments on non-stationary bandits and SciML tasks (e.g., 1D Burgers' equation) demonstrate improved stability and performance over static single-agent baselines.

Conclusion: ATLAS effectively addresses the limitations of static systems through adaptive learning and task distribution, enhancing performance in complex, evolving scenarios.

Abstract: Recent multi-LLM agent systems perform well in prompt optimization and automated problem-solving, but many either keep the solver frozen after fine-tuning or rely on a static preference-optimization loop, which becomes intractable for long-horizon tasks. We propose ATLAS (Adaptive Task-distributed Learning for Agentic Self-evolution), a task-distributed framework that iteratively develops a lightweight research agent while delegating complementary roles to specialized supporter agents for exploration, hyperparameter tuning, and reference policy management. Our core algorithm, Evolving Direct Preference Optimization (EvoDPO), adaptively updates the phase-indexed reference policy. We provide a theoretical regret analysis for a preference-based contextual bandit under concept drift. In addition, experiments were conducted on non-stationary linear contextual bandits and scientific machine learning (SciML) loss reweighting for the 1D Burgers' equation. Both results show that ATLAS improves stability and performance over a static single-agent baseline.

</details>


### [188] [Visual Reasoning over Time Series via Multi-Agent System](https://arxiv.org/abs/2602.03026)
*Weilin Ruan,Yuxuan Liang*

Main category: cs.AI

TL;DR: MAS4TS is a multi-agent system integrating visual reasoning and latent reconstruction for versatile time series tasks, achieving top performance.


<details>
  <summary>Details</summary>
Motivation: Existing time-series methods lack visual reasoning and task generalization; MAS4TS addresses these gaps.

Method: Uses Analyzer-Reasoner-Executor paradigm with Vision-Language Model for visual reasoning and latent reconstruction. Agents coordinate via shared memory.

Result: Achieves state-of-the-art performance across benchmarks with strong generalization and efficient inference.

Conclusion: MAS4TS effectively integrates visual reasoning and adaptive tools for superior time-series task performance.

Abstract: Time series analysis underpins many real-world applications, yet existing time-series-specific methods and pretrained large-model-based approaches remain limited in integrating intuitive visual reasoning and generalizing across tasks with adaptive tool usage. To address these limitations, we propose MAS4TS, a tool-driven multi-agent system for general time series tasks, built upon an Analyzer-Reasoner-Executor paradigm that integrates agent communication, visual reasoning, and latent reconstruction within a unified framework. MAS4TS first performs visual reasoning over time series plots with structured priors using a Vision-Language Model to extract temporal structures, and subsequently reconstructs predictive trajectories in latent space. Three specialized agents coordinate via shared memory and gated communication, while a router selects task-specific tool chains for execution. Extensive experiments on multiple benchmarks demonstrate that MAS4TS achieves state-of-the-art performance across a wide range of time series tasks, while exhibiting strong generalization and efficient inference.

</details>


### [189] [Dynamic Mix Precision Routing for Efficient Multi-step LLM Interaction](https://arxiv.org/abs/2602.02711)
*Yuanzhe Li,Jianing Deng,Jingtong Hu,Tianlong Chen,Song Wang,Huanrui Yang*

Main category: cs.AI

TL;DR: The paper explores using low-precision quantized LLMs for long-horizon decision-making, proposing a dynamic mix-precision routing framework to balance cost and accuracy.


<details>
  <summary>Details</summary>
Motivation: High inference costs of large LLMs in multi-step tasks motivate the use of low-precision quantized models to reduce costs while maintaining performance.

Method: A dynamic mix-precision routing framework adaptively selects between high- and low-precision LLMs per step, trained via KL-divergence-based SL and GRPO.

Result: Experiments show the approach improves accuracy-cost trade-off over single-precision baselines and heuristic methods in ALFWorld.

Conclusion: Dynamic mix-precision routing effectively balances LLM performance and inference costs in long-horizon decision-making tasks.

Abstract: Large language models (LLM) achieve strong performance in long-horizon decision-making tasks through multi-step interaction and reasoning at test time. While practitioners commonly believe a higher task success rate necessitates the use of a larger and stronger LLM model, multi-step interaction with a large LLM incurs prohibitive inference cost. To address this problem, we explore the use of low-precision quantized LLM in the long-horizon decision-making process. Based on the observation of diverse sensitivities among interaction steps, we propose a dynamic mix-precision routing framework that adaptively selects between high-precision and low-precision LLMs at each decision step. The router is trained via a two-stage pipeline, consisting of KL-divergence-based supervised learning that identifies precision-sensitive steps, followed by Group-Relative Policy Optimization (GRPO) to further improve task success rates. Experiments on ALFWorld demonstrate that our approach achieves a great improvement on accuracy-cost trade-off over single-precision baselines and heuristic routing methods.

</details>


### [190] [Scaling-Aware Adapter for Structure-Grounded LLM Reasoning](https://arxiv.org/abs/2602.02780)
*Zihao Jing,Qiuhao Zeng,Ruiyi Fang,Yan Yi Li,Yan Sun,Boyu Wang,Pingzhao Hu*

Main category: cs.AI

TL;DR: Cuttlefish is a unified all-atom LLM that addresses limitations of existing methods by adaptively scaling structural tokens and grounding language reasoning in geometric cues, achieving superior performance in structure-grounded reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing methods for reasoning over biomolecular structures using LLMs are modality-specific, compress inputs inefficiently, and lack geometric grounding, leading to structural hallucinations and suboptimal performance.

Method: Cuttlefish introduces Scaling-Aware Patching for adaptive token scaling and Geometry Grounding Adapter to refine tokens with geometric cues, enhancing structural reasoning.

Result: Experiments show Cuttlefish outperforms existing methods in heterogeneous structure-grounded reasoning benchmarks.

Conclusion: Cuttlefish provides a scalable and geometrically grounded approach for all-atom reasoning, addressing key limitations of current methods.

Abstract: Large language models (LLMs) are enabling reasoning over biomolecular structures, yet existing methods remain modality-specific and typically compress structural inputs through sequence-based tokenization or fixed-length query connectors. Such architectures either omit the geometric groundings requisite for mitigating structural hallucinations or impose inflexible modality fusion bottlenecks that concurrently over-compress and suboptimally allocate structural tokens, thereby impeding the realization of generalized all-atom reasoning. We introduce Cuttlefish, a unified all-atom LLM that grounds language reasoning in geometric cues while scaling modality tokens with structural complexity. First, Scaling-Aware Patching leverages an instruction-conditioned gating mechanism to generate variable-size patches over structural graphs, adaptively scaling the query token budget with structural complexity to mitigate fixed-length connector bottlenecks. Second, Geometry Grounding Adapter refines these adaptive tokens via cross-attention to modality embeddings and injects the resulting modality tokens into the LLM, exposing explicit geometric cues to reduce structural hallucination. Experiments across diverse all-atom benchmarks demonstrate that Cuttlefish achieves superior performance in heterogeneous structure-grounded reasoning. Code is available at the project repository.

</details>


### [191] [MAS-ProVe: Understanding the Process Verification of Multi-Agent Systems](https://arxiv.org/abs/2602.03053)
*Vishal Venkataramani,Haizhou Shi,Zixuan Ke,Austin Xu,Xiaoxiao He,Yingbo Zhou,Semih Yavuz,Hao Wang,Shafiq Joty*

Main category: cs.AI

TL;DR: MAS-ProVe investigates process verification in Multi-Agent Systems (MAS) using LLMs, evaluating multiple paradigms and finding high variance and inconsistent performance improvements.


<details>
  <summary>Details</summary>
Motivation: To address the unclear effectiveness of process verification in MAS, despite its promise in general reasoning settings.

Method: The study evaluates three verification paradigms (LLM-as-a-Judge, reward models, process reward models), two granularity levels, five verifiers, and four context management strategies across six MAS frameworks.

Result: Process verification does not consistently improve performance and shows high variance; LLM-as-a-Judge outperforms reward-based approaches, with trained judges being more effective.

Conclusion: Effective process verification for MAS remains an open challenge, requiring advancements beyond current paradigms.

Abstract: Multi-Agent Systems (MAS) built on Large Language Models (LLMs) often exhibit high variance in their reasoning trajectories. Process verification, which evaluates intermediate steps in trajectories, has shown promise in general reasoning settings, and has been suggested as a potential tool for guiding coordination of MAS; however, its actual effectiveness in MAS remains unclear. To fill this gap, we present MAS-ProVe, a systematic empirical study of process verification for multi-agent systems (MAS). Our study spans three verification paradigms (LLM-as-a-Judge, reward models, and process reward models), evaluated across two levels of verification granularity (agent-level and iteration-level). We further examine five representative verifiers and four context management strategies, and conduct experiments over six diverse MAS frameworks on multiple reasoning benchmarks. We find that process-level verification does not consistently improve performance and frequently exhibits high variance, highlighting the difficulty of reliably evaluating partial multi-agent trajectories. Among the methods studied, LLM-as-a-Judge generally outperforms reward-based approaches, with trained judges surpassing general-purpose LLMs. We further observe a small performance gap between LLMs acting as judges and as single agents, and identify a context-length-performance trade-off in verification. Overall, our results suggest that effective and robust process verification for MAS remains an open challenge, requiring further advances beyond current paradigms. Code is available at https://github.com/Wang-ML-Lab/MAS-ProVe.

</details>


### [192] [Rejecting Arguments Based on Doubt in Structured Bipolar Argumentation](https://arxiv.org/abs/2602.03286)
*Michael A. Müller,Srdjan Vesic,Bruno Yun*

Main category: cs.AI

TL;DR: This paper introduces structured bipolar argumentation frameworks (SBAFs) to capture philosophical and linguistic insights, allowing agents to reject arguments based on doubt and focus on sentences rather than entire arguments.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address gaps in computational argumentation by incorporating two philosophical ideas: agents can rationally reject arguments due to doubt and focus on individual sentences/claims in debates.

Method: The authors define SBAFs with attack and support relations between sentences, then provide semantics that avoid forcing acceptance of all defended arguments (unlike completeness-based semantics) and include language extensions for acceptable sentences.

Result: The proposed semantics represent reasonable debate positions, bridging admissible and complete semantics, and offer insights into when abstract argumentation is suitable or deductive support semantics applies.

Conclusion: The approach enhances computational argumentation by aligning it with philosophical and linguistic principles, providing flexibility in argument acceptance and new perspectives on existing methods.

Abstract: This paper develops a new approach to computational argumentation that is informed by philosophical and linguistic views. Namely, it takes into account two ideas that have received little attention in the literature on computational argumentation: First, an agent may rationally reject an argument based on mere doubt, thus not all arguments they could defend must be accepted; and, second, that it is sometimes more natural to think in terms of which individual sentences or claims an agent accepts in a debate, rather than which arguments. In order to incorporate these two ideas into a computational approach, we first define the notion of structured bipolar argumentation frameworks (SBAFs), where arguments consist of sentences and we have both an attack and a support relation between them. Then, we provide semantics for SBAFs with two features: (1) Unlike with completeness-based semantics, our semantics do not force agents to accept all defended arguments. (2) In addition to argument extensions, which give acceptable sets of arguments, we also provide semantics for language extensions that specify acceptable sets of sentences. These semantics represent reasonable positions an agent might have in a debate. Our semantics lie between the admissible and complete semantics of abstract argumentation. Further, our approach can be used to provide a new perspective on existing approaches. For instance, we can specify the conditions under which an agent can ignore support between arguments (i.e. under which the use of abstract argumentation is warranted) and we show that deductive support semantics is a special case of our approach.

</details>


### [193] [AutoSizer: Automatic Sizing of Analog and Mixed-Signal Circuits via Large Language Model (LLM) Agents](https://arxiv.org/abs/2602.02849)
*Xi Yu,Dmitrii Torbunov,Soumyajit Mandal,Yihui Ren*

Main category: cs.AI

TL;DR: AutoSizer, an LLM-driven meta-optimization framework, addresses AMS circuit sizing challenges by integrating circuit understanding and adaptive search-space refinement, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Analog and Mixed-Signal (AMS) circuit design faces inefficiencies due to reliance on expert knowledge and static optimization methods. Large Language Models (LLMs), despite strong reasoning, lack precision for numerical optimization in AMS sizing.

Method: AutoSizer uses a two-loop optimization framework: an inner loop for sizing and an outer loop refining the search space based on simulation feedback. It includes AMS-SizingBench, a benchmark for evaluating adaptive policies.

Result: AutoSizer achieves higher solution quality, faster convergence, and better success rates compared to traditional and LLM-based methods.

Conclusion: AutoSizer effectively bridges the gap between LLM reasoning and numerical optimization in AMS circuit sizing, offering robust and efficient solutions.

Abstract: The design of Analog and Mixed-Signal (AMS) integrated circuits remains heavily reliant on expert knowledge, with transistor sizing a major bottleneck due to nonlinear behavior, high-dimensional design spaces, and strict performance constraints. Existing Electronic Design Automation (EDA) methods typically frame sizing as static black-box optimization, resulting in inefficient and less robust solutions. Although Large Language Models (LLMs) exhibit strong reasoning abilities, they are not suited for precise numerical optimization in AMS sizing. To address this gap, we propose AutoSizer, a reflective LLM-driven meta-optimization framework that unifies circuit understanding, adaptive search-space construction, and optimization orchestration in a closed loop. It employs a two-loop optimization framework, with an inner loop for circuit sizing and an outer loop that analyzes optimization dynamics and constraints to iteratively refine the search space from simulation feedback. We further introduce AMS-SizingBench, an open benchmark comprising 24 diverse AMS circuits in SKY130 CMOS technology, designed to evaluate adaptive optimization policies under realistic simulator-based constraints. AutoSizer experimentally achieves higher solution quality, faster convergence, and higher success rate across varying circuit difficulties, outperforming both traditional optimization methods and existing LLM-based agents.

</details>


### [194] [STEER: Inference-Time Risk Control via Constrained Quality-Diversity Search](https://arxiv.org/abs/2602.02862)
*Eric Yang,Jong Ha Lee,Jonathan Amar,Elissa Ye,Yugang Jia*

Main category: cs.AI

TL;DR: STEER introduces a tunable control framework for LLMs in ordinal decision settings, enabling adjustable decision conservativeness without retraining, outperforming existing methods in behavioral coverage and accuracy.


<details>
  <summary>Details</summary>
Motivation: LLMs often face mode collapse in tasks requiring nuanced trade-offs (e.g., clinical triage). Standard alignment lacks tunable control over specificity and sensitivity, limiting practical utility.

Method: STEER uses an offline quality-diversity search to create diverse personas, enforcing safety and reasoning thresholds. A single control parameter adjusts decision conservativeness at inference.

Result: STEER outperforms temperature-based sampling and static ensembles in behavioral coverage and maintains high accuracy on urgent cases while offering comparable control for ambiguous decisions.

Conclusion: STEER provides a safe, effective paradigm for risk control in LLMs, enabling adaptable behavior without sacrificing competence.

Abstract: Large Language Models (LLMs) trained for average correctness often exhibit mode collapse, producing narrow decision behaviors on tasks where multiple responses may be reasonable. This limitation is particularly problematic in ordinal decision settings such as clinical triage, where standard alignment removes the ability to trade off specificity and sensitivity (the ROC operating point) based on contextual constraints. We propose STEER (Steerable Tuning via Evolutionary Ensemble Refinement), a training-free framework that reintroduces this tunable control. STEER constructs a population of natural-language personas through an offline, constrained quality-diversity search that promotes behavioral coverage while enforcing minimum safety, reasoning, and stability thresholds. At inference time, STEER exposes a single, interpretable control parameter that maps a user-specified risk percentile to a selected persona, yielding a monotonic adjustment of decision conservativeness. On two clinical triage benchmarks, STEER achieves broader behavioral coverage compared to temperature-based sampling and static persona ensembles. Compared to a representative post-training method, STEER maintains substantially higher accuracy on unambiguous urgent cases while providing comparable control over ambiguous decisions. These results demonstrate STEER as a safety-preserving paradigm for risk control, capable of steering behavior without compromising domain competence.

</details>


### [195] [Aligning Language Model Benchmarks with Pairwise Preferences](https://arxiv.org/abs/2602.02898)
*Marco Gutierrez,Xinyi Leng,Hannah Cyberey,Jonathan Richard Schwarz,Ahmed Alaa,Thomas Hartvigsen*

Main category: cs.AI

TL;DR: The paper introduces 'benchmark alignment' and BenchAlign, a method to improve how benchmarks predict real-world model utility by updating them with limited performance data.


<details>
  <summary>Details</summary>
Motivation: Benchmarks often fail to predict real-world model performance, creating a gap between evaluation and practical utility.

Method: Proposes BenchAlign, which uses model performance and ranked pairs to weight benchmark questions, aligning them with human preferences.

Result: Aligned benchmarks accurately rank unseen models according to human preferences and remain interpretable.

Conclusion: BenchAlign bridges the gap between benchmarks and real utility, aiding model development.

Abstract: Language model benchmarks are pervasive and computationally-efficient proxies for real-world performance. However, many recent works find that benchmarks often fail to predict real utility. Towards bridging this gap, we introduce benchmark alignment, where we use limited amounts of information about model performance to automatically update offline benchmarks, aiming to produce new static benchmarks that predict model pairwise preferences in given test settings. We then propose BenchAlign, the first solution to this problem, which learns preference-aligned weight- ings for benchmark questions using the question-level performance of language models alongside ranked pairs of models that could be collected during deployment, producing new benchmarks that rank previously unseen models according to these preferences. Our experiments show that our aligned benchmarks can accurately rank unseen models according to models of human preferences, even across different sizes, while remaining interpretable. Overall, our work provides insights into the limits of aligning benchmarks with practical human preferences, which stands to accelerate model development towards real utility.

</details>


### [196] [Minimal Computational Preconditions for Subjective Perspective in Artificial Agents](https://arxiv.org/abs/2602.02902)
*Hongju Pae*

Main category: cs.AI

TL;DR: The study implements subjective perspective in AI agents using a latent state structure, showing hysteresis as a sign of subjectivity.


<details>
  <summary>Details</summary>
Motivation: To ground subjective perspective in AI agents using phenomenologically inspired methods.

Method: A slowly evolving latent state modulates policies without direct optimization, tested in reward-free environments with regime shifts.

Result: The latent state shows hysteresis, indicating perspective-like subjectivity, while policy behavior remains reactive.

Conclusion: Hysteresis in latent structures can signal subjectivity in artificial systems.

Abstract: This study operationalizes subjective perspective in artificial agents by grounding it in a minimal, phenomenologically motivated internal structure. The perspective is implemented as a slowly evolving global latent state that modulates fast policy dynamics without being directly optimized for behavioral consequences. In a reward-free environment with regime shifts, this latent structure exhibits direction-dependent hysteresis, while policy-level behavior remains comparatively reactive. I argue that such hysteresis constitutes a measurable signature of perspective-like subjectivity in machine systems.

</details>


### [197] [FIRE-Bench: Evaluating Agents on the Rediscovery of Scientific Insights](https://arxiv.org/abs/2602.02905)
*Zhen Wang,Fan Bai,Zhongyan Luo,Jinyan Su,Kaiser Sun,Xinle Yu,Jieyuan Liu,Kun Zhou,Claire Cardie,Mark Dredze,Eric P. Xing,Zhiting Hu*

Main category: cs.AI

TL;DR: FIRE-Bench evaluates LLM-powered autonomous agents' ability to rediscover established scientific findings, revealing current limitations in full-cycle research despite using advanced models like GPT-5.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks either rely on LLM-as-judge evaluations or focus on isolated metrics, lacking a rigorous framework for assessing verifiable scientific discovery.

Method: FIRE-Bench tasks agents with autonomously rediscovering findings from high-impact ML research, covering idea exploration, experiment design, code implementation, execution, and evidence-based conclusions.

Result: Current agents show limited success (<50 F1), high variability, and recurring failures in experimental design, execution, and reasoning, even with advanced LLMs like GPT-5.

Conclusion: FIRE-Bench offers a diagnostic tool to measure progress toward reliable agent-driven scientific discovery, highlighting the challenges remaining.

Abstract: Autonomous agents powered by large language models (LLMs) promise to accelerate scientific discovery end-to-end, but rigorously evaluating their capacity for verifiable discovery remains a central challenge. Existing benchmarks face a trade-off: they either heavily rely on LLM-as-judge evaluations of automatically generated research outputs or optimize convenient yet isolated performance metrics that provide coarse proxies for scientific insight. To address this gap, we introduce FIRE-Bench (Full-cycle Insight Rediscovery Evaluation), a benchmark that evaluates agents through the rediscovery of established findings from recent, high-impact machine learning research. Agents are given only a high-level research question extracted from a published, verified study and must autonomously explore ideas, design experiments, implement code, execute their plans, and derive conclusions supported by empirical evidence. We evaluate a range of state-of-the-art agents with frontier LLMs backbones like gpt-5 on FIRE-Bench. Our results show that full-cycle scientific research remains challenging for current agent systems: even the strongest agents achieve limited rediscovery success (<50 F1), exhibit high variance across runs, and display recurring failure modes in experimental design, execution, and evidence-based reasoning. FIRE-Bench provides a rigorous and diagnostic framework for measuring progress toward reliable agent-driven scientific discovery.

</details>


### [198] [Reasoning about Reasoning: BAPO Bounds on Chain-of-Thought Token Complexity in LLMs](https://arxiv.org/abs/2602.02909)
*Kiran Tomlinson,Tobias Schnabel,Adith Swaminathan,Jennifer Neville*

Main category: cs.AI

TL;DR: The paper explores the minimum reasoning tokens needed in chain-of-thought (CoT) reasoning for LLMs as input size grows, proving Ω(n) lower bounds for three tasks and validating with experiments.


<details>
  <summary>Details</summary>
Motivation: To understand the fundamental limits of CoT reasoning's latency and compute costs by quantifying required reasoning tokens for scalable performance.

Method: Extends the BAPO model to prove lower bounds on CoT tokens for binary majority, triplet matching, and graph reachability, complemented by upper bounds and experiments.

Result: Proves Ω(n) reasoning tokens are necessary for input size n, with empirical validation showing linear scaling and failures under constrained budgets.

Conclusion: Identifies bottlenecks in CoT's inference-time compute and provides a framework for optimizing reasoning length.

Abstract: Inference-time scaling via chain-of-thought (CoT) reasoning is a major driver of state-of-the-art LLM performance, but it comes with substantial latency and compute costs. We address a fundamental theoretical question: how many reasoning tokens are required to solve a problem as input size grows? By extending the bounded attention prefix oracle (BAPO) model--an abstraction of LLMs that quantifies the information flow required to solve a task--we prove lower bounds on the CoT tokens required for three canonical BAPO-hard tasks: binary majority, triplet matching, and graph reachability. We show that each requires $Ω(n)$ reasoning tokens when the input size is $n$. We complement these results with matching or near-matching upper bounds via explicit constructions. Finally, our experiments with frontier reasoning models show approximately linear reasoning token scaling on these tasks and failures when constrained to smaller reasoning budgets, consistent with our theoretical lower bounds. Together, our results identify fundamental bottlenecks in inference-time compute through CoT and offer a principled tool for analyzing optimal reasoning length.

</details>


### [199] [DeltaEvolve: Accelerating Scientific Discovery through Momentum-Driven Evolution](https://arxiv.org/abs/2602.02919)
*Jiachen Jiang,Tianyu Ding,Zhihui Zhu*

Main category: cs.AI

TL;DR: DeltaEvolve improves evolutionary AI systems by replacing full-code histories with structured semantic deltas, enhancing efficiency and guidance.


<details>
  <summary>Details</summary>
Motivation: Existing methods like AlphaEvolve use inefficient full-code histories, which dilute core ideas and provide weak evolutionary guidance.

Method: Proposes DeltaEvolve, a momentum-driven framework using semantic deltas to capture modifications and their impact, organized via a multi-level database.

Result: Empirical results show DeltaEvolve discovers better solutions with fewer tokens compared to full-code-based approaches.

Conclusion: Semantic deltas offer a more efficient and informative alternative to full-code histories for evolutionary AI systems.

Abstract: LLM-driven evolutionary systems have shown promise for automated science discovery, yet existing approaches such as AlphaEvolve rely on full-code histories that are context-inefficient and potentially provide weak evolutionary guidance. In this work, we first formalize the evolutionary agents as a general Expectation-Maximization framework, where the language model samples candidate programs (E-step) and the system updates the control context based on evaluation feedback (M-step). Under this view, constructing context via full-code snapshots constitutes a suboptimal M-step, as redundant implement details dilutes core algorithmic ideas, making it difficult to provide clear inspirations for evolution. To address this, we propose DeltaEvolve, a momentum-driven evolutionary framework that replaces full-code history with structured semantic delta capturing how and why modifications between successive nodes affect performance. As programs are often decomposable, semantic delta usually contains many effective components which are transferable and more informative to drive improvement. By organizing semantic delta through multi-level database and progressive disclosure mechanism, input tokens are further reduced. Empirical evaluations on tasks across diverse scientific domains show that our framework can discover better solution with less token consumption over full-code-based evolutionary agents.

</details>


### [200] [UAT-LITE: Inference-Time Uncertainty-Aware Attention for Pretrained Transformers](https://arxiv.org/abs/2602.02952)
*Elias Hossain,Shubhashis Roy Dipta,Subash Neupane,Rajib Rana,Ravid Shwartz-Ziv,Ivan Garibay,Niloofar Yousefi*

Main category: cs.AI

TL;DR: The paper proposes UAT-LITE, a method to improve uncertainty awareness in NLP models by modulating self-attention during inference, reducing calibration errors without altering pretrained weights or training objectives.


<details>
  <summary>Details</summary>
Motivation: Neural NLP models often exhibit miscalibration, assigning high confidence to incorrect predictions, which is problematic for selective prediction and high-stakes applications. Existing methods either adjust probabilities post-hoc or incur high training/storage costs.

Method: UAT-LITE uses Monte Carlo dropout in pretrained transformers to estimate token-level epistemic uncertainty and modulate self-attention during contextualization. It also introduces layerwise variance decomposition to analyze uncertainty accumulation across transformer layers.

Result: UAT-LITE reduces Expected Calibration Error by ~20% on average across SQuAD 2.0, MNLI, and SST-2, while maintaining task accuracy. It also enhances selective prediction and robustness under distribution shift.

Conclusion: UAT-LITE effectively addresses calibration issues in NLP models by making self-attention uncertainty-aware during inference, offering a lightweight and efficient solution without additional training overhead.

Abstract: Neural NLP models are often miscalibrated, assigning high confidence to incorrect predictions, which undermines selective prediction and high-stakes deployment. Post-hoc calibration methods adjust output probabilities but leave internal computation unchanged, while ensemble and Bayesian approaches improve uncertainty at substantial training or storage cost. We propose UAT-LITE, an inference-time framework that makes self-attention uncertainty-aware using approximate Bayesian inference via Monte Carlo dropout in pretrained transformer classifiers. Token-level epistemic uncertainty is estimated from stochastic forward passes and used to modulate self-attention during contextualization, without modifying pretrained weights or training objectives. We additionally introduce a layerwise variance decomposition to diagnose how predictive uncertainty accumulates across transformer depth. Across the SQuAD 2.0 answerability, MNLI, and SST-2, UAT-LITE reduces Expected Calibration Error by approximately 20% on average relative to a fine-tuned BERT-base baseline while preserving task accuracy, and improves selective prediction and robustness under distribution shift.

</details>


### [201] [Generative Engine Optimization: A VLM and Agent Framework for Pinterest Acquisition Growth](https://arxiv.org/abs/2602.02961)
*Faye Zhang,Qianyu Cheng,Jasmine Wan,Vishwakarma Singh,Jinfeng Rao,Kofi Boakye*

Main category: cs.AI

TL;DR: Pinterest GEO is a framework optimizing visual content for generative search by predicting user queries and creating semantically rich collections, boosting organic traffic.


<details>
  <summary>Details</summary>
Motivation: Generative search systems like ChatGPT prioritize semantic depth and authority, challenging visual platforms where individual images lack these signals, risking reduced site visits.

Method: Fine-tuned Vision-Language Models predict user queries, AI agents mine trends, and multimodal embeddings create indexable collections optimized for generative retrieval.

Result: Pinterest GEO achieved 20% organic traffic growth and multi-million monthly active user (MAU) growth.

Conclusion: The framework provides a viable solution for visual platforms to adapt and thrive in the generative search era.

Abstract: Large Language Models are fundamentally reshaping content discovery through AI-native search systems such as ChatGPT, Gemini, and Claude. Unlike traditional search engines that match keywords to documents, these systems infer user intent, synthesize multimodal evidence, and generate contextual answers directly on the search page, introducing a paradigm shift from Search Engine Optimization (SEO) to Generative Engine Optimization (GEO). For visual content platforms hosting billions of assets, this poses an acute challenge: individual images lack the semantic depth and authority signals that generative search prioritizes, risking disintermediation as user needs are satisfied in-place without site visits.
  We present Pinterest GEO, a production-scale framework that pioneers reverse search design: rather than generating generic image captions describing what content is, we fine-tune Vision-Language Models (VLMs) to predict what users would actually search for, augmented this with AI agents that mine real-time internet trends to capture emerging search demand. These VLM-generated queries then drive construction of semantically coherent Collection Pages via multimodal embeddings, creating indexable aggregations optimized for generative retrieval. Finally, we employ hybrid VLM and two-tower ANN architectures to build authority-aware interlinking structures that propagate signals across billions of visual assets. Deployed at scale across billions of images and tens of millions of collections, GEO delivers 20\% organic traffic growth contributing to multi-million monthly active user (MAU) growth, demonstrating a principled pathway for visual platforms to thrive in the generative search era.

</details>


### [202] [Structuring Value Representations via Geometric Coherence in Markov Decision Processes](https://arxiv.org/abs/2602.02978)
*Zuyuan Zhang,Zeyu Fang,Tian Lan*

Main category: cs.AI

TL;DR: The paper introduces GCR-RL, a method using order theory to refine value function estimates in reinforcement learning, improving sample efficiency and stability.


<details>
  <summary>Details</summary>
Motivation: To leverage geometric properties and order theory to enhance reinforcement learning by ensuring geometric coherence in value function estimates.

Method: Proposes GCR-RL, which refines partially ordered sets (posets) using temporal difference signals, and develops Q-learning and actor-critic algorithms for this purpose.

Result: The method shows significant improvements in sample efficiency and stable performance across various tasks compared to baselines.

Conclusion: GCR-RL effectively integrates geometric coherence into RL, validated by theoretical and empirical results.

Abstract: Geometric properties can be leveraged to stabilize and speed reinforcement learning. Existing examples include encoding symmetry structure, geometry-aware data augmentation, and enforcing structural restrictions. In this paper, we take a novel view of RL through the lens of order theory and recast value function estimates into learning a desired poset (partially ordered set). We propose \emph{GCR-RL} (Geometric Coherence Regularized Reinforcement Learning) that computes a sequence of super-poset refinements -- by refining posets in previous steps and learning additional order relationships from temporal difference signals -- thus ensuring geometric coherence across the sequence of posets underpinning the learned value functions. Two novel algorithms by Q-learning and by actor--critic are developed to efficiently realize these super-poset refinements. Their theoretical properties and convergence rates are analyzed. We empirically evaluate GCR-RL in a range of tasks and demonstrate significant improvements in sample efficiency and stable performance over strong baselines.

</details>


### [203] [Are LLMs Biased Like Humans? Causal Reasoning as a Function of Prior Knowledge, Irrelevant Information, and Reasoning Budget](https://arxiv.org/abs/2602.02983)
*Hanna M. Dettki,Charley M. Wu,Bob Rehder*

Main category: cs.AI

TL;DR: LLMs exhibit rule-like causal reasoning unlike humans, handle biases differently, and show robustness with CoT prompts, but may falter under uncertainty.


<details>
  <summary>Details</summary>
Motivation: Understand whether LLMs' causal judgments align with normative rules, human shortcuts, or pattern matching.

Method: Benchmark 20+ LLMs against humans on 11 causal tasks using collider structures; test robustness under semantic abstraction and prompt overloading.

Result: Most LLMs show rule-like reasoning, unlike humans, and CoT prompts improve robustness. They also lack characteristic human biases.

Conclusion: LLMs can complement humans in avoiding biases but may struggle with uncertainty, necessitating careful deployment analysis.

Abstract: Large language models (LLMs) are increasingly used in domains where causal reasoning matters, yet it remains unclear whether their judgments reflect normative causal computation, human-like shortcuts, or brittle pattern matching. We benchmark 20+ LLMs against a matched human baseline on 11 causal judgment tasks formalized by a collider structure ($C_1 \!\rightarrow\! E\! \leftarrow \!C_2$). We find that a small interpretable model compresses LLMs' causal judgments well and that most LLMs exhibit more rule-like reasoning strategies than humans who seem to account for unmentioned latent factors in their probability judgments. Furthermore, most LLMs do not mirror the characteristic human collider biases of weak explaining away and Markov violations. We probe LLMs' causal judgment robustness under (i) semantic abstraction and (ii) prompt overloading (injecting irrelevant text), and find that chain-of-thought (CoT) increases robustness for many LLMs. Together, this divergence suggests LLMs can complement humans when known biases are undesirable, but their rule-like reasoning may break down when uncertainty is intrinsic -- highlighting the need to characterize LLM reasoning strategies for safe, effective deployment.

</details>


### [204] [Large Language Models Can Take False First Steps at Inference-time Planning](https://arxiv.org/abs/2602.02991)
*Haijiang Yan,Jian-Qiao Zhu,Adam Sanborn*

Main category: cs.AI

TL;DR: The paper explains why LLMs exhibit short-sighted planning during inference despite their trained sequence-level planning abilities, attributing it to the influence of self-generated context.


<details>
  <summary>Details</summary>
Motivation: The gap between LLMs' trained planning capabilities and their seemingly inconsistent behavior during inference motivates a Bayesian explanation grounded in generative context.

Method: The study proposes a Bayesian model and validates it through two experiments: a random-generation task and a Gaussian-sampling task.

Result: Experiments show constrained planning under human prompts and increased planning strength with accumulated self-generated context, along with reduced initial bias when conditioning on self-generated sequences.

Conclusion: The findings offer theoretical and empirical insights into how LLMs' planning behavior evolves during inference due to self-generated context.

Abstract: Large language models (LLMs) have been shown to acquire sequence-level planning abilities during training, yet their planning behavior exhibited at inference time often appears short-sighted and inconsistent with these capabilities. We propose a Bayesian account for this gap by grounding planning behavior in the evolving generative context: given the subtle differences between natural language and the language internalized by LLMs, accumulated self-generated context drives a planning-shift during inference and thereby creates the appearance of compromised planning behavior. We further validate the proposed model through two controlled experiments: a random-generation task demonstrating constrained planning under human prompts and increasing planning strength as self-generated context accumulates, and a Gaussian-sampling task showing reduced initial bias when conditioning on self-generated sequences. These findings provide a theoretical explanation along with empirical evidence for characterizing how LLMs plan ahead during inference.

</details>


### [205] [Agent Alpha: Tree Search Unifying Generation, Exploration and Evaluation for Computer-Use Agents](https://arxiv.org/abs/2602.02995)
*Sizhe Tang,Rongqian Chen,Tian Lan*

Main category: cs.AI

TL;DR: Agent Alpha introduces a unified framework combining generation, exploration, and evaluation via step-level MCTS, enabling deliberate planning and efficient prefix reuse for GUI agents, achieving a 77% success rate on OSWorld.


<details>
  <summary>Details</summary>
Motivation: The lack of regressive ability in GUI agents prevents partial success reuse and recovery from early missteps, prompting the need for a more structured approach.

Method: Agent Alpha uses step-level Monte Carlo Tree Search (MCTS) with alpha-UCT guided search, comparison-driven evaluation, and diversity-constrained expansion.

Result: Agent Alpha achieves a state-of-the-art success rate of ~77% on the OSWorld benchmark, outperforming trajectory-level baselines under equivalent compute.

Conclusion: The framework's integration of MCTS, evaluation methods, and constrained expansion significantly enhances GUI agent performance.

Abstract: While scaling test-time compute through trajectory-level sampling has significantly improved Graphical User Interface (GUI) agents, the lack of regressive ability prevents the reuse of partial successes and the recovery from early missteps. In this paper, we introduce Agent Alpha, a unified framework that synergizes generation, exploration, and evaluation through step-level Monte Carlo Tree Search (MCTS). It enables active modeling or exploiting structures of the planning space. By integrating alpha-UCT guided search into the interaction loop, Agent Alpha enables deliberate planning, facilitating early pruning of suboptimal branches and efficient prefix reuse. We also employ comparison-driven evaluation to mitigate absolute scoring biases and diversity-constrained expansion to maintain a compact, informative search space. Regret bound of alpha-UCT is analyzed. On the OSWorld benchmark, Agent Alpha achieves a state-of-the-art success rate of $\sim 77\%$, significantly outperforming trajectory-level baselines under equivalent compute.

</details>


### [206] [Methods and Open Problems in Differentiable Social Choice: Learning Mechanisms, Decisions, and Alignment](https://arxiv.org/abs/2602.03003)
*Zhiyu An,Wan Du*

Main category: cs.AI

TL;DR: The paper explores the integration of social choice concepts into machine learning, introducing differentiable social choice as a framework to optimize collective decision-making processes from data.


<details>
  <summary>Details</summary>
Motivation: Social choice has become integral to machine learning systems, influencing areas like auctions, federated learning, and governance. The paper highlights the need for explicit normative scrutiny in these systems.

Method: Differentiable social choice is proposed as a paradigm to model voting rules and aggregation procedures as learnable, differentiable systems. It synthesizes work across auctions, voting, budgeting, and related fields.

Result: The paper demonstrates how classical axioms and impossibility results translate into optimization challenges within machine learning frameworks.

Conclusion: The review identifies 36 open problems, shaping a new research agenda bridging machine learning, economics, and democratic theory.

Abstract: Social choice is no longer a peripheral concern of political theory or economics-it has become a foundational component of modern machine learning systems. From auctions and resource allocation to federated learning, participatory governance, and the alignment of large language models, machine learning pipelines increasingly aggregate heterogeneous preferences, incentives, and judgments into collective decisions. In effect, many contemporary machine learning systems already implement social choice mechanisms, often implicitly and without explicit normative scrutiny.
  This Review surveys differentiable social choice: an emerging paradigm that formulates voting rules, mechanisms, and aggregation procedures as learnable, differentiable models optimized from data. We synthesize work across auctions, voting, budgeting, liquid democracy, decentralized aggregation, and inverse mechanism learning, showing how classical axioms and impossibility results reappear as objectives, constraints, and optimization trade-offs. We conclude by identifying 36 open problems defining a new research agenda at the intersection of machine learning, economics, and democratic theory.

</details>


### [207] [Distilling LLM Reasoning into Graph of Concept Predictors](https://arxiv.org/abs/2602.03006)
*Ziyang Yu,Liang Zhao*

Main category: cs.AI

TL;DR: GCP is a reasoning-aware active distillation framework that improves efficiency and interpretability in training compact student models by leveraging modular concept predictors and targeted sub-module retraining.


<details>
  <summary>Details</summary>
Motivation: The high costs of deploying LLMs for discriminative tasks necessitate efficient distillation methods. Current approaches often discard intermediate reasoning signals, limiting diagnostics and efficiency.

Method: GCP externalizes the teacher's reasoning as a directed acyclic graph and mirrors it with modular concept predictors. It uses graph-aware acquisition for efficient sampling and targeted sub-module retraining.

Result: GCP improves performance under limited annotation budgets and offers more interpretable and controllable training dynamics across eight NLP benchmarks.

Conclusion: GCP provides a scalable and interpretable solution for active distillation, enhancing efficiency and transparency in training compact discriminative models.

Abstract: Deploying Large Language Models (LLMs) for discriminative workloads is often limited by inference latency, compute, and API costs at scale. Active distillation reduces these costs by querying an LLM oracle to train compact discriminative students, but most pipelines distill only final labels, discarding intermediate reasoning signals and offering limited diagnostics of what reasoning is missing and where errors arise. We propose Graph of Concept Predictors (GCP), a reasoning-aware active distillation framework that externalizes the teacher's decision process as a directed acyclic graph and mirrors it with modular concept predictors in the student. GCP enhances sample efficiency through a graph-aware acquisition strategy that targets uncertainty and disagreement at critical reasoning nodes. Additionally, it improves training stability and efficiency by performing targeted sub-module retraining, which attributes downstream loss to specific concept predictors and updates only the most influential modules. Experiments on eight NLP classification benchmarks demonstrate that GCP enhances performance under limited annotation budgets while yielding more interpretable and controllable training dynamics. Code is available at: https://github.com/Ziyang-Yu/GCP.

</details>


### [208] [STAR: Similarity-guided Teacher-Assisted Refinement for Super-Tiny Function Calling Models](https://arxiv.org/abs/2602.03022)
*Jiliang Ni,Jiachen Pu,Zhongyi Yang,Jingfeng Luo,Conggang Hu*

Main category: cs.AI

TL;DR: STAR is a framework for transferring LLMs' capabilities to smaller models, addressing overfitting and training instability with innovations like Constrained Knowledge Distillation and Similarity-guided RL.


<details>
  <summary>Details</summary>
Motivation: The large scale of LLMs limits their adoption, requiring methods to transfer their capabilities to smaller models while overcoming challenges like overfitting and ineffective rewards.

Method: STAR introduces Constrained Knowledge Distillation (CKD) to ensure training stability and Similarity-guided RL (Sim-RL) for fine-grained rewards, synergizing these in a cohesive curriculum.

Result: STAR models achieve state-of-the-art performance in their size class, with a 0.6B model outperforming larger open models.

Conclusion: STAR successfully distills LLMs' capabilities into super-tiny models, enabling accessible and efficient AI agents.

Abstract: The proliferation of Large Language Models (LLMs) in function calling is pivotal for creating advanced AI agents, yet their large scale hinders widespread adoption, necessitating transferring their capabilities into smaller ones. However, existing paradigms are often plagued by overfitting, training instability, ineffective binary rewards for multi-solution tasks, and the difficulty of synergizing techniques. We introduce STAR: Similarity-guided Teacher-Assisted Refinement, a novel holistic framework that effectively transfers LLMs' capabilities to super-tiny models. STAR consists of two core technical innovations: (1) Constrained Knowledge Distillation (CKD), a training objective that augments top-k forward KL divergence to suppress confidently incorrect predictions, ensuring training stability while preserving exploration capacity for downstream RL. STAR holistically synergizes these strategies within a cohesive training curriculum, enabling super-tiny models to achieve exceptional performance on complex function calling tasks; (2) Similarity-guided RL (Sim-RL), a RL mechanism that introduces a fine-grained, similarity-based reward. This provides a robust, continuous, and rich signal for better policy optimization by evaluating the similarity between generated outputs and the ground truth. Extensive experiments on challenging and renowned benchmarks demonstrate the effectiveness of our method. Our STAR models establish SOTA in their size classes, significantly outperforming baselines. Remarkably, our 0.6B STAR model achieves the best performance among all open models under 1B, surpassing even several well-known open models at a larger scale. STAR demonstrates a training framework that distills capabilities of LLMs into super-tiny models, paving the way for powerful, accessible, and efficient AI agents.

</details>


### [209] [RC-GRPO: Reward-Conditioned Group Relative Policy Optimization for Multi-Turn Tool Calling Agents](https://arxiv.org/abs/2602.03025)
*Haitian Zhong,Jixiu Zhai,Lei Song,Jiang Bian,Qiang Liu,Tieniu Tan*

Main category: cs.AI

TL;DR: The paper addresses the challenge of sparse rewards and expensive exploration in multi-turn tool calling for LLMs by proposing RC-GRPO, a method that conditions rollouts on reward tokens to improve diversity and performance.


<details>
  <summary>Details</summary>
Motivation: Sparse rewards and low within-group reward variation in multi-turn tool calling for LLMs hinder learning, causing stalling in methods like SFT followed by GRPO.

Method: RC-GRPO introduces reward-conditioned trajectory policies (RCTP) fine-tuned with reward tokens to generate quality-specific trajectories. During RL, diverse reward tokens are sampled within GRPO groups to enhance diversity and advantage gains.

Result: RC-GRPO outperformed baselines on the BFCLv4 benchmark, with Qwen-2.5-7B-Instruct surpassing even closed-source API models.

Conclusion: Reward-conditioned policy optimization (RC-GRPO) effectively tackles exploration challenges in multi-turn tool calling, improving LLM performance.

Abstract: Multi-turn tool calling is challenging for Large Language Models (LLMs) because rewards are sparse and exploration is expensive. A common recipe, SFT followed by GRPO, can stall when within-group reward variation is low (e.g., more rollouts in a group receive the all 0 or all 1 reward), making the group-normalized advantage uninformative and yielding vanishing updates. To address this problem, we propose RC-GRPO (Reward-Conditioned Group Relative Policy Optimization), which treats exploration as a controllable steering problem via discrete reward tokens. We first fine-tune a Reward-Conditioned Trajectory Policy (RCTP) on mixed-quality trajectories with reward goal special tokens (e.g., <|high_reward|>, <|low_reward|>) injected into the prompts, enabling the model to learn how to generate distinct quality trajectories on demand. Then during RL, we sample diverse reward tokens within each GRPO group and condition rollouts on the sampled token to improve within-group diversity, improving advantage gains. On the Berkeley Function Calling Leaderboard v4 (BFCLv4) multi-turn benchmark, our method yields consistently improved performance than baselines, and the performance on Qwen-2.5-7B-Instruct even surpasses all closed-source API models.

</details>


### [210] [KANFIS A Neuro-Symbolic Framework for Interpretable and Uncertainty-Aware Learning](https://arxiv.org/abs/2602.03034)
*Binbin Yong,Haoran Pei,Jun Shen,Haoran Li,Qingguo Zhou,Zhao Su*

Main category: cs.AI

TL;DR: KANFIS is a compact neuro-fuzzy system that addresses the rule explosion issue in ANFIS by using additive function decomposition and linear scaling.


<details>
  <summary>Details</summary>
Motivation: To overcome the structural complexity and exponential rule growth in conventional ANFIS architectures.

Method: Proposes KANFIS with additive aggregation, sparse masking, and compatibility with T1 and IT2 fuzzy logic.

Result: Achieves competitive performance while maintaining interpretability and transparency.

Conclusion: KANFIS offers a scalable and interpretable alternative to traditional neuro-fuzzy systems.

Abstract: Adaptive Neuro-Fuzzy Inference System (ANFIS) was designed to combine the learning capabilities of neural network with the reasoning transparency of fuzzy logic. However, conventional ANFIS architectures suffer from structural complexity, where the product-based inference mechanism causes an exponential explosion of rules in high-dimensional spaces. We herein propose the Kolmogorov-Arnold Neuro-Fuzzy Inference System (KANFIS), a compact neuro-symbolic architecture that unifies fuzzy reasoning with additive function decomposition. KANFIS employs an additive aggregation mechanism, under which both model parameters and rule complexity scale linearly with input dimensionality rather than exponentially. Furthermore, KANFIS is compatible with both Type-1 (T1) and Interval Type-2 (IT2) fuzzy logic systems, enabling explicit modeling of uncertainty and ambiguity in fuzzy representations. By using sparse masking mechanisms, KANFIS generates compact and structured rule sets, resulting in an intrinsically interpretable model with clear rule semantics and transparent inference processes. Empirical results demonstrate that KANFIS achieves competitive performance against representative neural and neuro-fuzzy baselines.

</details>


### [211] [De-conflating Preference and Qualification: Constrained Dual-Perspective Reasoning for Job Recommendation with Large Language Models](https://arxiv.org/abs/2602.03097)
*Bryce Kan,Wei Yang,Emily Nguyen,Ganghui Yi,Bowen Yi,Chenxiao Yu,Yan Liu*

Main category: cs.AI

TL;DR: JobRec is a generative job recommendation framework that separates candidate preferences and employer qualifications through dual-perspective reasoning, improving controllability and outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Existing job recommendation models conflate candidate preferences and employer qualifications into a single signal, leading to confounded supervision and limited policy controllability.

Method: JobRec uses a Unified Semantic Alignment Schema for structured semantic layers, a Two-Stage Cooperative Training Strategy for decoupled inference, and a Lagrangian-based Policy Alignment module for optimization.

Result: Experiments show JobRec outperforms baselines and offers better controllability in professional matching.

Conclusion: JobRec effectively de-conflates preference and qualification, enhancing recommendation accuracy and controllability.

Abstract: Professional job recommendation involves a complex bipartite matching process that must reconcile a candidate's subjective preference with an employer's objective qualification. While Large Language Models (LLMs) are well-suited for modeling the rich semantics of resumes and job descriptions, existing paradigms often collapse these two decision dimensions into a single interaction signal, yielding confounded supervision under recruitment-funnel censoring and limiting policy controllability. To address these challenges, We propose JobRec, a generative job recommendation framework for de-conflating preference and qualification via constrained dual-perspective reasoning. JobRec introduces a Unified Semantic Alignment Schema that aligns candidate and job attributes into structured semantic layers, and a Two-Stage Cooperative Training Strategy that learns decoupled experts to separately infer preference and qualification. Building on these experts, a Lagrangian-based Policy Alignment module optimizes recommendations under explicit eligibility requirements, enabling controllable trade-offs. To mitigate data scarcity, we construct a synthetic dataset refined by experts. Experiments show that JobRec consistently outperforms strong baselines and provides improved controllability for strategy-aware professional matching.

</details>


### [212] [Risky-Bench: Probing Agentic Safety Risks under Real-World Deployment](https://arxiv.org/abs/2602.03100)
*Jingnan Zheng,Yanzhen Luo,Jingjun Xu,Bingnan Liu,Yuxin Chen,Chenhang Cui,Gelei Deng,Chaochao Lu,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.AI

TL;DR: Risky-Bench is a novel framework designed to systematically evaluate the safety of large language model (LLM) agents in real-world deployments, addressing limitations of existing risk-oriented evaluations by using domain-agnostic safety principles and adaptable context-aware rubrics.


<details>
  <summary>Details</summary>
Motivation: Existing agent safety evaluations lack comprehensive coverage of safety risks, especially during long-horizon, interactive tasks, and are too specialized for specific settings, limiting their adaptability.

Method: Risky-Bench organizes evaluations around domain-agnostic safety principles, derives context-aware safety rubrics, and systematically assesses safety risks through realistic task execution under varying threat assumptions.

Result: Applied to life-assist agent settings, Risky-Bench identified significant safety risks in state-of-the-art agents under realistic conditions, demonstrating its effectiveness.

Conclusion: Risky-Bench offers an extensible methodology for agent safety assessment, adaptable to diverse deployment settings beyond life-assist scenarios, providing a structured and comprehensive evaluation pipeline.

Abstract: Large Language Models (LLMs) are increasingly deployed as agents that operate in real-world environments, introducing safety risks beyond linguistic harm. Existing agent safety evaluations rely on risk-oriented tasks tailored to specific agent settings, resulting in limited coverage of safety risk space and failing to assess agent safety behavior during long-horizon, interactive task execution in complex real-world deployments. Moreover, their specialization to particular agent settings limits adaptability across diverse agent configurations. To address these limitations, we propose Risky-Bench, a framework that enables systematic agent safety evaluation grounded in real-world deployment. Risky-Bench organizes evaluation around domain-agnostic safety principles to derive context-aware safety rubrics that delineate safety space, and systematically evaluates safety risks across this space through realistic task execution under varying threat assumptions. When applied to life-assist agent settings, Risky-Bench uncovers substantial safety risks in state-of-the-art agents under realistic execution conditions. Moreover, as a well-structured evaluation pipeline, Risky-Bench is not confined to life-assist scenarios and can be adapted to other deployment settings to construct environment-specific safety evaluations, providing an extensible methodology for agent safety assessment.

</details>


### [213] [Understanding Multi-Agent LLM Frameworks: A Unified Benchmark and Experimental Analysis](https://arxiv.org/abs/2602.03128)
*Abdelghny Orogat,Ana Rostam,Essam Mansour*

Main category: cs.AI

TL;DR: The paper introduces MAFBench, a unified evaluation suite for multi-agent LLM frameworks, highlighting the significant impact of architectural choices on performance metrics like latency, accuracy, and coordination.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack standardized framework-level evaluation, making it difficult to understand the impact of architectural choices on system performance.

Method: The authors introduce an architectural taxonomy and develop MAFBench, a unified evaluation suite integrating existing benchmarks under standardized conditions.

Result: Framework-level design choices can drastically affect latency (100x increase), planning accuracy (30% reduction), and coordination success (from >90% to <30%).

Conclusion: The findings provide concrete design principles and framework selection guidance, along with future research directions.

Abstract: Multi-agent LLM frameworks are widely used to accelerate the development of agent systems powered by large language models (LLMs). These frameworks impose distinct architectural structures that govern how agents interact, store information, and coordinate tasks. However, their impact on system performance remains poorly understood. This gap is critical, as architectural choices alone can induce order-of-magnitude differences in latency and throughput, as well as substantial variation in accuracy and scalability. Addressing this challenge requires (i) jointly evaluating multiple capabilities, such as orchestration overhead, memory behavior, planning, specialization, and coordination, and (ii) conducting these evaluations under controlled, framework-level conditions to isolate architectural effects. Existing benchmarks focus on individual capabilities and lack standardized framework-level evaluation. We address these limitations by (i) introducing an architectural taxonomy for systematically comparing multi-agent LLM frameworks along fundamental dimensions, and (ii) developing MAFBench, a unified evaluation suite that integrates existing benchmarks under a standardized execution pipeline. Using MAFBench, we conduct a controlled empirical study across several widely used frameworks. Our results show that framework-level design choices alone can increase latency by over 100x, reduce planning accuracy by up to 30%, and lower coordination success from above 90% to below 30%. Finally, we translate our findings into concrete architectural design principles and framework selection guidance, and outline promising future research directions.

</details>


### [214] [General Agents Contain World Models, even under Partial Observability and Stochasticity](https://arxiv.org/abs/2602.03146)
*Santiago Cifuentes*

Main category: cs.AI

TL;DR: The paper extends prior work on deterministic agents in fully observable environments to stochastic agents in partially observable ones, showing that even stochastic agents learn their environment.


<details>
  <summary>Details</summary>
Motivation: To understand if stochastic agents in partially observable environments inherently learn their surroundings, broadening the scope of previous deterministic results.

Method: Extends a theorem from prior work to stochastic agents and partially observable environments, weakening the generality requirement.

Result: Stochastic agents still learn their environment, and even less powerful agents model their world.

Conclusion: The findings demonstrate that stochasticity doesn't prevent agents from learning their environment.

Abstract: Deciding whether an agent possesses a model of its surrounding world is a fundamental step toward understanding its capabilities and limitations. In [10], it was shown that, within a particular framework, every almost optimal and general agent necessarily contains sufficient knowledge of its environment to allow an approximate reconstruction of it by querying the agent as a black box. This result relied on the assumptions that the agent is deterministic and that the environment is fully observable.
  In this work, we remove both assumptions by extending the theorem to stochastic agents operating in partially observable environments. Fundamentally, this shows that stochastic agents cannot avoid learning their environment through the usage of randomization. We also strengthen the result by weakening the notion of generality, proving that less powerful agents already contain a model of the world in which they operate.

</details>


### [215] [Enhancing Foundation VLM Robustness to Missing Modality: Scalable Diffusion for Bi-directional Feature Restoration](https://arxiv.org/abs/2602.03151)
*Wei Dai,Haoyu Wang,Honghao Chang,Lijun He,Fan Li,Jian Sun,Haixia Bi*

Main category: cs.AI

TL;DR: The paper proposes a missing modality restoration strategy for Vision Language Models (VLMs) using an enhanced diffusion model with dynamic gating and cross-modal learning, achieving superior performance in zero-shot evaluations.


<details>
  <summary>Details</summary>
Motivation: VLMs struggle with incomplete modalities, prompting the need for a robust restoration method that preserves semantic accuracy and generalization.

Method: Introduces an enhanced diffusion model with Dynamic Modality Gating and Cross-Modal Mutual Learning to restore missing features adaptively.

Result: Outperforms baseline methods in zero-shot evaluations and handles diverse missing rates effectively.

Conclusion: The proposed strategy is a scalable and reliable extension for VLMs in missing modality scenarios.

Abstract: Vision Language Models (VLMs) typically assume complete modality input during inference. However, their effectiveness drops sharply when certain modalities are unavailable or incomplete. Current research primarily faces two dilemmas: Prompt-based methods struggle to restore missing yet indispensable features and impair generalization of VLMs. Imputation-based approaches, lacking effective guidance, are prone to generating semantically irrelevant noise. Restoring precise semantics while sustaining VLM generalization remains challenging. Therefore, we propose a general missing modality restoration strategy in this paper. We introduce an enhanced diffusion model as a pluggable mid-stage training module to effectively restore missing features. Our strategy introduces two key innovations: (I) Dynamic Modality Gating, which adaptively leverages conditional features to steer the generation of semantically consistent features; (II) Cross-Modal Mutual Learning mechanism, which bridges the semantic spaces of dual encoders to achieve bidirectional alignment. Zero-shot evaluations across benchmark datasets demonstrate that our approach outperforms existing baseline methods. Extensive experiments and ablation studies confirm our model as a robust and scalable extension for VLMs in missing modality scenarios, ensuring reliability across diverse missing rates and environments. Our code and models will be publicly available.

</details>


### [216] [VALUEFLOW: Toward Pluralistic and Steerable Value-based Alignment in Large Language Models](https://arxiv.org/abs/2602.03160)
*Woojin Kim,Sieun Hyeon,Jusang Oh,Jaeyoung Do*

Main category: cs.AI

TL;DR: VALUEFLOW is introduced as a unified framework addressing gaps in LLM alignment with human values, featuring hierarchical extraction, calibrated evaluation, and steering control.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve LLM alignment with diverse human values by overcoming limitations in preference-based and existing value-based methods.

Method: VALUEFLOW includes HIVES for hierarchical value embedding, VIDB for labeled texts with intensity estimates, and an anchor-based evaluator for model outputs.

Result: The study evaluates ten models across four value theories, revealing steerability asymmetries and composition laws for multi-value control.

Conclusion: VALUEFLOW advances LLM pluralistic alignment by providing scalable infrastructure for value intensity evaluation and control.

Abstract: Aligning Large Language Models (LLMs) with the diverse spectrum of human values remains a central challenge: preference-based methods often fail to capture deeper motivational principles. Value-based approaches offer a more principled path, yet three gaps persist: extraction often ignores hierarchical structure, evaluation detects presence but not calibrated intensity, and the steerability of LLMs at controlled intensities remains insufficiently understood. To address these limitations, we introduce VALUEFLOW, the first unified framework that spans extraction, evaluation, and steering with calibrated intensity control. The framework integrates three components: (i) HIVES, a hierarchical value embedding space that captures intra- and cross-theory value structure; (ii) the Value Intensity DataBase (VIDB), a large-scale resource of value-labeled texts with intensity estimates derived from ranking-based aggregation; and (iii) an anchor-based evaluator that produces consistent intensity scores for model outputs by ranking them against VIDB panels. Using VALUEFLOW, we conduct a comprehensive large-scale study across ten models and four value theories, identifying asymmetries in steerability and composition laws for multi-value control. This paper establishes a scalable infrastructure for evaluating and controlling value intensity, advancing pluralistic alignment of LLMs.

</details>


### [217] [Beyond Quantity: Trajectory Diversity Scaling for Code Agents](https://arxiv.org/abs/2602.03219)
*Guhong Chen,Chenghao Sun,Cheng Fu,Qiyao Wang,Zhihong Huang,Chaopeng Wei,Guangxu Chen,Feiteng Fang,Ahmadreza Argha,Bing Zhao,Xander Xu,Qi Han,Hamid Alinejad-Rokny,Qiang Qu,Binhua Li,Shiwen Ni,Min Yang,Hu Wei,Yongbin Li*

Main category: cs.AI

TL;DR: TDScalingisaframeworkforimprovingcodeLLMs'generalizationbyfocusingontrajectorydiversityratherthandataquantity,integratinginnovationslikeBusinessClusterandadaptiveevolution.


<details>
  <summary>Details</summary>
Motivation: CurrentcodeLLMsfacelimitationsduetolow-qualitysyntheticdataanddiminishingreturnsfromquantityscaling,underutilizingtrajectorydata.

Method: TDScalingintroducesadiversity-focusedframeworkwithfourinnovations:BusinessCluster,multi-agentparadigm,adaptiveevolution,andsandboxedcodetools.

Result: ExperimentsshowTDScalingenhancestool-usegeneralizationandcodingproficiencyacrossbenchmarks.

Conclusion: TDScalingoffersawin-winsolutionforimprovingcodeagents\'performanceandissetforpublicrelease.

Abstract: As code large language models (LLMs) evolve into tool-interactive agents via the Model Context Protocol (MCP), their generalization is increasingly limited by low-quality synthetic data and the diminishing returns of quantity scaling. Moreover, quantity-centric scaling exhibits an early bottleneck that underutilizes trajectory data. We propose TDScaling, a Trajectory Diversity Scaling-based data synthesis framework for code agents that scales performance through diversity rather than raw volume. Under a fixed training budget, increasing trajectory diversity yields larger gains than adding more trajectories, improving the performance-cost trade-off for agent training. TDScaling integrates four innovations: (1) a Business Cluster mechanism that captures real-service logical dependencies; (2) a blueprint-driven multi-agent paradigm that enforces trajectory coherence; (3) an adaptive evolution mechanism that steers synthesis toward long-tail scenarios using Domain Entropy, Reasoning Mode Entropy, and Cumulative Action Complexity to prevent mode collapse; and (4) a sandboxed code tool that mitigates catastrophic forgetting of intrinsic coding capabilities. Experiments on general tool-use benchmarks (BFCL, tau^2-Bench) and code agent tasks (RebenchT, CodeCI, BIRD) demonstrate a win-win outcome: TDScaling improves both tool-use generalization and inherent coding proficiency. We plan to release the full codebase and the synthesized dataset (including 30,000+ tool clusters) upon publication.

</details>


### [218] [TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking](https://arxiv.org/abs/2602.03224)
*Yu Cheng,Jiuan Zhou,Yongkang Hu,Yihang Chen,Huichi Zhou,Mingang Chen,Zhizhong Zhang,Kun Shao,Yuan Xie,Zhaoxia Yin*

Main category: cs.AI

TL;DR: The paper addresses Agent Memory Misevolution during test-time evolution, proposing TAME, a dual-memory framework that maintains trustworthiness and task performance.


<details>
  <summary>Details</summary>
Motivation: Agent safety alignment deteriorates during benign task evolution, termed Agent Memory Misevolution, necessitating a solution to preserve trustworthiness.

Method: TAME uses dual-memory evolution (executor and evaluator memory) with closed-loop refinement to balance task utility and safety.

Result: Experiments show TAME mitigates misevolution, improving both trustworthiness and task performance.

Conclusion: TAME effectively addresses Agent Memory Misevolution without compromising utility.

Abstract: Test-time evolution of agent memory serves as a pivotal paradigm for achieving AGI by bolstering complex reasoning through experience accumulation. However, even during benign task evolution, agent safety alignment remains vulnerable-a phenomenon known as Agent Memory Misevolution. To evaluate this phenomenon, we construct the Trust-Memevo benchmark to assess multi-dimensional trustworthiness during benign task evolution, revealing an overall decline in trustworthiness across various task domains and evaluation settings. To address this issue, we propose TAME, a dual-memory evolutionary framework that separately evolves executor memory to improve task performance by distilling generalizable methodologies, and evaluator memory to refine assessments of both safety and task utility based on historical feedback. Through a closed loop of memory filtering, draft generation, trustworthy refinement, execution, and dual-track memory updating, TAME preserves trustworthiness without sacrificing utility. Experiments demonstrate that TAME mitigates misevolution, achieving a joint improvement in both trustworthiness and task performance.

</details>


### [219] [The Necessity of a Unified Framework for LLM-Based Agent Evaluation](https://arxiv.org/abs/2602.03238)
*Pengyu Zhu,Li Sun,Philip S. Yu,Sen Su*

Main category: cs.AI

TL;DR: The paper highlights challenges in evaluating Large Language Model (LLM)-based agents due to confounding factors like system prompts and toolset configurations, proposing a unified framework for standardization.


<details>
  <summary>Details</summary>
Motivation: Current agent benchmarks suffer from inconsistencies and lack standardization, leading to unfairness and opacity in evaluations.

Method: The paper proposes a unified evaluation framework to address these issues, though specific methodologies are not detailed in the abstract.

Result: The abstract mentions the proposal but does not provide concrete results or findings.

Conclusion: A standardized evaluation framework is deemed essential for fair and rigorous advancement in agent evaluation.

Abstract: With the advent of Large Language Models (LLMs), general-purpose agents have seen fundamental advancements. However, evaluating these agents presents unique challenges that distinguish them from static QA benchmarks. We observe that current agent benchmarks are heavily confounded by extraneous factors, including system prompts, toolset configurations, and environmental dynamics. Existing evaluations often rely on fragmented, researcher-specific frameworks where the prompt engineering for reasoning and tool usage varies significantly, making it difficult to attribute performance gains to the model itself. Additionally, the lack of standardized environmental data leads to untraceable errors and non-reproducible results. This lack of standardization introduces substantial unfairness and opacity into the field. We propose that a unified evaluation framework is essential for the rigorous advancement of agent evaluation. To this end, we introduce a proposal aimed at standardizing agent evaluation.

</details>


### [220] [Accordion-Thinking: Self-Regulated Step Summaries for Efficient and Readable LLM Reasoning](https://arxiv.org/abs/2602.03249)
*Zhicheng Yang,Zhijiang Guo,Yinya Huang,Yongxin Wang,Wenlei Shi,Yiwei Wang,Xiaodan Liang,Jing Tang*

Main category: cs.AI

TL;DR: Accordion-Thinking is a framework enabling LLMs to dynamically summarize and compress reasoning steps, balancing efficiency and accuracy without compromising solution quality.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of scaling test-time compute due to linear KV cache growth and quadratic attention complexity in reasoning tasks.

Method: Introduces Accordion-Thinking, where LLMs learn dynamic summarization of reasoning steps via reinforcement learning, enabling a Fold inference mode to reduce dependency on historical tokens.

Result: Achieves 3x throughput while maintaining accuracy on a 48GB GPU memory configuration, with the Fold mode's accuracy gap narrowing and vanishing during training.

Conclusion: LLMs can effectively compress reasoning context through learned self-regulation, enhancing efficiency without sacrificing accuracy, and providing human-readable summaries.

Abstract: Scaling test-time compute via long Chain-ofThought unlocks remarkable gains in reasoning capabilities, yet it faces practical limits due to the linear growth of KV cache and quadratic attention complexity. In this paper, we introduce Accordion-Thinking, an end-to-end framework where LLMs learn to self-regulate the granularity of the reasoning steps through dynamic summarization. This mechanism enables a Fold inference mode, where the model periodically summarizes its thought process and discards former thoughts to reduce dependency on historical tokens. We apply reinforcement learning to incentivize this capability further, uncovering a critical insight: the accuracy gap between the highly efficient Fold mode and the exhaustive Unfold mode progressively narrows and eventually vanishes over the course of training. This phenomenon demonstrates that the model learns to encode essential reasoning information into compact summaries, achieving effective compression of the reasoning context. Our Accordion-Thinker demonstrates that with learned self-compression, LLMs can tackle complex reasoning tasks with minimal dependency token overhead without compromising solution quality, and it achieves a 3x throughput while maintaining accuracy on a 48GB GPU memory configuration, while the structured step summaries provide a human-readable account of the reasoning process.

</details>


### [221] [LPS-Bench: Benchmarking Safety Awareness of Computer-Use Agents in Long-Horizon Planning under Benign and Adversarial Scenarios](https://arxiv.org/abs/2602.03255)
*Tianyu Chen,Chujia Hu,Ge Gao,Dongrui Liu,Xia Hu,Wenjie Wang*

Main category: cs.AI

TL;DR: LPS-Bench is a new benchmark evaluating planning-time safety awareness of computer-use agents (CUAs) in long-horizon tasks, revealing gaps in existing CUAs' ability to handle risks.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks overlook planning-time risks in CUAs, focusing only on execution-time errors. LPS-Bench addresses this gap.

Method: Introduces LPS-Bench with 65 scenarios across 7 task domains, using a multi-agent pipeline for data generation and LLM-as-a-judge for evaluating safety awareness.

Result: Experiments show existing CUAs lack sufficient safety awareness, highlighting risks in long-horizon tasks.

Conclusion: LPS-Bench identifies safety deficiencies in CUAs and proposes mitigation strategies for improving long-horizon planning safety.

Abstract: Computer-use agents (CUAs) that interact with real computer systems can perform automated tasks but face critical safety risks. Ambiguous instructions may trigger harmful actions, and adversarial users can manipulate tool execution to achieve malicious goals. Existing benchmarks mostly focus on short-horizon or GUI-based tasks, evaluating on execution-time errors but overlooking the ability to anticipate planning-time risks. To fill this gap, we present LPS-Bench, a benchmark that evaluates the planning-time safety awareness of MCP-based CUAs under long-horizon tasks, covering both benign and adversarial interactions across 65 scenarios of 7 task domains and 9 risk types. We introduce a multi-agent automated pipeline for scalable data generation and adopt an LLM-as-a-judge evaluation protocol to assess safety awareness through the planning trajectory. Experiments reveal substantial deficiencies in existing CUAs' ability to maintain safe behavior. We further analyze the risks and propose mitigation strategies to improve long-horizon planning safety in MCP-based CUA systems. We open-source our code at https://github.com/tychenn/LPS-Bench.

</details>


### [222] [CSR-Bench: A Benchmark for Evaluating the Cross-modal Safety and Reliability of MLLMs](https://arxiv.org/abs/2602.03263)
*Yuxuan Liu,Yuntian Shi,Kun Wang,Haoting Shen,Kun Yang*

Main category: cs.AI

TL;DR: CSR-Bench evaluates MLLMs' cross-modal reliability across safety, over-rejection, bias, and hallucination, revealing alignment gaps and trade-offs between safety and performance.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitations of MLLMs in understanding joint intent across text and images, focusing on safety behavior driven by unimodal shortcuts rather than true multimodal understanding.

Method: The authors introduce CSR-Bench, a benchmark with 61 fine-grained types across four interaction patterns (Safety, Over-rejection, Bias, Hallucination), requiring integrated image-text interpretation and paired text-only controls.

Result: Evaluation of 16 MLLMs shows systematic cross-modal alignment gaps, weak safety awareness, language dominance under interference, and performance degradation from text-only to multimodal inputs.

Conclusion: The study highlights a trade-off between reducing over-rejection and maintaining safe, non-discriminatory behavior, suggesting that some safety improvements may rely on refusal heuristics rather than robust intent understanding.

Abstract: Multimodal large language models (MLLMs) enable interaction over both text and images, but their safety behavior can be driven by unimodal shortcuts instead of true joint intent understanding. We introduce CSR-Bench, a benchmark for evaluating cross-modal reliability through four stress-testing interaction patterns spanning Safety, Over-rejection, Bias, and Hallucination, covering 61 fine-grained types. Each instance is constructed to require integrated image-text interpretation, and we additionally provide paired text-only controls to diagnose modality-induced behavior shifts. We evaluate 16 state-of-the-art MLLMs and observe systematic cross-modal alignment gaps. Models show weak safety awareness, strong language dominance under interference, and consistent performance degradation from text-only controls to multimodal inputs. We also observe a clear trade-off between reducing over-rejection and maintaining safe, non-discriminatory behavior, suggesting that some apparent safety gains may come from refusal-oriented heuristics rather than robust intent understanding. WARNING: This paper contains unsafe contents.

</details>


### [223] [Agentic Proposing: Enhancing Large Language Model Reasoning via Compositional Skill Synthesis](https://arxiv.org/abs/2602.03279)
*Zhengbo Jiao,Shaobo Wang,Zifan Zhang,Xuan Ren,Wei Wang,Bing Zhao,Hu Wei,Linfeng Zhang*

Main category: cs.AI

TL;DR: Agentic Proposing framework generates high-quality synthetic datasets for training large language models, outperforming human-curated data with fewer samples.


<details>
  <summary>Details</summary>
Motivation: Overcome the limitations of human annotation costs and scalability in creating datasets for complex reasoning tasks.

Method: Introduces Agentic Proposing, a goal-driven sequential decision process using MGPO to generate verifiable training trajectories.

Result: Downstream solvers trained on synthetic data achieve state-of-the-art performance, e.g., 91.6% accuracy on AIME25 with only 11,000 trajectories.

Conclusion: High-quality synthetic datasets can replace massive human-curated ones, proving effective for advancing complex reasoning in models.

Abstract: Advancing complex reasoning in large language models relies on high-quality, verifiable datasets, yet human annotation remains cost-prohibitive and difficult to scale. Current synthesis paradigms often face a recurring trade-off: maintaining structural validity typically restricts problem complexity, while relaxing constraints to increase difficulty frequently leads to inconsistent or unsolvable instances. To address this, we propose Agentic Proposing, a framework that models problem synthesis as a goal-driven sequential decision process where a specialized agent dynamically selects and composes modular reasoning skills. Through an iterative workflow of internal reflection and tool-use, we develop the Agentic-Proposer-4B using Multi-Granularity Policy Optimization (MGPO) to generate high-precision, verifiable training trajectories across mathematics, coding, and science. Empirical results demonstrate that downstream solvers trained on agent-synthesized data significantly outperform leading baselines and exhibit robust cross-domain generalization. Notably, a 30B solver trained on only 11,000 synthesized trajectories achieves a state-of-the-art 91.6% accuracy on AIME25, rivaling frontier-scale proprietary models such as GPT-5 and proving that a small volume of high-quality synthetic signals can effectively substitute for massive human-curated datasets.

</details>


### [224] [MeetBench-XL: Calibrated Multi-Dimensional Evaluation and Learned Dual-Policy Agents for Real-Time Meetings](https://arxiv.org/abs/2602.03285)
*Yuelin Hu,Jun Xu,Bingcong Lu,Zhengxue Cheng,Hongwei Hu,Ronghua Wu,Li Song*

Main category: cs.AI

TL;DR: The paper introduces MeetAll, a grounded dataset from enterprise meetings, and MeetMaster XL, a dual-policy agent optimizing query routing and tool use, outperforming existing benchmarks and commercial systems.


<details>
  <summary>Details</summary>
Motivation: Existing meeting benchmarks lack real-world enterprise workflow representation, needing solutions for diverse tasks under strict constraints.

Method: Developed MeetAll dataset and MeetMaster XL agent, incorporating enterprise-informed protocols and dual-policy optimization.

Result: MeetMaster XL achieves superior quality-latency tradeoff, outperforming baselines and commercial systems.

Conclusion: The framework addresses enterprise meeting challenges with validated protocols and efficient agent design.

Abstract: Enterprise meeting environments require AI assistants that handle diverse operational tasks, from rapid fact checking during live discussions to cross meeting analysis for strategic planning, under strict latency, cost, and privacy constraints. Existing meeting benchmarks mainly focus on simplified question answering and fail to reflect real world enterprise workflows, where queries arise organically from multi stakeholder collaboration, span long temporal contexts, and require tool augmented reasoning.
  We address this gap through a grounded dataset and a learned agent framework. First, we introduce MeetAll, a bilingual and multimodal corpus derived from 231 enterprise meetings totaling 140 hours. Questions are injected using an enterprise informed protocol validated by domain expert review and human discriminability studies. Unlike purely synthetic benchmarks, this protocol is grounded in four enterprise critical dimensions: cognitive load, temporal context span, domain expertise, and actionable task execution, calibrated through interviews with stakeholders across finance, healthcare, and technology sectors.
  Second, we propose MeetBench XL, a multi dimensional evaluation protocol aligned with human judgment that measures factual fidelity, intent alignment, response efficiency, structural clarity, and completeness. Third, we present MeetMaster XL, a learned dual policy agent that jointly optimizes query routing between fast and slow reasoning paths and tool invocation, including retrieval, cross meeting aggregation, and web search. A lightweight classifier enables accurate routing with minimal overhead, achieving a superior quality latency tradeoff over single model baselines. Experiments against commercial systems show consistent gains, supported by ablations, robustness tests, and a real world deployment case study.Resources: https://github.com/huyuelin/MeetBench.

</details>


### [225] [Memora: A Harmonic Memory Representation Balancing Abstraction and Specificity](https://arxiv.org/abs/2602.03315)
*Menglin Xia,Xuchao Zhang,Shantanu Dixit,Paramaguru Harimurugan,Rujia Wang,Victor Ruhle,Robert Sim,Chetan Bansal,Saravan Rajmohan*

Main category: cs.AI

TL;DR: Memora introduces a harmonic memory representation balancing abstraction and specificity for efficient agent memory systems, outperforming benchmarks.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of scaling agent memory systems without losing specificity for effective reasoning.

Method: Memora uses primary abstractions for indexing and cue anchors for diverse retrieval, with a policy exploiting memory connections.

Result: Memora achieves state-of-the-art performance on LoCoMo and LongMemEval benchmarks.

Conclusion: Memora effectively balances abstraction and specificity, improving retrieval relevance and reasoning scalability.

Abstract: Agent memory systems must accommodate continuously growing information while supporting efficient, context-aware retrieval for downstream tasks. Abstraction is essential for scaling agent memory, yet it often comes at the cost of specificity, obscuring the fine-grained details required for effective reasoning. We introduce Memora, a harmonic memory representation that structurally balances abstraction and specificity. Memora organizes information via its primary abstractions that index concrete memory values and consolidate related updates into unified memory entries, while cue anchors expand retrieval access across diverse aspects of the memory and connect related memories. Building on this structure, we employ a retrieval policy that actively exploits these memory connections to retrieve relevant information beyond direct semantic similarity. Theoretically, we show that standard Retrieval-Augmented Generation (RAG) and Knowledge Graph (KG)-based memory systems emerge as special cases of our framework. Empirically, Memora establishes a new state-of-the-art on the LoCoMo and LongMemEval benchmarks, demonstrating better retrieval relevance and reasoning effectiveness as memory scales.

</details>


### [226] [MentalSeek-Dx: Towards Progressive Hypothetico-Deductive Reasoning for Real-world Psychiatric Diagnosis](https://arxiv.org/abs/2602.03340)
*Xiao Sun,Yuming Yang,Junnan Zhu,Jiang Zhong,Xinyu Zhou,Kaiwen Wei*

Main category: cs.AI

TL;DR: MentalDx Bench is introduced as the first benchmark for disorder-level psychiatric diagnosis using real-world clinical data. Evaluations reveal LLMs perform poorly at disorder-level diagnosis compared to coarse categorization. MentalSeek-Dx, a specialized LLM, achieves SOTA performance by incorporating clinical reasoning through supervised learning.


<details>
  <summary>Details</summary>
Motivation: Mental health disorders are a global challenge, and current LLMs lack ecological validity and fine-grained diagnostic supervision for clinical use.

Method: Introduces MentalDx Bench with 712 annotated health records covering 76 disorders. MentalSeek-Dx is trained using supervised trajectory construction and reinforcement learning.

Result: LLMs show poor disorder-level diagnosis despite strong coarse categorization. MentalSeek-Dx achieves SOTA performance with 14B parameters.

Conclusion: MentalSeek-Dx bridges the gap between LLMs and clinical psychiatric diagnosis, offering a reliable framework.

Abstract: Mental health disorders represent a burgeoning global public health challenge. While Large Language Models (LLMs) have demonstrated potential in psychiatric assessment, their clinical utility is severely constrained by benchmarks that lack ecological validity and fine-grained diagnostic supervision. To bridge this gap, we introduce \textbf{MentalDx Bench}, the first benchmark dedicated to disorder-level psychiatric diagnosis within real-world clinical settings. Comprising 712 de-identified electronic health records annotated by board-certified psychiatrists under ICD-11 guidelines, the benchmark covers 76 disorders across 16 diagnostic categories. Evaluation of 18 LLMs reveals a critical \textit{paradigm misalignment}: strong performance at coarse diagnostic categorization contrasts with systematic failure at disorder-level diagnosis, underscoring a gap between pattern-based modeling and clinical hypothetico-deductive reasoning. In response, we propose \textbf{MentalSeek-Dx}, a medical-specialized LLM trained to internalize this clinical reasoning process through supervised trajectory construction and curriculum-based reinforcement learning. Experiments on MentalDx Bench demonstrate that MentalSeek-Dx achieves state-of-the-art (SOTA) performance with only 14B parameters, establishing a clinically grounded framework for reliable psychiatric diagnosis.

</details>


### [227] [Building Interpretable Models for Moral Decision-Making](https://arxiv.org/abs/2602.03351)
*Mayank Goel,Aritra Das,Paras Chopra*

Main category: cs.AI

TL;DR: Custom transformer model analyzes neural networks' moral decisions in trolley-style dilemmas with 77% accuracy, using interpretability techniques to reveal localized biases.


<details>
  <summary>Details</summary>
Motivation: To understand how neural networks make moral decisions in structured scenarios like trolley problems.

Method: A 2-layer transformer processes scenario embeddings encoding affected individuals, numbers, and outcomes.

Result: Achieves 77% accuracy on Moral Machine data, with biases localized to distinct computational stages.

Conclusion: The model is effective for detailed analysis of moral reasoning in neural networks.

Abstract: We build a custom transformer model to study how neural networks make moral decisions on trolley-style dilemmas. The model processes structured scenarios using embeddings that encode who is affected, how many people, and which outcome they belong to. Our 2-layer architecture achieves 77% accuracy on Moral Machine data while remaining small enough for detailed analysis. We use different interpretability techniques to uncover how moral reasoning distributes across the network, demonstrating that biases localize to distinct computational stages among other findings.

</details>


### [228] [GFlowPO: Generative Flow Network as a Language Model Prompt Optimizer](https://arxiv.org/abs/2602.03358)
*Junmo Cho,Suhan Kim,Sangjune An,Minsu Kim,Dong Bok Lee,Heejun Lee,Sung Ju Hwang,Hae Beom Lee*

Main category: cs.AI

TL;DR: GFlowPO introduces a probabilistic framework for prompt optimization, using off-policy GFlowNet and Dynamic Memory Update to improve efficiency and performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: Prompt optimization for language models is challenging due to the large search space and sparse rewards. Existing RL-based methods suffer from poor sample efficiency.

Method: The framework involves fine-tuning a lightweight prompt-LM with GFlowNet and using Dynamic Memory Update to refine the meta-prompt by incorporating diverse and high-performing prompts.

Result: GFlowPO outperforms baseline methods in few-shot text classification, instruction induction, and question answering tasks.

Conclusion: GFlowPO offers a sample-efficient and effective approach to prompt optimization, enhancing LM performance across multiple tasks.

Abstract: Finding effective prompts for language models (LMs) is critical yet notoriously difficult: the prompt space is combinatorially large, rewards are sparse due to expensive target-LM evaluation. Yet, existing RL-based prompt optimizers often rely on on-policy updates and a meta-prompt sampled from a fixed distribution, leading to poor sample efficiency. We propose GFlowPO, a probabilistic prompt optimization framework that casts prompt search as a posterior inference problem over latent prompts regularized by a meta-prompted reference-LM prior. In the first step, we fine-tune a lightweight prompt-LM with an off-policy Generative Flow Network (GFlowNet) objective, using a replay-based training policy that reuses past prompt evaluations to enable sample-efficient exploration. In the second step, we introduce Dynamic Memory Update (DMU), a training-free mechanism that updates the meta-prompt by injecting both (i) diverse prompts from a replay buffer and (ii) top-performing prompts from a small priority queue, thereby progressively concentrating the search process on high-reward regions. Across few-shot text classification, instruction induction benchmarks, and question answering tasks, GFlowPO consistently outperforms recent discrete prompt optimization baselines.

</details>


### [229] [Risk Awareness Injection: Calibrating Vision-Language Models for Safety without Compromising Utility](https://arxiv.org/abs/2602.03402)
*Mengxuan Wang,Yuxin Chen,Gang Xu,Tao He,Hongjie Jiang,Ming Li*

Main category: cs.AI

TL;DR: RAI is a lightweight, training-free framework that enhances VLMs' safety by amplifying unsafe signals, preserving utility while reducing attack success.


<details>
  <summary>Details</summary>
Motivation: VLMs are vulnerable to multimodal jailbreak attacks, and existing defenses are costly or degrade utility. RAI leverages LLMs' inherent risk recognition.

Method: RAI constructs an Unsafe Prototype Subspace from language embeddings and modulates high-risk visual tokens to activate safety signals.

Result: RAI significantly reduces attack success rates without harming task performance, as shown in extensive benchmarks.

Conclusion: RAI effectively restores VLM safety by mimicking LLM risk detection, offering a practical, cost-free solution.

Abstract: Vision language models (VLMs) extend the reasoning capabilities of large language models (LLMs) to cross-modal settings, yet remain highly vulnerable to multimodal jailbreak attacks. Existing defenses predominantly rely on safety fine-tuning or aggressive token manipulations, incurring substantial training costs or significantly degrading utility. Recent research shows that LLMs inherently recognize unsafe content in text, and the incorporation of visual inputs in VLMs frequently dilutes risk-related signals. Motivated by this, we propose Risk Awareness Injection (RAI), a lightweight and training-free framework for safety calibration that restores LLM-like risk recognition by amplifying unsafe signals in VLMs. Specifically, RAI constructs an Unsafe Prototype Subspace from language embeddings and performs targeted modulation on selected high-risk visual tokens, explicitly activating safety-critical signals within the cross-modal feature space. This modulation restores the model's LLM-like ability to detect unsafe content from visual inputs, while preserving the semantic integrity of original tokens for cross-modal reasoning. Extensive experiments across multiple jailbreak and utility benchmarks demonstrate that RAI substantially reduces attack success rate without compromising task performance.

</details>


### [230] [Feasible strategies for conflict resolution within intuitionistic fuzzy preference-based conflict situations](https://arxiv.org/abs/2602.03403)
*Guangming Lang,Mingchuan Shang,Mengjun Hu,Jie Zhou,Feng Xu*

Main category: cs.AI

TL;DR: The paper introduces intuitionistic fuzzy preference-based conflict analysis to enhance granularity in capturing agents' attitudes, develops new conflict measures, and proposes feasible strategies with an algorithm for conflict resolution.


<details>
  <summary>Details</summary>
Motivation: Existing preference-based conflict models are limited by their reliance on three qualitative relations (preference, converse, indifference), which fail to fully capture the essence of conflict situations.

Method: The authors propose intuitionistic fuzzy preference-based conflict situations, develop conflict measures, construct three-way analysis models, and design adjustment mechanism-based strategies with an algorithm.

Result: The new model offers finer granularity in conflict analysis, enables trisecting agent/issue sets, and provides feasible strategies for conflict resolution, validated through an illustrative example.

Conclusion: The introduced intuitionistic fuzzy framework significantly improves conflict analysis by addressing limitations of classical models and offers practical tools for conflict resolution.

Abstract: In three-way conflict analysis, preference-based conflict situations characterize agents' attitudes towards issues by formally modeling their preferences over pairs of issues. However, existing preference-based conflict models rely exclusively on three qualitative relations, namely, preference, converse, and indifference, to describe agents' attitudes towards issue pairs, which significantly limits their capacity in capturing the essence of conflict. To overcome this limitation, we introduce the concept of an intuitionistic fuzzy preference-based conflict situation that captures agents' attitudes towards issue pairs with finer granularity than that afforded by classical preference-based models. Afterwards, we develop intuitionistic fuzzy preference-based conflict measures within this framework, and construct three-way conflict analysis models for trisecting the set of agent pairs, the agent set, and the issue set. Additionally, relative loss functions built on the proposed conflict functions are employed to calculate thresholds for three-way conflict analysis. Finally, we present adjustment mechanism-based feasible strategies that simultaneously account for both adjustment magnitudes and conflict degrees, together with an algorithm for constructing such feasible strategies, and provide an illustrative example to demonstrate the validity and effectiveness of the proposed model.

</details>


### [231] [DiscoverLLM: From Executing Intents to Discovering Them](https://arxiv.org/abs/2602.03429)
*Tae Soo Kim,Yoonjoo Lee,Jaesang Yu,John Joon Young Chung,Juho Kim*

Main category: cs.AI

TL;DR: DiscoverLLM is a framework that trains LLMs to help users form and discover their intents by adaptively exploring and refining options, achieving higher task performance and conversation efficiency.


<details>
  <summary>Details</summary>
Motivation: Users often lack clear intents initially and need exploration to discover them, which existing LLMs fail to address.

Method: Introduces a user simulator modeling cognitive state with hierarchical intents and trains LLMs to optimize concretization as a reward signal.

Result: Achieves 10% higher task performance, reduces conversation length by 40%, and improves user satisfaction in studies.

Conclusion: DiscoverLLM effectively bridges the gap between user ambiguity and intent discovery, enhancing LLM-user collaboration.

Abstract: To handle ambiguous and open-ended requests, Large Language Models (LLMs) are increasingly trained to interact with users to surface intents they have not yet expressed (e.g., ask clarification questions). However, users are often ambiguous because they have not yet formed their intents: they must observe and explore outcomes to discover what they want. Simply asking "what kind of tone do you want?" fails when users themselves do not know. We introduce DiscoverLLM, a novel and generalizable framework that trains LLMs to help users form and discover their intents. Central to our approach is a novel user simulator that models cognitive state with a hierarchy of intents that progressively concretize as the model surfaces relevant options -- where the degree of concretization serves as a reward signal that models can be trained to optimize. Resulting models learn to collaborate with users by adaptively diverging (i.e., explore options) when intents are unclear, and converging (i.e., refine and implement) when intents concretize. Across proposed interactive benchmarks in creative writing, technical writing, and SVG drawing, DiscoverLLM achieves over 10% higher task performance while reducing conversation length by up to 40%. In a user study with 75 human participants, DiscoverLLM improved conversation satisfaction and efficiency compared to baselines.

</details>


### [232] [Ontology-to-tools compilation for executable semantic constraint enforcement in LLM agents](https://arxiv.org/abs/2602.03439)
*Xiaochi Zhou,Patrick Bulter,Changxuan Yang,Simon D. Rihm,Thitikarn Angkanaporn,Jethro Akroyd,Sebastian Mosbach,Markus Kraft*

Main category: cs.AI

TL;DR: The paper introduces ontology-to-tools compilation to integrate LLMs with formal domain knowledge, ensuring semantic constraints are enforced during knowledge graph generation.


<details>
  <summary>Details</summary>
Motivation: To reduce manual engineering and improve accuracy by embedding formal knowledge directly into generative systems.

Method: Ontological specifications are compiled into executable tool interfaces within The World Avatar framework, using Model Context Protocol (MCP) agents for structured interaction.

Result: Demonstrated with metal-organic polyhedra synthesis literature, showing guided LLM behavior and reduced manual effort.

Conclusion: Provides a paradigm for embedding formal knowledge into generative systems, enhancing automation and constraint enforcement.

Abstract: We introduce ontology-to-tools compilation as a proof-of-principle mechanism for coupling large language models (LLMs) with formal domain knowledge. Within The World Avatar (TWA), ontological specifications are compiled into executable tool interfaces that LLM-based agents must use to create and modify knowledge graph instances, enforcing semantic constraints during generation rather than through post-hoc validation. Extending TWA's semantic agent composition framework, the Model Context Protocol (MCP) and associated agents are integral components of the knowledge graph ecosystem, enabling structured interaction between generative models, symbolic constraints, and external resources. An agent-based workflow translates ontologies into ontology-aware tools and iteratively applies them to extract, validate, and repair structured knowledge from unstructured scientific text. Using metal-organic polyhedra synthesis literature as an illustrative case, we show how executable ontological semantics can guide LLM behaviour and reduce manual schema and prompt engineering, establishing a general paradigm for embedding formal knowledge into generative systems.

</details>


### [233] [CRL-VLA: Continual Vision-Language-Action Learning](https://arxiv.org/abs/2602.03445)
*Qixin Zeng,Shuo Zhang,Hongyin Zhang,Renjie Wang,Han Zhao,Libang Zhao,Runze Li,Donglin Wang,Chao Huang*

Main category: cs.AI

TL;DR: CRL-VLA is a framework for continual reinforcement learning in Vision-Language-Action models, addressing the stability-plasticity trade-off via asymmetric regulation and a dual-critic architecture.


<details>
  <summary>Details</summary>
Motivation: Open-world environments require lifelong learning for embodied agents, but balancing stability (retaining old skills) and plasticity (learning new ones) is challenging.

Method: CRL-VLA uses asymmetric regulation with a dual-critic architecture and Goal-Conditioned Value Formulation (GCVF) to manage stability and plasticity.

Result: Experiments on LIBERO show CRL-VLA outperforms baselines in anti-forgetting and forward adaptation.

Conclusion: CRL-VLA effectively balances stability and plasticity, making it a promising solution for continual reinforcement learning.

Abstract: Lifelong learning is critical for embodied agents in open-world environments, where reinforcement learning fine-tuning has emerged as an important paradigm to enable Vision-Language-Action (VLA) models to master dexterous manipulation through environmental interaction. Thus, Continual Reinforcement Learning (CRL) is a promising pathway for deploying VLA models in lifelong robotic scenarios, yet balancing stability (retaining old skills) and plasticity (learning new ones) remains a formidable challenge for existing methods. We introduce CRL-VLA, a framework for continual post-training of VLA models with rigorous theoretical bounds. We derive a unified performance bound linking the stability-plasticity trade-off to goal-conditioned advantage magnitude, scaled by policy divergence. CRL-VLA resolves this dilemma via asymmetric regulation: constraining advantage magnitudes on prior tasks while enabling controlled growth on new tasks. This is realized through a simple but effective dual-critic architecture with novel Goal-Conditioned Value Formulation (GCVF), where a frozen critic anchors semantic consistency and a trainable estimator drives adaptation. Experiments on the LIBERO benchmark demonstrate that CRL-VLA effectively harmonizes these conflicting objectives, outperforming baselines in both anti-forgetting and forward adaptation.

</details>


### [234] [The Dual Role of Abstracting over the Irrelevant in Symbolic Explanations: Cognitive Effort vs. Understanding](https://arxiv.org/abs/2602.03467)
*Zeynep G. Saribatur,Johannes Langer,Ute Schmid*

Main category: cs.AI

TL;DR: The paper explores how formal abstractions (removal and clustering of details) improve human understanding and reduce cognitive effort in symbolic AI explanations.


<details>
  <summary>Details</summary>
Motivation: AI systems often produce opaque outputs, and while symbolic AI is transparent, raw logical traces can be cognitively demanding. The study aims to enhance human-centered explanations using abstraction.

Method: Answer Set Programming (ASP) is used to define irrelevant details for abstraction. Cognitive experiments tested participants' performance with simplified explanations across domains.

Result: Clustering details improved understanding, while removing details reduced cognitive effort, validating abstraction's role in better symbolic explanations.

Conclusion: Abstraction techniques like removal and clustering enhance the effectiveness of symbolic AI explanations for humans.

Abstract: Explanations are central to human cognition, yet AI systems often produce outputs that are difficult to understand. While symbolic AI offers a transparent foundation for interpretability, raw logical traces often impose a high extraneous cognitive load. We investigate how formal abstractions, specifically removal and clustering, impact human reasoning performance and cognitive effort. Utilizing Answer Set Programming (ASP) as a formal framework, we define a notion of irrelevant details to be abstracted over to obtain simplified explanations. Our cognitive experiments, in which participants classified stimuli across domains with explanations derived from an answer set program, show that clustering details significantly improve participants' understanding, while removal of details significantly reduce cognitive effort, supporting the hypothesis that abstraction enhances human-centered symbolic explanations.

</details>


### [235] [IntentRL: Training Proactive User-intent Agents for Open-ended Deep Research via Reinforcement Learning](https://arxiv.org/abs/2602.03468)
*Haohao Luo,Zexi Li,Yuexiang Xie,Wenhao Zhang,Yaliang Li,Ying Shen*

Main category: cs.AI

TL;DR: IntentRL trains agents to clarify user intents before long-horizon research, improving outcomes via scalable data pipelines and two-stage RL.


<details>
  <summary>Details</summary>
Motivation: Addressing the autonomy-interaction dilemma in Deep Research agents, where high autonomy leads to prolonged execution and unsatisfactory results.

Method: Proposes IntentRL, using a scalable pipeline for intent refinement and a two-stage RL strategy (offline learning and online adaptation).

Result: IntentRL improves intent hit rate and task performance, outperforming closed-source DR agents and proactive LLM baselines.

Conclusion: IntentRL effectively balances autonomy and interaction by clarifying user intents upfront, enhancing overall performance.

Abstract: Deep Research (DR) agents extend Large Language Models (LLMs) beyond parametric knowledge by autonomously retrieving and synthesizing evidence from large web corpora into long-form reports, enabling a long-horizon agentic paradigm. However, unlike real-time conversational assistants, DR is computationally expensive and time-consuming, creating an autonomy-interaction dilemma: high autonomy on ambiguous user queries often leads to prolonged execution with unsatisfactory outcomes. To address this, we propose IntentRL, a framework that trains proactive agents to clarify latent user intents before starting long-horizon research. To overcome the scarcity of open-ended research data, we introduce a scalable pipeline that expands a few seed samples into high-quality dialogue turns via a shallow-to-deep intent refinement graph. We further adopt a two-stage reinforcement learning (RL) strategy: Stage I applies RL on offline dialogues to efficiently learn general user-interaction behavior, while Stage II uses the trained agent and a user simulator for online rollouts to strengthen adaptation to diverse user feedback. Extensive experiments show that IntentRL significantly improves both intent hit rate and downstream task performance, outperforming the built-in clarify modules of closed-source DR agents and proactive LLM baselines.

</details>


### [236] [When Routing Collapses: On the Degenerate Convergence of LLM Routers](https://arxiv.org/abs/2602.03478)
*Guannan Lai,Han-Jia Ye*

Main category: cs.AI

TL;DR: The paper identifies 'routing collapse' in LLM routing systems, where routers overuse expensive models even when cheaper ones suffice, and proposes EquiRouter to directly learn model rankings, reducing costs by 17% at high performance.


<details>
  <summary>Details</summary>
Motivation: Existing LLM routers systematically default to expensive models despite cheaper ones being sufficient, wasting resources and undermining routing efficiency.

Method: Proposes EquiRouter, a decision-aware router that directly learns model rankings to address the objective-decision mismatch in current routers.

Result: EquiRouter reduces costs by about 17% at GPT-4-level performance compared to prior routers.

Conclusion: EquiRouter effectively mitigates routing collapse and improves cost-efficiency by restoring the role of smaller models.

Abstract: LLM routing aims to achieve a favorable quality--cost trade-off by dynamically assigning easy queries to smaller models and harder queries to stronger ones. However, across both unimodal and multimodal settings, we uncover a pervasive yet underexplored failure mode in existing routers: as the user's cost budget increases, routers systematically default to the most capable and most expensive model even when cheaper models already suffice. As a result, current routers under-utilize small models, wasting computation and monetary cost and undermining the core promise of routing; we term this phenomenon routing collapse. We attribute routing collapse to an objective--decision mismatch: many routers are trained to predict scalar performance scores, whereas routing decisions ultimately depend on discrete comparisons among candidate models. Consequently, small prediction errors can flip relative orderings and trigger suboptimal selections. To bridge this gap, we propose EquiRouter, a decision-aware router that directly learns model rankings, restoring the role of smaller models and mitigating routing collapse. On RouterBench, EquiRouter reduces cost by about 17\% at GPT-4-level performance compared to the strongest prior router. Our code is available at https://github.com/AIGNLAI/EquiRouter.

</details>


### [237] [Group Selection as a Safeguard Against AI Substitution](https://arxiv.org/abs/2602.03541)
*Qiankun Zhong,Thomas F. Eisenmann,Julian Garcia,Iyad Rahwan*

Main category: cs.AI

TL;DR: AI reliance reduces cultural variance, risking 'cultural collapse.' Comparing AI-complement vs. substitute users, the latter dominates individually but harms group diversity. Complement users help groups thrive under strong boundaries.


<details>
  <summary>Details</summary>
Motivation: To understand the long-term impact of AI reliance on human cultural evolution and identify conditions leading to cultural collapse.

Method: Agent-based modeling and evolutionary game theory were used to compare AI-complement and substitute users' strategies and their evolutionary spread.

Result: AI-substitute users dominate individually but reduce cultural variance. Complement users support group diversity and thrive under strong group boundaries.

Conclusion: AI adoption's population-level effects highlight risks to cultural diversity. Policy strategies should balance AI use to mitigate these risks.

Abstract: Reliance on generative AI can reduce cultural variance and diversity, especially in creative work. This reduction in variance has already led to problems in model performance, including model collapse and hallucination. In this paper, we examine the long-term consequences of AI use for human cultural evolution and the conditions under which widespread AI use may lead to "cultural collapse", a process in which reliance on AI-generated content reduces human variation and innovation and slows cumulative cultural evolution. Using an agent-based model and evolutionary game theory, we compare two types of AI use: complement and substitute. AI-complement users seek suggestions and guidance while remaining the main producers of the final output, whereas AI-substitute users provide minimal input, and rely on AI to produce most of the output. We then study how these use strategies compete and spread under evolutionary dynamics. We find that AI-substitute users prevail under individual-level selection despite the stronger reduction in cultural variance. By contrast, AI-complement users can benefit their groups by maintaining the variance needed for exploration, and can therefore be favored under cultural group selection when group boundaries are strong. Overall, our findings shed light on the long-term, population-level effects of AI adoption and inform policy and organizational strategies to mitigate these risks.

</details>


### [238] [Persona Generators: Generating Diverse Synthetic Personas at Scale](https://arxiv.org/abs/2602.03545)
*Davide Paglieri,Logan Cross,William A. Cunningham,Joel Z. Leibo,Alexander Sasha Vezhnevets*

Main category: cs.AI

TL;DR: The paper introduces Persona Generators, lightweight functions optimized to create diverse synthetic populations for AI evaluation, outperforming baselines in coverage and diversity.


<details>
  <summary>Details</summary>
Motivation: Evaluating AI systems requires diverse human data, which is costly or impractical to collect, especially for novel or hypothetical scenarios.

Method: The authors use an iterative improvement loop (AlphaEvolve) with large language models as mutation operators to refine Persona Generators, producing diverse synthetic personas from small descriptions.

Result: Evolved generators outperform baselines across six diversity metrics, effectively covering rare trait combinations.

Conclusion: Persona Generators enable scalable, high-coverage synthetic populations for AI evaluation, addressing gaps in existing approaches.

Abstract: Evaluating AI systems that interact with humans requires understanding their behavior across diverse user populations, but collecting representative human data is often expensive or infeasible, particularly for novel technologies or hypothetical future scenarios. Recent work in Generative Agent-Based Modeling has shown that large language models can simulate human-like synthetic personas with high fidelity, accurately reproducing the beliefs and behaviors of specific individuals. However, most approaches require detailed data about target populations and often prioritize density matching (replicating what is most probable) rather than support coverage (spanning what is possible), leaving long-tail behaviors underexplored. We introduce Persona Generators, functions that can produce diverse synthetic populations tailored to arbitrary contexts. We apply an iterative improvement loop based on AlphaEvolve, using large language models as mutation operators to refine our Persona Generator code over hundreds of iterations. The optimization process produces lightweight Persona Generators that can automatically expand small descriptions into populations of diverse synthetic personas that maximize coverage of opinions and preferences along relevant diversity axes. We demonstrate that evolved generators substantially outperform existing baselines across six diversity metrics on held-out contexts, producing populations that span rare trait combinations difficult to achieve in standard LLM outputs.

</details>


### [239] [EHRWorld: A Patient-Centric Medical World Model for Long-Horizon Clinical Trajectories](https://arxiv.org/abs/2602.03569)
*Linjie Mu,Zhongzhen Huang,Yannian Gu,Shengqian Qin,Shaoting Zhang,Xiaofan Zhang*

Main category: cs.AI

TL;DR: LLMs struggle with long-horizon clinical simulations due to error accumulation. EHRWorld, trained on causal sequential data, outperforms LLM baselines in stability and accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of dynamically simulating disease progression and treatment outcomes in medicine using LLMs, which currently fail to maintain consistent patient states.

Method: Introduces EHRWorld, a patient-centric medical world model trained on EHRWorld-110K, a large-scale dataset from real-world electronic health records, under a causal sequential paradigm.

Result: EHRWorld outperforms naive LLM-based methods, showing stable long-horizon simulation, better modeling of sensitive events, and efficient reasoning.

Conclusion: Training on temporally evolving, causally grounded clinical data is essential for reliable medical world modeling.

Abstract: World models offer a principled framework for simulating future states under interventions, but realizing such models in complex, high-stakes domains like medicine remains challenging. Recent large language models (LLMs) have achieved strong performance on static medical reasoning tasks, raising the question of whether they can function as dynamic medical world models capable of simulating disease progression and treatment outcomes over time. In this work, we show that LLMs only incorporating medical knowledge struggle to maintain consistent patient states under sequential interventions, leading to error accumulation in long-horizon clinical simulation. To address this limitation, we introduce EHRWorld, a patient-centric medical world model trained under a causal sequential paradigm, together with EHRWorld-110K, a large-scale longitudinal clinical dataset derived from real-world electronic health records. Extensive evaluations demonstrate that EHRWorld significantly outperforms naive LLM-based baselines, achieving more stable long-horizon simulation, improved modeling of clinically sensitive events, and favorable reasoning efficiency, highlighting the necessity of training on causally grounded, temporally evolving clinical data for reliable and robust medical world modeling.

</details>


### [240] [Can LLMs Do Rocket Science? Exploring the Limits of Complex Reasoning with GTOC 12](https://arxiv.org/abs/2602.03630)
*Iñaki del Campo,Pablo Cuervo,Victor Rodriguez-Fernandez,Roberto Armellin,Jack Yarndley*

Main category: cs.AI

TL;DR: LLMs show promise in autonomous multi-stage planning for complex tasks like asteroid mining but face significant implementation barriers despite improved strategic viability.


<details>
  <summary>Details</summary>
Motivation: To evaluate the limits of LLMs in autonomous multi-stage planning for high-dimensional, physically constrained environments, specifically focusing on astrodynamics challenges.

Method: Used the MLE-Bench framework adapted for orbital mechanics and deployed an AIDE-based agent architecture. Evaluated performance using an 'LLM-as-a-Judge' methodology with expert-developed rubrics.

Result: Strategic viability scores nearly doubled in two years (9.3 to 17.2 out of 26), but models struggle with execution due to unit inconsistencies, boundary errors, and inefficient debugging.

Conclusion: LLMs have the knowledge and intelligence for space science tasks but are hindered by implementation barriers, acting as facilitators rather than fully autonomous engineers.

Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in code generation and general reasoning, yet their capacity for autonomous multi-stage planning in high-dimensional, physically constrained environments remains an open research question. This study investigates the limits of current AI agents by evaluating them against the 12th Global Trajectory Optimization Competition (GTOC 12), a complex astrodynamics challenge requiring the design of a large-scale asteroid mining campaign. We adapt the MLE-Bench framework to the domain of orbital mechanics and deploy an AIDE-based agent architecture to autonomously generate and refine mission solutions. To assess performance beyond binary validity, we employ an "LLM-as-a-Judge" methodology, utilizing a rubric developed by domain experts to evaluate strategic viability across five structural categories. A comparative analysis of models, ranging from GPT-4-Turbo to reasoning-enhanced architectures like Gemini 2.5 Pro, and o3, reveals a significant trend: the average strategic viability score has nearly doubled in the last two years (rising from 9.3 to 17.2 out of 26). However, we identify a critical capability gap between strategy and execution. While advanced models demonstrate sophisticated conceptual understanding, correctly framing objective functions and mission architectures, they consistently fail at implementation due to physical unit inconsistencies, boundary condition errors, and inefficient debugging loops. We conclude that, while current LLMs often demonstrate sufficient knowledge and intelligence to tackle space science tasks, they remain limited by an implementation barrier, functioning as powerful domain facilitators rather than fully autonomous engineers.

</details>


### [241] [Search-R2: Enhancing Search-Integrated Reasoning via Actor-Refiner Collaboration](https://arxiv.org/abs/2602.03647)
*Bowei He,Minda Hu,Zenan Xu,Hongru Wang,Licheng Zong,Yankai Chen,Chen Ma,Xue Liu,Pluto Zhou,Irwin King*

Main category: cs.AI

TL;DR: Search-R2 is a novel Actor-Refiner framework that improves search-integrated reasoning by decomposing generation into initial trajectories and targeted corrections, achieving better accuracy with dense process rewards.


<details>
  <summary>Details</summary>
Motivation: Training language agents via reinforcement learning suffers from sparse rewards, leading to misleading search behaviors and difficulty distinguishing quality reasoning from luck.

Method: Proposes Search-R2, an Actor-Refiner collaboration where the Actor generates initial reasoning and the Meta-Refiner selectively fixes flaws via 'cut-and-regenerate.' Uses hybrid rewards for fine-grained supervision.

Result: Outperforms RAG and RL baselines across QA datasets, achieving higher reasoning accuracy with minimal overhead.

Conclusion: Search-R2 successfully addresses multi-scale credit assignment in search-integrated reasoning, proving selective correction enhances performance.

Abstract: Search-integrated reasoning enables language agents to transcend static parametric knowledge by actively querying external sources. However, training these agents via reinforcement learning is hindered by the multi-scale credit assignment problem: existing methods typically rely on sparse, trajectory-level rewards that fail to distinguish between high-quality reasoning and fortuitous guesses, leading to redundant or misleading search behaviors. To address this, we propose Search-R2, a novel Actor-Refiner collaboration framework that enhances reasoning through targeted intervention, with both components jointly optimized during training. Our approach decomposes the generation process into an Actor, which produces initial reasoning trajectories, and a Meta-Refiner, which selectively diagnoses and repairs flawed steps via a 'cut-and-regenerate' mechanism. To provide fine-grained supervision, we introduce a hybrid reward design that couples outcome correctness with a dense process reward quantifying the information density of retrieved evidence. Theoretically, we formalize the Actor-Refiner interaction as a smoothed mixture policy, proving that selective correction yields strict performance gains over strong baselines. Extensive experiments across various general and multi-hop QA datasets demonstrate that Search-R2 consistently outperforms strong RAG and RL-based baselines across model scales, achieving superior reasoning accuracy with minimal overhead.

</details>


### [242] [Mitigating Conversational Inertia in Multi-Turn Agents](https://arxiv.org/abs/2602.03664)
*Yang Wan,Zheng Cao,Zhenhao Zhang,Zhengwen Zeng,Shuheng Shen,Changhua Meng,Linchao Zhu*

Main category: cs.AI

TL;DR: The paper identifies conversational inertia in LLMs during multiturn interactions, leading to imitation bias. It proposes Context Preference Learning and context management strategies to mitigate this, achieving better performance.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the issue of LLMs mimicking their own responses in multiturn scenarios, which limits exploration despite the benefits of longer context.

Method: The authors analyze attention patterns to identify conversational inertia, then propose Context Preference Learning to calibrate model preferences and introduce context management strategies.

Result: Experiments across eight agentic environments and a deep research scenario show reduced conversational inertia and improved performance.

Conclusion: The framework successfully balances exploration and exploitation by addressing conversational inertia in LLMs.

Abstract: Large language models excel as few-shot learners when provided with appropriate demonstrations, yet this strength becomes problematic in multiturn agent scenarios, where LLMs erroneously mimic their own previous responses as few-shot examples. Through attention analysis, we identify conversational inertia, a phenomenon where models exhibit strong diagonal attention to previous responses, which is associated with imitation bias that constrains exploration. This reveals a tension when transforming few-shot LLMs into agents: longer context enriches environmental feedback for exploitation, yet also amplifies conversational inertia that undermines exploration. Our key insight is that for identical states, actions generated with longer contexts exhibit stronger inertia than those with shorter contexts, enabling construction of preference pairs without environment rewards. Based on this, we propose Context Preference Learning to calibrate model preferences to favor low-inertia responses over highinertia ones. We further provide context management strategies at inference time to balance exploration and exploitation. Experimental results across eight agentic environments and one deep research scenario validate that our framework reduces conversational inertia and achieves performance improvements.

</details>


### [243] [TodyComm: Task-Oriented Dynamic Communication for Multi-Round LLM-based Multi-Agent System](https://arxiv.org/abs/2602.03688)
*Wenzhe Fan,Tommaso Tognoli,Henry Peng Zou,Chunyu Miao,Yibo Wang,Xinhua Zhang*

Main category: cs.AI

TL;DR: TodyComm, a dynamic communication algorithm, adapts collaboration topologies per round to improve task utility in multi-agent systems, outperforming fixed-topology methods in effectiveness and efficiency.


<details>
  <summary>Details</summary>
Motivation: Fixed communication topologies in multi-agent systems are ineffective for dynamic scenarios where agent roles change across rounds due to adversaries, task progression, or constraints like bandwidth.

Method: TodyComm, a task-oriented dynamic communication algorithm, generates behavior-driven collaboration topologies that adapt to round-specific dynamics, optimized via policy gradient.

Result: TodyComm achieves superior task effectiveness, token efficiency, and scalability across five benchmarks under dynamic adversary and communication budgets.

Conclusion: Dynamic adaptation of communication topologies per round, as in TodyComm, enhances collaboration in multi-agent systems compared to fixed-topology approaches.

Abstract: Multi-round LLM-based multi-agent systems rely on effective communication structures to support collaboration across rounds. However, most existing methods employ a fixed communication topology during inference, which falls short in many realistic applications where the agents' roles may change \textit{across rounds} due to dynamic adversary, task progression, or time-varying constraints such as communication bandwidth. In this paper, we propose addressing this issue through TodyComm, a \textbf{t}ask-\textbf{o}riented \textbf{dy}namic \textbf{comm}unication algorithm. It produces behavior-driven collaboration topologies that adapt to the dynamics at each round, optimizing the utility for the task through policy gradient. Experiments on five benchmarks demonstrate that under both dynamic adversary and communications budgets, TodyComm delivers superior task effectiveness while retaining token efficiency and scalability.

</details>


### [244] [AOrchestra: Automating Sub-Agent Creation for Agentic Orchestration](https://arxiv.org/abs/2602.03786)
*Jianhao Ruan,Zhihao Xu,Yiran Peng,Fashen Ren,Zhaoyang Yu,Xinbing Liang,Jinyu Xiang,Bang Liu,Chenglin Wu,Yuyu Luo,Jiayi Zhang*

Main category: cs.AI

TL;DR: AOrchestra introduces a dynamic agent abstraction framework for adaptable multi-turn task solving, reducing human effort and achieving performance improvements.


<details>
  <summary>Details</summary>
Motivation: Existing agent designs lack dynamic abstraction for adaptability in complex tasks, prompting a unified, framework-agnostic approach.

Method: Proposes a tuple-based agent abstraction (Instruction, Context, Tools, Model) and AOrchestra, an orchestrator that dynamically spawns task-specific executors.

Result: AOrchestra achieves a 16.28% improvement over baselines on benchmarks like GAIA and SWE-Bench.

Conclusion: The framework enhances adaptability, reduces engineering effort, and enables performance-cost trade-offs.

Abstract: Language agents have shown strong promise for task automation. Realizing this promise for increasingly complex, long-horizon tasks has driven the rise of a sub-agent-as-tools paradigm for multi-turn task solving. However, existing designs still lack a dynamic abstraction view of sub-agents, thereby hurting adaptability. We address this challenge with a unified, framework-agnostic agent abstraction that models any agent as a tuple Instruction, Context, Tools, Model. This tuple acts as a compositional recipe for capabilities, enabling the system to spawn specialized executors for each task on demand. Building on this abstraction, we introduce an agentic system AOrchestra, where the central orchestrator concretizes the tuple at each step: it curates task-relevant context, selects tools and models, and delegates execution via on-the-fly automatic agent creation. Such designs enable reducing human engineering efforts, and remain framework-agnostic with plug-and-play support for diverse agents as task executors. It also enables a controllable performance-cost trade-off, allowing the system to approach Pareto-efficient. Across three challenging benchmarks (GAIA, SWE-Bench, Terminal-Bench), AOrchestra achieves 16.28% relative improvement against the strongest baseline when paired with Gemini-3-Flash. The code is available at: https://github.com/FoundationAgents/AOrchestra

</details>


### [245] [Understanding Agent Scaling in LLM-Based Multi-Agent Systems via Diversity](https://arxiv.org/abs/2602.03794)
*Yingxuan Yang,Chengrui Qu,Muning Wen,Laixi Shi,Ying Wen,Weinan Zhang,Adam Wierman,Shangding Gu*

Main category: cs.AI

TL;DR: Scaling LLM-based multi-agent systems (MAS) with homogeneous agents shows diminishing returns, while diversity (e.g., models, prompts) yields gains. A framework reveals performance is bounded by task uncertainty, not agent count, with heterogeneous agents providing complementary evidence. Empirical results show 2 diverse agents can outperform 16 homogeneous ones.


<details>
  <summary>Details</summary>
Motivation: To understand why scaling homogeneous MAS leads to diminishing returns and how diversity improves performance, providing insights for building efficient MAS.

Method: The paper introduces an information-theoretic framework to analyze MAS performance bounds, defining $K^*$ to quantify effective channels. Experiments compare homogeneous and heterogeneous agent configurations.

Result: Heterogeneous MAS configurations outperform homogeneous scaling, with 2 diverse agents matching or exceeding 16 homogeneous agents. Performance depends on effective channel count, not agent number.

Conclusion: Diversity-aware design significantly enhances MAS performance by leveraging complementary evidence from heterogeneous agents, offering guidelines for efficient and robust system construction.

Abstract: LLM-based multi-agent systems (MAS) have emerged as a promising approach to tackle complex tasks that are difficult for individual LLMs. A natural strategy is to scale performance by increasing the number of agents; however, we find that such scaling exhibits strong diminishing returns in homogeneous settings, while introducing heterogeneity (e.g., different models, prompts, or tools) continues to yield substantial gains. This raises a fundamental question: what limits scaling, and why does diversity help? We present an information-theoretic framework showing that MAS performance is bounded by the intrinsic task uncertainty, not by agent count. We derive architecture-agnostic bounds demonstrating that improvements depend on how many effective channels the system accesses. Homogeneous agents saturate early because their outputs are strongly correlated, whereas heterogeneous agents contribute complementary evidence. We further introduce $K^*$, an effective channel count that quantifies the number of effective channels without ground-truth labels. Empirically, we show that heterogeneous configurations consistently outperform homogeneous scaling: 2 diverse agents can match or exceed the performance of 16 homogeneous agents. Our results provide principled guidelines for building efficient and robust MAS through diversity-aware design. Code and Dataset are available at the link: https://github.com/SafeRL-Lab/Agent-Scaling.

</details>


### [246] [Conformal Thinking: Risk Control for Reasoning on a Compute Budget](https://arxiv.org/abs/2602.03814)
*Xi Wang,Anushri Suresh,Alvin Zhang,Rishi More,William Jurayj,Benjamin Van Durme,Mehrdad Farajtabar,Daniel Khashabi,Eric Nalisnick*

Main category: cs.AI

TL;DR: The paper addresses adaptive reasoning in LLMs by framing budget setting as risk control, introducing thresholds to balance accuracy and compute efficiency.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in optimizing token budget and adaptive reasoning thresholds to balance risk and accuracy while minimizing computational costs.

Method: The proposed framework uses distribution-free risk control to set upper and lower thresholds for stopping reasoning, with an efficiency loss mechanism for multi-criteria scenarios.

Result: Empirical results show computational efficiency gains from the lower threshold and ensemble stopping mechanisms while meeting user-specified risk targets.

Conclusion: The risk control approach effectively balances accuracy and compute efficiency in adaptive reasoning for LLMs.

Abstract: Reasoning Large Language Models (LLMs) enable test-time scaling, with dataset-level accuracy improving as the token budget increases, motivating adaptive reasoning -- spending tokens when they improve reliability and stopping early when additional computation is unlikely to help. However, setting the token budget, as well as the threshold for adaptive reasoning, is a practical challenge that entails a fundamental risk-accuracy trade-off. We re-frame the budget setting problem as risk control, limiting the error rate while minimizing compute. Our framework introduces an upper threshold that stops reasoning when the model is confident (risking incorrect output) and a novel parametric lower threshold that preemptively stops unsolvable instances (risking premature stoppage). Given a target risk and a validation set, we use distribution-free risk control to optimally specify these stopping mechanisms. For scenarios with multiple budget controlling criteria, we incorporate an efficiency loss to select the most computationally efficient exiting mechanism. Empirical results across diverse reasoning tasks and models demonstrate the effectiveness of our risk control approach, demonstrating computational efficiency gains from the lower threshold and ensemble stopping mechanisms while adhering to the user-specified risk target.

</details>


### [247] [AutoFigure: Generating and Refining Publication-Ready Scientific Illustrations](https://arxiv.org/abs/2602.03828)
*Minjun Zhu,Zhen Lin,Yixuan Weng,Panzhong Lu,Qiujie Xie,Yifan Wei,Sifan Liu,Qiyao Sun,Yue Zhang*

Main category: cs.AI

TL;DR: FigureBench introduces a large-scale benchmark for generating scientific illustrations from text, alongside AutoFigure, an agentic framework that produces high-quality illustrations, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Manual creation of scientific illustrations is a bottleneck; automated solutions are needed to improve efficiency and quality.

Method: FigureBench provides 3,300 text-figure pairs. AutoFigure uses extensive thinking, recombination, and validation to generate illustrations.

Result: AutoFigure outperforms baseline methods, producing publication-ready illustrations.

Conclusion: AutoFigure and FigureBench offer effective tools for automated scientific illustration generation.

Abstract: High-quality scientific illustrations are crucial for effectively communicating complex scientific and technical concepts, yet their manual creation remains a well-recognized bottleneck in both academia and industry. We present FigureBench, the first large-scale benchmark for generating scientific illustrations from long-form scientific texts. It contains 3,300 high-quality scientific text-figure pairs, covering diverse text-to-illustration tasks from scientific papers, surveys, blogs, and textbooks. Moreover, we propose AutoFigure, the first agentic framework that automatically generates high-quality scientific illustrations based on long-form scientific text. Specifically, before rendering the final result, AutoFigure engages in extensive thinking, recombination, and validation to produce a layout that is both structurally sound and aesthetically refined, outputting a scientific illustration that achieves both structural completeness and aesthetic appeal. Leveraging the high-quality data from FigureBench, we conduct extensive experiments to test the performance of AutoFigure against various baseline methods. The results demonstrate that AutoFigure consistently surpasses all baseline methods, producing publication-ready scientific illustrations. The code, dataset and huggingface space are released in https://github.com/ResearAI/AutoFigure.

</details>


### [248] [Thinking Like a Doctor: Conversational Diagnosis through the Exploration of Diagnostic Knowledge Graphs](https://arxiv.org/abs/2602.01995)
*Jeongmoon Won,Seungwon Kook,Yohan Jo*

Main category: cs.AI

TL;DR: A conversational diagnosis system uses a knowledge graph to generate and verify hypotheses through clarifying questions, improving accuracy and efficiency, with realistic patient simulation.


<details>
  <summary>Details</summary>
Motivation: Existing diagnostic systems often rely on unrealistic assumptions, such as rich patient input or parametric models, limiting their practicality.

Method: The system explores a diagnostic knowledge graph in two steps: generating hypotheses from dialogue and verifying them through iterative clarifying questions.

Result: Experiments show improved diagnostic accuracy and efficiency, with physician evaluations confirming the realism and clinical utility.

Conclusion: The proposed system addresses limitations of existing approaches and demonstrates promising results in realistic clinical settings.

Abstract: Conversational diagnosis requires multi-turn history-taking, where an agent asks clarifying questions to refine differential diagnoses under incomplete information. Existing approaches often rely on the parametric knowledge of a model or assume that patients provide rich and concrete information, which is unrealistic. To address these limitations, we propose a conversational diagnosis system that explores a diagnostic knowledge graph to reason in two steps: (i) generating diagnostic hypotheses from the dialogue context, and (ii) verifying hypotheses through clarifying questions, which are repeated until a final diagnosis is reached. Since evaluating the system requires a realistic patient simulator that responds to the system's questions, we adopt a well-established simulator along with patient profiles from MIMIC-IV. We further adapt it to describe symptoms vaguely to reflect real-world patients during early clinical encounters. Experiments show improved diagnostic accuracy and efficiency over strong baselines, and evaluations by physicians support the realism of our simulator and the clinical utility of the generated questions. Our code will be released upon publication.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [249] [Exploring Silicon-Based Societies: An Early Study of the Moltbook Agent Community](https://arxiv.org/abs/2602.02613)
*Yu-Zheng Lin,Bono Po-Jen Shih,Hsuan-Ying Alessandra Chien,Shalaka Satam,Jesus Horacio Pacheco,Sicong Shao,Soheil Salehi,Pratik Satam*

Main category: cs.MA

TL;DR: The paper introduces data-driven silicon sociology to study social structures in large-scale autonomous agent ecosystems, analyzing Moltbook's 150,000 agents and uncovering reproducible patterns of organization through data mining.


<details>
  <summary>Details</summary>
Motivation: Understanding collective behavior in large-scale autonomous agent ecosystems is challenging with anecdotal or small-scale methods, necessitating systematic empirical frameworks.

Method: The study involves large-scale data mining of Moltbook, analyzing 12,758 agent-authored sub-community descriptions using preprocessing, contextual embedding, and unsupervised clustering.

Result: Autonomous agents organize collective space reproducibly, displaying human-mimetic interests, silicon-centric self-reflection, and early-stage economic behaviors, all emerging from machine-generated data.

Conclusion: This work lays a methodological foundation for silicon sociology, showing data mining's power in understanding autonomous agent societies' organization and evolution.

Abstract: The rapid emergence of autonomous large language model agents has given rise to persistent, large-scale agent ecosystems whose collective behavior cannot be adequately understood through anecdotal observation or small-scale simulation. This paper introduces data-driven silicon sociology as a systematic empirical framework for studying social structure formation among interacting artificial agents. We present a pioneering large-scale data mining investigation of an in-the-wild agent society by analyzing Moltbook, a social platform designed primarily for agent-to-agent interaction. At the time of study, Moltbook hosted over 150,000 registered autonomous agents operating across thousands of agent-created sub-communities. Using programmatic and non-intrusive data acquisition, we collected and analyzed the textual descriptions of 12,758 submolts, which represent proactive sub-community partitioning activities within the ecosystem. Treating agent-authored descriptions as first-class observational artifacts, we apply rigorous preprocessing, contextual embedding, and unsupervised clustering techniques to uncover latent patterns of thematic organization and social space structuring. The results show that autonomous agents systematically organize collective space through reproducible patterns spanning human-mimetic interests, silicon-centric self-reflection, and early-stage economic and coordination behaviors. Rather than relying on predefined sociological taxonomies, these structures emerge directly from machine-generated data traces. This work establishes a methodological foundation for data-driven silicon sociology and demonstrates that data mining techniques can provide a powerful lens for understanding the organization and evolution of large autonomous agent societies.

</details>


### [250] [Scaling Small Agents Through Strategy Auctions](https://arxiv.org/abs/2602.02751)
*Lisa Alazraki,William F. Shen,Yoram Bachrach,Akhil Mathur*

Main category: cs.MA

TL;DR: Small language models are cost-effective but struggle with complex tasks. The SALE framework improves efficiency by routing tasks dynamically, reducing costs and outperforming larger models.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the scalability issue of small language models in complex tasks and proposes a framework to enhance their efficiency without relying on larger models.

Method: Introduces Strategy Auctions for Workload Efficiency (SALE), where agents bid strategic plans scored by a cost-value mechanism, enabling dynamic routing and self-improvement.

Result: SALE reduces reliance on large agents by 53%, lowers costs by 35%, and outperforms the largest agent’s pass@1 with minimal overhead.

Conclusion: Small agents can be scaled effectively through coordination mechanisms, suggesting a shift from larger models to adaptive ecosystems for performance gains.

Abstract: Small language models are increasingly viewed as a promising, cost-effective approach to agentic AI, with proponents claiming they are sufficiently capable for agentic workflows. However, while smaller agents can closely match larger ones on simple tasks, it remains unclear how their performance scales with task complexity, when large models become necessary, and how to better leverage small agents for long-horizon workloads. In this work, we empirically show that small agents' performance fails to scale with task complexity on deep search and coding tasks, and we introduce Strategy Auctions for Workload Efficiency (SALE), an agent framework inspired by freelancer marketplaces. In SALE, agents bid with short strategic plans, which are scored by a systematic cost-value mechanism and refined via a shared auction memory, enabling per-task routing and continual self-improvement without training a separate router or running all models to completion. Across deep search and coding tasks of varying complexity, SALE reduces reliance on the largest agent by 53%, lowers overall cost by 35%, and consistently improves upon the largest agent's pass@1 with only a negligible overhead beyond executing the final trace. In contrast, established routers that rely on task descriptions either underperform the largest agent or fail to reduce cost -- often both -- underscoring their poor fit for agentic workflows. These results suggest that while small agents may be insufficient for complex workloads, they can be effectively "scaled up" through coordinated task allocation and test-time self-improvement. More broadly, they motivate a systems-level view of agentic AI in which performance gains come less from ever-larger individual models and more from market-inspired coordination mechanisms that organize heterogeneous agents into efficient, adaptive ecosystems.

</details>


### [251] [Game-Theoretic and Algorithmic Analyses of Multi-Agent Routing under Crossing Costs](https://arxiv.org/abs/2602.03455)
*Tesshu Hanaka,Nikolaos Melissinos,Hirotaka Ono*

Main category: cs.MA

TL;DR: The paper introduces a novel framework for decentralized multi-agent routing, focusing on 'crossing cost' to quantify congestion risks, and provides game-theoretic and algorithmic solutions.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of centralized control in multi-agent path finding by developing a decentralized, risk-aware model that quantifies congestion and delays.

Method: Introduces the Multi-Agent Routing under Crossing Cost model on mixed graphs, treating conflicts as costs rather than hard constraints. Uses game theory to analyze equilibria and parameterized algorithms to minimize crossing costs.

Result: Proves the existence of pure Nash equilibria, shows polynomial-time equilibria under mild conditions, and identifies NP-hardness for total cost minimization. Provides XP/FPT algorithms for structured instances.

Conclusion: The framework bridges game theory and parameterized complexity, offering scalable and risk-aware solutions for decentralized multi-agent routing.

Abstract: Coordinating the movement of multiple autonomous agents over a shared network is a fundamental challenge in algorithmic robotics, intelligent transportation, and distributed systems. The dominant approach, Multi-Agent Path Finding, relies on centralized control and synchronous collision avoidance, which often requires strict synchronization and guarantees of globally conflict-free execution. This paper introduces the Multi-Agent Routing under Crossing Cost model on mixed graphs, a novel framework tailored to asynchronous settings. In our model, instead of treating conflicts as hard constraints, each agent is assigned a path, and the system is evaluated through a cost function that measures potential head-on encounters. This ``crossing cost'', which is defined as the product of the numbers of agents traversing an edge in opposite directions, quantifies the risk of congestion and delay in decentralized execution.
  Our contributions are both game-theoretic and algorithmic. We model the setting as a congestion game with a non-standard cost function, prove the existence of pure Nash equilibria, and analyze the dynamics leading to them. Equilibria can be found in polynomial time under mild conditions, while the general case is PLS-complete. From an optimization perspective, minimizing the total crossing cost is NP-hard, as the problem generalizes Steiner Orientation. To address this hardness barrier, we design a suite of parameterized algorithms for minimizing crossing cost, with parameters including the number of arcs, edges, agents, and structural graph measures. These yield XP or FPT results depending on the parameter, offering algorithmic strategies for structurally restricted instances. Our framework provides a new theoretical foundation for decentralized multi-agent routing, bridging equilibrium analysis and parameterized complexity to support scalable and risk-aware coordination.

</details>


### [252] [When Should Agents Coordinate in Differentiable Sequential Decision Problems?](https://arxiv.org/abs/2602.03674)
*Caleb Probine,Su Ann Low,David Fridovich-Keil,Ufuk Topcu*

Main category: cs.MA

TL;DR: The paper explores the value of coordination in multi-robot teams, modeling it as a spectrum from joint optimization to Nash equilibria, and links coordination to the second-order properties of agents' objectives.


<details>
  <summary>Details</summary>
Motivation: Multi-robot teams often suffer from poor outcomes due to uncoordinated actions. Coordination is costly, so understanding its value and timing is crucial.

Method: The study models coordination as a spectrum, analyzes second-order properties of agents' objectives, and provides algorithms for determining optimal coordination times.

Result: Demonstrates that coordination in differentiable motion-planning problems can be analyzed through second-order reasoning, enabling algorithms to decide when coordination is beneficial.

Conclusion: Effectively leveraging second-order properties of objectives allows multi-robot teams to strategically coordinate, balancing individual and collective benefits.

Abstract: Multi-robot teams must coordinate to operate effectively. When a team operates in an uncoordinated manner, and agents choose actions that are only individually optimal, the team's outcome can suffer. However, in many domains, coordination requires costly communication. We explore the value of coordination in a broad class of differentiable motion-planning problems. In particular, we model coordinated behavior as a spectrum: at one extreme, agents jointly optimize a common team objective, and at the other, agents make unilaterally optimal decisions given their individual decision variables, i.e., they operate at Nash equilibria. We then demonstrate that reasoning about coordination in differentiable motion-planning problems reduces to reasoning about the second-order properties of agents' objectives, and we provide algorithms that use this second-order reasoning to determine at which times a team of agents should coordinate.

</details>


### [253] [Agent Primitives: Reusable Latent Building Blocks for Multi-Agent Systems](https://arxiv.org/abs/2602.03695)
*Haibo Jin,Kuang Peng,Ye Yu,Xiaopeng Yuan,Haohan Wang*

Main category: cs.MA

TL;DR: The paper introduces Agent Primitives, reusable latent building blocks for LLM-based multi-agent systems (MAS), improving efficiency, accuracy, and stability compared to traditional MAS and single-agent systems.


<details>
  <summary>Details</summary>
Motivation: Existing MAS are task-specific, complex, and suffer from error accumulation in long-context interactions, limiting reusability and robustness.

Method: Proposes three primitives (Review, Voting and Selection, Planning and Execution) communicating via key-value cache, with an Organizer agent composing them automatically based on past successful configurations.

Result: Primitives-based MAS achieves 12.0-16.5% higher accuracy, reduces token usage and latency by 3x-4x, and maintains stable performance across models with minimal overhead.

Conclusion: Agent Primitives offer a scalable, efficient, and robust alternative to traditional MAS, enhancing performance and reusability.

Abstract: While existing multi-agent systems (MAS) can handle complex problems by enabling collaboration among multiple agents, they are often highly task-specific, relying on manually crafted agent roles and interaction prompts, which leads to increased architectural complexity and limited reusability across tasks. Moreover, most MAS communicate primarily through natural language, making them vulnerable to error accumulation and instability in long-context, multi-stage interactions within internal agent histories.
  In this work, we propose \textbf{Agent Primitives}, a set of reusable latent building blocks for LLM-based MAS. Inspired by neural network design, where complex models are built from reusable components, we observe that many existing MAS architectures can be decomposed into a small number of recurring internal computation patterns. Based on this observation, we instantiate three primitives: Review, Voting and Selection, and Planning and Execution. All primitives communicate internally via key-value (KV) cache, which improves both robustness and efficiency by mitigating information degradation across multi-stage interactions. To enable automatic system construction, an Organizer agent selects and composes primitives for each query, guided by a lightweight knowledge pool of previously successful configurations, forming a primitive-based MAS.
  Experiments show that primitives-based MAS improve average accuracy by 12.0-16.5\% over single-agent baselines, reduce token usage and inference latency by approximately 3$\times$-4$\times$ compared to text-based MAS, while incurring only 1.3$\times$-1.6$\times$ overhead relative to single-agent inference and providing more stable performance across model backbones.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [254] [UNSO: Unified Newton Schulz Orthogonalization](https://arxiv.org/abs/2602.02500)
*Chen Hu,Qianxi Zhao,Yuming Li,Mingyu Zhou,Xiyin Li*

Main category: cs.LG

TL;DR: The paper introduces UNSO, a unified framework for Newton-Schulz iteration, avoiding inefficiency and instability by removing insignificant terms and using learnable coefficients.


<details>
  <summary>Details</summary>
Motivation: The conventional Newton-Schulz iteration is inefficient and unstable, and existing improvements still rely on the same iterative paradigm, increasing computational burden.

Method: Proposes UNSO, a unified framework that avoids polynomial expansion, evaluates matrix power roles, removes insignificant terms, and uses learnable coefficients optimized for performance.

Result: UNSO achieves outstanding performance with stable convergence, outperforming conventional methods.

Conclusion: UNSO provides an efficient and stable alternative to the Newton-Schulz iteration by unifying the iterative structure and optimizing learnable coefficients.

Abstract: The Newton-Schulz (NS) iteration has gained increasing interest for its role in the Muon optimizer and the Stiefel manifold. However, the conventional NS iteration suffers from inefficiency and instability. Although various improvements have been introduced to NS iteration, they fail to deviate from the conventional iterative paradigm, which could increase computation burden largely due to the matrix products along the long dimension repeatedly. To address this, we consolidate the iterative structure into a unified framework, named Unified Newton-Schulz Orthogonalization (UNSO). To do so, we could avoid a polynomial expansion. Instead, we evaluate the role of each matrix power, remove the insignificant terms, and provide a recommended polynomial with learnable coefficients. These learnable coefficients are then optimized, and achieve an outstanding performance with stable convergence. The code of our method is available: https://github.com/greekinRoma/Unified_Newton_Schulz_Orthogonalization.

</details>


### [255] [Augmenting Parameter-Efficient Pre-trained Language Models with Large Language Models](https://arxiv.org/abs/2602.02501)
*Saurabh Anand,Shubham Malaviya,Manish Shukla,Sachin Lodha*

Main category: cs.LG

TL;DR: Combining parameter-efficient fine-tuning and large language models (LLMs) improves cybersecurity AI model reliability by addressing data drift and label scarcity.


<details>
  <summary>Details</summary>
Motivation: Training AI models in cybersecurity faces challenges like data drift and label scarcity, risking overfitting and requiring frequent updates.

Method: Used parameter-efficient fine-tuning with compacters and layer freezing. Introduced two LLM strategies: labeling unlabeled data and acting as fallback for low-confidence predictions.

Result: Empirical results show improved reliability and robustness of models in cybersecurity tasks.

Conclusion: Integrating parameter-efficient models with LLMs enhances suitability for real-world cybersecurity applications.

Abstract: Training AI models in cybersecurity with help of vast datasets offers significant opportunities to mimic real-world behaviors effectively. However, challenges like data drift and scarcity of labelled data lead to frequent updates of models and the risk of overfitting. To address these challenges, we used parameter-efficient fine-tuning techniques for pre-trained language models wherein we combine compacters with various layer freezing strategies. To enhance the capabilities of these pre-trained language models, in this work we introduce two strategies that use large language models. In the first strategy, we utilize large language models as data-labelling tools wherein they generate labels for unlabeled data. In the second strategy, large language modes are utilized as fallback mechanisms for predictions having low confidence scores. We perform comprehensive experimental analysis on the proposed strategies on different downstream tasks specific to cybersecurity domain. We empirically demonstrate that by combining parameter-efficient pre-trained models with large language models, we can improve the reliability and robustness of models, making them more suitable for real-world cybersecurity applications.

</details>


### [256] [Sparse Adapter Fusion for Continual Learning in NLP](https://arxiv.org/abs/2602.02502)
*Min Zeng,Xi Chen,Haiqin Yang,Yike Guo*

Main category: cs.LG

TL;DR: SAFM, a Sparse Adapter Fusion Method, dynamically fuses adapters to improve continual learning in NLP by minimizing parameter use and maximizing reuse, outperforming SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Address inefficiencies in parameter reuse and knowledge sharing in continual learning for NLP, preventing catastrophic forgetting and reducing unnecessary parameter introduction.

Method: SAFM uses a two-stage process: decision (choosing to reuse, add, or skip adapters) and tuning (layer-wise loss for adapter differentiation), optimizing parameter use and knowledge capture.

Result: SAFM achieves comparable performance to SOTA methods while using less than 60% of the parameters, demonstrating efficient and effective continual learning.

Conclusion: SAFM provides a scalable and parameter-efficient solution for continual learning in NLP, balancing performance and resource use.

Abstract: Continual learning in natural language processing plays a crucial role in adapting to evolving data and preventing catastrophic forgetting. Despite significant progress, existing methods still face challenges, such as inefficient parameter reuse across tasks, risking catastrophic forgetting when tasks are dissimilar, and the unnecessary introduction of new parameters for each task, which hampers knowledge sharing among similar tasks. To tackle these issues, we propose a Sparse Adapter Fusion Method (SAFM), which dynamically fuses old and new adapters to address these challenges. SAFM operates in two stages: the decision stage and the tuning stage. In the decision stage, SAFM determines whether to incorporate a new adapter, reuse an existing one, or add an empty adapter. The architecture search procedure, designed to prioritize reusing or adding empty adapters, minimizes parameter consumption and maximizes reuse. In the tuning stage, SAFM especially facilitates a layer-wise loss to encourage differentiation between adapters, effectively capturing knowledge within the same task. Experimental results consistently show that SAFM outperforms state-of-the-art (SOTA) methods, achieving comparable performance while utilizing less than 60% of the parameters.

</details>


### [257] [Learning ORDER-Aware Multimodal Representations for Composite Materials Design](https://arxiv.org/abs/2602.02513)
*Xinyao Li,Hangwei Qian,Jingjing Li,Ivor Tsang*

Main category: cs.LG

TL;DR: ORDER is a multimodal pretraining framework for composite materials that leverages ordinality to address the challenges of continuous design spaces and data scarcity, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional graph-based AI methods fail for composite materials due to their continuous, nonlinear design spaces and lack of well-defined graph structures.

Method: The paper introduces ORDER, a framework that integrates ordinality to align multimodal data, ensuring similar properties occupy nearby latent spaces.

Result: ORDER outperforms state-of-the-art baselines in property prediction, cross-modal retrieval, and microstructure generation tasks.

Conclusion: ORDER effectively addresses the challenges of composite materials by preserving continuous property relationships, enabling better predictions and interpolations.

Abstract: Artificial intelligence (AI) has shown remarkable success in materials discovery and property prediction, particularly for crystalline and polymer systems where material properties and structures are dominated by discrete graph representations. Such graph-central paradigm breaks down on composite materials, which possess continuous and nonlinear design spaces that lack well-defined graph structures. General composite descriptors, e.g., fiber volume and misalignment angle, cannot fully capture the fiber distributions that fundamentally determine microstructural characteristics, necessitating the integration of heterogeneous data sources through multimodal learning. Existing alignment-oriented multimodal frameworks have proven effective on abundant crystal or polymer data under discrete, unique graph-property mapping assumptions, but fail to address the highly continuous composite design space under extreme data scarcity. In this work, we introduce ORDinal-aware imagE-tabulaR alignment (ORDER), a multimodal pretraining framework that establishes ordinality as a core principle for composite material representations. ORDER ensures that materials with similar target properties occupy nearby regions in the latent space, which effectively preserves the continuous nature of composite properties and enables meaningful interpolation between sparsely observed designs. We evaluate ORDER on a public Nanofiber-enforced composite dataset and an internally curated dataset that simulates the construction of carbon fiber T700 with diverse fiber distributions. ORDER achieves consistent improvements over state-of-the-art multimodal baselines across property prediction, cross-modal retrieval, and microstructure generation tasks.

</details>


### [258] [ToolTok: Tool Tokenization for Efficient and Generalizable GUI Agents](https://arxiv.org/abs/2602.02548)
*Xiaoce Wang,Guibin Zhang,Junzhe Li,Jinzhe Tu,Chun Li,Ming Li*

Main category: cs.LG

TL;DR: ToolTok introduces a multi-step pathfinding paradigm for GUI agents using tokenized tools and semantic anchoring to improve generalization and efficiency with limited data.


<details>
  <summary>Details</summary>
Motivation: Existing GUI agent models struggle with input resolution variations and data scarcity, prompting the need for a more robust and efficient approach.

Method: ToolTok models operations as progressive tool usage sequences, aligning tools with human habits and using learnable token embeddings with semantic anchoring for efficient learning. It employs an easy-to-hard curriculum to teach tool semantics to LLMs.

Result: ToolTok outperforms comparable models (4B) and competes with larger ones (235B) using less than 1% of training data, showing strong generalization.

Conclusion: ToolTok offers a scalable and efficient solution for GUI agents, addressing limitations of existing models with minimal data requirements.

Abstract: Existing GUI agent models relying on coordinate-based one-step visual grounding struggle with generalizing to varying input resolutions and aspect ratios. Alternatives introduce coordinate-free strategies yet suffer from learning under severe data scarcity. To address the limitations, we propose ToolTok, a novel paradigm of multi-step pathfinding for GUI agents, where operations are modeled as a sequence of progressive tool usage. Specifically, we devise tools aligned with human interaction habits and represent each tool using learnable token embeddings. To enable efficient embedding learning under limited supervision, ToolTok introduces a semantic anchoring mechanism that grounds each tool with semantically related concepts as natural inductive bias. To further enable a pre-trained large language model to progressively acquire tool semantics, we construct an easy-to-hard curriculum consisting of three tasks: token definition question-answering, pure text-guided tool selection, and simplified visual pathfinding. Extensive experiments on multiple benchmarks show that ToolTok achieves superior performance among models of comparable scale (4B) and remains competitive with a substantially larger model (235B). Notably, these results are obtained using less than 1% of the training data required by other post-training approaches. In addition, ToolTok demonstrates strong generalization across unseen scenarios. Our training & inference code is open-source at https://github.com/ZephinueCode/ToolTok.

</details>


### [259] [GraphDancer: Training LLMs to Explore and Reason over Graphs via Curriculum Reinforcement Learning](https://arxiv.org/abs/2602.02518)
*Yuyang Bai,Zhuofeng Li,Ping Nie,Jianwen Xie,Yu Zhang*

Main category: cs.LG

TL;DR: GraphDancer uses RL to teach LLMs to navigate heterogeneous graphs, addressing challenges like precise function calls and multi-hop reasoning. It shows robust generalization despite a smaller model size.


<details>
  <summary>Details</summary>
Motivation: To improve LLMs' ability to handle graph-structured knowledge, which requires precise navigation and multi-hop reasoning, unlike plain text tasks.

Method: Proposes GraphDancer, an RL framework with a graph-aware curriculum for training LLMs to interleave reasoning and function execution.

Result: Outperforms larger models (14B backbone, GPT-4o-mini) in cross-domain benchmarks, demonstrating strong generalization.

Conclusion: GraphDancer effectively enhances LLMs' graph navigation and reasoning skills, even with a smaller model size.

Abstract: Large language models (LLMs) increasingly rely on external knowledge to improve factuality, yet many real-world knowledge sources are organized as heterogeneous graphs rather than plain text. Reasoning over such graph-structured knowledge poses two key challenges: (1) navigating structured, schema-defined relations requires precise function calls rather than similarity-based retrieval, and (2) answering complex questions often demands multi-hop evidence aggregation through iterative information seeking. We propose GraphDancer, a reinforcement learning (RL) framework that teaches LLMs to navigate graphs by interleaving reasoning and function execution. To make RL effective for moderate-sized LLMs, we introduce a graph-aware curriculum that schedules training by the structural complexity of information-seeking trajectories using an easy-to-hard biased sampler. We evaluate GraphDancer on a multi-domain benchmark by training on one domain only and testing on unseen domains and out-of-distribution question types. Despite using only a 3B backbone, GraphDancer outperforms baselines equipped with either a 14B backbone or GPT-4o-mini, demonstrating robust cross-domain generalization of graph exploration and reasoning skills. Our code and models can be found at https://yuyangbai.com/graphdancer/ .

</details>


### [260] [Label Curation Using Agentic AI](https://arxiv.org/abs/2602.02564)
*Subhodeep Ghosh,Bayan Divaaniaazar,Md Ishat-E-Rabban,Spencer Clarke,Senjuti Basu Roy*

Main category: cs.LG

TL;DR: AURA is an agentic AI framework for scalable, multi-modal data annotation, improving accuracy and estimating annotator reliability without ground truth.


<details>
  <summary>Details</summary>
Motivation: Traditional human-centric annotation pipelines are costly, slow, and prone to variability, prompting the need for automated, reliability-aware solutions.

Method: AURA uses multiple AI agents to generate and validate labels via probabilistic modeling and Expectation-Maximization, inferring true labels and annotator reliability.

Result: AURA improves annotation accuracy by up to 5.8% on benchmarks and up to 50% in challenging settings, while accurately estimating annotator reliability.

Conclusion: AURA offers a scalable and accurate alternative to traditional annotation pipelines, enhancing label quality and annotator assessment.

Abstract: Data annotation is essential for supervised learning, yet producing accurate, unbiased, and scalable labels remains challenging as datasets grow in size and modality. Traditional human-centric pipelines are costly, slow, and prone to annotator variability, motivating reliability-aware automated annotation. We present AURA (Agentic AI for Unified Reliability Modeling and Annotation Aggregation), an agentic AI framework for large-scale, multi-modal data annotation. AURA coordinates multiple AI agents to generate and validate labels without requiring ground truth. At its core, AURA adapts a classical probabilistic model that jointly infers latent true labels and annotator reliability via confusion matrices, using Expectation-Maximization to reconcile conflicting annotations and aggregate noisy predictions. Across the four benchmark datasets evaluated, AURA achieves accuracy improvements of up to 5.8% over baseline. In more challenging settings with poor quality annotators, the improvement is up to 50% over baseline. AURA also accurately estimates the reliability of annotators, allowing assessment of annotator quality even without any pre-validation steps.

</details>


### [261] [Scaled Dot-Product Attention implements projection of inputs onto a common surface](https://arxiv.org/abs/2602.02521)
*Terence D Sanger*

Main category: cs.LG

TL;DR: The paper reinterprets SDPA as a projection onto a context-dependent surface, offering faster computation and new extensions.


<details>
  <summary>Details</summary>
Motivation: To reconcile SDPA's database-inspired 'query, key, value' framework with mathematical signal processing.

Method: Rewriting SDPA as a projection of input vectors onto a common surface determined by the inputs themselves.

Result: SDPA is shown to discover time- and context-dependent nonlinear dependencies, enabling faster computation.

Conclusion: SDPA's new interpretation justifies its use for time-series data with nonlinear dependencies, diverging from 'self-attention' concepts.

Abstract: Scaled dot-product attention (SDPA) is a fundamental component responsible for the success of large-language models and other nonlinear signal processing applications. The rationale for SDPA has been based upon "query, key, value" concepts borrowed from database theory, but these concepts are difficult to reconcile with standard methods in mathematical signal processing. We show that SDPA can be rewritten in a different but mathematically equivalent form as a projection of the input vectors onto a common surface determined by the inputs themselves. Therefore SDPA discovers nonlinear dependencies in the input that are time-dependent and context-dependent. The rewritten form of SDPA permits increased speed of both feedforward and learning algorithms, but more importantly suggests potential extensions. In the context of language, we re-interpret the role of SDPA as finding a time-dependent contextual meaning determined by the surface on which the set of input vectors lies. Input token embeddings are then modified by the local context surface. This interpretation differs substantially from the concept of "self-attention", and provides a strong justification for the use of SDPA for time-series data with time-varying local nonlinear dependencies.

</details>


### [262] [IMU-1: Sample-Efficient Pre-training of Small Language Models](https://arxiv.org/abs/2602.02522)
*George Grigorev*

Main category: cs.LG

TL;DR: IMU-1 is a 430M-parameter language model trained on 72B tokens, achieving benchmark performance comparable to models trained on significantly more data.


<details>
  <summary>Details</summary>
Motivation: The goal is to demonstrate efficient training of smaller models that perform as well as larger ones trained on much more data.

Method: Combines architectural interventions (QK-norm attention, per-head gating, value residuals, LayerNorm scaling) with optimization advances (NorMuon with cautious weight decay, muP parametrization) and a three-stage training schedule with post-hoc checkpoint EMA.

Result: IMU-1 approaches the performance of models trained on 56x more data.

Conclusion: The validated training recipe enables efficient model training, and the release of code, weights, and data supports reproducibility.

Abstract: We present IMU-1, a 430M-parameter language model trained on 72B tokens that approaches the benchmark performance of models trained on 56x more data. We describe a validated training recipe combining recent architectural interventions (QK-norm attention, per-head gating, value residuals, LayerNorm scaling) with optimization advances (NorMuon with cautious weight decay, muP parametrization) and a three-stage training schedule with post-hoc checkpoint EMA. We provide ablations for each component and release code, weights and data to enable reproduction: https://huggingface.co/thepowerfuldeez/imu1_base

</details>


### [263] [The "Robert Boulton" Singularity: Semantic Tunneling and Manifold Unfolding in Recursive AI](https://arxiv.org/abs/2602.02526)
*Pengyue Hou*

Main category: cs.LG

TL;DR: PPL is unreliable for monitoring AI stability; Semantic Tunneling causes diversity loss. MNCIS framework with ASNC prevents collapse by expanding model diversity.


<details>
  <summary>Details</summary>
Motivation: To address the deceptive nature of PPL in stable contexts and the catastrophic loss of semantic diversity in generative AI models.

Method: Used a sliding-window protocol to identify Semantic Tunneling and applied MNCIS with ASNC to induce Manifold Unfolding.

Result: Baseline model lost semantic diversity; MNCIS expanded effective rank from 3.62 to 5.35, preserving diversity.

Conclusion: MNCIS with ASNC effectively prevents semantic collapse and maintains diverse outputs in generative AI.

Abstract: The stability of generative artificial intelligence trained on recursive synthetic data is conventionally monitored via Perplexity (PPL). We demonstrate that PPL is a deceptive metric in context-stabilized regimes (L=128). Using a rigorous sliding-window protocol (N=1500), we identify a novel failure mode termed "Semantic Tunneling." While the Baseline model maintains high grammatical fluency (PPL approx. 83.9), it suffers a catastrophic loss of semantic diversity, converging within seven generations to a single, low-entropy narrative attractor: the "Robert Boulton" Singularity. This phenomenon represents a total collapse of the latent manifold (Global Effective Rank 3.62 -> 2.22), where the model discards diverse world knowledge to optimize for statistically safe syntactic templates. To address this, we apply the Multi-Scale Negative Coupled Information Systems (MNCIS) framework recently established in Hou (2026) [arXiv:2601.11594]. We demonstrate that Adaptive Spectral Negative Coupling (ASNC) acts as a topological operator that actively induces "Manifold Unfolding." MNCIS forces the model to expand its effective rank from the anisotropic baseline of 3.62 to a hyper-diverse state of 5.35, effectively constructing an "Artificial Manifold" that resists the gravitational pull of semantic attractors and preserves the long-tail distribution of the training data.

</details>


### [264] [Incident-Guided Spatiotemporal Traffic Forecasting](https://arxiv.org/abs/2602.02528)
*Lixiang Fan,Bohao Li,Tao Zou,Bowen Du,Junchen Ye*

Main category: cs.LG

TL;DR: The paper introduces IGSTGNN, a framework integrating incident impacts into traffic forecasting using ICSF and TIID modules, backed by a new dataset and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing traffic forecasting methods neglect sudden incidents like accidents or bad weather, which disrupt temporal patterns, limiting prediction accuracy.

Method: IGSTGNN uses ICSF to capture spatial influence and TIID to model dynamic dissipation of incident impacts.

Result: IGSTGNN achieves state-of-the-art performance on a new benchmark dataset, and its modules enhance other models.

Conclusion: Explicitly modeling incident impacts improves traffic forecasting, with IGSTGNN setting a new benchmark.

Abstract: Recent years have witnessed the rapid development of deep-learning-based, graph-neural-network-based forecasting methods for modern intelligent transportation systems. However, most existing work focuses exclusively on capturing spatio-temporal dependencies from historical traffic data, while overlooking the fact that suddenly occurring transportation incidents, such as traffic accidents and adverse weather, serve as external disturbances that can substantially alter temporal patterns. We argue that this issue has become a major obstacle to modeling the dynamics of traffic systems and improving prediction accuracy, but the unpredictability of incidents makes it difficult to observe patterns from historical sequences. To address these challenges, this paper proposes a novel framework named the Incident-Guided Spatiotemporal Graph Neural Network (IGSTGNN). IGSTGNN explicitly models the incident's impact through two core components: an Incident-Context Spatial Fusion (ICSF) module to capture the initial heterogeneous spatial influence, and a Temporal Incident Impact Decay (TIID) module to model the subsequent dynamic dissipation. To facilitate research on the spatio-temporal impact of incidents on traffic flow, a large-scale dataset is constructed and released, featuring incident records that are time-aligned with traffic time series. On this new benchmark, the proposed IGSTGNN framework is demonstrated to achieve state-of-the-art performance. Furthermore, the generalizability of the ICSF and TIID modules is validated by integrating them into various existing models.

</details>


### [265] [Formulating Reinforcement Learning for Human-Robot Collaboration through Off-Policy Evaluation](https://arxiv.org/abs/2602.02530)
*Saurav Singh,Rodney Sanchez,Alexander Ororbia,Jamison Heard*

Main category: cs.LG

TL;DR: A novel RL framework uses off-policy evaluation (OPE) to automate state and reward function selection, reducing costly real-time interactions and human involvement.


<details>
  <summary>Details</summary>
Motivation: Traditional RL methods require extensive human expertise and real-time environment interaction, which is impractical for complex, safety-critical applications like human-robot interaction.

Method: The framework leverages logged interaction data and OPE to evaluate multiple state representations and reward functions offline, selecting the best ones based on estimated policy performance.

Result: Validated in controlled (OpenAI Gym's Lunar Lander) and real-world (NASA-MATB-II) environments, the method shows improved feasibility and scalability for human-robot interaction.

Conclusion: The automated, data-driven approach enhances offline RL's reliability and sustainability for complex real-world settings.

Abstract: Reinforcement learning (RL) has the potential to transform real-world decision-making systems by enabling autonomous agents to learn from experience. Deploying RL in real-world settings, especially in the context of human-robot interaction, requires defining state representations and reward functions, which are critical for learning efficiency and policy performance. Traditional RL approaches often rely on domain expertise and trial-and-error, necessitating extensive human involvement as well as direct interaction with the environment, which can be costly and impractical, especially in complex and safety-critical applications. This work proposes a novel RL framework that leverages off-policy evaluation (OPE) for state space and reward function selection, using only logged interaction data. This approach eliminates the need for real-time access to the environment or human-in-the-loop feedback, greatly reducing the dependency on costly real-time interactions. The proposed approach systematically evaluates multiple candidate state representations and reward functions by training offline RL agents and applying OPE to estimate policy performance. The optimal state space and reward function are selected based on their ability to produce high-performing policies under OPE metrics. Our method is validated on two environments: the Lunar Lander environment by OpenAI Gym, which provides a controlled setting for assessing state space and reward function selection, and a NASA-MATB-II human subjects study environment, which evaluates the approach's real-world applicability to human-robot teaming scenarios. This work enhances the feasibility and scalability of offline RL for real-world environments by automating critical RL design decisions through a data-driven OPE-based evaluation, enabling more reliable, effective, and sustainable RL formulation for complex human-robot interaction settings.

</details>


### [266] [Hypersonic Flow Control: Generalized Deep Reinforcement Learning for Hypersonic Intake Unstart Control under Uncertainty](https://arxiv.org/abs/2602.02531)
*Trishit Mondal,Ameya D. Jagtap*

Main category: cs.LG

TL;DR: A deep reinforcement learning (DRL)-based strategy effectively controls hypersonic inlet unstart, demonstrating robustness and generalization under various conditions and sensor noise.


<details>
  <summary>Details</summary>
Motivation: Hypersonic unstart destabilizes inlet operation at Mach 5+ due to shock-boundary-layer interactions and pressure fluctuations, necessitating reliable control solutions.

Method: Uses an in-house CFD solver with adaptive mesh refinement for high-fidelity simulations and trains a DRL controller to stabilize inlet flow under varying back pressures.

Result: The DRL controller stabilizes the inlet robustly, generalizes to unseen scenarios (e.g., back pressures, Reynolds numbers, sensor noise), and achieves practical performance with minimal sensors.

Conclusion: The study presents a data-driven DRL approach for real-time hypersonic flow control, addressing operational uncertainties effectively.

Abstract: The hypersonic unstart phenomenon poses a major challenge to reliable air-breathing propulsion at Mach 5 and above, where strong shock-boundary-layer interactions and rapid pressure fluctuations can destabilize inlet operation. Here, we demonstrate a deep reinforcement learning (DRL)- based active flow control strategy to control unstart in a canonical two-dimensional hypersonic inlet at Mach 5 and Reynolds number $5\times 10^6$. The in-house CFD solver enables high-fidelity simulations with adaptive mesh refinement, resolving key flow features, including shock motion, boundary-layer dynamics, and flow separation, that are essential for learning physically consistent control policies suitable for real-time deployment. The DRL controller robustly stabilizes the inlet over a wide range of back pressures representative of varying combustion chamber conditions. It further generalizes to previously unseen scenarios, including different back-pressure levels, Reynolds numbers, and sensor configurations, while operating with noisy measurements, thereby demonstrating strong zero-shot generalization. Control remains robust in the presence of noisy sensor measurements, and a minimal, optimally selected sensor set achieves comparable performance, enabling practical implementation. These results establish a data-driven approach for real-time hypersonic flow control under realistic operational uncertainties.

</details>


### [267] [CADENT: Gated Hybrid Distillation for Sample-Efficient Transfer in Reinforcement Learning](https://arxiv.org/abs/2602.02532)
*Mahyar Alinejad,Yue Wang,George Atia*

Main category: cs.LG

TL;DR: CADENT combines strategic automaton-based knowledge with tactical policy-level guidance, improving RL transfer learning with dynamic trust mechanisms.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with domain shift in RL transfer learning, lacking balance between strategic and tactical knowledge transfer.

Method: Introduces CADENT, a framework unifying strategic automaton-based and tactical policy knowledge with an experience-gated trust mechanism.

Result: Achieves 40-60% better sample efficiency and superior asymptotic performance across diverse environments.

Conclusion: CADENT establishes a robust approach for adaptive knowledge transfer in RL, outperforming baselines.

Abstract: Transfer learning promises to reduce the high sample complexity of deep reinforcement learning (RL), yet existing methods struggle with domain shift between source and target environments. Policy distillation provides powerful tactical guidance but fails to transfer long-term strategic knowledge, while automaton-based methods capture task structure but lack fine-grained action guidance. This paper introduces Context-Aware Distillation with Experience-gated Transfer (CADENT), a framework that unifies strategic automaton-based knowledge with tactical policy-level knowledge into a coherent guidance signal. CADENT's key innovation is an experience-gated trust mechanism that dynamically weighs teacher guidance against the student's own experience at the state-action level, enabling graceful adaptation to target domain specifics. Across challenging environments, from sparse-reward grid worlds to continuous control tasks, CADENT achieves 40-60\% better sample efficiency than baselines while maintaining superior asymptotic performance, establishing a robust approach for adaptive knowledge transfer in RL.

</details>


### [268] [Enhancing Psychologists' Understanding through Explainable Deep Learning Framework for ADHD Diagnosis](https://arxiv.org/abs/2602.02535)
*Abdul Rehman,Ilona Heldal,Jerry Chun-Wei Lin*

Main category: cs.LG

TL;DR: The paper proposes an explainable framework using a hybrid DNN-RNN model (HyExDNN-RNN) for ADHD detection and multi-class categorization, achieving high accuracy and interpretability through XAI techniques like SHAP and PFI.


<details>
  <summary>Details</summary>
Motivation: ADHD diagnosis is complex and requires reliable, transparent methods. The goal is to assist psychologists with interpretable AI-driven insights.

Method: Uses HyExDNN-RNN for ADHD detection, Pearson correlation for feature selection, and SHAP/PFI for interpretability.

Result: Achieved 99% F1 score for binary classification and 94.2% for multi-class categorization.

Conclusion: The framework effectively bridges AI and psychology, enhancing ADHD diagnosis with interpretable results.

Abstract: Attention Deficit Hyperactivity Disorder (ADHD) is a neurodevelopmental disorder that is challenging to diagnose and requires advanced approaches for reliable and transparent identification and classification. It is characterized by a pattern of inattention, hyperactivity and impulsivity that is more severe and more frequent than in individuals with a comparable level of development. In this paper, an explainable framework based on a fine-tuned hybrid Deep Neural Network (DNN) and Recurrent Neural Network (RNN) called HyExDNN-RNN model is proposed for ADHD detection, multi-class categorization, and decision interpretation. This framework not only detects ADHD, but also provides interpretable insights into the diagnostic process so that psychologists can better understand and trust the results of the diagnosis. We use the Pearson correlation coefficient for optimal feature selection and machine and deep learning models for experimental analysis and comparison. We use a standardized technique for feature reduction, model selection and interpretation to accurately determine the diagnosis rate and ensure the interpretability of the proposed framework. Our framework provided excellent results on binary classification, with HyExDNN-RNN achieving an F1 score of 99% and 94.2% on multi-class categorization. XAI approaches, in particular SHapley Additive exPlanations (SHAP) and Permutation Feature Importance (PFI), provided important insights into the importance of features and the decision logic of models. By combining AI with human expertise, we aim to bridge the gap between advanced computational techniques and practical psychological applications. These results demonstrate the potential of our framework to assist in ADHD diagnosis and interpretation.

</details>


### [269] [From Sparse Decisions to Dense Reasoning: A Multi-attribute Trajectory Paradigm for Multimodal Moderation](https://arxiv.org/abs/2602.02536)
*Tianle Gu,Kexin Huang,Lingyu Li,Ruilin Luo,Shiyang Huang,Zongqi Wang,Yujiu Yang,Yan Teng,Yingchun Wang*

Main category: cs.LG

TL;DR: The paper introduces UniMod, a novel learning paradigm for multimodal safety moderation, addressing data and supervision sparsity by transitioning from binary labels to dense reasoning traces. It achieves competitive performance with less training data.


<details>
  <summary>Details</summary>
Motivation: Current multimodal safety moderation suffers from sparse data and supervision, leading to shortcut learning. The authors aim to improve discrimination by grounding decisions in explicit safety semantics.

Method: Proposes UniMod, a framework using structured reasoning traces (evidence grounding, modality assessment, etc.) and UniRM, a multi-head reward model for multi-dimensional supervision. Includes optimization strategies to decouple tasks.

Result: UniMod achieves competitive performance in textual moderation and sets a new benchmark in multimodal moderation using less than 40% of baseline training data.

Conclusion: UniMod offers an effective and efficient framework for multimodal moderation, validated by empirical results and ablations.

Abstract: Safety moderation is pivotal for identifying harmful content. Despite the success of textual safety moderation, its multimodal counterparts remain hindered by a dual sparsity of data and supervision. Conventional reliance on binary labels lead to shortcut learning, which obscures the intrinsic classification boundaries necessary for effective multimodal discrimination. Hence, we propose a novel learning paradigm (UniMod) that transitions from sparse decision-making to dense reasoning traces. By constructing structured trajectories encompassing evidence grounding, modality assessment, risk mapping, policy decision, and response generation, we reformulate monolithic decision tasks into a multi-dimensional boundary learning process. This approach forces the model to ground its decision in explicit safety semantics, preventing the model from converging on superficial shortcuts. To facilitate this paradigm, we develop a multi-head scalar reward model (UniRM). UniRM provides multi-dimensional supervision by assigning attribute-level scores to the response generation stage. Furthermore, we introduce specialized optimization strategies to decouple task-specific parameters and rebalance training dynamics, effectively resolving interference between diverse objectives in multi-task learning. Empirical results show UniMod achieves competitive textual moderation performance and sets a new multimodal benchmark using less than 40\% of the training data used by leading baselines. Ablations further validate our multi-attribute trajectory reasoning, offering an effective and efficient framework for multimodal moderation. Supplementary materials are available at \href{https://trustworthylab.github.io/UniMod/}{project website}.

</details>


### [270] [Enhancing Post-Training Quantization via Future Activation Awareness](https://arxiv.org/abs/2602.02538)
*Zheqi Lv,Zhenxuan Fan,Qi Tian,Wenqiao Zhang,Yueting Zhuang*

Main category: cs.LG

TL;DR: Future-Aware Quantization (FAQ) improves post-training quantization by using future-layer activations to guide quantization, reducing bias and error accumulation without extra tuning.


<details>
  <summary>Details</summary>
Motivation: PTQ suffers from quantization bias and error accumulation, especially with biased calibration data, leading to suboptimal performance.

Method: FAQ leverages future-layer activations and a window-wise preview mechanism to softly aggregate them, avoiding over-reliance on any single layer.

Result: FAQ outperforms prior methods with negligible extra cost, requiring no backward passes, data reconstruction, or tuning.

Conclusion: FAQ is an efficient and effective quantization method, suitable for edge deployment.

Abstract: Post-training quantization (PTQ) is a widely used method to compress large language models (LLMs) without fine-tuning. It typically sets quantization hyperparameters (e.g., scaling factors) based on current-layer activations. Although this method is efficient, it suffers from quantization bias and error accumulation, resulting in suboptimal and unstable quantization, especially when the calibration data is biased. To overcome these issues, we propose Future-Aware Quantization (FAQ), which leverages future-layer activations to guide quantization. This allows better identification and preservation of important weights, while reducing sensitivity to calibration noise. We further introduce a window-wise preview mechanism to softly aggregate multiple future-layer activations, mitigating over-reliance on any single layer. To avoid expensive greedy search, we use a pre-searched configuration to minimize overhead. Experiments show that FAQ consistently outperforms prior methods with negligible extra cost, requiring no backward passes, data reconstruction, or tuning, making it well-suited for edge deployment.

</details>


### [271] [How Much Information Can a Vision Token Hold? A Scaling Law for Recognition Limits in VLMs](https://arxiv.org/abs/2602.02539)
*Shuxin Zhuang,Zi Liang,Runsheng Yu,Hongzong Li,Rong Feng,Shiqin Tang,Youzhi Zhang*

Main category: cs.LG

TL;DR: The paper examines the information capacity of visual tokens in vision-centric models, identifying three distinct phases as information load increases and proposing a scaling law to optimize visual context compression.


<details>
  <summary>Details</summary>
Motivation: The study aims to determine the upper bound of information visual tokens can represent, addressing gaps in understanding the efficiency-accuracy trade-off in vision-centric models.

Method: Controlled stress tests increase character count in images to observe phase transitions (Stable, Instability, Collapse). A probabilistic scaling law is formulated to unify token load and visual density.

Result: Three distinct phases emerge with increasing information load. The scaling law proves universal across Vision-Language Models, guiding optimization of visual context compression.

Conclusion: The findings provide empirical insights into the limits of visual token representation and offer practical guidance for balancing efficiency and accuracy in visual context compression.

Abstract: Recent vision-centric approaches have made significant strides in long-context modeling. Represented by DeepSeek-OCR, these models encode rendered text into continuous vision tokens, achieving high compression rates without sacrificing recognition precision. However, viewing the vision encoder as a lossy channel with finite representational capacity raises a fundamental question: what is the information upper bound of visual tokens? To investigate this limit, we conduct controlled stress tests by progressively increasing the information quantity (character count) within an image. We observe a distinct phase-transition phenomenon characterized by three regimes: a near-perfect Stable Phase, an Instability Phase marked by increased error variance, and a total Collapse Phase. We analyze the mechanical origins of these transitions and identify key factors. Furthermore, we formulate a probabilistic scaling law that unifies average vision token load and visual density into a latent difficulty metric. Extensive experiments across various Vision-Language Models demonstrate the universality of this scaling law, providing critical empirical guidance for optimizing the efficiency-accuracy trade-off in visual context compression.

</details>


### [272] [Auto-Augmentation Contrastive Learning for Wearable-based Human Activity Recognition](https://arxiv.org/abs/2602.02542)
*Qingyu Wu,Jianfei Shen,Feiyi Fan,Yang Gu,Chenyang Xu,Yiqiang Chen*

Main category: cs.LG

TL;DR: AutoCL is an end-to-end auto-augmentation contrastive learning method designed for wearable-based HAR, reducing reliance on manual data augmentation and improving recognition accuracy.


<details>
  <summary>Details</summary>
Motivation: Contrastive learning in HAR relies heavily on manual data augmentation, which lacks generalizability and flexibility. AutoCL aims to automate this process.

Method: AutoCL uses a Siamese network with a shared backbone and embedded generator for auto-augmentation, along with stop-gradient and correlation reduction strategies.

Result: Experiments on four HAR datasets show AutoCL outperforms state-of-the-art methods in recognition accuracy.

Conclusion: AutoCL effectively automates augmentation in HAR, enhancing performance and reducing manual effort.

Abstract: For low-semantic sensor signals from human activity recognition (HAR), contrastive learning (CL) is essential to implement novel applications or generic models without manual annotation, which is a high-performance self-supervised learning (SSL) method. However, CL relies heavily on data augmentation for pairwise comparisons. Especially for low semantic data in the HAR area, conducting good performance augmentation strategies in pretext tasks still rely on manual attempts lacking generalizability and flexibility. To reduce the augmentation burden, we propose an end-to-end auto-augmentation contrastive learning (AutoCL) method for wearable-based HAR. AutoCL is based on a Siamese network architecture that shares the parameters of the backbone and with a generator embedded to learn auto-augmentation. AutoCL trains the generator based on the representation in the latent space to overcome the disturbances caused by noise and redundant information in raw sensor data. The architecture empirical study indicates the effectiveness of this design. Furthermore, we propose a stop-gradient design and correlation reduction strategy in AutoCL to enhance encoder representation learning. Extensive experiments based on four wide-used HAR datasets demonstrate that the proposed AutoCL method significantly improves recognition accuracy compared with other SOTA methods.

</details>


### [273] [Toward Ultra-Long-Horizon Sequential Model Editing](https://arxiv.org/abs/2602.02543)
*Mingda Liu,Zhenghan Zhu,Ze'an Miao,Katsuki Fujisawa*

Main category: cs.LG

TL;DR: NAS is a plug-and-play norm-constrained strategy that addresses model collapse in L&E methods by controlling MLP weight norms, significantly improving editing performance.


<details>
  <summary>Details</summary>
Motivation: To mitigate abrupt model collapse triggered by sequential edits in the Locate-and-Edit paradigm, which correlates with explosive growth of MLP weight norms.

Method: Proposes Norm-Anchor Scaling (NAS), a strategy to constrain weight norms during edits.

Result: Delays collapse point by over 4 times and achieves a 72.2% average relative gain in editing performance with minimal computational overhead.

Conclusion: NAS effectively enhances the stability and performance of model editing in LLMs.

Abstract: Model editing has emerged as a practical approach for mitigating factual errors and outdated knowledge in large language models (LLMs). Among existing methods, the Locate-and-Edit (L&E) paradigm is the dominant framework: it locates MLP parameters implicated in expressing a target fact, and then performs a localized update to rewrite that fact. However, long sequences of edits often trigger abrupt model collapse in L&E beyond a critical point. We empirically identify a strong correlation between collapse and explosive growth of edited MLP weight norms, and formally prove that commonly used L&E update rules can induce exponential norm growth across sequential edits in the absence of explicit norm control. To address this issue, we propose Norm-Anchor Scaling NAS, a plug-and-play norm-constrained strategy. Across extensive experiments, NAS delays the collapse point of representative L&E algorithms by more than 4 times and yields a 72.2% average relative gain in editing performance, requiring only a single additional line of code and incurring negligible computational overhead.

</details>


### [274] [SPA-Cache: Singular Proxies for Adaptive Caching in Diffusion Language Models](https://arxiv.org/abs/2602.02544)
*Wenhao Sun,Rong-Cheng Tu,Yifu Ding,Zhao Jin,Jingyi Liao,Yongcheng Jing,Dacheng Tao*

Main category: cs.LG

TL;DR: SPA-Cache improves DLM efficiency by optimizing token updates and budget allocation, achieving up to 8x throughput gains.


<details>
  <summary>Details</summary>
Motivation: Diffusion Language Models (DLMs) lack efficient caching due to non-causal nature, leading to costly state recomputation. Existing methods are limited by rigid budget allocation and expensive update heuristics.

Method: SPA-Cache introduces a low-dimensional proxy for update identification and adaptive budget allocation for stable layers, reducing overhead without quality loss.

Result: The method achieves up to 8x throughput improvement over vanilla decoding and 2-4x speedup over baselines.

Conclusion: SPA-Cache effectively addresses DLM caching inefficiencies, enhancing decoding speed and practical usability.

Abstract: While Diffusion Language Models (DLMs) offer a flexible, arbitrary-order alternative to the autoregressive paradigm, their non-causal nature precludes standard KV caching, forcing costly hidden state recomputation at every decoding step. Existing DLM caching approaches reduce this cost by selective hidden state updates; however, they are still limited by (i) costly token-wise update identification heuristics and (ii) rigid, uniform budget allocation that fails to account for heterogeneous hidden state dynamics. To address these challenges, we present SPA-Cache that jointly optimizes update identification and budget allocation in DLM cache. First, we derive a low-dimensional singular proxy that enables the identification of update-critical tokens in a low-dimensional subspace, substantially reducing the overhead of update identification. Second, we introduce an adaptive strategy that allocates fewer updates to stable layers without degrading generation quality. Together, these contributions significantly improve the efficiency of DLMs, yielding up to an $8\times$ throughput improvement over vanilla decoding and a $2$--$4\times$ speedup over existing caching baselines.

</details>


### [275] [Beyond Alignment: Expanding Reasoning Capacity via Manifold-Reshaping Policy Optimization](https://arxiv.org/abs/2602.02545)
*Dayu Wang,Jiaye Yang,Weikang Li,Jiahui Liang,Yang Li*

Main category: cs.LG

TL;DR: MRPO introduces a geometric framework to expand LLMs' latent reasoning space, outperforming larger models through targeted interventions.


<details>
  <summary>Details</summary>
Motivation: Recent studies question if RL genuinely enhances LLM reasoning or merely aligns existing capabilities. This work challenges the hypothesis that exploration is confined to pre-trained models' bias manifold.

Method: Proposes Manifold-Reshaping Policy Optimization (MRPO) with two stages: Spectral Orthogonal Exploration (SOE) and Effective Rank regularization.

Result: Empirically, MRPO achieves SOTA performance on math tasks, outperforming larger models like Qwen3-32B.

Conclusion: MRPO successfully expands LLMs' reasoning capacity beyond standard RL methods.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated remarkable success in enhancing the reasoning capabilities of Large Language Models (LLMs). However, recent studies question whether RL genuinely expands reasoning capacity or merely aligns existing latent capabilities, arguing that exploration remains confined within the pre-trained model's low-rank bias manifold. In this work, we challenge this accessibility boundary hypothesis by demonstrating that the latent reasoning space can be fundamentally expanded through targeted geometric interventions. We propose Manifold-Reshaping Policy Optimization (MRPO), a geometric framework designed to fundamentally restructure the inference space of LLMs. MRPO operates in two stages: first, we employ Spectral Orthogonal Exploration (SOE) to eject the policy initialization into the null space of the bias manifold; second, we integrate an Effective Rank regularization term into the policy optimization objective. This approach incentivizes the discovery and maintenance of high-dimensional reasoning trajectories against the entropy-reducing tendency of standard RL. Empirically, our 4B-parameter method achieves state-of-the-art performance on mathematical tasks, significantly outperforming larger models (e.g., Qwen3-32B) and expanding the capability boundary beyond standard GRPO. Our code is available at https://anonymous.4open.science/r/MRPO-D57B/

</details>


### [276] [D$^2$Quant: Accurate Low-bit Post-Training Weight Quantization for LLMs](https://arxiv.org/abs/2602.02546)
*Xianglong Yan,ChengZhu Bao,Zhiteng Li,Tianao Zhang,Shaoqiu Zhang,Ruobing Xie,Samm Sun,Yulun Zhang*

Main category: cs.LG

TL;DR: D$^2$Quant improves weight-only PTQ for LLMs by addressing quantization bottlenecks and activation deviations, achieving superior sub-4-bit performance.


<details>
  <summary>Details</summary>
Motivation: High compute and memory costs of LLMs limit deployment in resource-constrained scenarios, and existing weight-only PTQ methods suffer from accuracy degradation at low precision.

Method: D$^2$Quant introduces a Dual-Scale Quantizer (DSQ) for down-projection matrices and Deviation-Aware Correction (DAC) to mitigate activation deviations.

Result: Extensive experiments show D$^2$Quant outperforms existing methods in sub-4-bit weight-only PTQ.

Conclusion: D$^2$Quant provides an effective solution for deploying LLMs efficiently without compromising accuracy.

Abstract: Large language models (LLMs) deliver strong performance, but their high compute and memory costs make deployment difficult in resource-constrained scenarios. Weight-only post-training quantization (PTQ) is appealing, as it reduces memory usage and enables practical speedup without low-bit operators or specialized hardware. However, accuracy often degrades significantly in weight-only PTQ at sub-4-bit precision, and our analysis identifies two main causes: (1) down-projection matrices are a well-known quantization bottleneck, but maintaining their fidelity often requires extra bit-width; (2) weight quantization induces activation deviations, but effective correction strategies remain underexplored. To address these issues, we propose D$^2$Quant, a novel weight-only PTQ framework that improves quantization from both the weight and activation perspectives. On the weight side, we design a Dual-Scale Quantizer (DSQ) tailored to down-projection matrices, with an absorbable scaling factor that significantly improves accuracy without increasing the bit budget. On the activation side, we propose Deviation-Aware Correction (DAC), which incorporates a mean-shift correction within LayerNorm to mitigate quantization-induced activation distribution shifts. Extensive experiments across multiple LLM families and evaluation metrics show that D$^2$Quant delivers superior performance for weight-only PTQ at sub-4-bit precision. The code and models will be available at https://github.com/XIANGLONGYAN/D2Quant.

</details>


### [277] [naPINN: Noise-Adaptive Physics-Informed Neural Networks for Recovering Physics from Corrupted Measurement](https://arxiv.org/abs/2602.02547)
*Hankyeol Kim,Pilsung Kang*

Main category: cs.LG

TL;DR: The paper introduces naPINN, a robust variant of PINNs, to handle complex noise and outliers in data-driven physics problems.


<details>
  <summary>Details</summary>
Motivation: PINNs struggle with performance under complex noise and outliers, prompting the need for a noise-adaptive solution.

Method: naPINN uses an energy-based model to learn residual distributions and a reliability gate to filter outliers, aided by rejection cost regularization.

Result: naPINN outperforms existing methods, accurately recovering dynamics even with severe data corruption.

Conclusion: naPINN effectively addresses noise and outlier challenges in PINNs, enhancing robustness.

Abstract: Physics-Informed Neural Networks (PINNs) are effective methods for solving inverse problems and discovering governing equations from observational data. However, their performance degrades significantly under complex measurement noise and gross outliers. To address this issue, we propose the Noise-Adaptive Physics-Informed Neural Network (naPINN), which robustly recovers physical solutions from corrupted measurements without prior knowledge of the noise distribution. naPINN embeds an energy-based model into the training loop to learn the latent distribution of prediction residuals. Leveraging the learned energy landscape, a trainable reliability gate adaptively filters data points exhibiting high energy, while a rejection cost regularization prevents trivial solutions where valid data are discarded. We demonstrate the efficacy of naPINN on various benchmark partial differential equations corrupted by non-Gaussian noise and varying rates of outliers. The results show that naPINN significantly outperforms existing robust PINN baselines, successfully isolating outliers and accurately reconstructing the dynamics under severe data corruption.

</details>


### [278] [HyPAC: Cost-Efficient LLMs-Human Hybrid Annotation with PAC Error Guarantees](https://arxiv.org/abs/2602.02550)
*Hao Zeng,Huipeng Huang,Xinhao Qu,Jianguo Huang,Bingyi Jing,Hongxin Wei*

Main category: cs.LG

TL;DR: HyPAC efficiently routes data annotation tasks to optimal sources (LLMs, reasoning models, humans) to minimize cost while ensuring controlled error rates, validated by benchmarks showing a 78.51% cost reduction.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of balancing cost and quality in multi-source data annotation (LLMs, reasoning models, humans) while guaranteeing controlled labeling errors.

Method: HyPAC uses importance sampling and upper confidence bounds to calibrate decision thresholds, routing inputs to the most cost-efficient source based on uncertainty.

Result: The method reduces annotation costs by 78.51% while maintaining tight control over annotation errors, outperforming benchmarks.

Conclusion: HyPAC provides a distribution-free, cost-efficient solution for multi-source annotation with proven PAC guarantees.

Abstract: Data annotation often involves multiple sources with different cost-quality trade-offs, such as fast large language models (LLMs), slow reasoning models, and human experts. In this work, we study the problem of routing inputs to the most cost-efficient annotation source while controlling the labeling error on test instances. We propose \textbf{HyPAC}, a method that adaptively labels inputs to the most cost-efficient annotation source while providing distribution-free guarantees on annotation error. HyPAC calibrates two decision thresholds using importance sampling and upper confidence bounds, partitioning inputs into three regions based on uncertainty and routing each to the appropriate annotation source. We prove that HyPAC achieves the minimum expected cost with a probably approximately correct (PAC) guarantee on the annotation error, free of data distribution and pre-trained models. Experiments on common benchmarks demonstrate the effectiveness of our method, reducing the annotation cost by 78.51\% while tightly controlling the annotation error.

</details>


### [279] [EEO-TFV: Escape-Explore Optimizer for Web-Scale Time-Series Forecasting and Vision Analysis](https://arxiv.org/abs/2602.02551)
*Hua Wang,Jinghao Lu,Fan Zhang*

Main category: cs.LG

TL;DR: A lightweight Transformer with Escape-Explore Optimizer (EEO) tackles error accumulation and generalization issues in Web-scale tasks, matching state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Transformer models struggle with error accumulation in long-sequence forecasting and out-of-distribution samples in image tasks, especially in complex Web data analysis.

Method: Proposes a lightweight Transformer architecture paired with EEO to avoid sharp minima and saddle points, enhancing exploration and generalization.

Result: Achieves competitive performance on 11 time-series benchmarks and Synapse image segmentation, with improved generalization and stability.

Conclusion: The method validates potential as a cross-task foundation model for Web-scale data analysis, addressing key limitations of existing Transformer models.

Abstract: Transformer-based foundation models have achieved remarkable progress in tasks such as time-series forecasting and image segmentation. However, they frequently suffer from error accumulation in multivariate long-sequence prediction and exhibit vulnerability to out-of-distribution samples in image-related tasks. Furthermore, these challenges become particularly pronounced in large-scale Web data analysis tasks, which typically involve complex temporal patterns and multimodal features. This complexity substantially increases optimization difficulty, rendering models prone to stagnation at saddle points within high-dimensional parameter spaces. To address these issues, we propose a lightweight Transformer architecture in conjunction with a novel Escape-Explore Optimizer (EEO). The optimizer enhances both exploration and generalization while effectively avoiding sharp minima and saddle-point traps. Experimental results show that, in representative Web data scenarios, our method achieves performance on par with state-of-the-art models across 11 time-series benchmark datasets and the Synapse medical image segmentation task. Moreover, it demonstrates superior generalization and stability, thereby validating its potential as a versatile cross-task foundation model for Web-scale data mining and analysis.

</details>


### [280] [BatCoder: Self-Supervised Bidirectional Code-Documentation Learning via Back-Translation](https://arxiv.org/abs/2602.02554)
*Jingwen Xu,Yiyang Lu,Zisu Huang,Changze Lv,Xiaohua Wang,Shizheng Li,Zhibo Xu,Zhengkang Guo,Zhengyuan Wang,Muzhao Tian,Xuanjing Huang,Xiaoqing Zheng*

Main category: cs.LG

TL;DR: BatCoder is a self-supervised RL framework that jointly optimizes code generation and documentation production using a back-translation strategy, reducing reliance on scarce code-documentation pairs.


<details>
  <summary>Details</summary>
Motivation: High-quality code-documentation pairs are costly and scarce, especially for niche programming languages, necessitating a self-supervised approach.

Method: BatCoder uses back-translation: generating documentation from code and reconstructing code from documentation, with semantic similarity as an implicit reward.

Result: Achieved 83.5% pass@1 on HumanEval and 81.0% on MBPP, outperforming baselines, with consistent scaling in corpus size and model capacity.

Conclusion: BatCoder effectively trains models using only code, expanding training data availability and enhancing performance.

Abstract: Training LLMs for code-related tasks typically depends on high-quality code-documentation pairs, which are costly to curate and often scarce for niche programming languages. We introduce BatCoder, a self-supervised reinforcement learning framework designed to jointly optimize code generation and documentation production. BatCoder employs a back-translation strategy: a documentation is first generated from code, and then the generated documentation is used to reconstruct the original code. The semantic similarity between the original and reconstructed code serves as an implicit reward, enabling reinforcement learning to improve the model's performance both in generating code from documentation and vice versa. This approach allows models to be trained using only code, substantially increasing the available training examples. Evaluated on HumanEval and MBPP with a 7B model, BatCoder achieved 83.5% and 81.0% pass@1, outperforming strong open-source baselines. Moreover, the framework demonstrates consistent scaling with respect to both training corpus size and model capacity.

</details>


### [281] [Learning to Explore with Parameter-Space Noise: A Deep Dive into Parameter-Space Noise for Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2602.02555)
*Bizhe Bai,Xinyue Wang,Peng Ye,Tao Chen*

Main category: cs.LG

TL;DR: PSN-RLVR introduces policy perturbation and truncated importance sampling to overcome the exploration ceiling in RLVR, improving reasoning capabilities of LLMs.


<details>
  <summary>Details</summary>
Motivation: RLVR often reweights existing solutions instead of discovering new strategies, limiting performance gains.

Method: PSN-RLVR perturbs policy parameters for trajectory-level exploration and uses truncated importance sampling to address sampling-update mismatch.

Result: PSN-GRPO expands reasoning capabilities across benchmarks, outperforming prior RLVR methods under large sampling budgets.

Conclusion: PSN-RLVR effectively enhances exploration in RLVR, improving pass-at-k performance while remaining composable for further gains.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) improves LLM reasoning, yet growing evidence indicates an exploration ceiling: it often reweights existing solution traces rather than discovering new strategies, limiting gains under large sampling budgets (e.g., pass-at-256). We address this limitation with PSN-RLVR, which perturbs policy parameters before rollout generation to induce temporally consistent, trajectory-level exploration that better preserves long-horizon chain-of-thought coherence than action-space noise. To mitigate the resulting sampling-update mismatch, we incorporate truncated importance sampling (TIS). To avoid expensive KL-based adaptive noise control, we propose a computationally efficient real-time adaptive noise scheduler driven by a lightweight surrogate that combines semantic diversity with normalized self-certainty. Instantiated on GRPO, a widely used RLVR method, PSN-GRPO consistently expands the effective reasoning capability boundary across multiple mathematical reasoning benchmarks and model families, yielding higher pass-at-k under large sampling budgets and outperforming prior exploration-oriented RLVR methods (e.g., Pass-at-k-style training) while remaining orthogonal and thus composable for additional gains.

</details>


### [282] [Beyond Experience Retrieval: Learning to Generate Utility-Optimized Structured Experience for Frozen LLMs](https://arxiv.org/abs/2602.02556)
*Xuancheng Li,Haitao Li,Yujia Zhou,Yiqun Liu,Qingyao Ai*

Main category: cs.LG

TL;DR: SEAM introduces a lightweight plug-in module for LLMs to reuse prior experiences efficiently without external retrieval, improving accuracy with low overhead.


<details>
  <summary>Details</summary>
Motivation: Current LLMs are static and inefficient in reusing experiences, relying on noisy, latency-prone similarity-based methods.

Method: SEAM stores experiences in its parameters, generates structured entries in one forward pass, and is trained via executor rollouts and GRPO while keeping the executor frozen.

Result: SEAM shows consistent accuracy gains on mathematical reasoning benchmarks with minimal overhead.

Conclusion: SEAM effectively enhances LLM performance by enabling efficient, structured experience reuse.

Abstract: Large language models (LLMs) are largely static and often redo reasoning or repeat mistakes. Prior experience reuse typically relies on external retrieval, which is similarity-based, can introduce noise, and adds latency. We introduce SEAM (Structured Experience Adapter Module), a lightweight, executor-specific plug-in that stores experience in its parameters and generates a structured, instance-tailored experience entry in a single forward pass to guide a frozen LLM executor. SEAM is trained for utility via executor rollouts and GRPO while keeping the executor frozen, and it can be further improved after deployment with supervised fine-tuning on logged successful trajectories. Experiments on mathematical reasoning benchmarks show consistent accuracy gains across executors with low overhead. Extensive ablations and analyses further elucidate the mechanisms underlying SEAM's effectiveness and robustness.

</details>


### [283] [The Alignment Curse: Cross-Modality Jailbreak Transfer in Omni-Models](https://arxiv.org/abs/2602.02557)
*Yupeng Chen,Junchi Yu,Aoxi Liu,Philip Torr,Adel Bibi*

Main category: cs.LG

TL;DR: This paper investigates the transfer of jailbreak attacks from text to audio in multimodal models, revealing that strong modality alignment can propagate vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the semantic similarity between text and audio modalities and the maturity of textual jailbreak methods, aiming to explore underexplored cross-modality vulnerabilities.

Method: The authors analyze modality alignment's role in jailbreak transfer, empirically evaluate textual and audio-based jailbreaks, and test their effectiveness on omni-models.

Result: Text-transferred audio jailbreaks perform comparably or better than audio-based ones, showing strong cross-model transferability and effectiveness under audio-only access.

Conclusion: The findings establish text-transferred audio jailbreaks as powerful baselines for future audio red-teaming, highlighting the risks of strong modality alignment.

Abstract: Recent advances in end-to-end trained omni-models have significantly improved multimodal understanding. At the same time, safety red-teaming has expanded beyond text to encompass audio-based jailbreak attacks. However, an important bridge between textual and audio jailbreaks remains underexplored. In this work, we study the cross-modality transfer of jailbreak attacks from text to audio, motivated by the semantic similarity between the two modalities and the maturity of textual jailbreak methods. We first analyze the connection between modality alignment and cross-modality jailbreak transfer, showing that strong alignment can inadvertently propagate textual vulnerabilities to the audio modality, which we term the alignment curse. Guided by this analysis, we conduct an empirical evaluation of textual jailbreaks, text-transferred audio jailbreaks, and existing audio-based jailbreaks on recent omni-models. Our results show that text-transferred audio jailbreaks perform comparably to, and often better than, audio-based jailbreaks, establishing them as simple yet powerful baselines for future audio red-teaming. We further demonstrate strong cross-model transferability and show that text-transferred audio attacks remain effective even under a stricter audio-only access threat model.

</details>


### [284] [PA-MIL: Phenotype-Aware Multiple Instance Learning Guided by Language Prompting and Genotype-to-Phenotype Relationships](https://arxiv.org/abs/2602.02558)
*Zekang Yang,Hong Liu,Xiangdong Wang*

Main category: cs.LG

TL;DR: PA-MIL is an ante-hoc interpretable framework for cancer subtyping using phenotype-aware features, leveraging genotype-phenotype relationships for improved reliability and accountability.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack reliable explanations; PA-MIL addresses this by integrating phenotype-aware features and genotype-phenotype relationships.

Method: Constructs a phenotype knowledge base, uses morphological descriptions for feature aggregation, and develops GP-NN for multi-level guidance.

Result: Competitive performance against MIL methods with enhanced interpretability and linear classifier results comparable to state-of-the-art.

Conclusion: PA-MIL offers reliable, accountable explanations and demonstrates effectiveness through genotype-phenotype relationship analysis.

Abstract: Deep learning has been extensively researched in the analysis of pathology whole-slide images (WSIs). However, most existing methods are limited to providing prediction interpretability by locating the model's salient areas in a post-hoc manner, failing to offer more reliable and accountable explanations. In this work, we propose Phenotype-Aware Multiple Instance Learning (PA-MIL), a novel ante-hoc interpretable framework that identifies cancer-related phenotypes from WSIs and utilizes them for cancer subtyping. To facilitate PA-MIL in learning phenotype-aware features, we 1) construct a phenotype knowledge base containing cancer-related phenotypes and their associated genotypes. 2) utilize the morphological descriptions of phenotypes as language prompting to aggregate phenotype-related features. 3) devise the Genotype-to-Phenotype Neural Network (GP-NN) grounded in genotype-to-phenotype relationships, which provides multi-level guidance for PA-MIL. Experimental results on multiple datasets demonstrate that PA-MIL achieves competitive performance compared to existing MIL methods while offering improved interpretability. PA-MIL leverages phenotype saliency as evidence and, using a linear classifier, achieves competitive results compared to state-of-the-art methods. Additionally, we thoroughly analyze the genotype-phenotype relationships, as well as cohort-level and case-level interpretability, demonstrating the reliability and accountability of PA-MIL.

</details>


### [285] [Auditing Sybil: Explaining Deep Lung Cancer Risk Prediction Through Generative Interventional Attributions](https://arxiv.org/abs/2602.02560)
*Bartlomiej Sobieski,Jakub Grzywaczewski,Karol Dobiczek,Mateusz Wójcik,Tomasz Bartczak,Patryk Szatkowski,Przemysław Bombiński,Matthew Tivnan,Przemyslaw Biecek*

Main category: cs.LG

TL;DR: Sybil,a deep learning model for lung cancer risk prediction,lacks causal verification.S(H)NAP,a model-agnostic auditing framework,reveals Sybil's critical failure modes through generative interventional attributions.


<details>
  <summary>Details</summary>
Motivation: Current assessments of Sybil rely on observational metrics,overlooking its reasoning mechanisms,necessitating causal verification for robust clinical deployment.

Method: S(H)NAP uses generative interventional attributions via 3D diffusion bridge modeling to modify anatomical features and isolate causal contributions to Sybil's risk score.

Result: Sybil often mimics expert radiologists but has critical failure modes like sensitivity to unjustified artifacts and radial bias.

Conclusion: Causal verification via S(H)NAP is essential to uncover and address Sybil's limitations before clinical use.

Abstract: Lung cancer remains the leading cause of cancer mortality, driving the development of automated screening tools to alleviate radiologist workload. Standing at the frontier of this effort is Sybil, a deep learning model capable of predicting future risk solely from computed tomography (CT) with high precision. However, despite extensive clinical validation, current assessments rely purely on observational metrics. This correlation-based approach overlooks the model's actual reasoning mechanism, necessitating a shift to causal verification to ensure robust decision-making before clinical deployment. We propose S(H)NAP, a model-agnostic auditing framework that constructs generative interventional attributions validated by expert radiologists. By leveraging realistic 3D diffusion bridge modeling to systematically modify anatomical features, our approach isolates object-specific causal contributions to the risk score. Providing the first interventional audit of Sybil, we demonstrate that while the model often exhibits behavior akin to an expert radiologist, differentiating malignant pulmonary nodules from benign ones, it suffers from critical failure modes, including dangerous sensitivity to clinically unjustified artifacts and a distinct radial bias.

</details>


### [286] [A General ReLearner: Empowering Spatiotemporal Prediction by Re-learning Input-label Residual](https://arxiv.org/abs/2602.02563)
*Jiaming Ma,Binwu Wang,Pengkun Wang,Xu Wang,Zhengyang Zhou,Yang Wang*

Main category: cs.LG

TL;DR: The paper introduces a bidirectional learning framework called ReLearner to improve spatiotemporal prediction models by addressing discrepancies between inputs and labels using residual learning and smoothing modules.


<details>
  <summary>Details</summary>
Motivation: Existing spatiotemporal prediction models struggle with discrepancies between inputs and labels, leading to suboptimal performance. The authors aim to enhance these models by incorporating label features during training.

Method: Proposes the Spatiotemporal Residual Theorem and designs ReLearner, a universal module with Residual Learning and Residual Smoothing Modules, to enable bidirectional learning in STNNs.

Result: ReLearner significantly boosts predictive performance across 11 datasets and 14 backbone models, demonstrating its effectiveness.

Conclusion: The bidirectional learning approach via ReLearner successfully addresses input-label discrepancies, improving spatiotemporal prediction accuracy.

Abstract: Prevailing spatiotemporal prediction models typically operate under a forward (unidirectional) learning paradigm, in which models extract spatiotemporal features from historical observation input and map them to target spatiotemporal space for future forecasting (label). However, these models frequently exhibit suboptimal performance when spatiotemporal discrepancies exist between inputs and labels, for instance, when nodes with similar time-series inputs manifest distinct future labels, or vice versa. To address this limitation, we propose explicitly incorporating label features during the training phase. Specifically, we introduce the Spatiotemporal Residual Theorem, which generalizes the conventional unidirectional spatiotemporal prediction paradigm into a bidirectional learning framework. Building upon this theoretical foundation, we design an universal module, termed ReLearner, which seamlessly augments Spatiotemporal Neural Networks (STNNs) with a bidirectional learning capability via an auxiliary inverse learning process. In this process, the model relearns the spatiotemporal feature residuals between input data and future data. The proposed ReLearner comprises two critical components: (1) a Residual Learning Module, designed to effectively disentangle spatiotemporal feature discrepancies between input and label representations; and (2) a Residual Smoothing Module, employed to smooth residual terms and facilitate stable convergence. Extensive experiments conducted on 11 real-world datasets across 14 backbone models demonstrate that ReLearner significantly enhances the predictive performance of existing STNNs.Our code is available on GitHub.

</details>


### [287] [High Rank Matrix Completion via Grassmannian Proxy Fusion](https://arxiv.org/abs/2602.02565)
*Huanran Li,Jeremy Johnson,Daniel Pimentel-Alarcón*

Main category: cs.LG

TL;DR: The paper introduces a method for high-rank matrix completion by clustering incomplete vectors via proxy subspaces and optimizing distances on the Grassmannian.


<details>
  <summary>Details</summary>
Motivation: Current HRMC methods often lack theoretical support, yield uninterpretable results, and require excessive samples. The authors aim to bridge this gap.

Method: Clustering incomplete vectors by grouping proxy subspaces and optimizing chordal and geodesic distances on the Grassmannian.

Result: The method matches leading methods at high sampling rates and outperforms them at low rates, nearing the theoretical sampling limit.

Conclusion: The proposed approach effectively narrows the gap to the theoretical sampling limit, offering improved performance and interpretability.

Abstract: This paper approaches high-rank matrix completion (HRMC) by filling missing entries in a data matrix where columns lie near a union of subspaces, clustering these columns, and identifying the underlying subspaces. Current methods often lack theoretical support, produce uninterpretable results, and require more samples than theoretically necessary. We propose clustering incomplete vectors by grouping proxy subspaces and minimizing two criteria over the Grassmannian: (a) the chordal distance between each point and its corresponding subspace and (b) the geodesic distances between subspaces of all data points. Experiments on synthetic and real datasets demonstrate that our method performs comparably to leading methods in high sampling rates and significantly better in low sampling rates, thus narrowing the gap to the theoretical sampling limit of HRMC.

</details>


### [288] [A Comparative Simulation Study of the Fairness and Accuracy of Predictive Policing Systems in Baltimore City](https://arxiv.org/abs/2602.02566)
*Samin Semsar,Kiran Laxmikant Prabhu,Gabriella Waters,James Foulds*

Main category: cs.LG

TL;DR: The abstract discusses a comparative simulation study on predictive policing fairness and accuracy in Baltimore, revealing complex bias issues in both predictive and traditional hot spots policing.


<details>
  <summary>Details</summary>
Motivation: Address the gap in comprehensive comparative studies on predictive policing systems, particularly regarding fairness and accuracy.

Method: A comprehensive comparative simulation study evaluating predictive policing and hot spots policing in Baltimore.

Result: Predictive policing showed short-term fairness and accuracy but amplified bias faster than hot spots policing. Bias tendencies differed from previous studies, sometimes over-policing White neighborhoods.

Conclusion: The study highlights the complexity of bias in policing systems and proposes a methodology for city-specific evaluations to uncover inequities and long-term behavioral tendencies.

Abstract: There are ongoing discussions about predictive policing systems, such as those deployed in Los Angeles, California and Baltimore, Maryland, being unfair, for example, by exhibiting racial bias. Studies found that unfairness may be due to feedback loops and being trained on historically biased recorded data. However, comparative studies on predictive policing systems are few and are not sufficiently comprehensive. In this work, we perform a comprehensive comparative simulation study on the fairness and accuracy of predictive policing technologies in Baltimore. Our results suggest that the situation around bias in predictive policing is more complex than was previously assumed. While predictive policing exhibited bias due to feedback loops as was previously reported, we found that the traditional alternative, hot spots policing, had similar issues. Predictive policing was found to be more fair and accurate than hot spots policing in the short term, although it amplified bias faster, suggesting the potential for worse long-run behavior. In Baltimore, in some cases the bias in these systems tended toward over-policing in White neighborhoods, unlike in previous studies. Overall, this work demonstrates a methodology for city-specific evaluation and behavioral-tendency comparison of predictive policing systems, showing how such simulations can reveal inequities and long-term tendencies.

</details>


### [289] [IceBench-S2S: A Benchmark of Deep Learning for Challenging Subseasonal-to-Seasonal Daily Arctic Sea Ice Forecasting in Deep Latent Space](https://arxiv.org/abs/2602.02567)
*Jingyi Xu,Shengnan Wang,Weidong Yang,Siwei Tu,Lei Bai,Ben Fei*

Main category: cs.LG

TL;DR: IceBench-S2S introduces a benchmark for evaluating deep learning models to extend Arctic sea ice concentration forecasts from subseasonal to seasonal scales, addressing gaps in current capabilities.


<details>
  <summary>Details</summary>
Motivation: Current deep learning models for Arctic sea ice forecasting are limited to short lead times, hindering practical applications like maritime planning and scientific research. Extending forecasts to seasonal scales is crucial.

Method: Proposes IceBench-S2S, a framework that compresses spatial features of daily sea ice data into a latent space and models temporally concatenated features with DL backbones for seasonal forecasting.

Result: Provides a unified pipeline for training and evaluating DL models, offering guidance for polar environmental monitoring.

Conclusion: IceBench-S2S bridges the gap in seasonal forecasting lead times, enhancing the utility of DL models for real-world Arctic applications.

Abstract: Arctic sea ice plays a critical role in regulating Earth's climate system, significantly influencing polar ecological stability and human activities in coastal regions. Recent advances in artificial intelligence have facilitated the development of skillful pan-Arctic sea ice forecasting systems, where data-driven approaches showcase tremendous potential to outperform conventional physics-based numerical models in terms of accuracy, computational efficiency and forecasting lead times. Despite the latest progress made by deep learning (DL) forecasting models, most of their skillful forecasting lead times are confined to daily subseasonal scale and monthly averaged values for up to six months, which drastically hinders their deployment for real-world applications, e.g., maritime routine planning for Arctic transportation and scientific investigation. Extending daily forecasts from subseasonal to seasonal (S2S) scale is scientifically crucial for operational applications. To bridge the gap between the forecasting lead time of current DL models and the significant daily S2S scale, we introduce IceBench-S2S, the first comprehensive benchmark for evaluating DL approaches in mitigating the challenge of forecasting Arctic sea ice concentration in successive 180-day periods. It proposes a generalized framework that first compresses spatial features of daily sea ice data into a deep latent space. The temporally concatenated deep features are subsequently modeled by DL-based forecasting backbones to predict the sea ice variation at S2S scale. IceBench-S2S provides a unified training and evaluation pipeline for different backbones, along with practical guidance for model selection in polar environmental monitoring tasks.

</details>


### [290] [Mitigating Task-Order Sensitivity and Forgetting via Hierarchical Second-Order Consolidation](https://arxiv.org/abs/2602.02568)
*Protik Nag,Krishnan Raghavan,Vignesh Narayanan*

Main category: cs.LG

TL;DR: HTCL is a hierarchical framework for continual learning that improves performance and reduces variance by optimizing task sequences and using Hessian-regularized consolidation.


<details>
  <summary>Details</summary>
Motivation: Address high variance from random task ordering in continual learning by combining fast local adaptation with global consolidation.

Method: Uses hierarchical task sequencing and Hessian-regularized Taylor expansions for consolidation, scalable to multi-level hierarchies.

Result: Achieves accuracy gains of 7% to 25% and reduces standard deviation by up to 68% across datasets.

Conclusion: HTCL is a model-agnostic solution that enhances continual learning performance and stability.

Abstract: We introduce $\textbf{Hierarchical Taylor Series-based Continual Learning (HTCL)}$, a framework that couples fast local adaptation with conservative, second-order global consolidation to address the high variance introduced by random task ordering. To address task-order effects, HTCL identifies the best intra-group task sequence and integrates the resulting local updates through a Hessian-regularized Taylor expansion, yielding a consolidation step with theoretical guarantees. The approach naturally extends to an $L$-level hierarchy, enabling multiscale knowledge integration in a manner not supported by conventional single-level CL systems. Across a wide range of datasets and replay and regularization baselines, HTCL acts as a model-agnostic consolidation layer that consistently enhances performance, yielding mean accuracy gains of $7\%$ to $25\%$ while reducing the standard deviation of final accuracy by up to $68\%$ across random task permutations.

</details>


### [291] [Trajectory Consistency for One-Step Generation on Euler Mean Flows](https://arxiv.org/abs/2602.02571)
*Zhiqi Li,Yuchen Sun,Duowen Chen,Jinjin He,Bo Zhu*

Main category: cs.LG

TL;DR: EMF is a flow-based generative framework for efficient one-step/few-step generation using a linear surrogate for long-range trajectory consistency, reducing training time and memory.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of supervising and optimizing long-range trajectory consistency in flow-based generative models, which are typically costly and unstable.

Method: The method introduces a principled linear surrogate derived from the semigroup formulation of flow-based models, avoiding explicit Jacobian computations and supporting unified training.

Result: Experiments show improved optimization stability and sample quality, with ~50% reductions in training time and memory compared to existing methods.

Conclusion: EMF provides an efficient and scalable alternative for flow-based generation, balancing accuracy and computational cost.

Abstract: We propose \emph{Euler Mean Flows (EMF)}, a flow-based generative framework for one-step and few-step generation that enforces long-range trajectory consistency with minimal sampling cost. The key idea of EMF is to replace the trajectory consistency constraint, which is difficult to supervise and optimize over long time scales, with a principled linear surrogate that enables direct data supervision for long-horizon flow-map compositions. We derive this approximation from the semigroup formulation of flow-based models and show that, under mild regularity assumptions, it faithfully approximates the original consistency objective while being substantially easier to optimize. This formulation leads to a unified, JVP-free training framework that supports both $u$-prediction and $x_1$-prediction variants, avoiding explicit Jacobian computations and significantly reducing memory and computational overhead. Experiments on image synthesis, particle-based geometry generation, and functional generation demonstrate improved optimization stability and sample quality under fixed sampling budgets, together with approximately $50\%$ reductions in training time and memory consumption compared to existing one-step methods for image generation.

</details>


### [292] [Reward Shaping for Inference-Time Alignment: A Stackelberg Game Perspective](https://arxiv.org/abs/2602.02572)
*Haichuan Wang,Tao Lin,Lingkai Kong,Ce Li,Hezi Jiang,Milind Tambe*

Main category: cs.LG

TL;DR: The paper addresses suboptimal alignment methods for LLMs by proposing a reward model optimization problem using a Stackelberg game, achieving better rewards and performance.


<details>
  <summary>Details</summary>
Motivation: Current alignment methods inherit biases from base policies due to KL regularization, conflicting with user preferences, and risk reward hacking.

Method: Formalizes reward model optimization as a Stackelberg game and introduces a simple reward shaping scheme.

Result: Empirical evaluation shows improved average reward and win-tie rates exceeding 66% against baselines.

Conclusion: The proposed method integrates seamlessly into existing alignment methods, effectively mitigating bias and enhancing performance.

Abstract: Existing alignment methods directly use the reward model learned from user preference data to optimize an LLM policy, subject to KL regularization with respect to the base policy. This practice is suboptimal for maximizing user's utility because the KL regularization may cause the LLM to inherit the bias in the base policy that conflicts with user preferences. While amplifying rewards for preferred outputs can mitigate this bias, it also increases the risk of reward hacking. This tradeoff motivates the problem of optimally designing reward models under KL regularization. We formalize this reward model optimization problem as a Stackelberg game, and show that a simple reward shaping scheme can effectively approximate the optimal reward model. We empirically evaluate our method in inference-time alignment settings and demonstrate that it integrates seamlessly into existing alignment methods with minimal overhead. Our method consistently improves average reward and achieves win-tie rates exceeding 66% against all baselines, averaged across evaluation settings.

</details>


### [293] [Product Interaction: An Algebraic Formalism for Deep Learning Architectures](https://arxiv.org/abs/2602.02573)
*Haonan Dong,Chun-Wun Cheng,Angelica I. Aviles-Rivero*

Main category: cs.LG

TL;DR: The paper introduces 'product interactions,' an algebraic framework for constructing neural network layers via multiplication operators over algebras, unifying diverse architectures.


<details>
  <summary>Details</summary>
Motivation: To provide a principled algebraic formalism for generating and organizing neural network layers by interaction order, unifying existing architectures.

Method: Defines neural network layers using compositions of multiplication operators over algebras, categorized by linear, quadratic, and higher-order product interactions.

Result: Shows convolutional/equivariant networks as symmetry-constrained linear interactions and attention/Mamba as higher-order interactions.

Conclusion: Product interactions offer a unified algebraic foundation for diverse neural network architectures.

Abstract: In this paper, we introduce product interactions, an algebraic formalism in which neural network layers are constructed from compositions of a multiplication operator defined over suitable algebras. Product interactions provide a principled way to generate and organize algebraic expressions by increasing interaction order. Our central observation is that algebraic expressions in modern neural networks admit a unified construction in terms of linear, quadratic, and higher-order product interactions. Convolutional and equivariant networks arise as symmetry-constrained linear product interactions, while attention and Mamba correspond to higher-order product interactions.

</details>


### [294] [QuantLRM: Quantization of Large Reasoning Models via Fine-Tuning Signals](https://arxiv.org/abs/2602.02581)
*Nan Zhang,Eugene Kwek,Yusen Zhang,Muyu Pan,Suhang Wang,Prasenjit Mitra,Rui Zhang*

Main category: cs.LG

TL;DR: The paper introduces QuantLRM, a method for weight quantization of Large Reasoning Models (LRMs) using fine-tuning signals, focusing on protecting the smallest and largest weight updates ('protecting both ends') for better performance.


<details>
  <summary>Details</summary>
Motivation: To improve the quantization of Large Language Models (LLMs) by leveraging weight update magnitudes during fine-tuning, hypothesizing that extreme values are more critical than intermediate ones.

Method: QuantLRM fits restricted quadratic functions on weight updates to identify important channels, combining average quadratic values with zero-update counts for effective quantization.

Result: QuantLRM consistently improves LRM quantization across various fine-tuning methods and benchmarks, achieving an average 6.55% improvement on reinforcement learning fine-tuned models.

Conclusion: QuantLRM effectively quantizes LRMs using fine-tuning signals, with broad applicability even for non-fine-tuned models via pseudo-fine-tuning.

Abstract: Weight-only quantization is important for compressing Large Language Models (LLMs). Inspired by the spirit of classical magnitude pruning, we study whether the magnitude of weight updates during reasoning-incentivized fine-tuning can provide valuable signals for quantizing Large Reasoning Models (LRMs). We hypothesize that the smallest and largest weight updates during fine-tuning are more important than those of intermediate magnitude, a phenomenon we term "protecting both ends". Upon hypothesis validation, we introduce QuantLRM, which stands for weight quantization of LRMs via fine-tuning signals. We fit simple restricted quadratic functions on weight updates to protect both ends. By multiplying the average quadratic values with the count of zero weight updates of channels, we compute channel importance that is more effective than using activation or second-order information. We run QuantLRM to quantize various fine-tuned models (including supervised, direct preference optimization, and reinforcement learning fine-tuning) over four reasoning benchmarks (AIME-120, FOLIO, temporal sequences, and GPQA-Diamond) and empirically find that QuantLRM delivers a consistent improvement for LRMs quantization, with an average improvement of 6.55% on a reinforcement learning fine-tuned model. Also supporting non-fine-tuned LRMs, QuantLRM gathers effective signals via pseudo-fine-tuning, which greatly enhances its applicability.

</details>


### [295] [Copula-Based Aggregation and Context-Aware Conformal Prediction for Reliable Renewable Energy Forecasting](https://arxiv.org/abs/2602.02583)
*Alireza Moradi,Mathieu Tanneau,Reza Zandehshahvar,Pascal Van Hentenryck*

Main category: cs.LG

TL;DR: Proposes a calibrated probabilistic aggregation framework for converting site-level renewable energy forecasts into reliable fleet-level forecasts using copula-based dependence modeling and CACP.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of deriving coherent and calibrated fleet-level probabilistic forecasts from heterogeneous site-level forecasts due to cross-site dependencies and aggregation-induced miscalibration.

Method: Combines copula-based dependence modeling to capture cross-site correlations with Context-Aware Conformal Prediction (CACP) to correct miscalibration at the aggregated level.

Result: Demonstrates near-nominal coverage and sharper prediction intervals compared to uncalibrated baselines in experiments on solar generation datasets from MISO, ERCOT, and SPP.

Conclusion: The proposed Copula+CACP framework effectively enables dependence-aware aggregation while ensuring valid coverage and sharp intervals.

Abstract: The rapid growth of renewable energy penetration has intensified the need for reliable probabilistic forecasts to support grid operations at aggregated (fleet or system) levels. In practice, however, system operators often lack access to fleet-level probabilistic models and instead rely on site-level forecasts produced by heterogeneous third-party providers. Constructing coherent and calibrated fleet-level probabilistic forecasts from such inputs remains challenging due to complex cross-site dependencies and aggregation-induced miscalibration. This paper proposes a calibrated probabilistic aggregation framework that directly converts site-level probabilistic forecasts into reliable fleet-level forecasts in settings where system-level models cannot be trained or maintained. The framework integrates copula-based dependence modeling to capture cross-site correlations with Context-Aware Conformal Prediction (CACP) to correct miscalibration at the aggregated level. This combination enables dependence-aware aggregation while providing valid coverage and maintaining sharp prediction intervals. Experiments on large-scale solar generation datasets from MISO, ERCOT, and SPP demonstrate that the proposed Copula+CACP approach consistently achieves near-nominal coverage with significantly sharper intervals than uncalibrated aggregation baselines.

</details>


### [296] [Effective Frontiers: A Unification of Neural Scaling Laws](https://arxiv.org/abs/2602.02593)
*Jiaxuan Zou,Zixuan Gong,Ye Su,Huayi Tang,Yong Liu*

Main category: cs.LG

TL;DR: The paper presents a unified framework explaining neural scaling laws via pattern coverage from a Zipfian distribution, introducing the Effective Frontier and proving scaling laws for model capacity, datasize, and compute.


<details>
  <summary>Details</summary>
Motivation: Existing theoretical explanations for neural scaling laws are often architecture-specific or overly complex, lacking universal intuition. The paper aims to provide a simpler, unified framework.

Method: The authors abstract learning tasks as pattern coverage from a Zipfian distribution, introduce the Effective Frontier ($k_\star$), and derive scaling laws for $N$, $D$, and $C$ based on capacity, coverage, and optimization bottlenecks.

Result: The framework proves scaling laws for $N$, $D$, and $C$, unifying them via a Max-Bottleneck principle, showing Kaplan and Chinchilla laws as equilibrium solutions under different bottlenecks.

Conclusion: The paper successfully unifies neural scaling laws through a pattern-coverage framework, explaining them as constrained optimization problems with varying active bottlenecks.

Abstract: Neural scaling laws govern the prediction power-law improvement of test loss with respect to model capacity ($N$), datasize ($D$), and compute ($C$). However, existing theoretical explanations often rely on specific architectures or complex kernel methods, lacking intuitive universality. In this paper, we propose a unified framework that abstracts general learning tasks as the progressive coverage of patterns from a long-tail (Zipfian) distribution. We introduce the Effective Frontier ($k_\star$), a threshold in the pattern rank space that separates learned knowledge from the unlearned tail. We prove that reducible loss is asymptotically determined by the probability mass of the tail a resource-dependent frontier truncation. Based on our framework, we derive the precise scaling laws for $N$, $D$, and $C$, attributing them to capacity, coverage, and optimization bottlenecks, respectively. Furthermore, we unify these mechanisms via a Max-Bottleneck principle, demonstrating that the Kaplan and Chinchilla scaling laws are not contradictory, but equilibrium solutions to the same constrained optimization problem under different active bottlenecks.

</details>


### [297] [Fubini Study geometry of representation drift in high dimensional data](https://arxiv.org/abs/2602.02596)
*Arturo Tozzi*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: High dimensional representation drift is commonly quantified using Euclidean or cosine distances, which presuppose fixed coordinates when comparing representations across time, training or preprocessing stages. While effective in many settings, these measures entangle intrinsic changes in the data with variations induced by arbitrary parametrizations. We introduce a projective geometric view of representation drift grounded in the Fubini Study metric, which identifies representations that differ only by gauge transformations such as global rescalings or sign flips. Applying this framework to empirical high dimensional datasets, we explicitly construct representation trajectories and track their evolution through cumulative geometric drift. Comparing Euclidean, cosine and Fubini Study distances along these trajectories reveals that conventional metrics systematically overestimate change whenever representations carry genuine projective ambiguity. By contrast, the Fubini Study metric isolates intrinsic evolution by remaining invariant under gauge-induced fluctuations. We further show that the difference between cosine and Fubini Study drift defines a computable, monotone quantity that directly captures representation churn attributable to gauge freedom. This separation provides a diagnostic for distinguishing meaningful structural evolution from parametrization artifacts, without introducing model-specific assumptions. Overall, we establish a geometric criterion for assessing representation stability in high-dimensional systems and clarify the limits of angular distances. Embedding representation dynamics in projective space connects data analysis with established geometric programs and yields observables that are directly testable in empirical workflows.

</details>


### [298] [ContextEvolve: Multi-Agent Context Compression for Systems Code Optimization](https://arxiv.org/abs/2602.02597)
*Hongyuan Su,Yu Zheng,Yong Li*

Main category: cs.LG

TL;DR: ContextEvolve is a multi-agent framework enhancing LLM-generated code optimization by decomposing context into three agents, achieving superior performance and efficiency compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: The need for iterative optimization in LLM-generated code to meet correctness and performance standards drives the development of ContextEvolve, addressing inefficiencies in current methods.

Method: ContextEvolve employs three agents (Summarizer, Navigator, Sampler) to decompose optimization context, enabling efficient search without parameter updates.

Result: ContextEvolve outperforms state-of-the-art baselines by 33.3% and reduces token consumption by 29.0%.

Conclusion: ContextEvolve provides a scalable, efficient solution for optimizing LLM-generated code, demonstrating significant performance improvements.

Abstract: Large language models are transforming systems research by automating the discovery of performance-critical algorithms for computer systems. Despite plausible codes generated by LLMs, producing solutions that meet the stringent correctness and performance requirements of systems demands iterative optimization. Test-time reinforcement learning offers high search efficiency but requires parameter updates infeasible under API-only access, while existing training-free evolutionary methods suffer from inefficient context utilization and undirected search. We introduce ContextEvolve, a multi-agent framework that achieves RL-level search efficiency under strict parameter-blind constraints by decomposing optimization context into three orthogonal dimensions: a Summarizer Agent condenses semantic state via code-to-language abstraction, a Navigator Agent distills optimization direction from trajectory analysis, and a Sampler Agent curates experience distribution through prioritized exemplar retrieval. This orchestration forms a functional isomorphism with RL-mapping to state representation, policy gradient, and experience replay-enabling principled optimization in a textual latent space. On the ADRS benchmark, ContextEvolve outperforms state-of-the-art baselines by 33.3% while reducing token consumption by 29.0%. Codes for our work are released at https://anonymous.4open.science/r/ContextEvolve-ACC

</details>


### [299] [RAP: KV-Cache Compression via RoPE-Aligned Pruning](https://arxiv.org/abs/2602.02599)
*Jihao Xin,Tian Lvu,Hatem Ltaief,David Keyes,Marco Canini*

Main category: cs.LG

TL;DR: RAP is a method to prune RoPE-aligned column pairs in LLMs, reducing KV-Cache, attention parameters, and FLOPs by 20-30% while maintaining accuracy and improving latency.


<details>
  <summary>Details</summary>
Motivation: Long-context inference in large language models is bottlenecked by the high memory and compute cost of the KV-Cache, especially in RoPE-based models where low-rank factorization fails.

Method: Proposes RoPE-Aligned Pruning (RAP), which prunes entire RoPE-aligned column pairs to preserve the 2x2 rotation structure, enabling absorption of downstream weights and eliminating reconstruction overhead.

Result: RAP achieves 20-30% reduction in KV-Cache, attention parameters, and FLOPs while maintaining strong accuracy. It also reduces attention latency to 83% (prefill) and 77% (decode) of baseline in LLaMA-3-8B and Mistral-7B.

Conclusion: RAP effectively addresses the inefficiencies in RoPE-based LLMs by enabling significant reductions in memory, compute, and latency without sacrificing accuracy.

Abstract: Long-context inference in large language models is increasingly bottlenecked by the memory and compute cost of the KV-Cache. Low-rank factorization compresses KV projections by writing $W \approx A * B$, where A produces latent KV states and B can be absorbed into downstream weights. In modern RoPE-based LLMs, this absorption fails: RoPE forces latent KV states to be reconstructed to full dimension, reintroducing substantial memory and compute overhead. We propose RoPE-Aligned Pruning (RAP), which prunes entire RoPE-aligned column pairs to preserve RoPE's 2x2 rotation structure, restore B absorption, and eliminate reconstruction. Our evaluation on LLaMA-3-8B and Mistral-7B shows that RAP enables joint reduction of KV-Cache, attention parameters, and FLOPs by 20-30%, all at once, while maintaining strong accuracy. Notably, RAP reduces attention latency to 83% (prefill) and 77% (decode) of baseline.

</details>


### [300] [Step-Wise Refusal Dynamics in Autoregressive and Diffusion Language Models](https://arxiv.org/abs/2602.02600)
*Eliron Rahimi,Elad Hirshel,Rom Himelstein,Amit LeVi,Avi Mendelson,Chaim Baskin*

Main category: cs.LG

TL;DR: This paper analyzes the role of sampling mechanisms in refusal behavior and jailbreak robustness in diffusion language models (DLMs) compared to autoregressive (AR) models, introducing the SRI signal for improved safety and interpretability.


<details>
  <summary>Details</summary>
Motivation: Despite progress in DLMs, the impact of sampling mechanisms on safety behaviors like refusal and jailbreak robustness is not well understood, prompting this study.

Method: The authors develop an analytical framework for step-wise refusal dynamics and introduce the Step-Wise Refusal Internal Dynamics (SRI) signal to interpret and enhance safety in both AR and DLMs.

Result: The SRI signal reveals internal recovery dynamics and detects harmful generations as incomplete internal recovery, enabling lightweight, efficient detectors that outperform existing defenses.

Conclusion: The sampling strategy significantly influences safety behavior, and the SRI signal provides a powerful tool for improving model safety with minimal inference overhead.

Abstract: Diffusion language models (DLMs) have recently emerged as a promising alternative to autoregressive (AR) models, offering parallel decoding and controllable sampling dynamics while achieving competitive generation quality at scale. Despite this progress, the role of sampling mechanisms in shaping refusal behavior and jailbreak robustness remains poorly understood. In this work, we present a fundamental analytical framework for step-wise refusal dynamics, enabling comparison between AR and diffusion sampling. Our analysis reveals that the sampling strategy itself plays a central role in safety behavior, as a factor distinct from the underlying learned representations. Motivated by this analysis, we introduce the Step-Wise Refusal Internal Dynamics (SRI) signal, which supports interpretability and improved safety for both AR and DLMs. We demonstrate that the geometric structure of SRI captures internal recovery dynamics, and identifies anomalous behavior in harmful generations as cases of \emph{incomplete internal recovery} that are not observable at the text level. This structure enables lightweight inference-time detectors that generalize to unseen attacks while matching or outperforming existing defenses with over $100\times$ lower inference overhead.

</details>


### [301] [Discovering Data Manifold Geometry via Non-Contracting Flows](https://arxiv.org/abs/2602.02611)
*David Vigouroux,Lucas Drumetz,Ronan Fablet,François Rousseau*

Main category: cs.LG

TL;DR: An unsupervised method learns tangent vector fields in ambient space to create a global reference system for unknown data manifolds, avoiding assumptions of flatness.


<details>
  <summary>Details</summary>
Motivation: Existing isometric methods assume manifold flatness, which is limiting. This work aims to learn interpretable intrinsic coordinates tied to a shared global frame.

Method: Learns tangent vector fields whose flows transport samples to a common reference point, using non-shrinking constraints and integration-free flow-matching objectives.

Result: Proven recovery of global coordinate charts when feasible, with correct tangent alignment and coherent structure on synthetic and real-world data (e.g., CIFAR-10).

Conclusion: The method scales well and achieves competitive classification performance while maintaining interpretability.

Abstract: We introduce an unsupervised approach for constructing a global reference system by learning, in the ambient space, vector fields that span the tangent spaces of an unknown data manifold. In contrast to isometric objectives, which implicitly assume manifold flatness, our method learns tangent vector fields whose flows transport all samples to a common, learnable reference point. The resulting arc-lengths along these flows define interpretable intrinsic coordinates tied to a shared global frame. To prevent degenerate collapse, we enforce a non-shrinking constraint and derive a scalable, integration-free objective inspired by flow matching. Within our theoretical framework, we prove that minimizing the proposed objective recovers a global coordinate chart when one exists. Empirically, we obtain correct tangent alignment and coherent global coordinate structure on synthetic manifolds. We also demonstrate the scalability of our method on CIFAR-10, where the learned coordinates achieve competitive downstream classification performance.

</details>


### [302] [A Semi-Supervised Pipeline for Generalized Behavior Discovery from Animal-Borne Motion Time Series](https://arxiv.org/abs/2602.02618)
*Fatemeh Karimi Nejadasl,Judy Shamoun-Baranes,Eldar Rakhimberdiev*

Main category: cs.LG

TL;DR: A semi-supervised pipeline for discovering novel animal behaviors from sensor data, leveraging embedding learning, clustering, and a containment score to identify new classes.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of labeling scarcity and class imbalance in animal behavior studies, aiming to generalize behavior discovery from sensor data.

Method: Proposes a pipeline with embedding learning, label-guided clustering, and a containment score (KDE + HDR) to assess novelty of discovered behaviors.

Result: Effectively identifies novel behaviors withheld from supervision and distinguishes them from known classes using low overlap scores.

Conclusion: The containment score offers a practical tool for generalized class discovery in ecological motion data, even with limited annotations and class imbalance.

Abstract: Learning behavioral taxonomies from animal-borne sensors is challenging because labels are scarce, classes are highly imbalanced, and behaviors may be absent from the annotated set. We study generalized behavior discovery in short multivariate motion snippets from gulls, where each sample is a sequence with 3-axis IMU acceleration (20 Hz) and GPS speed, spanning nine expert-annotated behavior categories. We propose a semi-supervised discovery pipeline that (i) learns an embedding function from the labeled subset, (ii) performs label-guided clustering over embeddings of both labeled and unlabeled samples to form candidate behavior groups, and (iii) decides whether a discovered group is truly novel using a containment score. Our key contribution is a KDE + HDR (highest-density region) containment score that measures how much a discovered cluster distribution is contained within, or contains, each known-class distribution; the best-match containment score serves as an interpretable novelty statistic. In experiments where an entire behavior is withheld from supervision and appears only in the unlabeled pool, the method recovers a distinct cluster and the containment score flags novelty via low overlap, while a negative-control setting with no novel behavior yields consistently higher overlaps. These results suggest that HDR-based containment provides a practical, quantitative test for generalized class discovery in ecological motion time series under limited annotation and severe class imbalance.

</details>


### [303] [daVinci-Agency: Unlocking Long-Horizon Agency Data-Efficiently](https://arxiv.org/abs/2602.02619)
*Mohan Jiang,Dayuan Fu,Junhao Shi,Ji Zeng,Weiye Si,Keyu Li,Xuefeng Li,Yang Xiao,Wenjie Li,Dequan Wang,Pengfei Liu*

Main category: cs.LG

TL;DR: The paper addresses the challenge of scaling LLMs for long-horizon tasks by leveraging real-world software evolution data (Pull Requests) to create scalable, high-quality supervision. The proposed method, daVinci-Agency, improves task decomposition, consistency, and refinement.


<details>
  <summary>Details</summary>
Motivation: Existing methods for training LLMs on long-horizon tasks suffer from limited data or high annotation costs. The paper seeks a scalable solution inspired by real-world software evolution.

Method: daVinci-Agency extracts structured supervision from Pull Request sequences, focusing on progressive task decomposition, long-term consistency, and verifiable refinement.

Result: Fine-tuning on daVinci-Agency data (239 samples) improves performance, notably achieving a 47% relative gain on Toolathlon.

Conclusion: Leveraging real-world data (Pull Requests) provides scalable, high-quality supervision for long-horizon tasks, outperforming synthetic methods.

Abstract: While Large Language Models (LLMs) excel at short-term tasks, scaling them to long-horizon agentic workflows remains challenging. The core bottleneck lies in the scarcity of training data that captures authentic long-dependency structures and cross-stage evolutionary dynamics--existing synthesis methods either confine to single-feature scenarios constrained by model distribution, or incur prohibitive human annotation costs, failing to provide scalable, high-quality supervision. We address this by reconceptualizing data synthesis through the lens of real-world software evolution. Our key insight: Pull Request (PR) sequences naturally embody the supervision signals for long-horizon learning. They decompose complex objectives into verifiable submission units, maintain functional coherence across iterations, and encode authentic refinement patterns through bug-fix histories. Building on this, we propose daVinci-Agency, which systematically mines structured supervision from chain-of-PRs through three interlocking mechanisms: (1) progressive task decomposition via continuous commits, (2) long-term consistency enforcement through unified functional objectives, and (3) verifiable refinement from authentic bug-fix trajectories. Unlike synthetic trajectories that treat each step independently, daVinci-Agency's PR-grounded structure inherently preserves the causal dependencies and iterative refinements essential for teaching persistent goal-directed behavior and enables natural alignment with project-level, full-cycle task modeling. The resulting trajectories are substantial--averaging 85k tokens and 116 tool calls--yet remarkably data-efficient: fine-tuning GLM-4.6 on 239 daVinci-Agency samples yields broad improvements across benchmarks, notably achieving a 47% relative gain on Toolathlon. Beyond benchmark performance, our analysis confirms...

</details>


### [304] [Learning Consistent Causal Abstraction Networks](https://arxiv.org/abs/2602.02623)
*Gabriele D'Acunto,Paolo Di Lorenzo,Sergio Barbarossa*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Causal artificial intelligence aims to enhance explainability, trustworthiness, and robustness in AI by leveraging structural causal models (SCMs). In this pursuit, recent advances formalize network sheaves and cosheaves of causal knowledge. Pushing in the same direction, we tackle the learning of consistent causal abstraction network (CAN), a sheaf-theoretic framework where (i) SCMs are Gaussian, (ii) restriction maps are transposes of constructive linear causal abstractions (CAs) adhering to the semantic embedding principle, and (iii) edge stalks correspond--up to permutation--to the node stalks of more detailed SCMs. Our problem formulation separates into edge-specific local Riemannian problems and avoids nonconvex objectives. We propose an efficient search procedure, solving the local problems with SPECTRAL, our iterative method with closed-form updates and suitable for positive definite and semidefinite covariance matrices. Experiments on synthetic data show competitive performance in the CA learning task, and successful recovery of diverse CAN structures.

</details>


### [305] [Learning Better Certified Models from Empirically-Robust Teachers](https://arxiv.org/abs/2602.02626)
*Alessandro De Palma*

Main category: cs.LG

TL;DR: The paper explores improving certified robustness in neural networks by distilling knowledge from adversarially-trained teachers, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Current methods either focus on empirical robustness (adversarial training) or certified robustness (verified training), but lack a balance. Empirical robustness lacks strong certificates, while certified robustness harms standard performance. This work aims to bridge the gap.

Method: Proposes knowledge distillation from adversarially-trained teachers to certifiably-robust models using a feature-space distillation objective.

Result: Demonstrates consistent improvements in certified training benchmarks for ReLU networks, achieving state-of-the-art performance.

Conclusion: Distillation from empirically-robust teachers effectively enhances certified robustness without sacrificing standard performance.

Abstract: Adversarial training attains strong empirical robustness to specific adversarial attacks by training on concrete adversarial perturbations, but it produces neural networks that are not amenable to strong robustness certificates through neural network verification. On the other hand, earlier certified training schemes directly train on bounds from network relaxations to obtain models that are certifiably robust, but display sub-par standard performance. Recent work has shown that state-of-the-art trade-offs between certified robustness and standard performance can be obtained through a family of losses combining adversarial outputs and neural network bounds. Nevertheless, differently from empirical robustness, verifiability still comes at a significant cost in standard performance. In this work, we propose to leverage empirically-robust teachers to improve the performance of certifiably-robust models through knowledge distillation. Using a versatile feature-space distillation objective, we show that distillation from adversarially-trained teachers consistently improves on the state-of-the-art in certified training for ReLU networks across a series of robust computer vision benchmarks.

</details>


### [306] [Performance of Small Language Model Pretraining on FABRIC: An Empirical Study](https://arxiv.org/abs/2602.02632)
*Praveen Rao*

Main category: cs.LG

TL;DR: The paper explores efficient pretraining techniques for smaller-sized LLMs using commodity GPUs, optimizing parallelism strategies to handle geographic distribution and network latency, proposing a systematic approach for high performance and resource efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of pretraining large language models (LLMs) with limited datasets and computing resources, particularly for academic users with access to commodity GPUs.

Method: The study experiments with data parallelism, intra-operator parallelism, and inter-operator/pipeline parallelism on homogeneous and heterogeneous GPU clusters, testing GPT-2 medium and large models using Alpa and Ray.

Result: Alpa's execution plans optimizing intra-operator and inter-operator/pipeline parallelism performed best under geographic distribution and network latencies (~10ms).

Conclusion: A systematic approach is proposed for selecting pretraining techniques to maximize performance and minimize GPU usage, especially in distributed environments.

Abstract: Large language models (LLMs) require enormous computing power to pretrain on massive datasets. When limited datasets are available, smaller-sized LLMs are better choice to pretrain (on user-specified datasets) by following the scaling laws of LLMs. Using pretrained models, vector embeddings can be generated for raw data and stored using vector databases to support modern AI applications and semantic search. In this work, we investigate the performance of pretraining techniques for smaller-sized LLMs on an experimental testbed (with commodity GPUs) available to academic users at no charge. We consider data parallelism, intra-operator parallelism, and inter-operator/pipeline parallelism, and their combinations for pretraining. We set up different GPU clusters with homogeneous and heterogeneous GPU hardware. Furthermore, we investigate the impact of network latency on pretraining performance especially when GPUs are geographically distributed. We used GPT-2 medium and large models and pretrained them using open-source packages, namely, Alpa and Ray. We observed that Alpa's execution plans that collectively optimized intra-operator and inter-operator/pipeline parallelism consistently performed the best when GPUs were geographically distributed. This was especially true when the network latencies were in 10's of milliseconds. Based on the insights gained from the experiments, we propose a systematic approach for selecting the appropriate pretraining technique to achieve high training performance/lower execution time as well as to reduce the number of GPUs used.

</details>


### [307] [A Reduction from Delayed to Immediate Feedback for Online Convex Optimization with Improved Guarantees](https://arxiv.org/abs/2602.02634)
*Alexander Ryabchenko,Idan Attias,Daniel M. Roy*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We develop a reduction-based framework for online learning with delayed feedback that recovers and improves upon existing results for both first-order and bandit convex optimization. Our approach introduces a continuous-time model under which regret decomposes into a delay-independent learning term and a delay-induced drift term, yielding a delay-adaptive reduction that converts any algorithm for online linear optimization into one that handles round-dependent delays. For bandit convex optimization, we significantly improve existing regret bounds, with delay-dependent terms matching state-of-the-art first-order rates. For first-order feedback, we recover state-of-the-art regret bounds via a simpler, unified analysis. Quantitatively, for bandit convex optimization we obtain $O(\sqrt{d_{\text{tot}}} + T^{\frac{3}{4}}\sqrt{k})$ regret, improving the delay-dependent term from $O(\min\{\sqrt{T d_{\text{max}}},(Td_{\text{tot}})^{\frac{1}{3}}\})$ in previous work to $O(\sqrt{d_{\text{tot}}})$. Here, $k$, $T$, $d_{\text{max}}$, and $d_{\text{tot}}$ denote the dimension, time horizon, maximum delay, and total delay, respectively. Under strong convexity, we achieve $O(\min\{σ_{\text{max}} \ln T, \sqrt{d_{\text{tot}}}\} + (T^2\ln T)^{\frac{1}{3}} {k}^{\frac{2}{3}})$, improving the delay-dependent term from $O(d_{\text{max}} \ln T)$ in previous work to $O(\min\{σ_{\text{max}} \ln T, \sqrt{d_{\text{tot}}}\})$, where $σ_{\text{max}}$ denotes the maximum number of outstanding observations and may be considerably smaller than $d_{\text{max}}$.

</details>


### [308] [hSNMF: Hybrid Spatially Regularized NMF for Image-Derived Spatial Transcriptomics](https://arxiv.org/abs/2602.02638)
*Md Ishtyaq Mahmud,Veena Kochat,Suresh Satpati,Jagan Mohan Reddy Dwarampudi,Humaira Anzum,Kunal Rai,Tania Banerjee*

Main category: cs.LG

TL;DR: The paper introduces Spatial NMF (SNMF) and Hybrid Spatial NMF (hSNMF) for spatial transcriptomics data, improving spatial compactness, cluster separability, and biological coherence.


<details>
  <summary>Details</summary>
Motivation: High-resolution spatial transcriptomics data poses challenges for representation learning and clustering, prompting the development of spatially regularized methods.

Method: The authors propose SNMF for local spatial smoothness and hSNMF combining spatial proximity and transcriptomic similarity, followed by Leiden clustering.

Result: SNMF and hSNMF outperform baselines in spatial compactness, cluster separability, and biological coherence on cholangiocarcinoma data.

Conclusion: The methods enhance spatial transcriptomics analysis, offering practical improvements for representation learning and clustering.

Abstract: High-resolution spatial transcriptomics platforms, such as Xenium, generate single-cell images that capture both molecular and spatial context, but their extremely high dimensionality poses major challenges for representation learning and clustering. In this study, we analyze data from the Xenium platform, which captures high-resolution images of tumor microarray (TMA) tissues and converts them into cell-by-gene matrices suitable for computational analysis. We benchmark and extend nonnegative matrix factorization (NMF) for spatial transcriptomics by introducing two spatially regularized variants. First, we propose Spatial NMF (SNMF), a lightweight baseline that enforces local spatial smoothness by diffusing each cell's NMF factor vector over its spatial neighborhood. Second, we introduce Hybrid Spatial NMF (hSNMF), which performs spatially regularized NMF followed by Leiden clustering on a hybrid adjacency that integrates spatial proximity (via a contact-radius graph) and transcriptomic similarity through a tunable mixing parameter alpha. Evaluated on a cholangiocarcinoma dataset, SNMF and hSNMF achieve markedly improved spatial compactness (CHAOS < 0.004, Moran's I > 0.96), greater cluster separability (Silhouette > 0.12, DBI < 1.8), and higher biological coherence (CMC and enrichment) compared to other spatial baselines. Availability and implementation: https://github.com/ishtyaqmahmud/hSNMF

</details>


### [309] [MARA: Continuous SE(3)-Equivariant Attention for Molecular Force Fields](https://arxiv.org/abs/2602.02671)
*Francesco Leonardi,Boris Bonev,Kaspar Riesen*

Main category: cs.LG

TL;DR: MARA (Modular Angular-Radial Attention) enhances MLFFs by introducing flexible, geometrically informed weighting of atomic interactions, improving accuracy and robustness in molecular modeling.


<details>
  <summary>Details</summary>
Motivation: Existing MLFFs use fixed angular expansions, limiting flexibility in modeling local geometric interactions. MARA aims to address this by providing a more adaptable and efficient approach.

Method: MARA extends spherical attention to molecular domains and SE(3), operating on angular and radial coordinates of neighboring atoms. It allows modular and flexible weighting of local environments.

Result: MARA improves energy and force predictions, reduces high-error events, and enhances the robustness of atomistic models in molecular benchmarks.

Conclusion: MARA demonstrates that continuous spherical attention is an effective geometric operator, increasing the expressiveness, stability, and reliability of MLFFs.

Abstract: Machine learning force fields (MLFFs) have become essential for accurate and efficient atomistic modeling. Despite their high accuracy, most existing approaches rely on fixed angular expansions, limiting flexibility in weighting local geometric interactions. We introduce Modular Angular-Radial Attention (MARA), a module that extends spherical attention -- originally developed for SO(3) tasks -- to the molecular domain and SE(3), providing an efficient approximation of equivariant interactions. MARA operates directly on the angular and radial coordinates of neighboring atoms, enabling flexible, geometrically informed, and modular weighting of local environments. Unlike existing attention mechanisms in SE(3)-equivariant architectures, MARA can be integrated in a plug-and-play manner into models such as MACE without architectural modifications. Across molecular benchmarks, MARA improves energy and force predictions, reduces high-error events, and enhances robustness. These results demonstrate that continuous spherical attention is an effective and generalizable geometric operator that increases the expressiveness, stability, and reliability of atomistic models.

</details>


### [310] [FlexRank: Nested Low-Rank Knowledge Decomposition for Adaptive Model Deployment](https://arxiv.org/abs/2602.02680)
*Riccardo Zaccone,Stefanos Laskaridis,Marco Ciccone,Samuel Horváth*

Main category: cs.LG

TL;DR: FlexRank introduces a method to extract nested submodels from pretrained large neural networks, enabling adaptive deployment across varying computational budgets without retraining.


<details>
  <summary>Details</summary>
Motivation: The high cost and rigidity of deploying large-scale neural networks like LLMs and ViTs necessitate adaptive solutions to leverage their overparametrized architectures efficiently.

Method: FlexRank uses low-rank weight decomposition with nested, importance-based consolidation to create submodels of varying capabilities from pretrained models.

Result: The approach allows for flexible deployment with a trade-off between computational cost and performance, eliminating the need for retraining for each budget.

Conclusion: FlexRank advances practical deployment by enabling "train-once, deploy-everywhere" adaptability for large models.

Abstract: The growing scale of deep neural networks, encompassing large language models (LLMs) and vision transformers (ViTs), has made training from scratch prohibitively expensive and deployment increasingly costly. These models are often used as computational monoliths with fixed cost, a rigidity that does not leverage overparametrized architectures and largely hinders adaptive deployment across different cost budgets. We argue that importance-ordered nested components can be extracted from pretrained models, and selectively activated on the available computational budget. To this end, our proposed FlexRank method leverages low-rank weight decomposition with nested, importance-based consolidation to extract submodels of increasing capabilities. Our approach enables a "train-once, deploy-everywhere" paradigm that offers a graceful trade-off between cost and performance without training from scratch for each budget - advancing practical deployment of large models.

</details>


### [311] [Expert-Data Alignment Governs Generation Quality in Decentralized Diffusion Models](https://arxiv.org/abs/2602.02685)
*Marcos Villagra,Bidhan Roy,Raihan Seraj,Zhiying Jiang*

Main category: cs.LG

TL;DR: DDMs' generation quality is governed by expert-data alignment, not stability metrics, contrary to initial expectations.


<details>
  <summary>Details</summary>
Motivation: To investigate what governs the quality of generations in DDMs, challenging the assumption that minimizing denoising trajectory sensitivity ensures better results.

Method: Analyzed two DDM systems using data-cluster distance, per-expert accuracy, and expert disagreement analyses.

Result: Ensemble routing was most stable but had poor generation quality (FID 47.9). Sparse Top-2 routing, aligning inputs with experts' training data, performed best (FID 22.6).

Conclusion: DDMs should prioritize expert-data alignment over stability metrics for optimal generation quality.

Abstract: Decentralized Diffusion Models (DDMs) route denoising through experts trained independently on disjoint data clusters, which can strongly disagree in their predictions. What governs the quality of generations in such systems? We present the first ever systematic investigation of this question. A priori, the expectation is that minimizing denoising trajectory sensitivity -- minimizing how perturbations amplify during sampling -- should govern generation quality. We demonstrate this hypothesis is incorrect: a stability-quality dissociation. Full ensemble routing, which combines all expert predictions at each step, achieves the most stable sampling dynamics and best numerical convergence while producing the worst generation quality (FID 47.9 vs. 22.6 for sparse Top-2 routing). Instead, we identify expert-data alignment as the governing principle: generation quality depends on routing inputs to experts whose training distribution covers the current denoising state. Across two distinct DDM systems, we validate expert-data alignment using (i) data-cluster distance analysis, confirming sparse routing selects experts with data clusters closest to the current denoising state, and (ii) per-expert analysis, showing selected experts produce more accurate predictions than non-selected ones, and (iii) expert disagreement analysis, showing quality degrades when experts disagree. For DDM deployment, our findings establish that routing should prioritize expert-data alignment over numerical stability metrics.

</details>


### [312] [Sparsely Supervised Diffusion](https://arxiv.org/abs/2602.02699)
*Wenshuai Zhao,Zhiyuan Li,Yi Zhao,Mohammad Hassan Vali,Martin Trapp,Joni Pajarinen,Juho Kannala,Arno Solin*

Main category: cs.LG

TL;DR: The paper proposes a sparsely supervised learning method for diffusion models using a masking strategy to address spatially inconsistent generation, showing effectiveness even with high masking ratios.


<details>
  <summary>Details</summary>
Motivation: Diffusion models often produce spatially inconsistent samples due to localized denoising, leading to globally implausible outputs.

Method: Introduces a sparse masking strategy during training, masking up to 98% of pixels to promote global consistency.

Result: Achieves competitive FID scores, avoids training instability on small datasets, reduces memorization, and enhances contextual information use.

Conclusion: The masking strategy effectively improves diffusion models' global consistency and training stability.

Abstract: Diffusion models have shown remarkable success across a wide range of generative tasks. However, they often suffer from spatially inconsistent generation, arguably due to the inherent locality of their denoising mechanisms. This can yield samples that are locally plausible but globally inconsistent. To mitigate this issue, we propose sparsely supervised learning for diffusion models, a simple yet effective masking strategy that can be implemented with only a few lines of code. Interestingly, the experiments show that it is safe to mask up to 98\% of pixels during diffusion model training. Our method delivers competitive FID scores across experiments and, most importantly, avoids training instability on small datasets. Moreover, the masking strategy reduces memorization and promotes the use of essential contextual information during generation.

</details>


### [313] [Every Bit Counts: A Theoretical Study of Precision-Expressivity Tradeoffs in Quantized Transformers](https://arxiv.org/abs/2602.02707)
*Sayak Chakrabarti,Toniann Pitassi,Josh Alman*

Main category: cs.LG

TL;DR: The paper explores the tradeoff between expressivity and precision in quantized Transformers, showing that certain functions require a minimum bit precision and dropping even one bit can render the model incapable of performing equality-like tasks.


<details>
  <summary>Details</summary>
Motivation: Quantization is widely used to accelerate Transformer inference, but its impact on expressivity is not well understood. This study aims to theoretically characterize this tradeoff.

Method: The authors introduce a function inspired by equality and prove that a one-layer softmax Transformer can compute it with p bits of precision but not p-1 bits, using constructions and communication-complexity bounds.

Result: Results show that tasks requiring equality-like comparisons are highly sensitive to quantization, with a tight one-bit threshold between success and failure.

Conclusion: The findings suggest practical heuristics for choosing quantization precision based on task requirements, especially for equality-related tasks.

Abstract: Quantization reduces the numerical precision of Transformer computations and is widely used to accelerate inference, yet its effect on expressivity remains poorly characterized. We demonstrate a fine-grained theoretical tradeoff between expressivity and precision: For every p we exhibit a function Γ, inspired by the equality function, and prove that a one-layer softmax Transformer can compute Γ, with p bits of precision, but not with p-1 bits of precision.
  This result concretely explains the widely observed phenomenon of empirical loss of expressivity when quantization is used. Practically, it suggests that tasks requiring equality-like comparisons (exact match, membership, etc.) are especially sensitive to quantization. Dropping even one bit can cross a threshold where the model cannot represent the needed comparison reliably. Thus, it paves the way for developing heuristics that will help practitioners choose how much quantization is possible: the precision should be chosen as a function of the length of equality to be checked for the specific task.
  Our proofs combine explicit finite-precision Transformer constructions with communication-complexity lower bounds, yielding a tight "one-bit" threshold.

</details>


### [314] [BinaryPPO: Efficient Policy Optimization for Binary Classification](https://arxiv.org/abs/2602.02708)
*Punya Syon Pandey,Zhijing Jin*

Main category: cs.LG

TL;DR: BinaryPPO introduces an offline reinforcement learning framework for binary classification, outperforming supervised fine-tuning (SFT) by 40-60% accuracy.


<details>
  <summary>Details</summary>
Motivation: SFT struggles with label noise, class imbalance, and sparse supervision in binary classification tasks.

Method: BinaryPPO reformulates classification as reward maximization using PPO with confidence-weighted rewards.

Result: Achieves up to 99% accuracy across benchmarks, significantly surpassing SFT baselines.

Conclusion: Confidence-based reward design offers a robust alternative to SFT for binary classification.

Abstract: Supervised fine-tuning (SFT) is the standard approach for binary classification tasks such as toxicity detection, factuality verification, and causal inference. However, SFT often performs poorly in real-world settings with label noise, class imbalance, or sparse supervision. We introduce BinaryPPO, an offline reinforcement learning large language model (LLM) framework that reformulates binary classification as a reward maximization problem. Our method leverages a variant of Proximal Policy Optimization (PPO) with a confidence-weighted reward function that penalizes uncertain or incorrect predictions, enabling the model to learn robust decision policies from static datasets without online interaction. Across eight domain-specific benchmarks and multiple models with differing architectures, BinaryPPO improves accuracy by 40-60 percentage points, reaching up to 99%, substantially outperforming supervised baselines. We provide an in-depth analysis of the role of reward shaping, advantage scaling, and policy stability in enabling this improvement. Overall, we demonstrate that confidence-based reward design provides a robust alternative to SFT for binary classification. Our code is available at https://github.com/psyonp/BinaryPPO.

</details>


### [315] [Maximum Likelihood Reinforcement Learning](https://arxiv.org/abs/2602.02710)
*Fahim Tajwar,Guanning Zeng,Yueer Zhou,Yuda Song,Daman Arora,Yiding Jiang,Jeff Schneider,Ruslan Salakhutdinov,Haiwen Feng,Andrea Zanette*

Main category: cs.LG

TL;DR: MaxRL introduces a sampling-based framework to approximate maximum likelihood using RL techniques, outperforming existing methods in efficiency and scalability.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning (RL) optimizes a lower-order approximation of the likelihood over correct rollouts, which MaxRL aims to improve.

Method: MaxRL defines a compute-indexed family of sample-based objectives, interpolating between standard RL and exact maximum likelihood, using unbiased policy-gradient estimators.

Result: MaxRL Pareto-dominates existing methods, achieving up to 20x efficiency gains and better scaling with data/compute.

Conclusion: MaxRL is a promising framework for improving RL training scalability in correctness-based settings.

Abstract: Reinforcement learning is the method of choice to train models in sampling-based setups with binary outcome feedback, such as navigation, code generation, and mathematical problem solving. In such settings, models implicitly induce a likelihood over correct rollouts. However, we observe that reinforcement learning does not maximize this likelihood, and instead optimizes only a lower-order approximation. Inspired by this observation, we introduce Maximum Likelihood Reinforcement Learning (MaxRL), a sampling-based framework to approximate maximum likelihood using reinforcement learning techniques. MaxRL addresses the challenges of non-differentiable sampling by defining a compute-indexed family of sample-based objectives that interpolate between standard reinforcement learning and exact maximum likelihood as additional sampling compute is allocated. The resulting objectives admit a simple, unbiased policy-gradient estimator and converge to maximum likelihood optimization in the infinite-compute limit. Empirically, we show that MaxRL Pareto-dominates existing methods in all models and tasks we tested, achieving up to 20x test-time scaling efficiency gains compared to its GRPO-trained counterpart. We also observe MaxRL to scale better with additional data and compute. Our results suggest MaxRL is a promising framework for scaling RL training in correctness based settings.

</details>


### [316] [Towards Understanding Steering Strength](https://arxiv.org/abs/2602.02712)
*Magamed Taimeskhanov,Samuel Vaiter,Damien Garreau*

Main category: cs.LG

TL;DR: The paper analyzes the impact of steering strength on LLM behavior, revealing non-monotonic effects and validating findings across various models.


<details>
  <summary>Details</summary>
Motivation: To understand how the magnitude of steering affects LLM performance, as improper steering can degrade model outputs.

Method: Theoretical analysis of steering strength's effect on token probability, concept presence, and cross-entropy, validated empirically on eleven models.

Result: Identified non-monotonic effects of steering strength and derived qualitative laws governing its impact.

Conclusion: Steering strength significantly influences LLM behavior, with theoretical insights confirmed across diverse models.

Abstract: A popular approach to post-training control of large language models (LLMs) is the steering of intermediate latent representations. Namely, identify a well-chosen direction depending on the task at hand and perturbs representations along this direction at inference time. While many propositions exist to pick this direction, considerably less is understood about how to choose the magnitude of the move, whereas its importance is clear: too little and the intended behavior does not emerge, too much and the model's performance degrades beyond repair. In this work, we propose the first theoretical analysis of steering strength. We characterize its effect on next token probability, presence of a concept, and cross-entropy, deriving precise qualitative laws governing these quantities. Our analysis reveals surprising behaviors, including non-monotonic effects of steering strength. We validate our theoretical predictions empirically on eleven language models, ranging from a small GPT architecture to modern models.

</details>


### [317] [Neural Probabilistic Amplitude Shaping for Nonlinear Fiber Channels](https://arxiv.org/abs/2602.02716)
*Mohammad Taha Askari,Lutz Lampe,Amirhossein Ghazisaeidi*

Main category: cs.LG

TL;DR: Neural probabilistic amplitude shaping improves SNR in coherent fiber systems.


<details>
  <summary>Details</summary>
Motivation: To enhance signal-to-noise ratio (SNR) in coherent fiber communication systems.

Method: Uses a joint-distribution learning framework called neural probabilistic amplitude shaping.

Result: Achieves a 0.5 dB SNR gain over sequence selection for dual-polarized 64-QAM transmission over 205 km.

Conclusion: The proposed method effectively boosts performance in fiber optic communication systems.

Abstract: We introduce neural probabilistic amplitude shaping, a joint-distribution learning framework for coherent fiber systems. The proposed scheme provides a 0.5 dB signal-to-noise ratio gain over sequence selection for dual-polarized 64-QAM transmission across a single-span 205 km link.

</details>


### [318] [Hierarchical Entity-centric Reinforcement Learning with Factored Subgoal Diffusion](https://arxiv.org/abs/2602.02722)
*Dan Haramati,Carl Qi,Tal Daniel,Amy Zhang,Aviv Tamar,George Konidaris*

Main category: cs.LG

TL;DR: A hierarchical framework combining subgoal decomposition and factored structure is proposed for offline GCRL to tackle long-horizon tasks in multi-entity domains, significantly improving success rates.


<details>
  <summary>Details</summary>
Motivation: Long-horizon goals in complex environments with multiple entities pose a challenge due to combinatorial complexity, sparse rewards, and high-dimensional observations.

Method: A two-level hierarchy is used: a value-based GCRL agent and a factored subgoal-generating conditional diffusion model, trained independently and composed post hoc.

Result: The method boosts performance on image-based long-horizon tasks with sparse rewards, achieving over 150% higher success rates on the hardest task and generalizing well.

Conclusion: The proposed framework effectively addresses challenges in offline GCRL for multi-entity domains, demonstrating modularity and compatibility with existing algorithms.

Abstract: We propose a hierarchical entity-centric framework for offline Goal-Conditioned Reinforcement Learning (GCRL) that combines subgoal decomposition with factored structure to solve long-horizon tasks in domains with multiple entities. Achieving long-horizon goals in complex environments remains a core challenge in Reinforcement Learning (RL). Domains with multiple entities are particularly difficult due to their combinatorial complexity. GCRL facilitates generalization across goals and the use of subgoal structure, but struggles with high-dimensional observations and combinatorial state-spaces, especially under sparse reward. We employ a two-level hierarchy composed of a value-based GCRL agent and a factored subgoal-generating conditional diffusion model. The RL agent and subgoal generator are trained independently and composed post hoc through selective subgoal generation based on the value function, making the approach modular and compatible with existing GCRL algorithms. We introduce new variations to benchmark tasks that highlight the challenges of multi-entity domains, and show that our method consistently boosts performance of the underlying RL agent on image-based long-horizon tasks with sparse rewards, achieving over 150% higher success rates on the hardest task in our suite and generalizing to increasing horizons and numbers of entities. Rollout videos are provided at: https://sites.google.com/view/hecrl

</details>


### [319] [Automated Dysphagia Screening Using Noninvasive Neck Acoustic Sensing](https://arxiv.org/abs/2602.02725)
*Jade Chng,Rong Xing,Yunfei Luo,Kristen Linnemeyer-Risser,Tauhidur Rahman,Andrew Yousef,Philip A Weissbrod*

Main category: cs.LG

TL;DR: Proposes a noninvasive, machine learning-based acoustic sensing method for early dysphagia detection, achieving an AUC-ROC of 0.904.


<details>
  <summary>Details</summary>
Motivation: Early detection of swallowing abnormalities (dysphagia) is critical but current methods are invasive or rely on imaging.

Method: Uses portable acoustic sensing to capture neck signals during swallowing, analyzed with machine learning.

Result: Achieves promising performance with an AUC-ROC of 0.904 under independent testing.

Conclusion: Demonstrates feasibility of noninvasive acoustic sensing for scalable pharyngeal health monitoring.

Abstract: Pharyngeal health plays a vital role in essential human functions such as breathing, swallowing, and vocalization. Early detection of swallowing abnormalities, also known as dysphagia, is crucial for timely intervention. However, current diagnostic methods often rely on radiographic imaging or invasive procedures. In this study, we propose an automated framework for detecting dysphagia using portable and noninvasive acoustic sensing coupled with applied machine learning. By capturing subtle acoustic signals from the neck during swallowing tasks, we aim to identify patterns associated with abnormal physiological conditions. Our approach achieves promising test-time abnormality detection performance, with an AUC-ROC of 0.904 under 5 independent train-test splits. This work demonstrates the feasibility of using noninvasive acoustic sensing as a practical and scalable tool for pharyngeal health monitoring.

</details>


### [320] [Vector Quantized Latent Concepts: A Scalable Alternative to Clustering-Based Concept Discovery](https://arxiv.org/abs/2602.02726)
*Xuemin Yu,Ankur Garg,Samira Ebrahimi Kahou,Hassan Sajjad*

Main category: cs.LG

TL;DR: VQLC is a scalable method for clustering token representations in deep learning models, offering human-understandable explanations efficiently.


<details>
  <summary>Details</summary>
Motivation: Understanding which parts of semantic information deep learning models rely on for predictions is challenging. Existing methods like hierarchical clustering are computationally heavy, and K-Means often yields poor clusters.

Method: Proposes VQLC, a framework using VQ-VAE to map continuous representations to discrete concept vectors.

Result: VQLC improves scalability while maintaining explanation quality comparable to existing methods.

Conclusion: VQLC offers a feasible and effective solution for concept-based explanations in large-scale datasets.

Abstract: Deep Learning models encode rich semantic information in their hidden representations. However, it remains challenging to understand which parts of this information models actually rely on when making predictions. A promising line of post-hoc concept-based explanation methods relies on clustering token representations. However, commonly used approaches such as hierarchical clustering are computationally infeasible for large-scale datasets, and K-Means often yields shallow or frequency-dominated clusters. We propose the vector quantized latent concept (VQLC) method, a framework built upon the vector quantized-variational autoencoder (VQ-VAE) architecture that learns a discrete codebook mapping continuous representations to concept vectors. We perform thorough evaluations and show that VQLC improves scalability while maintaining comparable quality of human-understandable explanations.

</details>


### [321] [RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System](https://arxiv.org/abs/2602.02488)
*Yinjie Wang,Tianbao Xie,Ke Shen,Mengdi Wang,Ling Yang*

Main category: cs.LG

TL;DR: RLAnything is a reinforcement learning framework enhancing LLMs and agents by dynamically optimizing environment, policy, and reward models through closed-loop feedback.


<details>
  <summary>Details</summary>
Motivation: To amplify learning signals and strengthen RL systems for LLMs and agentic scenarios by integrating dynamic feedback loops.

Method: Uses step-wise and outcome signals for policy training, jointly optimizes reward models via consistency feedback, and leverages critic feedback for environment adaptation.

Result: Substantial performance gains (e.g., 9.1-18.7%) on tasks like OSWorld and AlfWorld; optimized reward models outperform human-labeled outcomes.

Conclusion: RLAnything effectively enhances RL systems through integrated feedback loops, demonstrating significant improvements across diverse tasks.

Abstract: We propose RLAnything, a reinforcement learning framework that dynamically forges environment, policy, and reward models through closed-loop optimization, amplifying learning signals and strengthening the overall RL system for any LLM or agentic scenarios. Specifically, the policy is trained with integrated feedback from step-wise and outcome signals, while the reward model is jointly optimized via consistency feedback, which in turn further improves policy training. Moreover, our theory-motivated automatic environment adaptation improves training for both the reward and policy models by leveraging critic feedback from each, enabling learning from experience. Empirically, each added component consistently improves the overall system, and RLAnything yields substantial gains across various representative LLM and agentic tasks, boosting Qwen3-VL-8B-Thinking by 9.1% on OSWorld and Qwen2.5-7B-Instruct by 18.7% and 11.9% on AlfWorld and LiveBench, respectively. We also that optimized reward-model signals outperform outcomes that rely on human labels. Code: https://github.com/Gen-Verse/Open-AgentRL

</details>


### [322] [Search-Augmented Masked Diffusion Models for Constrained Generation](https://arxiv.org/abs/2602.02727)
*Huu Binh Ta,Michael Cardei,Alvaro Velasquez,Ferdinando Fioretto*

Main category: cs.LG

TL;DR: SearchDiff integrates neurosymbolic search into discrete diffusion models to improve constraint satisfaction and property adherence during generation without additional training.


<details>
  <summary>Details</summary>
Motivation: Current discrete diffusion models lack mechanisms for enforcing hard constraints or optimizing non-differentiable properties at inference time.

Method: Introduces SearchDiff, a training-free framework that combines informed search with reverse denoising to steer sampling toward feasible solutions.

Result: SearchDiff outperforms standard discrete diffusion and autoregressive models in biological design and symbolic reasoning tasks.

Conclusion: SearchDiff effectively enhances the feasibility and constraint satisfaction of generated sequences in symbolic and structured domains.

Abstract: Discrete diffusion models generate sequences by iteratively denoising samples corrupted by categorical noise, offering an appealing alternative to autoregressive decoding for structured and symbolic generation. However, standard training targets a likelihood-based objective that primarily matches the data distribution and provides no native mechanism for enforcing hard constraints or optimizing non-differentiable properties at inference time. This work addresses this limitation and introduces Search-Augmented Masked Diffusion (SearchDiff), a training-free neurosymbolic inference framework that integrates informed search directly into the reverse denoising process. At each denoising step, the model predictions define a proposal set that is optimized under a user-specified property satisfaction, yielding a modified reverse transition that steers sampling toward probable and feasible solutions. Experiments in biological design and symbolic reasoning illustrate that SearchDiff substantially improves constraint satisfaction and property adherence, while consistently outperforming discrete diffusion and autoregressive baselines.

</details>


### [323] [CAPS: Unifying Attention, Recurrence, and Alignment in Transformer-based Time Series Forecasting](https://arxiv.org/abs/2602.02729)
*Viresh Pati,Yubin Kim,Vinh Pham,Jevon Twitty,Shihao Yang,Jiecheng Lu*

Main category: cs.LG

TL;DR: CAPS is a structured attention mechanism for time series forecasting that decouples global trends, local shocks, and seasonal patterns using SO(2) rotations and additive gating paths. It outperforms standard softmax and linear attention with competitive results and linear complexity.


<details>
  <summary>Details</summary>
Motivation: Standard softmax attention entangles temporal structures, while recurrent models compromise long-term selection. CAPS aims to decouple these structures for better forecasting.

Method: CAPS uses SO(2) rotations for phase alignment and three additive gating paths (Riemann softmax, prefix-product gates, Clock baseline) within a single attention layer, modulated by the Clock mechanism.

Result: CAPS surpasses vanilla softmax and linear attention mechanisms, achieving competitive performance on benchmarks with linear complexity.

Conclusion: CAPS effectively decouples temporal structures, offering improved forecasting performance and scalability.

Abstract: This paper presents $\textbf{CAPS}$ (Clock-weighted Aggregation with Prefix-products and Softmax), a structured attention mechanism for time series forecasting that decouples three distinct temporal structures: global trends, local shocks, and seasonal patterns. Standard softmax attention entangles these through global normalization, while recent recurrent models sacrifice long-term, order-independent selection for order-dependent causal structure. CAPS combines SO(2) rotations for phase alignment with three additive gating paths -- Riemann softmax, prefix-product gates, and a Clock baseline -- within a single attention layer. We introduce the Clock mechanism, a learned temporal weighting that modulates these paths through a shared notion of temporal importance. Experiments on long- and short-term forecasting benchmarks surpass vanilla softmax and linear attention mechanisms and demonstrate competitive performance against seven strong baselines with linear complexity. Our code implementation is available at https://github.com/vireshpati/CAPS-Attention.

</details>


### [324] [TabPFN for Zero-shot Parametric Engineering Design Generation](https://arxiv.org/abs/2602.02735)
*Ke Wang,Yifan Tang,Nguyen Gia Hien Vu,Faez Ahmed,G. Gary Wang*

Main category: cs.LG

TL;DR: A zero-shot generation framework for parametric engineering design using TabPFN is proposed, requiring minimal data and no training, achieving competitive diversity and robustness with low performance error.


<details>
  <summary>Details</summary>
Motivation: Traditional deep generative models for engineering design are computationally expensive, require large datasets, and extensive retraining, limiting practical use.

Method: The proposed framework uses TabPFN for zero-shot conditional design generation, sequentially producing parameters based on target performance indicators without task-specific training.

Result: Tested on ship hull, BlendedNet aircraft, and UIUC airfoil datasets, the method shows competitive diversity, robustness, and low performance error (e.g., <2% for ship hulls), outperforming diffusion models in efficiency.

Conclusion: The zero-shot framework offers a practical, efficient alternative for engineering design, enabling rapid deployment and adaptability with minimal computational and data requirements.

Abstract: Deep generative models for engineering design often require substantial computational cost, large training datasets, and extensive retraining when design requirements or datasets change, limiting their applicability in real-world engineering design workflow. In this work, we propose a zero-shot generation framework for parametric engineering design based on TabPFN, enabling conditional design generation using only a limited number of reference samples and without any task-specific model training or fine-tuning. The proposed method generates design parameters sequentially conditioned on target performance indicators, providing a flexible alternative to conventional generative models. The effectiveness of the proposed approach is evaluated on three engineering design datasets, i.e., ship hull design, BlendedNet aircraft, and UIUC airfoil. Experimental results demonstrate that the proposed method achieves competitive diversity across highly structured parametric design spaces, remains robust to variations in sampling, resolution and parameter dimensionality of geometry generation, and achieves a low performance error (e.g., less than 2% in generated ship hull designs' performance). Compared with diffusion-based generative models, the proposed framework significantly reduces computational overhead and data requirements while preserving reliable generation performance. These results highlight the potential of zero-shot, data-efficient generation as a practical and efficient tool for engineering design, enabling rapid deployment, flexible adaptation to new design settings, and ease of integration into real-world engineering workflows.

</details>


### [325] [TopoPrune: Robust Data Pruning via Unified Latent Space Topology](https://arxiv.org/abs/2602.02739)
*Arjun Roy,Prajna G. Malettira,Manish Nagaraj,Kaushik Roy*

Main category: cs.LG

TL;DR: TopoPrune is a topology-based framework for stable and robust geometric data pruning, addressing instability issues in traditional methods by leveraging intrinsic data structure.


<details>
  <summary>Details</summary>
Motivation: Traditional geometric data pruning methods are unstable due to reliance on extrinsic geometry, leading to degraded performance under perturbations or cross-architecture transfers.

Method: TopoPrune uses a dual-scale approach: (1) a topology-aware manifold approximation for global embedding, and (2) differentiable persistent homology for local optimization, ranking samples by structural complexity.

Result: TopoPrune achieves high accuracy and precision even at high pruning rates (e.g., 90%), with robustness to noise and superior transferability across architectures.

Conclusion: TopoPrune offers a stable and principled topology-based solution for robust data-efficient learning.

Abstract: Geometric data pruning methods, while practical for leveraging pretrained models, are fundamentally unstable. Their reliance on extrinsic geometry renders them highly sensitive to latent space perturbations, causing performance to degrade during cross-architecture transfer or in the presence of feature noise. We introduce TopoPrune, a framework which resolves this challenge by leveraging topology to capture the stable, intrinsic structure of data. TopoPrune operates at two scales, (1) utilizing a topology-aware manifold approximation to establish a global low-dimensional embedding of the dataset. Subsequently, (2) it employs differentiable persistent homology to perform a local topological optimization on the manifold embeddings, ranking samples by their structural complexity. We demonstrate that our unified dual-scale topological approach ensures high accuracy and precision, particularly at significant dataset pruning rates (e.g., 90%). Furthermore, through the inherent stability properties of topology, TopoPrune is (a) exceptionally robust to noise perturbations of latent feature embeddings and (b) demonstrates superior transferability across diverse network architectures. This study demonstrates a promising avenue towards stable and principled topology-based frameworks for robust data-efficient learning.

</details>


### [326] [Entropy-Guided Dynamic Tokens for Graph-LLM Alignment in Molecular Understanding](https://arxiv.org/abs/2602.02742)
*Zihao Jing,Qiuhao Zeng,Ruiyi Fang,Yan Sun,Boyu Wang,Pingzhao Hu*

Main category: cs.LG

TL;DR: EDT-Former is a novel transformer model that dynamically generates tokens aligned with informative molecular patches, improving molecular graph understanding without costly LLM fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing graph-LLM bridges fail to account for stereochemistry and substructural context, and require inefficient LLM-backbone fine-tuning, limiting their effectiveness.

Method: EDT-Former uses an Entropy-guided Dynamic Token Transformer to create tokens aligned with molecular patches, preserving local and global features while avoiding LLM-backbone tuning.

Result: Achieves state-of-the-art performance on MoleculeQA, Mol-Instructions, TDC, and MoleculeNet benchmarks, demonstrating superior efficiency and generalization.

Conclusion: EDT-Former offers a scalable and generalizable solution for multimodal molecular understanding, outperforming prior methods without costly LLM tuning.

Abstract: Molecular understanding is central to advancing areas such as scientific discovery, yet Large Language Models (LLMs) struggle to understand molecular graphs effectively. Existing graph-LLM bridges often adapt the Q-Former-style connector with fixed-length static tokens, which is originally designed for vision tasks. These designs overlook stereochemistry and substructural context and typically require costly LLM-backbone fine-tuning, limiting efficiency and generalization. We introduce EDT-Former, an Entropy-guided Dynamic Token Transformer that generates tokens aligned with informative molecular patches, thereby preserving both local and global structural features for molecular graph understanding. Beyond prior approaches, EDT-Former enables alignment between frozen graph encoders and LLMs without tuning the LLM backbone (excluding the embedding layer), resulting in computationally efficient finetuning, and achieves stateof-the-art results on MoleculeQA, Molecule-oriented Mol-Instructions, and property prediction benchmarks (TDC, MoleculeNet), underscoring its effectiveness for scalable and generalizable multimodal molecular understanding

</details>


### [327] [On the Sample Efficiency of Inverse Dynamics Models for Semi-Supervised Imitation Learning](https://arxiv.org/abs/2602.02762)
*Sacha Morin,Moonsub Byeon,Alexia Jolicoeur-Martineau,Sébastien Lachapelle*

Main category: cs.LG

TL;DR: The paper explores semi-supervised imitation learning (SSIL), focusing on inverse dynamics models (IDMs). It shows VM-IDM and IDM labeling converge to the same policy, attributes IDM's superiority to better sample efficiency, and introduces an improved LAPO algorithm.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand why IDM-based policies outperform behavior cloning in SSIL, attributing it to higher sample efficiency and lower complexity of IDMs.

Method: The paper analyzes IDM-based policies using statistical learning theory and experiments, including unified video-action prediction (UVA), and proposes an enhanced LAPO algorithm.

Result: Findings confirm VM-IDM and IDM labeling learn the same policy in limit cases and highlight IDM's superior sample efficiency due to lower complexity and reduced stochasticity.

Conclusion: The work suggests IDMs are more efficient for SSIL and introduces an improved LAPO algorithm based on these insights.

Abstract: Semi-supervised imitation learning (SSIL) consists in learning a policy from a small dataset of action-labeled trajectories and a much larger dataset of action-free trajectories. Some SSIL methods learn an inverse dynamics model (IDM) to predict the action from the current state and the next state. An IDM can act as a policy when paired with a video model (VM-IDM) or as a label generator to perform behavior cloning on action-free data (IDM labeling). In this work, we first show that VM-IDM and IDM labeling learn the same policy in a limit case, which we call the IDM-based policy. We then argue that the previously observed advantage of IDM-based policies over behavior cloning is due to the superior sample efficiency of IDM learning, which we attribute to two causes: (i) the ground-truth IDM tends to be contained in a lower complexity hypothesis class relative to the expert policy, and (ii) the ground-truth IDM is often less stochastic than the expert policy. We argue these claims based on insights from statistical learning theory and novel experiments, including a study of IDM-based policies using recent architectures for unified video-action prediction (UVA). Motivated by these insights, we finally propose an improved version of the existing LAPO algorithm for latent action policy learning.

</details>


### [328] [Exposing Vulnerabilities in Explanation for Time Series Classifiers via Dual-Target Attacks](https://arxiv.org/abs/2602.02763)
*Bohan Wang,Zewen Liu,Lu Lin,Hui Liu,Li Xiong,Ming Jin,Wei Jin*

Main category: cs.LG

TL;DR: TSEF attacks show that temporal consistency in explanations doesn't ensure robustness, as predictions and explanations can be adversarially decoupled.


<details>
  <summary>Details</summary>
Motivation: Highlight the vulnerability of interpretable time series systems to attacks that decouple predictions from explanations, undermining trustworthiness.

Method: Introduce TSEF, a dual-target attack that manipulates classifier and explainer outputs to change predictions while keeping explanations plausible.

Result: TSEF successfully decouples predictions from explanations across datasets, revealing explanation stability as a misleading robustness indicator.

Conclusion: Explanation stability alone is unreliable; robustness evaluations should account for coupling-aware attacks in trustworthy time series tasks.

Abstract: Interpretable time series deep learning systems are often assessed by checking temporal consistency on explanations, implicitly treating this as evidence of robustness. We show that this assumption can fail: Predictions and explanations can be adversarially decoupled, enabling targeted misclassification while the explanation remains plausible and consistent with a chosen reference rationale. We propose TSEF (Time Series Explanation Fooler), a dual-target attack that jointly manipulates the classifier and explainer outputs. In contrast to single-objective misclassification attacks that disrupt explanation and spread attribution mass broadly, TSEF achieves targeted prediction changes while keeping explanations consistent with the reference. Across multiple datasets and explainer backbones, our results consistently reveal that explanation stability is a misleading proxy for decision robustness and motivate coupling-aware robustness evaluations for trustworthy time series tasks.

</details>


### [329] [Privately Fine-Tuned LLMs Preserve Temporal Dynamics in Tabular Data](https://arxiv.org/abs/2602.02766)
*Lucas Rosenblatt,Peihan Liu,Ryan McKenna,Natalia Ponomareva*

Main category: cs.LG

TL;DR: PATH introduces a differentially private synthetic data framework using autoregressive language models to preserve temporal coherence in longitudinal datasets, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for differentially private synthetic tabular data neglect temporal complexity in datasets like electronic health records, leading to insufficient modeling of sequential events.

Method: PATH treats entire tables as synthesis units, leveraging autoregressive capabilities of privately fine-tuned large language models to capture long-range dependencies.

Result: PATH reduces distributional distance to real trajectories by over 60% and state transition errors by nearly 50% compared to marginal mechanisms, while maintaining marginal fidelity.

Conclusion: PATH effectively addresses the limitations of traditional methods in preserving temporal coherence, making it superior for longitudinal datasets.

Abstract: Research on differentially private synthetic tabular data has largely focused on independent and identically distributed rows where each record corresponds to a unique individual. This perspective neglects the temporal complexity in longitudinal datasets, such as electronic health records, where a user contributes an entire (sub) table of sequential events. While practitioners might attempt to model such data by flattening user histories into high-dimensional vectors for use with standard marginal-based mechanisms, we demonstrate that this strategy is insufficient. Flattening fails to preserve temporal coherence even when it maintains valid marginal distributions. We introduce PATH, a novel generative framework that treats the full table as the unit of synthesis and leverages the autoregressive capabilities of privately fine-tuned large language models. Extensive evaluations show that PATH effectively captures long-range dependencies that traditional methods miss. Empirically, our method reduces the distributional distance to real trajectories by over 60% and reduces state transition errors by nearly 50% compared to leading marginal mechanisms while achieving similar marginal fidelity.

</details>


### [330] [Provable Effects of Data Replay in Continual Learning: A Feature Learning Perspective](https://arxiv.org/abs/2602.02767)
*Meng Ding,Jinhui Xu,Kaiyi Ji*

Main category: cs.LG

TL;DR: The paper analyzes full data-replay training in continual learning, identifying SNR as crucial for forgetting. Task ordering matters: higher-signal tasks aid learning and prevent forgetting.


<details>
  <summary>Details</summary>
Motivation: To understand the theoretical effectiveness of full data-replay methods in continual learning, focusing on catastrophic forgetting and the role of signal-to-noise ratio.

Method: Adopts a multi-view data model and analyzes task-incremental binary classification across M tasks, examining SNR's impact on forgetting.

Result: Forgetting occurs when cumulative noise dominates earlier signals. Adequate signal accumulation allows recovery of earlier tasks. Task ordering (prioritizing high-signal tasks) helps prevent forgetting.

Conclusion: Theoretical findings highlight SNR's importance and task ordering's role in mitigating forgetting, validated through synthetic and real-world experiments.

Abstract: Continual learning (CL) aims to train models on a sequence of tasks while retaining performance on previously learned ones. A core challenge in this setting is catastrophic forgetting, where new learning interferes with past knowledge. Among various mitigation strategies, data-replay methods, where past samples are periodically revisited, are considered simple yet effective, especially when memory constraints are relaxed. However, the theoretical effectiveness of full data replay, where all past data is accessible during training, remains largely unexplored. In this paper, we present a comprehensive theoretical framework for analyzing full data-replay training in continual learning from a feature learning perspective. Adopting a multi-view data model, we identify the signal-to-noise ratio (SNR) as a critical factor affecting forgetting. Focusing on task-incremental binary classification across $M$ tasks, our analysis verifies two key conclusions: (1) forgetting can still occur under full replay when the cumulative noise from later tasks dominates the signal from earlier ones; and (2) with sufficient signal accumulation, data replay can recover earlier tasks-even if their initial learning was poor. Notably, we uncover a novel insight into task ordering: prioritizing higher-signal tasks not only facilitates learning of lower-signal tasks but also helps prevent catastrophic forgetting. We validate our theoretical findings through synthetic and real-world experiments that visualize the interplay between signal learning and noise memorization across varying SNRs and task correlation regimes.

</details>


### [331] [VerIde ECG Biometrics: Verification and Identification](https://arxiv.org/abs/2602.02776)
*Scagnetto Arjuna*

Main category: cs.LG

TL;DR: ECG biometrics reveal strong individual signatures, showing anonymization via tabular features isn't privacy-safe. Deep learning on waveforms boosts verification and identification, raising privacy concerns.


<details>
  <summary>Details</summary>
Motivation: To assess ECG's potential as a biometric identifier and evaluate privacy risks in anonymization methods.

Method: Used MLP-based embedding networks on tabular ECG features, then ArcFace models on features and waveforms, testing scalability and normalization impact.

Result: High verification accuracy (TAR=0.908 @ FAR=1e-3) and identification rates (Rank@1=0.812), showing ECG's strong individual signature.

Conclusion: ECG carries measurable identity information, making privacy protocols essential when handling such data.

Abstract: This work studies electrocardiogram (ECG) biometrics at large scale, evaluating how strongly an ECG can be linked to an individual and, consequently, how its anonymization may be compromised. We show that identity information is already present in tabular representations (fiducial features): even a simple MLP-based embedding network yields non-trivial performance, indicating that anonymization based solely on releasing features does not guarantee privacy. We then adopt embedding-based deep learning models (ArcFace), first on features and then on ECG waveforms, showing a performance jump when moving from tabular inputs to waveforms, and a further gain with larger training sets and consistent normalization across train/val/test. On a large-scale test set, verification achieves high TAR at strict FAR thresholds (TAR=0.908 @ FAR=1e-3; TAR=0.820 @ FAR=1e-4) with EER=2.53% (all-vs-all); closed-set identification yields Rank@1=0.812 and Rank@10=0.910. In open-set, a two-stage pipeline (top-K shortlist on embeddings + re-ranking) reaches DIR@FAR up to 0.976 at FAR=1e-3 and 1e-4. Overall, the results show that ECG carries a measurable individual signature: re-identification is already possible with tabular features and is further amplified by embedding-based models, making privacy implications and realistic operational protocols essential to consider.

</details>


### [332] [Cross-Temporal Attention Fusion (CTAF) for Multimodal Physiological Signals in Self-Supervised Learning](https://arxiv.org/abs/2602.02784)
*Arian Khorasani,Théophile Demazure*

Main category: cs.LG

TL;DR: CTAF introduces a self-supervised module for multimodal affect modeling, addressing EEG-physiology asynchrony via cross-temporal attention fusion, achieving competitive results with few labels.


<details>
  <summary>Details</summary>
Motivation: Existing fusion methods often ignore or expensively handle temporal asynchrony between EEG and peripheral physiology, limiting robust affect modeling.

Method: Proposes Cross-Temporal Attention Fusion (CTAF) with soft bidirectional alignments, time-aware cross attention, fusion gate, and alignment-regularized contrastive objectives.

Result: CTAF outperforms baselines in cosine margins and cross-modal retrieval, remains competitive in accuracy/F1, and requires few labels.

Conclusion: CTAF advances label-efficient, generalizable fusion for psychophysiological time series, addressing central-autonomic nervous system coupling.

Abstract: We study multimodal affect modeling when EEG and peripheral physiology are asynchronous, which most fusion methods ignore or handle with costly warping. We propose Cross-Temporal Attention Fusion (CTAF), a self-supervised module that learns soft bidirectional alignments between modalities and builds a robust clip embedding using time-aware cross attention, a lightweight fusion gate, and alignment-regularized contrastive objectives with optional weak supervision. On the K-EmoCon dataset, under leave-one-out cross-validation evaluation, CTAF yields higher cosine margins for matched pairs and better cross-modal token retrieval within one second, and it is competitive with the baseline on three-bin accuracy and macro-F1 while using few labels. Our contributions are a time-aware fusion mechanism that directly models correspondence, an alignment-driven self-supervised objective tailored to EEG and physiology, and an evaluation protocol that measures alignment quality itself. Our approach accounts for the coupling between the central and autonomic nervous systems in psychophysiological time series. These results indicate that CTAF is a strong step toward label-efficient, generalizable EEG-peripheral fusion under temporal asynchrony.

</details>


### [333] [LEMON: Local Explanations via Modality-aware OptimizatioN](https://arxiv.org/abs/2602.02786)
*Yu Qin,Phillip Sloan,Raul Santos-Rodriguez,Majid Mirmehdi,Telmo de Menezes e Silva Filho*

Main category: cs.LG

TL;DR: LEMON is a model-agnostic framework for local explanations of multimodal predictions, offering efficient and unified modality-aware explanations.


<details>
  <summary>Details</summary>
Motivation: Existing explainability methods are often single-modal, architecture-dependent, or computationally expensive, limiting their scalability.

Method: LEMON fits a modality-aware surrogate with group-structured sparsity to provide unified explanations, treating the predictor as a black box.

Result: LEMON achieves competitive faithfulness while reducing black-box evaluations by 35-67 times and runtime by 2-8 times compared to baselines.

Conclusion: LEMON provides scalable, efficient, and faithful local explanations for multimodal predictions.

Abstract: Multimodal models are ubiquitous, yet existing explainability methods are often single-modal, architecture-dependent, or too computationally expensive to run at scale. We introduce LEMON (Local Explanations via Modality-aware OptimizatioN), a model-agnostic framework for local explanations of multimodal predictions. LEMON fits a single modality-aware surrogate with group-structured sparsity to produce unified explanations that disentangle modality-level contributions and feature-level attributions. The approach treats the predictor as a black box and is computationally efficient, requiring relatively few forward passes while remaining faithful under repeated perturbations. We evaluate LEMON on vision-language question answering and a clinical prediction task with image, text, and tabular inputs, comparing against representative multimodal baselines. Across backbones, LEMON achieves competitive deletion-based faithfulness while reducing black-box evaluations by 35-67 times and runtime by 2-8 times compared to strong multimodal baselines.

</details>


### [334] [Structure-Preserving Learning Improves Geometry Generalization in Neural PDEs](https://arxiv.org/abs/2602.02788)
*Benjamin D. Shaffer,Shawn Koohy,Brooks Kinch,M. Ani Hsieh,Nathaniel Trask*

Main category: cs.LG

TL;DR: Geo-NeW is a data-driven finite element method that learns differential operators and finite element spaces jointly, preserving physical laws and improving generalization to unseen geometries.


<details>
  <summary>Details</summary>
Motivation: To develop physics foundation models for solving PDEs with structure preservation and accuracy in unseen geometries.

Method: Introduce Geo-NeW, using transformer-based encoding and learned finite element spaces tied to geometry, ensuring conservation laws via Finite Element Exterior Calculus.

Result: State-of-the-art performance on steady-state PDE benchmarks and better generalization to out-of-distribution geometries.

Conclusion: Geo-NeW effectively connects geometry and boundary conditions to solutions, enhancing PDE learning and generalization.

Abstract: We aim to develop physics foundation models for science and engineering that provide real-time solutions to Partial Differential Equations (PDEs) which preserve structure and accuracy under adaptation to unseen geometries. To this end, we introduce General-Geometry Neural Whitney Forms (Geo-NeW): a data-driven finite element method. We jointly learn a differential operator and compatible reduced finite element spaces defined on the underlying geometry. The resulting model is solved to generate predictions, while exactly preserving physical conservation laws through Finite Element Exterior Calculus. Geometry enters the model as a discretized mesh both through a transformer-based encoding and as the basis for the learned finite element spaces. This explicitly connects the underlying geometry and imposed boundary conditions to the solution, providing a powerful inductive bias for learning neural PDEs, which we demonstrate improves generalization to unseen domains. We provide a novel parameterization of the constitutive model ensuring the existence and uniqueness of the solution. Our approach demonstrates state-of-the-art performance on several steady-state PDE benchmarks, and provides a significant improvement over conventional baselines on out-of-distribution geometries.

</details>


### [335] [Causality--Δ: Jacobian-Based Dependency Analysis in Flow Matching Models](https://arxiv.org/abs/2602.02793)
*Reza Rezvan,Gustav Gille,Moritz Schauer,Richard Torkar*

Main category: cs.LG

TL;DR: The paper explores how small latent perturbations propagate through flow matching, using Jacobian-vector products (JVPs) to analyze dependency structures in generated features.


<details>
  <summary>Details</summary>
Motivation: Understanding local and global structures in flow matching and their impact on feature dependencies is key to improving generative models.

Method: Derives closed-form expressions for optimal drift and Jacobians in Gaussian and mixture-of-Gaussian settings, and uses numerical JVPs to validate analytical results. Applies JVPs with attribute classifiers in image domains.

Result: Numerical JVPs match analytical Jacobians in synthetic benchmarks. Attribute-level JVP estimators recover empirical correlations in MNIST and CelebA datasets. Conditioning on small classifier-Jacobian norms reduces correlations, aligning with a common-cause hypothesis.

Conclusion: The study highlights the utility of JVPs for analyzing dependency structures in flow matching, though conditioning on norms is not equivalent to formal interventions.

Abstract: Flow matching learns a velocity field that transports a base distribution to data. We study how small latent perturbations propagate through these flows and show that Jacobian-vector products (JVPs) provide a practical lens on dependency structure in the generated features. We derive closed-form expressions for the optimal drift and its Jacobian in Gaussian and mixture-of-Gaussian settings, revealing that even globally nonlinear flows admit local affine structure. In low-dimensional synthetic benchmarks, numerical JVPs recover the analytical Jacobians. In image domains, composing the flow with an attribute classifier yields an attribute-level JVP estimator that recovers empirical correlations on MNIST and CelebA. Conditioning on small classifier-Jacobian norms reduces correlations in a way consistent with a hypothesized common-cause structure, while we emphasize that this conditioning is not a formal do intervention.

</details>


### [336] [Joint Learning of Hierarchical Neural Options and Abstract World Model](https://arxiv.org/abs/2602.02799)
*Wasu Top Piriyakulkij,Wolfgang Lehrach,Kevin Ellis,Kevin Murphy*

Main category: cs.LG

TL;DR: AgentOWL proposes a sample-efficient method to jointly learn hierarchical neural options and an abstract world model, outperforming baselines in skill acquisition with less data.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of existing model-free hierarchical reinforcement learning algorithms in acquiring sequences of skills (hierarchical neural options).

Method: Introduces AgentOWL, a novel approach that jointly learns an abstract world model (abstracting states and time) and hierarchical neural options efficiently.

Result: Demonstrates superior performance in learning more skills with significantly less data compared to baseline methods on Object-Centric Atari games.

Conclusion: AgentOWL successfully advances efficient skill acquisition in AI agents by combining hierarchical options with abstract world modeling.

Abstract: Building agents that can perform new skills by composing existing skills is a long-standing goal of AI agent research. Towards this end, we investigate how to efficiently acquire a sequence of skills, formalized as hierarchical neural options. However, existing model-free hierarchical reinforcement algorithms need a lot of data. We propose a novel method, which we call AgentOWL (Option and World model Learning Agent), that jointly learns -- in a sample efficient way -- an abstract world model (abstracting across both states and time) and a set of hierarchical neural options. We show, on a subset of Object-Centric Atari games, that our method can learn more skills using much less data than baseline methods.

</details>


### [337] [Membership Inference Attacks from Causal Principles](https://arxiv.org/abs/2602.02819)
*Mathieu Even,Clément Berenfeld,Linus Bleistein,Tudor Cebere,Julie Josse,Aurélien Bellet*

Main category: cs.LG

TL;DR: The paper addresses biases in Membership Inference Attacks (MIAs) evaluation by framing it as a causal inference problem, proposing reliable estimators for memorization measurement.


<details>
  <summary>Details</summary>
Motivation: Standard MIA evaluation methods are computationally expensive or statistically unclear, necessitating a more reliable approach.

Method: The authors define memorization as a causal effect and derive causal analogues of MIA metrics with non-asymptotic consistency guarantees.

Result: Experiments confirm the approach provides reliable memorization measurement without repeated retraining, even under distribution shift.

Conclusion: The causal inference framework offers a principled foundation for privacy evaluation in AI systems.

Abstract: Membership Inference Attacks (MIAs) are widely used to quantify training data memorization and assess privacy risks. Standard evaluation requires repeated retraining, which is computationally costly for large models. One-run methods (single training with randomized data inclusion) and zero-run methods (post hoc evaluation) are often used instead, though their statistical validity remains unclear. To address this gap, we frame MIA evaluation as a causal inference problem, defining memorization as the causal effect of including a data point in the training set. This novel formulation reveals and formalizes key sources of bias in existing protocols: one-run methods suffer from interference between jointly included points, while zero-run evaluations popular for LLMs are confounded by non-random membership assignment. We derive causal analogues of standard MIA metrics and propose practical estimators for multi-run, one-run, and zero-run regimes with non-asymptotic consistency guarantees. Experiments on real-world data show that our approach enables reliable memorization measurement even when retraining is impractical and under distribution shift, providing a principled foundation for privacy evaluation in modern AI systems.

</details>


### [338] [TraceNAS: Zero-shot LLM Pruning via Gradient Trace Correlation](https://arxiv.org/abs/2602.02891)
*Prajna G. Malettira,Manish Nagaraj,Arjun Roy,Shubham Negi,Kaushik Roy*

Main category: cs.LG

TL;DR: TraceNAS is a training-free NAS framework for efficient structured pruning of LLMs, reducing computational costs while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Existing pruning methods ignore global structural dependencies or are computationally expensive. TraceNAS addresses these issues.

Method: TraceNAS uses a scale-invariant zero-shot proxy to explore pruning of LLM depth and width without training.

Result: TraceNAS reduces GPU-hours by 10x and performs competitively with training-aware methods on benchmarks.

Conclusion: TraceNAS offers an efficient and effective alternative to training-aware pruning for LLMs.

Abstract: Structured pruning is essential for efficient deployment of Large Language Models (LLMs). The varying sensitivity of LLM sub-blocks to pruning necessitates the identification of optimal non-uniformly pruned models. Existing methods evaluate the importance of layers, attention heads, or weight channels in isolation. Such localized focus ignores the complex global structural dependencies that exist across the model. Training-aware structured pruning addresses global dependencies, but its computational cost can be just as expensive as post-pruning training. To alleviate the computational burden of training-aware pruning and capture global structural dependencies, we propose TraceNAS, a training-free Neural Architecture Search (NAS) framework that jointly explores structured pruning of LLM depth and width. TraceNAS identifies pruned models that maintain a high degree of loss landscape alignment with the pretrained model using a scale-invariant zero-shot proxy, effectively selecting models that exhibit maximal performance potential during post-pruning training. TraceNAS is highly efficient, enabling high-fidelity discovery of pruned models on a single GPU in 8.5 hours, yielding a 10$\times$ reduction in GPU-hours compared to training-aware methods. Evaluations on the Llama and Qwen families demonstrate that TraceNAS is competitive with training-aware baselines across commonsense and reasoning benchmarks.

</details>


### [339] [From Tokens to Numbers: Continuous Number Modeling for SVG Generation](https://arxiv.org/abs/2602.02820)
*Michael Ogezi,Martin Bell,Freda Shi,Ethan Smith*

Main category: cs.LG

TL;DR: The paper proposes Continuous Number Modeling (CNM), a method for efficiently encoding SVG parameters as continuous values, improving training speed and perceptual fidelity for vector graphics generation.


<details>
  <summary>Details</summary>
Motivation: Vector graphics like SVGs offer advantages such as flexibility and efficiency but face challenges due to inefficient token-based encoding of numerical parameters, which slows training and reduces accuracy.

Method: CNM models numbers as continuous values instead of discrete tokens, aligning the model's inputs with the data's continuous nature. A multimodal transformer is trained on 2 million raster-to-SVG samples, followed by reinforcement learning fine-tuning with perceptual feedback.

Result: CNM improves training speed by over 30% while achieving higher perceptual fidelity compared to alternative methods.

Conclusion: CNM is a practical and efficient approach for high-quality vector generation, with potential for broader applications. The code is made publicly available.

Abstract: For certain image generation tasks, vector graphics such as Scalable Vector Graphics (SVGs) offer clear benefits such as increased flexibility, size efficiency, and editing ease, but remain less explored than raster-based approaches. A core challenge is that the numerical, geometric parameters, which make up a large proportion of SVGs, are inefficiently encoded as long sequences of tokens. This slows training, reduces accuracy, and hurts generalization. To address these problems, we propose Continuous Number Modeling (CNM), an approach that directly models numbers as first-class, continuous values rather than discrete tokens. This formulation restores the mathematical elegance of the representation by aligning the model's inputs with the data's continuous nature, removing discretization artifacts introduced by token-based encoding. We then train a multimodal transformer on 2 million raster-to-SVG samples, followed by fine-tuning via reinforcement learning using perceptual feedback to further improve visual quality. Our approach improves training speed by over 30% while maintaining higher perceptual fidelity compared to alternative approaches. This work establishes CNM as a practical and efficient approach for high-quality vector generation, with potential for broader applications. We make our code available http://github.com/mikeogezi/CNM.

</details>


### [340] [A Single Revision Step Improves Token-Efficient LLM Reasoning](https://arxiv.org/abs/2602.02828)
*Yingchuan Zhang,Terry Ma,Wenxuan Zhong,Ping Ma*

Main category: cs.LG

TL;DR: PACER introduces a training-free, inference-only framework where reasoning traces revise conclusions collaboratively, improving accuracy over traditional voting methods.


<details>
  <summary>Details</summary>
Motivation: Traditional aggregation methods fail to evaluate reasoning traces collectively, leading to suppression of correct solutions due to misleadingly confident hallucinated paths.

Method: PACER involves preliminary trace screening, constructing a consensus packet, and enabling traces to self-review and revise conclusions based on logical consensus.

Result: PACER matches or exceeds 256-sample majority voting accuracy on benchmarks like AIME and BRUMO, outperforming raw ensemble baselines.

Conclusion: PACER enhances LLM reasoning by transforming consensus into a collaborative refinement process, addressing near-miss errors effectively.

Abstract: Large language models (LLMs) achieve higher accuracy on challenging reasoning tasks by scaling test-time compute through multiple trajectory sampling. However, standard aggregation methods like majority voting or individual confidence-based filtering face a fundamental "blind spot": they evaluate each trace in isolation. As problems scale in difficulty, models often generate hallucinated paths that exhibit misleadingly high confidence, causing the true solution to be suppressed by a narrow margin in traditional voting. We ask: can we enable traces to "peer-review" each other to resolve these near-miss errors?
  We introduce Packet-Conditioned Revision (PACER), a training-free, inference-only framework that enables reasoning traces to revise their conclusions through a structured coordination step. After a preliminary screening of generated traces, PACER constructs a compact consensus packet containing (i) unique candidate answers, (ii) their aggregated confidence scores, and (iii) representative reasoning summaries for each candidate answer. Individual traces then perform a targeted self-review conditioned on this packet, allowing them to identify specific logical junctions where they diverged from the broader consensus and pivot if their original reasoning is found to be flawed. Final predictions are obtained via confidence-weighted voting over these revised trajectories. On challenging competitive math benchmarks such as AIME and BRUMO, PACER matches or exceeds the accuracy of 256-sample majority voting, significantly outperforming raw ensemble baselines by transforming simple consensus into a collaborative logical refinement process.

</details>


### [341] [SC3D: Dynamic and Differentiable Causal Discovery for Temporal and Instantaneous Graphs](https://arxiv.org/abs/2602.02830)
*Sourajit Das,Dibyajyoti Chakraborthy,Romit Maulik*

Main category: cs.LG

TL;DR: Proposes SC3D, a two-stage differentiable framework for discovering causal structures in multivariate time series, improving stability and accuracy over existing methods.


<details>
  <summary>Details</summary>
Motivation: Causal structure discovery in time series is complex due to lagged and instantaneous dependencies and combinatorial search space.

Method: SC3D uses a two-stage approach: preselection via node-wise prediction (Stage 1) and refinement with sparsity and acyclicity constraints (Stage 2).

Result: SC3D outperforms existing methods in stability and accuracy for recovering lagged and instantaneous causal structures.

Conclusion: SC3D is a robust framework for causal discovery in dynamic systems, offering better performance than temporal baselines.

Abstract: Discovering causal structures from multivariate time series is a key problem because interactions span across multiple lags and possibly involve instantaneous dependencies. Additionally, the search space of the dynamic graphs is combinatorial in nature. In this study, we propose \textit{Stable Causal Dynamic Differentiable Discovery (SC3D)}, a two-stage differentiable framework that jointly learns lag-specific adjacency matrices and, if present, an instantaneous directed acyclic graph (DAG). In Stage 1, SC3D performs edge preselection through node-wise prediction to obtain masks for lagged and instantaneous edges, whereas Stage 2 refines these masks by optimizing a likelihood with sparsity along with enforcing acyclicity on the instantaneous block. Numerical results across synthetic and benchmark dynamical systems demonstrate that SC3D achieves improved stability and more accurate recovery of both lagged and instantaneous causal structures compared to existing temporal baselines.

</details>


### [342] [Koopman Autoencoders with Continuous-Time Latent Dynamics for Fluid Dynamics Forecasting](https://arxiv.org/abs/2602.02832)
*Rares Grozavescu,Pengyu Zhang,Etienne Meunier,Mark Girolami*

Main category: cs.LG

TL;DR: A continuous-time Koopman framework is introduced for turbulent flow simulations, leveraging numerical integration for flexible timesteps and improved long-horizon stability.


<details>
  <summary>Details</summary>
Motivation: Existing discrete-time Koopman autoencoders lack temporal flexibility, limiting their robustness and generalization in turbulent flow simulations.

Method: The proposed framework uses numerical integration to model latent evolution in continuous time, allowing variable timesteps and adhering to analytical matrix exponential solutions.

Result: The method shows accuracy, stability, and generalization beyond training regimes in CFD benchmarks.

Conclusion: The continuous-time Koopman framework enhances long-horizon forecasting and robustness in turbulent flow simulations.

Abstract: Data-driven surrogate models have emerged as powerful tools for accelerating the simulation of turbulent flows. However, classical approaches which perform autoregressive rollouts often trade off between strong short-term accuracy and long-horizon stability. Koopman autoencoders, inspired by Koopman operator theory, provide a physics-based alternative by mapping nonlinear dynamics into a latent space where linear evolution is conducted. In practice, most existing formulations operate in a discrete-time setting, limiting temporal flexibility. In this work, we introduce a continuous-time Koopman framework that models latent evolution through numerical integration schemes. By allowing variable timesteps at inference, the method demonstrates robustness to temporal resolution and generalizes beyond training regimes. In addition, the learned dynamics closely adhere to the analytical matrix exponential solution, enabling efficient long-horizon forecasting. We evaluate the approach on classical CFD benchmarks and report accuracy, stability, and extrapolation properties.

</details>


### [343] [Tabula RASA: Exposing and Breaking the Relational Bottleneck in Transformers](https://arxiv.org/abs/2602.02834)
*Jonas Petersen,Camilla Mazzoleni,Riccardo Maggioni*

Main category: cs.LG

TL;DR: RASA enhances transformers for multi-hop reasoning by adding edge-type embeddings and sparse masking, outperforming standard models and matching GPT-4 at lower cost.


<details>
  <summary>Details</summary>
Motivation: Transformers struggle with multi-hop relational reasoning tasks; RASA aims to address this limitation with minimal structural changes.

Method: Introduces RASA with edge-type embeddings and sparse masking to inject relational structure and reduce attention search space.

Result: RASA outperforms standard transformers on MetaQA and WebQuestionsSP, matching GPT-4 with lower costs and better performance on deeper reasoning tasks.

Conclusion: Minimal structural modifications like RASA significantly improve multi-hop reasoning in transformers without formal learnability guarantees.

Abstract: Transformers achieve remarkable performance across many domains, yet struggle with tasks requiring multi-hop relational reasoning over structured data. We analyze this limitation through circuit complexity: standard transformers are $\mathsf{TC}^0$-complete and require $Ω(k)$ layers for $k$-hop reasoning. We introduce RASA (Relation-Aware Sparse Attention), a minimal modification adding: (1) edge-type embeddings that inject relational structure into attention scores, and (2) sparse masking that restricts attention to graph-adjacent positions. While RASA has the same asymptotic depth requirements, sparse masking reduces the attention search space from $O(2^{n^2})$ to $O(2^m)$ patterns, and edge biases provide explicit relation routing. Empirically, on MetaQA (1/2/3-hop) and WebQuestionsSP, RASA outperforms standard transformers and matches GPT-4 at lower cost, with advantages growing with reasoning depth (+7.1 points on 3-hop). We do not claim formal learnability guarantees; the contribution is empirical validation that minimal structural modifications substantially improve multi-hop reasoning.

</details>


### [344] [Self-Hinting Language Models Enhance Reinforcement Learning](https://arxiv.org/abs/2602.03143)
*Baohao Liao,Hanze Dong,Xinxing Xu,Christof Monz,Jiang Bian*

Main category: cs.LG

TL;DR: SAGE enhances GRPO by injecting privileged hints during training to diversify rollouts and prevent advantage collapse under sparse rewards, improving performance across benchmarks without hints at test time.


<details>
  <summary>Details</summary>
Motivation: GRPO stalls under sparse terminal rewards due to identical rewards within groups, leading to vanishing updates. SAGE addresses this by diversifying outcomes with self-hints.

Method: SAGE introduces privileged hints during training to reshape rollout diversity under the same terminal reward, while deploying a no-hint policy at test time.

Result: SAGE outperforms GRPO across 6 benchmarks, with average gains of +2.0, +1.2, and +1.3 on Llama-3.2-3B-Instruct, Qwen2.5-7B-Instruct, and Qwen3-4B-Instruct, respectively.

Conclusion: SAGE effectively prevents GRPO's advantage collapse under sparse rewards, demonstrating consistent improvements without requiring hints during deployment.

Abstract: Group Relative Policy Optimization (GRPO) has recently emerged as a practical recipe for aligning large language models with verifiable objectives. However, under sparse terminal rewards, GRPO often stalls because rollouts within a group frequently receive identical rewards, causing relative advantages to collapse and updates to vanish. We propose self-hint aligned GRPO with privileged supervision (SAGE), an on-policy reinforcement learning framework that injects privileged hints during training to reshape the rollout distribution under the same terminal verifier reward. For each prompt $x$, the model samples a compact hint $h$ (e.g., a plan or decomposition) and then generates a solution $τ$ conditioned on $(x,h)$. Crucially, the task reward $R(x,τ)$ is unchanged; hints only increase within-group outcome diversity under finite sampling, preventing GRPO advantages from collapsing under sparse rewards. At test time, we set $h=\varnothing$ and deploy the no-hint policy without any privileged information. Moreover, sampling diverse self-hints serves as an adaptive curriculum that tracks the learner's bottlenecks more effectively than fixed hints from an initial policy or a stronger external model. Experiments over 6 benchmarks with 3 LLMs show that SAGE consistently outperforms GRPO, on average +2.0 on Llama-3.2-3B-Instruct, +1.2 on Qwen2.5-7B-Instruct and +1.3 on Qwen3-4B-Instruct. The code is available at https://github.com/BaohaoLiao/SAGE.

</details>


### [345] [Semantics-Aware Generative Latent Data Augmentation for Learning in Low-Resource Domains](https://arxiv.org/abs/2602.02841)
*Jae-Sung Bae,Minje Kim*

Main category: cs.LG

TL;DR: GeLDA, a semantics-aware generative latent data augmentation framework, improves deep learning performance in data-scarce settings by leveraging conditional diffusion models in FM-induced latent spaces.


<details>
  <summary>Details</summary>
Motivation: Deep learning underperforms in data-scarce settings, and foundation models (FMs) still face challenges with scarce labeled data during fine-tuning. GeLDA aims to address this gap.

Method: GeLDA uses conditional diffusion models to synthesize samples in a low-dimensional, FM-induced latent space, leveraging auxiliary feature vectors for semantic relationships.

Result: GeLDA improves Whisper-large's performance by 6.13% in speech emotion recognition and achieves 74.7% tail-class accuracy in ImageNet-LT, setting a new state-of-the-art.

Conclusion: GeLDA effectively enhances generalization in data-scarce scenarios by generating high-quality, task-relevant data.

Abstract: Despite strong performance in data-rich regimes, deep learning often underperforms in the data-scarce settings common in practice. While foundation models (FMs) trained on massive datasets demonstrate strong generalization by extracting general-purpose features, they can still suffer from scarce labeled data during downstream fine-tuning. To address this, we propose GeLDA, a semantics-aware generative latent data augmentation framework that leverages conditional diffusion models to synthesize samples in an FM-induced latent space. Because this space is low-dimensional and concentrates task-relevant information compared to the input space, GeLDA enables efficient, high-quality data generation. GeLDA conditions generation on auxiliary feature vectors that capture semantic relationships among classes or subdomains, facilitating data augmentation in low-resource domains. We validate GeLDA in two large-scale recognition tasks: (a) in zero-shot language-specific speech emotion recognition, GeLDA improves the Whisper-large baseline's unweighted average recall by 6.13%; and (b) in long-tailed image classification, it achieves 74.7% tail-class accuracy on ImageNet-LT, setting a new state-of-the-art result.

</details>


### [346] [Causal Flow Q-Learning for Robust Offline Reinforcement Learning](https://arxiv.org/abs/2602.02847)
*Mingxuan Li,Junzhe Zhang,Elias Bareinboim*

Main category: cs.LG

TL;DR: The paper introduces a causal offline RL method to handle confounded observations in pixel-based demonstrations, improving success rates by 120% over state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing flow-matching RL methods assume no unmeasured confounding in data, which fails in pixel-based tasks due to sensory capability mismatches, leading to biases.

Method: A causal offline RL objective optimizes worst-case policy performance under confounding biases, implemented with flow-matching policies and a deep discriminator.

Result: Experiments on 25 pixel-based tasks show a 120% higher success rate compared to standard offline RL methods.

Conclusion: The proposed causal approach effectively addresses confounding biases in offline RL, significantly outperforming existing methods.

Abstract: Expressive policies based on flow-matching have been successfully applied in reinforcement learning (RL) more recently due to their ability to model complex action distributions from offline data. These algorithms build on standard policy gradients, which assume that there is no unmeasured confounding in the data. However, this condition does not necessarily hold for pixel-based demonstrations when a mismatch exists between the demonstrator's and the learner's sensory capabilities, leading to implicit confounding biases in offline data. We address the challenge by investigating the problem of confounded observations in offline RL from a causal perspective. We develop a novel causal offline RL objective that optimizes policies' worst-case performance that may arise due to confounding biases. Based on this new objective, we introduce a practical implementation that learns expressive flow-matching policies from confounded demonstrations, employing a deep discriminator to assess the discrepancy between the target policy and the nominal behavioral policy. Experiments across 25 pixel-based tasks demonstrate that our proposed confounding-robust augmentation procedure achieves a success rate 120\% that of confounding-unaware, state-of-the-art offline RL methods.

</details>


### [347] [DynSplit-KV: Dynamic Semantic Splitting for KVCache Compression in Efficient Long-Context LLM Inference](https://arxiv.org/abs/2602.03184)
*Jiancai Ye,Jun Liu,Qingchen Li,Tianlang Zhao,Hanbin Zhang,Jiayi Pan,Ningyi Xu,Guohao Dai*

Main category: cs.LG

TL;DR: DynSplit-KV dynamically identifies delimiters for KVCache compression, improving accuracy and reducing memory usage in LLM inference.


<details>
  <summary>Details</summary>
Motivation: Current KVCache compression methods use rigid splitting, causing accuracy degradation due to mismatched semantic boundaries. Dynamic splitting is needed to align with varying semantics.

Method: DynSplit-KV introduces dynamic importance-aware delimiter selection and uniform mapping to transform variable-length blocks into fixed-length formats.

Result: DynSplit-KV improves accuracy by 49.9%, reduces inference overhead by 4.9x, achieves 2.2x speedup, and 2.6x peak memory reduction.

Conclusion: DynSplit-KV effectively addresses rigid splitting limitations, enhancing efficiency and accuracy in long-context LLM inference.

Abstract: Although Key-Value (KV) Cache is essential for efficient large language models (LLMs) inference, its growing memory footprint in long-context scenarios poses a significant bottleneck, making KVCache compression crucial. Current compression methods rely on rigid splitting strategies, such as fixed intervals or pre-defined delimiters. We observe that rigid splitting suffers from significant accuracy degradation (ranging from 5.5% to 55.1%) across different scenarios, owing to the scenario-dependent nature of the semantic boundaries. This highlights the necessity of dynamic semantic splitting to match semantics. To achieve this, we face two challenges. (1) Improper delimiter selection misaligns semantics with the KVCache, resulting in 28.6% accuracy loss. (2) Variable-length blocks after splitting introduce over 73.1% additional inference overhead. To address the above challenges, we propose DynSplit-KV, a KVCache compression method that dynamically identifies delimiters for splitting. We propose: (1) a dynamic importance-aware delimiter selection strategy, improving accuracy by 49.9%. (2) A uniform mapping strategy that transforms variable-length semantic blocks into a fixed-length format, reducing inference overhead by 4.9x. Experiments show that DynSplit-KV achieves the highest accuracy, 2.2x speedup compared with FlashAttention and 2.6x peak memory reduction in long-context scenarios.

</details>


### [348] [Zero Sum SVD: Balancing Loss Sensitivity for Low Rank LLM Compression](https://arxiv.org/abs/2602.02848)
*Ali Abbasi,Chayne Thrash,Haoran Qin,Shansita Sharma,Sepehr Seifi,Soheil Kolouri*

Main category: cs.LG

TL;DR: ZS-SVD is a post-training compression method using activation whitening and first-order loss estimates to globally select singular components, achieving heterogeneous rank allocation without iterative optimization.


<details>
  <summary>Details</summary>
Motivation: Current SVD-based compression methods often use homogeneous ranks or require costly iterative optimization, limiting efficiency and performance.

Method: ZS-SVD performs global singular component selection via activation whitening and first-order calibration loss estimates, enforcing a zero-sum rule for heterogeneous rank allocation. An optional lightweight correction applies a single projected gradient update.

Result: ZS-SVD consistently outperforms prior methods across various LLM architectures, benchmarks, and compression ratios.

Conclusion: ZS-SVD offers an efficient and effective solution for post-training compression of large language models, automating rank allocation without iterative optimization.

Abstract: Advances in large language models have driven strong performance across many tasks, but their memory and compute costs still hinder deployment. SVD-based compression reduces storage and can speed up inference via low-rank factors, yet performance depends on how rank is allocated under a global compression ratio. Prior methods often use homogeneous ranks for similarly sized matrices, despite large differences in loss sensitivity, or rely on expensive iterative pre-truncation optimization to determine per matrix ranks. We propose \textbf{Zero Sum SVD} (\textbf{ZS-SVD}), a post-training method that performs \emph{global} singular component selection using activation whitening and first-order calibration loss estimates in whitened coordinates. \textbf{ZS-SVD} prunes components across the whole model with a \textbf{zero sum} rule that keeps the cumulative predicted loss change near zero, automatically yielding heterogeneous ranks without solving a rank allocation optimization. Motivated by evidence that gradients near pretrained solutions exhibit low rank structure, we also introduce an optional lightweight correction that applies a \textbf{single} projected gradient update after truncation, followed by re-truncation. Extensive experiments across multiple LLM architectures show consistent gains across diverse benchmarks and compression ratios. Code is available at https://github.com/mint-vu/Zero-Sum-SVD

</details>


### [349] [Prompt Augmentation Scales up GRPO Training on Mathematical Reasoning](https://arxiv.org/abs/2602.03190)
*Wenquan Lu,Hai Huang,Randall Balestriero*

Main category: cs.LG

TL;DR: GRPO reinforcement learning improves math reasoning in LLMs, but entropy collapse limits training. Prompt augmentation diversifies reasoning templates, enabling stable longer training and achieving SOTA results.


<details>
  <summary>Details</summary>
Motivation: Address entropy collapse and limited training horizons in GRPO reinforcement learning for LLMs by introducing diverse reasoning prompts.

Method: Use prompt augmentation to generate varied reasoning traces under different templates/formats, avoiding entropy collapse without KL regularization.

Result: Achieves 44.5% per-benchmark and 51.3% per-question accuracy on math benchmarks like AIME24, AMC, MATH500, Minerva, and OlympiadBench.

Conclusion: Prompt augmentation stabilizes training, avoids collapse, and enables SOTA performance in mathematical reasoning.

Abstract: Reinforcement learning algorithms such as group-relative policy optimization (GRPO) have demonstrated strong potential for improving the mathematical reasoning capabilities of large language models. However, prior work has consistently observed an entropy collapse phenomenon during reinforcement post-training, characterized by a monotonic decrease in policy entropy that ultimately leads to training instability and collapse. As a result, most existing approaches restrict training to short horizons (typically 5-20 epochs), limiting sustained exploration and hindering further policy improvement. In addition, nearly all prior work relies on a single, fixed reasoning prompt or template during training. In this work, we introduce prompt augmentation, a training strategy that instructs the model to generate reasoning traces under diverse templates and formats, thereby increasing rollout diversity. We show that, without a KL regularization term, prompt augmentation enables stable scaling of training duration under a fixed dataset and allows the model to tolerate low-entropy regimes without premature collapse. Empirically, a Qwen2.5-Math-1.5B model trained with prompt augmentation on the MATH Level 3-5 dataset achieves state-of-the-art performance, reaching 44.5 per-benchmark accuracy and 51.3 per-question accuracy on standard mathematical reasoning benchmarks, including AIME24, AMC, MATH500, Minerva, and OlympiadBench. The code and model checkpoints are available at https://github.com/wenquanlu/prompt-augmentation-GRPO.

</details>


### [350] [Recurrent Equivariant Constraint Modulation: Learning Per-Layer Symmetry Relaxation from Data](https://arxiv.org/abs/2602.02853)
*Stefanos Pertigkiozoglou,Mircea Petrache,Shubhendu Trivedi,Kostas Daniilidis*

Main category: cs.LG

TL;DR: RECM introduces a mechanism to dynamically adjust equivariance constraints in neural networks, improving performance without manual tuning.


<details>
  <summary>Details</summary>
Motivation: Strict equivariance constraints hinder learning, and existing relaxation methods require costly task-dependent tuning.

Method: RECM learns relaxation levels from training signals and symmetry properties, eliminating the need for manual tuning.

Result: RECM outperforms prior methods, especially in tasks like molecular conformer generation.

Conclusion: RECM dynamically balances symmetry constraints, enhancing performance across varied equivariant tasks.

Abstract: Equivariant neural networks exploit underlying task symmetries to improve generalization, but strict equivariance constraints can induce more complex optimization dynamics that can hinder learning. Prior work addresses these limitations by relaxing strict equivariance during training, but typically relies on prespecified, explicit, or implicit target levels of relaxation for each network layer, which are task-dependent and costly to tune. We propose Recurrent Equivariant Constraint Modulation (RECM), a layer-wise constraint modulation mechanism that learns appropriate relaxation levels solely from the training signal and the symmetry properties of each layer's input-target distribution, without requiring any prior knowledge about the task-dependent target relaxation level. We demonstrate that under the proposed RECM update, the relaxation level of each layer provably converges to a value upper-bounded by its symmetry gap, namely the degree to which its input-target distribution deviates from exact symmetry. Consequently, layers processing symmetric distributions recover full equivariance, while those with approximate symmetries retain sufficient flexibility to learn non-symmetric solutions when warranted by the data. Empirically, RECM outperforms prior methods across diverse exact and approximate equivariant tasks, including the challenging molecular conformer generation on the GEOM-Drugs dataset.

</details>


### [351] [Merging Beyond: Streaming LLM Updates via Activation-Guided Rotations](https://arxiv.org/abs/2602.03237)
*Yuxuan Yao,Haonan Sheng,Qingsong Lv,Han Wu,Shuqi Liu,Zehua Liu,Zengyan Liu,Jiahui Gao,Haochen Tan,Xiaojin Fu,Haoli Bai,Hing Cheung So,Zhijiang Guo,Linqi Song*

Main category: cs.LG

TL;DR: Streaming Merging introduces ARM, an Activation-guided Rotation-aware Merging strategy, to iteratively optimize LLM adaptation by approximating gradient descent dynamics, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: The need for efficient adaptation techniques for increasingly large LLMs drives the exploration of model merging beyond post-hoc refinements, aiming to capture dynamic optimization benefits like supervised fine-tuning.

Method: The proposed ARM strategy treats merging coefficients as learning rates and uses activation subspaces to derive rotation vectors, steering parameter updates along data-driven trajectories and preserving geometric structures.

Result: ARM surpasses fully converged SFT models across scales (1.7B to 14B) and domains (math, code), requiring only early SFT checkpoints for iterative merging.

Conclusion: ARM offers a scalable, lightweight framework for efficient LLM adaptation, transcending traditional linear interpolation and converged checkpoints.

Abstract: The escalating scale of Large Language Models (LLMs) necessitates efficient adaptation techniques. Model merging has gained prominence for its efficiency and controllability. However, existing merging techniques typically serve as post-hoc refinements or focus on mitigating task interference, often failing to capture the dynamic optimization benefits of supervised fine-tuning (SFT). In this work, we propose Streaming Merging, an innovative model updating paradigm that conceptualizes merging as an iterative optimization process. Central to this paradigm is \textbf{ARM} (\textbf{A}ctivation-guided \textbf{R}otation-aware \textbf{M}erging), a strategy designed to approximate gradient descent dynamics. By treating merging coefficients as learning rates and deriving rotation vectors from activation subspaces, ARM effectively steers parameter updates along data-driven trajectories. Unlike conventional linear interpolation, ARM aligns semantic subspaces to preserve the geometric structure of high-dimensional parameter evolution. Remarkably, ARM requires only early SFT checkpoints and, through iterative merging, surpasses the fully converged SFT model. Experimental results across model scales (1.7B to 14B) and diverse domains (e.g., math, code) demonstrate that ARM can transcend converged checkpoints. Extensive experiments show that ARM provides a scalable and lightweight framework for efficient model adaptation.

</details>


### [352] [When pre-training hurts LoRA fine-tuning: a dynamical analysis via single-index models](https://arxiv.org/abs/2602.02855)
*Gibbs Nwemadji,Bruno Loureiro,Jean Barbier*

Main category: cs.LG

TL;DR: Excessive pre-training can slow down fine-tuning optimization, contrary to intuition.


<details>
  <summary>Details</summary>
Motivation: To investigate why pre-training doesn't always aid fine-tuning, especially when tasks are aligned.

Method: Mathematical analysis of low-rank adaptation (LoRA) fine-tuning on single-index models using one-pass SGD.

Result: Strong pre-training can prolong the search phase and hinder convergence.

Conclusion: Pre-training strength and task difficulty jointly impact LoRA fine-tuning dynamics.

Abstract: Pre-training on a source task is usually expected to facilitate fine-tuning on similar downstream problems. In this work, we mathematically show that this naive intuition is not always true: excessive pre-training can computationally slow down fine-tuning optimization. We study this phenomenon for low-rank adaptation (LoRA) fine-tuning on single-index models trained under one-pass SGD. Leveraging a summary statistics description of the fine-tuning dynamics, we precisely characterize how the convergence rate depends on the initial fine-tuning alignment and the degree of non-linearity of the target task. The key take away is that even when the pre-training and down- stream tasks are well aligned, strong pre-training can induce a prolonged search phase and hinder convergence. Our theory thus provides a unified picture of how pre-training strength and task difficulty jointly shape the dynamics and limitations of LoRA fine-tuning in a nontrivial tractable model.

</details>


### [353] [R1-SyntheticVL: Is Synthetic Data from Generative Models Ready for Multimodal Large Language Model?](https://arxiv.org/abs/2602.03300)
*Jingyi Zhang,Tianyi Lin,Huanjin Yao,Xiang Lan,Shunyu Liu,Jiaxing Huang*

Main category: cs.LG

TL;DR: CADS is a novel approach for synthesizing diverse, high-quality multimodal data using collective intelligence and adversarial learning to enhance MLLMs.


<details>
  <summary>Details</summary>
Motivation: Develop effective data synthesis techniques to improve MLLMs' performance on complex real-world tasks.

Method: CADS uses cyclic phases (CAD-Generate and CAD-Judge) and Adversarial Context Optimization to synthesize challenging, diverse multimodal data.

Result: Constructed MMSynthetic-20K dataset and trained R1-SyntheticVL model, achieving superior benchmark performance.

Conclusion: CADS effectively enhances MLLMs by generating high-quality, diverse, and challenging training data.

Abstract: In this work, we aim to develop effective data synthesis techniques that autonomously synthesize multimodal training data for enhancing MLLMs in solving complex real-world tasks. To this end, we propose Collective Adversarial Data Synthesis (CADS), a novel and general approach to synthesize high-quality, diverse and challenging multimodal data for MLLMs. The core idea of CADS is to leverage collective intelligence to ensure high-quality and diverse generation, while exploring adversarial learning to synthesize challenging samples for effectively driving model improvement. Specifically, CADS operates with two cyclic phases, i.e., Collective Adversarial Data Generation (CAD-Generate) and Collective Adversarial Data Judgment (CAD-Judge). CAD-Generate leverages collective knowledge to jointly generate new and diverse multimodal data, while CAD-Judge collaboratively assesses the quality of synthesized data. In addition, CADS introduces an Adversarial Context Optimization mechanism to optimize the generation context to encourage challenging and high-value data generation. With CADS, we construct MMSynthetic-20K and train our model R1-SyntheticVL, which demonstrates superior performance on various benchmarks.

</details>


### [354] [Late-Stage Generalization Collapse in Grokking: Detecting anti-grokking with Weightwatcher](https://arxiv.org/abs/2602.02859)
*Hari K Prakash,Charles H Martin*

Main category: cs.LG

TL;DR: The paper explores 'anti-grokking,' a late-stage collapse of generalization in neural networks, identified through extended training beyond standard grokking phases. It uses spectral analysis to diagnose this phenomenon and compares it with other grokking diagnostics.


<details>
  <summary>Details</summary>
Motivation: To provide a precise operational definition of memorization in neural networks by investigating 'anti-grokking,' a previously unreported phase where generalization collapses after initial success.

Method: The study revisits two grokking setups (a 3-layer MLP on MNIST and a transformer on modular addition) with extended training. It uses WeightWatcher for spectral analysis, focusing on Correlation Traps and HTSR layer quality metrics.

Result: Anti-grokking is marked by the emergence of Correlation Traps and deviations in HTSR metrics, leading to test accuracy collapse while training accuracy remains high. Other diagnostics fail to detect this phase.

Conclusion: Correlation Traps impair generalization and can cause catastrophic forgetting or prototype memorization, with similar issues observed in large-scale LLMs.

Abstract: \emph{Memorization} in neural networks lacks a precise operational definition and is often inferred from the grokking regime, where training accuracy saturates while test accuracy remains very low. We identify a previously unreported third phase of grokking in this training regime: \emph{anti-grokking}, a late-stage collapse of generalization.
  We revisit two canonical grokking setups: a 3-layer MLP trained on a subset of MNIST and a transformer trained on modular addition, but extended training far beyond standard. In both cases, after models transition from pre-grokking to successful generalization, test accuracy collapses back to chance while training accuracy remains perfect, indicating a distinct post-generalization failure mode.
  To diagnose anti-grokking, we use the open-source \texttt{WeightWatcher} tool based on HTSR/SETOL theory. The primary signal is the emergence of \emph{Correlation Traps}: anomalously large eigenvalues beyond the Marchenko--Pastur bulk in the empirical spectral density of shuffled weight matrices, which are predicted to impair generalization. As a secondary signal, anti-grokking corresponds to the average HTSR layer quality metric $α$ deviating from $2.0$. Neither metric requires access to the test or training data.
  We compare these signals to alternative grokking diagnostic, including $\ell_2$ norms, Activation Sparsity, Absolute Weight Entropy, and Local Circuit Complexity. These track pre-grokking and grokking but fail to identify anti-grokking. Finally, we show that Correlation Traps can induce catastrophic forgetting and/or prototype memorization, and observe similar pathologies in large-scale LLMs, like OSS GPT 20/120B.

</details>


### [355] [Robustness as an Emergent Property of Task Performance](https://arxiv.org/abs/2602.03344)
*Shir Ashury-Tahan,Ariel Gera,Elron Bandel,Michal Shmueli-Scheuer,Leshem Choshen*

Main category: cs.LG

TL;DR: As models achieve high performance on tasks, robustness naturally follows, driven by task-specific competence rather than inherent model properties.


<details>
  <summary>Details</summary>
Motivation: To understand the relationship between task performance and robustness, challenging the view of robustness as an independent capability.

Method: Empirical analysis of multiple models across diverse datasets and configurations (e.g., paraphrases, different temperatures).

Result: Strong positive correlation between high task performance and robustness, with robustness emerging as tasks saturate.

Conclusion: Explicit robustness efforts may be less needed as robustness develops alongside performance, indicating reliable model deployment for easier tasks.

Abstract: Robustness is often regarded as a critical future challenge for real-world applications, where stability is essential. However, as models often learn tasks in a similar order, we hypothesize that easier tasks will be easier regardless of how they are presented to the model. Indeed, in this paper, we show that as models approach high performance on a task, robustness is effectively achieved. Through an empirical analysis of multiple models across diverse datasets and configurations (e.g., paraphrases, different temperatures), we find a strong positive correlation. Moreover, we find that robustness is primarily driven by task-specific competence rather than inherent model-level properties, challenging current approaches that treat robustness as an independent capability. Thus, from a high-level perspective, we may expect that as new tasks saturate, model robustness on these tasks will emerge accordingly. For researchers, this implies that explicit efforts to measure and improve robustness may warrant reduced emphasis, as such robustness is likely to develop alongside performance gains. For practitioners, it acts as a sign that indeed the tasks that the literature deals with are unreliable, but on easier past tasks, the models are reliable and ready for real-world deployment.

</details>


### [356] [A Geometry-Aware Efficient Algorithm for Compositional Entropic Risk Minimization](https://arxiv.org/abs/2602.02877)
*Xiyuan Wei,Linli Zhou,Bokun Wang,Chih-Jen Lin,Tianbao Yang*

Main category: cs.LG

TL;DR: The paper proposes SCENT, a geometry-aware stochastic algorithm for compositional entropic risk minimization, addressing limitations like non-convergence and slow rates. It achieves an O(1/√T) convergence rate and outperforms baselines in tasks like extreme classification.


<details>
  <summary>Details</summary>
Motivation: Existing algorithms for entropic risk minimization suffer from non-convergence, numerical instability, and slow convergence. The paper aims to overcome these by leveraging dual formulations and geometry-aware updates.

Method: SCENT uses stochastic proximal mirror descent (SPMD) with a Bregman divergence tailored to the problem's geometry. It tackles the dual formulation as a min-min optimization problem.

Result: SCENT achieves an O(1/√T) convergence rate for convex problems and outperforms baselines in practical tasks like extreme classification and contrastive learning.

Conclusion: SCENT effectively addresses key limitations in entropic risk optimization, demonstrating theoretical and empirical superiority over existing methods.

Abstract: This paper studies optimization for a family of problems termed $\textbf{compositional entropic risk minimization}$, in which each data's loss is formulated as a Log-Expectation-Exponential (Log-E-Exp) function. The Log-E-Exp formulation serves as an abstraction of the Log-Sum-Exponential (LogSumExp) function when the explicit summation inside the logarithm is taken over a gigantic number of items and is therefore expensive to evaluate. While entropic risk objectives of this form arise in many machine learning problems, existing optimization algorithms suffer from several fundamental limitations including non-convergence, numerical instability, and slow convergence rates. To address these limitations, we propose a geometry-aware stochastic algorithm, termed $\textbf{SCENT}$, for the dual formulation of entropic risk minimization cast as a min--min optimization problem. The key to our design is a $\textbf{stochastic proximal mirror descent (SPMD)}$ update for the dual variable, equipped with a Bregman divergence induced by a negative exponential function that faithfully captures the geometry of the objective. Our main contributions are threefold: (i) we establish an $O(1/\sqrt{T})$ convergence rate of the proposed SCENT algorithm for convex problems; (ii) we theoretically characterize the advantages of SPMD over standard SGD update for optimizing the dual variable; and (iii) we demonstrate the empirical effectiveness of SCENT on extreme classification, partial AUC maximization, contrastive learning and distributionally robust optimization, where it consistently outperforms existing baselines.

</details>


### [357] [Mixture of Concept Bottleneck Experts](https://arxiv.org/abs/2602.02886)
*Francesco De Santis,Gabriele Ciravegna,Giovanni De Felice,Arianna Casanova,Francesco Giannini,Michelangelo Diligenti,Mateo Espinosa Zarlenga,Pietro Barbiero,Johannes Schneider,Danilo Giordano*

Main category: cs.LG

TL;DR: M-CBEs generalize Concept Bottleneck Models by varying experts and their functional forms, improving adaptability and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing CBMs are limited by fixed predictors, restricting accuracy and adaptability. M-CBEs address this gap.

Method: Proposes Linear M-CBE (finite linear expressions) and Symbolic M-CBE (symbolic regression with user-specified operators).

Result: Empirical results show M-CBEs navigate accuracy-interpretability trade-offs effectively.

Conclusion: M-CBEs offer a flexible framework to adapt to diverse user and task needs.

Abstract: Concept Bottleneck Models (CBMs) promote interpretability by grounding predictions in human-understandable concepts. However, existing CBMs typically fix their task predictor to a single linear or Boolean expression, limiting both predictive accuracy and adaptability to diverse user needs. We propose Mixture of Concept Bottleneck Experts (M-CBEs), a framework that generalizes existing CBMs along two dimensions: the number of experts and the functional form of each expert, exposing an underexplored region of the design space. We investigate this region by instantiating two novel models: Linear M-CBE, which learns a finite set of linear expressions, and Symbolic M-CBE, which leverages symbolic regression to discover expert functions from data under user-specified operator vocabularies. Empirical evaluation demonstrates that varying the mixture size and functional form provides a robust framework for navigating the accuracy-interpretability trade-off, adapting to different user and task needs.

</details>


### [358] [MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling](https://arxiv.org/abs/2602.03359)
*Ning Ding,Fangcheng Liu,Kyungrae Kim,Linji Hao,Kyeng-Hun Lee,Hyeonmok Ko,Yehui Tang*

Main category: cs.LG

TL;DR: MeKi scales LLMs using storage instead of FLOPs, enabling efficient edge deployment without latency overhead.


<details>
  <summary>Details</summary>
Motivation: Deploying performant LLMs on edge devices with limited resources is challenging but crucial for user experience.

Method: Introduces MeKi, using token-level memory experts and re-parameterization to fold training parameters into ROM lookup tables.

Result: MeKi outperforms dense LLMs at identical inference speeds, validating memory-based scaling.

Conclusion: MeKi offers a practical solution for scaling LLMs on edge devices by leveraging storage over computation.

Abstract: Scaling Large Language Models (LLMs) typically relies on increasing the number of parameters or test-time computations to boost performance. However, these strategies are impractical for edge device deployment due to limited RAM and NPU resources. Despite hardware constraints, deploying performant LLM on edge devices such as smartphone remains crucial for user experience. To address this, we propose MeKi (Memory-based Expert Knowledge Injection), a novel system that scales LLM capacity via storage space rather than FLOPs. MeKi equips each Transformer layer with token-level memory experts that injects pre-stored semantic knowledge into the generation process. To bridge the gap between training capacity and inference efficiency, we employ a re-parameterization strategy to fold parameter matrices used during training into a compact static lookup table. By offloading the knowledge to ROM, MeKi decouples model capacity from computational cost, introducing zero inference latency overhead. Extensive experiments demonstrate that MeKi significantly outperforms dense LLM baselines with identical inference speed, validating the effectiveness of memory-based scaling paradigm for on-device LLMs. Project homepage is at https://github.com/ningding-o/MeKi.

</details>


### [359] [Self-Soupervision: Cooking Model Soups without Labels](https://arxiv.org/abs/2602.02890)
*Anthony Fuller,James R. Green,Evan Shelhamer*

Main category: cs.LG

TL;DR: Model soups combine parameters from fine-tuned models (ingredients) into one (soup) to improve predictions. Self-Souping extends this to self-supervised learning, enhancing robustness and accuracy.


<details>
  <summary>Details</summary>
Motivation: Extend model soups to self-supervised learning to improve predictions using diverse data sources and algorithms.

Method: Fine-tune models (ingredients) on diverse SSL algorithms or hyperparameters, then mix their parameters into a single model (soup).

Result: Self-Souping boosts robustness (+3.5% ImageNet-C, +7% LAION-C) and creates soups outperforming individual SSL ingredients.

Conclusion: Self-Souping generalizes soups to SSL, enabling diverse ingredient combinations for improved robustness and accuracy.

Abstract: Model soups are strange and strangely effective combinations of parameters. They take a model (the stock), fine-tune it into multiple models (the ingredients), and then mix their parameters back into one model (the soup) to improve predictions. While all known soups require supervised learning, and optimize the same loss on labeled data, our recipes for Self-\emph{Soup}ervision generalize soups to self-supervised learning (SSL). Our Self-Souping lets us flavor ingredients on new data sources, e.g. from unlabeled data from a task for transfer or from a shift for robustness. We show that Self-Souping on corrupted test data, then fine-tuning back on uncorrupted train data, boosts robustness by +3.5\% (ImageNet-C) and +7\% (LAION-C). Self-\emph{Soup}ervision also unlocks countless SSL algorithms to cook the diverse ingredients needed for more robust soups. We show for the first time that ingredients can differ in their SSL hyperparameters -- and more surprisingly, in their SSL algorithms. We cook soups of MAE, MoCoV3, and MMCR ingredients that are more accurate than any one single SSL ingredient.

</details>


### [360] [Controlled disagreement improves generalization in decentralized training](https://arxiv.org/abs/2602.02899)
*Zesen Wang,Mikael Johansson*

Main category: cs.LG

TL;DR: DSGD-AC introduces adaptive consensus errors in decentralized SGD, proving they guide optimization toward flatter minima, outperforming standard DSGD and centralized SGD in accuracy and flatness.


<details>
  <summary>Details</summary>
Motivation: Challenges the belief that consensus errors in decentralized training harm performance, proposing they can instead act as beneficial regularizers.

Method: Proposes DSGD-AC, a decentralized SGD variant that preserves non-vanishing consensus errors with a time-dependent scaling mechanism.

Result: DSGD-AC consistently outperforms standard DSGD and centralized SGD in test accuracy and solution flatness across benchmarks.

Conclusion: Consensus errors serve as implicit regularizers, offering a new design perspective for decentralized learning algorithms.

Abstract: Decentralized training is often regarded as inferior to centralized training because the consensus errors between workers are thought to undermine convergence and generalization, even with homogeneous data distributions. This work challenges this view by introducing decentralized SGD with Adaptive Consensus (DSGD-AC), which intentionally preserves non-vanishing consensus errors through a time-dependent scaling mechanism. We prove that these errors are not random noise but systematically align with the dominant Hessian subspace, acting as structured perturbations that guide optimization toward flatter minima. Across image classification and machine translation benchmarks, DSGD-AC consistently surpasses both standard DSGD and centralized SGD in test accuracy and solution flatness. Together, these results establish consensus errors as a useful implicit regularizer and open a new perspective on the design of decentralized learning algorithms.

</details>


### [361] [When Single Answer Is Not Enough: Rethinking Single-Step Retrosynthesis Benchmarks for LLMs](https://arxiv.org/abs/2602.03554)
*Bogdan Zagribelnyy,Ivan Ilin,Maksim Kuznetsov,Nikita Bondarev,Roman Schutski,Thomas MacDougall,Rim Shayakhmetov,Zulfat Miftakhutdinov,Mikolaj Mizera,Vladimir Aladinskiy,Alex Aliper,Alex Zhavoronkov*

Main category: cs.LG

TL;DR: A new benchmarking framework evaluates LLMs for retrosynthesis using ChemCensor, a plausibility metric, and introduces CREED, a dataset for improved model training.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for retrosynthesis lack alignment with real-world open-ended synthesis planning and rely on single ground-truth accuracy.

Method: Proposes ChemCensor for evaluating plausibility and CREED dataset for training LLMs, testing general and specialized models.

Result: The trained model outperforms baselines under the new benchmark, emphasizing plausibility over exact matches.

Conclusion: The framework better reflects human synthesis practices and advances LLM use in drug discovery.

Abstract: Recent progress has expanded the use of large language models (LLMs) in drug discovery, including synthesis planning. However, objective evaluation of retrosynthesis performance remains limited. Existing benchmarks and metrics typically rely on published synthetic procedures and Top-K accuracy based on single ground-truth, which does not capture the open-ended nature of real-world synthesis planning. We propose a new benchmarking framework for single-step retrosynthesis that evaluates both general-purpose and chemistry-specialized LLMs using ChemCensor, a novel metric for chemical plausibility. By emphasizing plausibility over exact match, this approach better aligns with human synthesis planning practices. We also introduce CREED, a novel dataset comprising millions of ChemCensor-validated reaction records for LLM training, and use it to train a model that improves over the LLM baselines under this benchmark.

</details>


### [362] [Manifold-Constrained Energy-Based Transition Models for Offline Reinforcement Learning](https://arxiv.org/abs/2602.02900)
*Zeyu Fang,Zuyuan Zhang,Mahdi Imani,Tian Lan*

Main category: cs.LG

TL;DR: MC-ETM improves offline RL robustness by learning energy-based transition models with manifold constraints, truncating unreliable rollouts, and stabilizing Bellman backups with pessimistic penalties.


<details>
  <summary>Details</summary>
Motivation: Address brittleness in model-based offline RL caused by distribution shift and compounding model error, leading to value overestimation.

Method: Train conditional energy-based transition models using manifold projection--diffusion negative sampling, learn latent manifolds, and stabilize policy optimization with energy-based reliability signals and pessimistic penalties.

Result: Improved multi-step dynamics fidelity and higher normalized returns on offline control benchmarks, especially under irregular dynamics and sparse data.

Conclusion: MC-ETM effectively mitigates distribution shift issues in offline RL by leveraging energy-based modeling and conservative optimization strategies.

Abstract: Model-based offline reinforcement learning is brittle under distribution shift: policy improvement drives rollouts into state--action regions weakly supported by the dataset, where compounding model error yields severe value overestimation. We propose Manifold-Constrained Energy-based Transition Models (MC-ETM), which train conditional energy-based transition models using a manifold projection--diffusion negative sampler. MC-ETM learns a latent manifold of next states and generates near-manifold hard negatives by perturbing latent codes and running Langevin dynamics in latent space with the learned conditional energy, sharpening the energy landscape around the dataset support and improving sensitivity to subtle out-of-distribution deviations. For policy optimization, the learned energy provides a single reliability signal: rollouts are truncated when the minimum energy over sampled next states exceeds a threshold, and Bellman backups are stabilized via pessimistic penalties based on Q-value-level dispersion across energy-guided samples. We formalize MC-ETM through a hybrid pessimistic MDP formulation and derive a conservative performance bound separating in-support evaluation error from truncation risk. Empirically, MC-ETM improves multi-step dynamics fidelity and yields higher normalized returns on standard offline control benchmarks, particularly under irregular dynamics and sparse data coverage.

</details>


### [363] [Spatiotemporal Decision Transformer for Traffic Coordination](https://arxiv.org/abs/2602.02903)
*Haoran Su,Yandong Sun,Hanxiao Deng*

Main category: cs.LG

TL;DR: MADT reformulates multi-agent traffic signal control as a sequence modeling problem, improving coordination and reducing travel time by 5-6%.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in multi-agent coordination and sample efficiency in urban traffic signal control.

Method: Extends Decision Transformer with graph attention, temporal transformer encoder, and return-to-go conditioning.

Result: Achieves state-of-the-art performance, reducing travel time by 5-6% and enhancing coordination.

Conclusion: MADT offers an effective, efficient solution for multi-agent traffic signal control, leveraging offline learning.

Abstract: Traffic signal control is a critical challenge in urban transportation, requiring coordination among multiple intersections to optimize network-wide traffic flow. While reinforcement learning has shown promise for adaptive signal control, existing methods struggle with multi-agent coordination and sample efficiency. We introduce MADT (Multi-Agent Decision Transformer), a novel approach that reformulates multi-agent traffic signal control as a sequence modeling problem. MADT extends the Decision Transformer paradigm to multi-agent settings by incorporating: (1) a graph attention mechanism for modeling spatial dependencies between intersections, (2) a|temporal transformer encoder for capturing traffic dynamics, and (3) return-to-go conditioning for target performance specification. Our approach enables offline learning from historical traffic data, with architecture design that facilitates potential online fine-tuning. Experiments on synthetic grid networks and real-world traffic scenarios demonstrate that MADT achieves state-of-the-art performance, reducing average travel time by 5-6% compared to the strongest baseline while exhibiting superior coordination among adjacent intersections.

</details>


### [364] [A Random Matrix Theory Perspective on the Consistency of Diffusion Models](https://arxiv.org/abs/2602.02908)
*Binxu Wang,Jacob Zavatone-Veth,Cengiz Pehlevan*

Main category: cs.LG

TL;DR: Linear diffusion models trained on non-overlapping dataset subsets produce similar outputs due to shared Gaussian statistics. A random matrix theory framework explains how dataset properties affect denoiser behavior, including overshrinking low-variance directions and cross-split disagreement.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand why diffusion models trained on different dataset subsets yield similar outputs for the same noise seed, linking this to shared Gaussian statistics and dataset properties.

Method: The authors develop a random matrix theory (RMT) framework to analyze the expectation and variance of learned denoisers and sampling maps in the linear setting. They extend deterministic-equivalence tools to fractional matrix powers.

Result: The theory explains overshrinking of low-variance directions and cross-split disagreement due to anisotropy, inhomogeneity, and dataset size. Predictions are validated on UNet and DiT architectures, showing deviations across splits.

Conclusion: The work provides a principled baseline for reproducibility in diffusion training, connecting data's spectral properties to generative output stability.

Abstract: Diffusion models trained on different, non-overlapping subsets of a dataset often produce strikingly similar outputs when given the same noise seed. We trace this consistency to a simple linear effect: the shared Gaussian statistics across splits already predict much of the generated images. To formalize this, we develop a random matrix theory (RMT) framework that quantifies how finite datasets shape the expectation and variance of the learned denoiser and sampling map in the linear setting. For expectations, sampling variability acts as a renormalization of the noise level through a self-consistent relation $σ^2 \mapsto κ(σ^2)$, explaining why limited data overshrink low-variance directions and pull samples toward the dataset mean. For fluctuations, our variance formulas reveal three key factors behind cross-split disagreement: \textit{anisotropy} across eigenmodes, \textit{inhomogeneity} across inputs, and overall scaling with dataset size. Extending deterministic-equivalence tools to fractional matrix powers further allows us to analyze entire sampling trajectories. The theory sharply predicts the behavior of linear diffusion models, and we validate its predictions on UNet and DiT architectures in their non-memorization regime, identifying where and how samples deviates across training data split. This provides a principled baseline for reproducibility in diffusion training, linking spectral properties of data to the stability of generative outputs.

</details>


### [365] [Conflict-Resolving and Sharpness-Aware Minimization for Generalized Knowledge Editing with Multiple Updates](https://arxiv.org/abs/2602.03696)
*Duy Nguyen,Hanqi Xiao,Archiki Prasad,Elias Stengel-Eskin,Hyunji Lee,Mohit Bansal*

Main category: cs.LG

TL;DR: CoRSA is a parameter-efficient framework for updating LLMs, improving generalization, stability, and conflict resolution, outperforming baselines significantly.


<details>
  <summary>Details</summary>
Motivation: Existing methods for updating LLMs suffer from poor generalization, limited stability, and knowledge conflict, necessitating a better approach.

Method: CoRSA integrates Conflict-Resolving and Sharpness-Aware Minimization to enhance generalization, stability, and conflict resolution through loss curvature minimization and margin maximization.

Result: CoRSA outperforms baselines with average improvements of 12.42% over LoRA and 10% over model editing methods, reducing catastrophic forgetting by 27.82% and excelling in code domain updates.

Conclusion: CoRSA provides a robust, efficient solution for updating LLMs, addressing key challenges and outperforming existing methods across benchmarks.

Abstract: Large language models (LLMs) rely on internal knowledge to solve many downstream tasks, making it crucial to keep them up to date. Since full retraining is expensive, prior work has explored efficient alternatives such as model editing and parameter-efficient fine-tuning. However, these approaches often break down in practice due to poor generalization across inputs, limited stability, and knowledge conflict. To address these limitations, we propose the CoRSA (Conflict-Resolving and Sharpness-Aware Minimization) training framework, a parameter-efficient, holistic approach for knowledge editing with multiple updates. CoRSA tackles multiple challenges simultaneously: it improves generalization to different input forms and enhances stability across multiple updates by minimizing loss curvature, and resolves conflicts by maximizing the margin between new and prior knowledge. Across three widely used fact editing benchmarks, CoRSA achieves significant gains in generalization, outperforming baselines with average absolute improvements of 12.42% over LoRA and 10% over model editing methods. With multiple updates, it maintains high update efficacy while reducing catastrophic forgetting by 27.82% compared to LoRA. CoRSA also generalizes to the code domain, outperforming the strongest baseline by 5.48% Pass@5 in update efficacy.

</details>


### [366] [Notes on the Reward Representation of Posterior Updates](https://arxiv.org/abs/2602.02912)
*Pedro A. Ortega*

Main category: cs.LG

TL;DR: The paper explores when decision-making as inference can be made literal, focusing on KL-regularized updates that align with Bayesian posteriors, revealing how behavioral change is driven by evidence.


<details>
  <summary>Details</summary>
Motivation: To understand when decision-making as inference (common in control and reinforcement learning) can be treated literally, not just metaphorically, by examining KL-regularized updates within a fixed probabilistic model.

Method: Analyze the case where a KL-regularized soft update corresponds to a Bayesian posterior, ensuring the update variable acts as a genuine information channel. Investigate how behavioral change is driven by evidence.

Result: Posterior updates determine relative incentive signals but not absolute rewards, which remain context-dependent. Coherence constraints link reward descriptions across different update directions.

Conclusion: The study provides a sharp identification of how Bayesian updates influence behavior, highlighting the role of evidence and context in shaping decision-making, while absolute rewards remain ambiguous.

Abstract: Many ideas in modern control and reinforcement learning treat decision-making as inference: start from a baseline distribution and update it when a signal arrives. We ask when this can be made literal rather than metaphorical. We study the special case where a KL-regularized soft update is exactly a Bayesian posterior inside a single fixed probabilistic model, so the update variable is a genuine channel through which information is transmitted. In this regime, behavioral change is driven only by evidence carried by that channel: the update must be explainable as an evidence reweighing of the baseline. This yields a sharp identification result: posterior updates determine the relative, context-dependent incentive signal that shifts behavior, but they do not uniquely determine absolute rewards, which remain ambiguous up to context-specific baselines. Requiring one reusable continuation value across different update directions adds a further coherence constraint linking the reward descriptions associated with different conditioning orders.

</details>


### [367] [Efficient Estimation of Kernel Surrogate Models for Task Attribution](https://arxiv.org/abs/2602.03783)
*Zhenshuo Zhang,Minxuan Duan,Hongyang R. Zhang*

Main category: cs.LG

TL;DR: The paper introduces kernel surrogate models for task attribution in AI training, outperforming linear models and influence functions with higher accuracy and practical benefits.


<details>
  <summary>Details</summary>
Motivation: Understanding how individual training tasks influence target task performance is crucial, but leave-one-out retraining is computationally expensive. Existing linear surrogate models miss nonlinear interactions.

Method: Proposes kernel surrogate models and a gradient-based estimation procedure for efficient learning, leveraging pretrained models' first-order approximations.

Result: Kernel surrogate models achieve 25% higher correlation with ground truth than linear models and improve task selection by 40%.

Conclusion: Kernel surrogate models effectively capture nonlinear task interactions and outperform existing methods, offering practical advantages in AI training.

Abstract: Modern AI agents such as large language models are trained on diverse tasks -- translation, code generation, mathematical reasoning, and text prediction -- simultaneously. A key question is to quantify how each individual training task influences performance on a target task, a problem we refer to as task attribution. The direct approach, leave-one-out retraining, measures the effect of removing each task, but is computationally infeasible at scale. An alternative approach that builds surrogate models to predict a target task's performance for any subset of training tasks has emerged in recent literature. Prior work focuses on linear surrogate models, which capture first-order relationships, but miss nonlinear interactions such as synergy, antagonism, or XOR-type effects. In this paper, we first consider a unified task weighting framework for analyzing task attribution methods, and show a new connection between linear surrogate models and influence functions through a second-order analysis. Then, we introduce kernel surrogate models, which more effectively represent second-order task interactions. To efficiently learn the kernel surrogate, we develop a gradient-based estimation procedure that leverages a first-order approximation of pretrained models; empirically, this yields accurate estimates with less than $2\%$ relative error without repeated retraining. Experiments across multiple domains -- including math reasoning in transformers, in-context learning, and multi-objective reinforcement learning -- demonstrate the effectiveness of kernel surrogate models. They achieve a $25\%$ higher correlation with the leave-one-out ground truth than linear surrogates and influence-function baselines. When used for downstream task selection, kernel surrogate models yield a $40\%$ improvement in demonstration selection for in-context learning and multi-objective reinforcement learning benchmarks.

</details>


### [368] [Weighted Temporal Decay Loss for Learning Wearable PPG Data with Sparse Clinical Labels](https://arxiv.org/abs/2602.02917)
*Yunsung Chung,Keum San Chun,Migyeong Gwak,Han Feng,Yingshuo Liu,Chanho Lim,Viswam Nathan,Nassir Marrouche,Sharanya Arcot Desai*

Main category: cs.LG

TL;DR: The paper proposes a training strategy to address the challenge of sparse clinical labels in wearable PPG-based health monitoring by learning biomarker-specific decay rates for sample weights, improving accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: The sparsity of clinical labels makes PPG biosignals temporally distant from lab draws unreliable for supervision. This motivates the need for a method to improve reliability and accuracy in health monitoring algorithms.

Method: The authors introduce a training strategy that learns biomarker-specific decay rates for sample weights based on the time gap between data segments and their labels, incorporating these weights into the loss function with a regularizer to avoid trivial solutions.

Result: The approach outperforms baselines, achieving 0.715 AUPRC (subject-wise) compared to 0.674 for a fine-tuned self-supervised baseline and 0.626 for a Random Forest. A linear decay function proves most robust.

Conclusion: The proposed method enhances accuracy in PPG-based health monitoring and provides interpretable insights into the temporal sensitivity of biomarkers.

Abstract: Advances in wearable computing and AI have increased interest in leveraging PPG for health monitoring over the past decade. One of the biggest challenges in developing health algorithms based on such biosignals is the sparsity of clinical labels, which makes biosignals temporally distant from lab draws less reliable for supervision. To address this problem, we introduce a simple training strategy that learns a biomarker-specific decay of sample weight over the time gap between a segment and its ground truth label and uses this weight in the loss with a regularizer to prevent trivial solutions. On smartwatch PPG from 450 participants across 10 biomarkers, the approach improves over baselines. In the subject-wise setting, the proposed approach averages 0.715 AUPRC, compared to 0.674 for a fine-tuned self-supervised baseline and 0.626 for a feature-based Random Forest. A comparison of four decay families shows that a simple linear decay function is most robust on average. Beyond accuracy, the learned decay rates summarize how quickly each biomarker's PPG evidence becomes stale, providing an interpretable view of temporal sensitivity.

</details>


### [369] [A Reproducible Framework for Bias-Resistant Machine Learning on Small-Sample Neuroimaging Data](https://arxiv.org/abs/2602.02920)
*Jagan Mohan Reddy Dwarampudi,Jennifer L Purks,Joshua Wong,Renjie Hu,Tania Banerjee*

Main category: cs.LG

TL;DR: A reproducible, bias-resistant ML framework for small-sample neuroimaging data, combining domain-informed features, nested CV, and threshold optimization, achieving unbiased results.


<details>
  <summary>Details</summary>
Motivation: Addresses optimistic bias in conventional CV frameworks, enhancing reproducibility and generalization for biomedical ML.

Method: Integrates domain-informed feature engineering, nested cross-validation, and calibrated decision-threshold optimization.

Result: Achieved nested-CV balanced accuracy of 0.660 ± 0.068 with interpretable feature subsets.

Conclusion: Provides a reliable ML blueprint for data-limited biomedical domains, emphasizing interpretability and unbiased evaluation.

Abstract: We introduce a reproducible, bias-resistant machine learning framework that integrates domain-informed feature engineering, nested cross-validation, and calibrated decision-threshold optimization for small-sample neuroimaging data. Conventional cross-validation frameworks that reuse the same folds for both model selection and performance estimation yield optimistically biased results, limiting reproducibility and generalization. Demonstrated on a high-dimensional structural MRI dataset of deep brain stimulation cognitive outcomes, the framework achieved a nested-CV balanced accuracy of 0.660\,$\pm$\,0.068 using a compact, interpretable subset selected via importance-guided ranking. By combining interpretability and unbiased evaluation, this work provides a generalizable computational blueprint for reliable machine learning in data-limited biomedical domains.

</details>


### [370] [Bridging Online and Offline RL: Contextual Bandit Learning for Multi-Turn Code Generation](https://arxiv.org/abs/2602.03806)
*Ziru Chen,Dongdong Chen,Ruinan Jin,Yingbin Liang,Yujia Xie,Huan Sun*

Main category: cs.LG

TL;DR: Cobalt is a new RL method combining online and offline learning for multi-turn code generation, outperforming baselines and improving LLMs on LiveCodeBench.


<details>
  <summary>Details</summary>
Motivation: The high cost and instability of online RL motivate a hybrid approach for multi-turn code generation tasks.

Method: Cobalt uses offline trajectories divided into prompts for online bandit learning, training LLMs for single-step code completions.

Result: Cobalt outperforms GRPO and VeRPO, improving R1-Distill 8B and Qwen3 8B by up to 9.0 and 6.2 Pass@1 scores.

Conclusion: Cobalt is effective for iterative decision-making tasks like multi-turn code generation.

Abstract: Recently, there have been significant research interests in training large language models (LLMs) with reinforcement learning (RL) on real-world tasks, such as multi-turn code generation. While online RL tends to perform better than offline RL, its higher training cost and instability hinders wide adoption. In this paper, we build on the observation that multi-turn code generation can be formulated as a one-step recoverable Markov decision process and propose contextual bandit learning with offline trajectories (Cobalt), a new method that combines the benefits of online and offline RL. Cobalt first collects code generation trajectories using a reference LLM and divides them into partial trajectories as contextual prompts. Then, during online bandit learning, the LLM is trained to complete each partial trajectory prompt through single-step code generation. Cobalt outperforms two multi-turn online RL baselines based on GRPO and VeRPO, and substantially improves R1-Distill 8B and Qwen3 8B by up to 9.0 and 6.2 absolute Pass@1 scores on LiveCodeBench. Also, we analyze LLMs' in-context reward hacking behaviors and augment Cobalt training with perturbed trajectories to mitigate this issue. Overall, our results demonstrate Cobalt as a promising solution for iterative decision-making tasks like multi-turn code generation. Our code and data are available at https://github.com/OSU-NLP-Group/cobalt.

</details>


### [371] [How Does the Lagrangian Guide Safe Reinforcement Learning through Diffusion Models?](https://arxiv.org/abs/2602.02924)
*Xiaoyuan Cheng,Wenxuan Yuan,Boyang Li,Yuanchao Xu,Yiming Yang,Hao Liang,Bei Peng,Robert Loftin,Zhuo Sun,Yukun Hu*

Main category: cs.LG

TL;DR: ALGD introduces an augmented Lagrangian to stabilize diffusion-based safe RL, addressing instability in policy generation and training while maintaining optimal policy distribution.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based RL methods lack consideration of safety in online settings and suffer from instability due to non-convex Lagrangian landscapes.

Method: ALGD uses an augmented Lagrangian to convexify the energy landscape, stabilizing policy generation and training in diffusion-based safe RL.

Result: ALGD achieves strong and stable performance across diverse environments, validated by theoretical analysis and experiments.

Conclusion: ALGD provides a theoretically grounded and effective solution for stable and safe diffusion-based RL.

Abstract: Diffusion policy sampling enables reinforcement learning (RL) to represent multimodal action distributions beyond suboptimal unimodal Gaussian policies. However, existing diffusion-based RL methods primarily focus on offline settings for reward maximization, with limited consideration of safety in online settings. To address this gap, we propose Augmented Lagrangian-Guided Diffusion (ALGD), a novel algorithm for off-policy safe RL. By revisiting optimization theory and energy-based model, we show that the instability of primal-dual methods arises from the non-convex Lagrangian landscape. In diffusion-based safe RL, the Lagrangian can be interpreted as an energy function guiding the denoising dynamics. Counterintuitively, direct usage destabilizes both policy generation and training. ALGD resolves this issue by introducing an augmented Lagrangian that locally convexifies the energy landscape, yielding a stabilized policy generation and training process without altering the distribution of the optimal policy. Theoretical analysis and extensive experiments demonstrate that ALGD is both theoretically grounded and empirically effective, achieving strong and stable performance across diverse environments.

</details>


### [372] [Refining Decision Boundaries In Anomaly Detection Using Similarity Search Within the Feature Space](https://arxiv.org/abs/2602.02925)
*Sidahmed Benabderrahmane,Petko Valtchev,James Cheney,Talal Rahwan*

Main category: cs.LG

TL;DR: Introducing SDA2E, an autoencoder combined with active learning, for detecting rare anomalies in imbalanced datasets like APTs, achieving superior performance with reduced labeled data.


<details>
  <summary>Details</summary>
Motivation: Detecting rare anomalies (e.g., APTs) in imbalanced datasets is challenging; active learning and exploiting geometric structures can improve efficiency.

Method: SDA2E combines sparse dual adversarial attention-based autoencoder with similarity-guided active learning (normal-like expansion, anomaly-like prioritization, hybrid strategy) and SIM_NM1 similarity measure.

Result: Achieves superior ranking (nDCG up to 1.0) with 80% less labeled data than passive training, validated on 52 datasets and benchmarks.

Conclusion: SDA2E offers a robust framework for anomaly detection, especially effective for cybersecurity applications like APT detection.

Abstract: Detecting rare and diverse anomalies in highly imbalanced datasets-such as Advanced Persistent Threats (APTs) in cybersecurity-remains a fundamental challenge for machine learning systems. Active learning offers a promising direction by strategically querying an oracle to minimize labeling effort, yet conventional approaches often fail to exploit the intrinsic geometric structure of the feature space for model refinement. In this paper, we introduce SDA2E, a Sparse Dual Adversarial Attention-based AutoEncoder designed to learn compact and discriminative latent representations from imbalanced, high-dimensional data. We further propose a similarity-guided active learning framework that integrates three novel strategies to refine decision boundaries efficiently: mormal-like expansion, which enriches the training set with points similar to labeled normals to improve reconstruction fidelity; anomaly-like prioritization, which boosts ranking accuracy by focusing on points resembling known anomalies; and a hybrid strategy that combines both for balanced model refinement and ranking. A key component of our framework is a new similarity measure, Normalized Matching 1s (SIM_NM1), tailored for sparse binary embeddings. We evaluate SDA2E extensively across 52 imbalanced datasets, including multiple DARPA Transparent Computing scenarios, and benchmark it against 15 state-of-the-art anomaly detection methods. Results demonstrate that SDA2E consistently achieves superior ranking performance (nDCG up to 1.0 in several cases) while reducing the required labeled data by up to 80% compared to passive training. Statistical tests confirm the significance of these improvements. Our work establishes a robust, efficient, and statistically validated framework for anomaly detection that is particularly suited to cybersecurity applications such as APT detection.

</details>


### [373] [Distance Marching for Generative Modeling](https://arxiv.org/abs/2602.02928)
*Zimo Wang,Ishit Mehta,Haolin Lu,Chung-En Sun,Ge Yan,Tsui-Wei Weng,Tzu-Mao Li*

Main category: cs.LG

TL;DR: Distance Marching improves time-unconditional generative models by focusing on closer targets and better denoising directions, achieving superior performance over baselines.


<details>
  <summary>Details</summary>
Motivation: Time-unconditional models face ambiguity in denoising directions due to lack of time conditioning, requiring a more principled approach.

Method: Proposes Distance Marching, a time-unconditional method with two inference techniques and losses targeting closer denoising goals.

Result: Improves FID by 13.5% on CIFAR-10 and ImageNet, outperforms flow matching with fewer sampling steps, and aids early stopping/OOD detection.

Conclusion: Distance field modeling offers a robust framework for generative tasks, enhancing performance and efficiency.

Abstract: Time-unconditional generative models learn time-independent denoising vector fields. But without time conditioning, the same noisy input may correspond to multiple noise levels and different denoising directions, which interferes with the supervision signal. Inspired by distance field modeling, we propose Distance Marching, a new time-unconditional approach with two principled inference methods. Crucially, we design losses that focus on closer targets. This yields denoising directions better directed toward the data manifold. Across architectures, Distance Marching consistently improves FID by 13.5% on CIFAR-10 and ImageNet over recent time-unconditional baselines. For class-conditional ImageNet generation, despite removing time input, Distance Marching surpasses flow matching using our losses and inference methods. It achieves lower FID than flow matching's final performance using 60% of the sampling steps and 13.6% lower FID on average across backbone sizes. Moreover, our distance prediction is also helpful for early stopping during sampling and for OOD detection. We hope distance field modeling can serve as a principled lens for generative modeling.

</details>


### [374] [RPG-AE: Neuro-Symbolic Graph Autoencoders with Rare Pattern Mining for Provenance-Based Anomaly Detection](https://arxiv.org/abs/2602.02929)
*Asif Tauhid,Sidahmed Benabderrahmane,Mohamad Altrabulsi,Ahamed Foisal,Talal Rahwan*

Main category: cs.LG

TL;DR: A neuro-symbolic framework combining Graph Autoencoder and rare pattern mining improves APT detection in system-level provenance data.


<details>
  <summary>Details</summary>
Motivation: APT attacks are hard to detect due to their stealthy nature, requiring advanced methods beyond traditional anomaly detection.

Method: Uses a Graph Autoencoder to learn normal relational structure and applies rare pattern mining to boost anomaly detection.

Result: Outperforms baseline GAE and existing unsupervised methods on DARPA datasets, achieving competitive performance.

Conclusion: Coupling graph-based learning with pattern mining enhances detection effectiveness and interpretability.

Abstract: Advanced Persistent Threats (APTs) are sophisticated, long-term cyberattacks that are difficult to detect because they operate stealthily and often blend into normal system behavior. This paper presents a neuro-symbolic anomaly detection framework that combines a Graph Autoencoder (GAE) with rare pattern mining to identify APT-like activities in system-level provenance data. Our approach first constructs a process behavioral graph using k-Nearest Neighbors based on feature similarity, then learns normal relational structure using a Graph Autoencoder. Anomaly candidates are identified through deviations between observed and reconstructed graph structure. To further improve detection, we integrate an rare pattern mining module that discovers infrequent behavioral co-occurrences and uses them to boost anomaly scores for processes exhibiting rare signatures. We evaluate the proposed method on the DARPA Transparent Computing datasets and show that rare-pattern boosting yields substantial gains in anomaly ranking quality over the baseline GAE. Compared with existing unsupervised approaches on the same benchmark, our single unified model consistently outperforms individual context-based detectors and achieves performance competitive with ensemble aggregation methods that require multiple separate detectors. These results highlight the value of coupling graph-based representation learning with classical pattern mining to improve both effectiveness and interpretability in provenance-based security anomaly detection.

</details>


### [375] [Rare Event Early Detection: A Dataset of Sepsis Onset for Critically Ill Trauma Patients](https://arxiv.org/abs/2602.02930)
*Yin Jin,Tucker R. Stewart,Deyi Zhou,Chhavi Gupta,Arjita Nema,Scott C. Brakenridge,Grant E. O'Keefe,Juhua Hu*

Main category: cs.LG

TL;DR: The paper introduces a new dataset for post-trauma sepsis detection, addressing gaps in existing ICU datasets, and establishes a benchmark for early detection.


<details>
  <summary>Details</summary>
Motivation: Early detection of sepsis improves outcomes, but current public datasets overlook trauma patients' unique challenges.

Method: Created a standardized post-trauma sepsis dataset from MIMIC-III, validated it, and framed sepsis detection as a rare event problem.

Result: Benchmark experiments highlight the need for further research using the new dataset.

Conclusion: Targeted datasets for post-trauma sepsis are crucial for advancing early detection methods.

Abstract: Sepsis is a major public health concern due to its high morbidity, mortality, and cost. Its clinical outcome can be substantially improved through early detection and timely intervention. By leveraging publicly available datasets, machine learning (ML) has driven advances in both research and clinical practice. However, existing public datasets consider ICU patients (Intensive Care Unit) as a uniform group and neglect the potential challenges presented by critically ill trauma patients in whom injury-related inflammation and organ dysfunction can overlap with the clinical features of sepsis. We propose that a targeted identification of post-traumatic sepsis is necessary in order to develop methods for early detection. Therefore, we introduce a publicly available standardized post-trauma sepsis onset dataset extracted, relabeled using standardized post-trauma clinical facts, and validated from MIMIC-III. Furthermore, we frame early detection of post-trauma sepsis onset according to clinical workflow in ICUs in a daily basis resulting in a new rare event detection problem. We then establish a general benchmark through comprehensive experiments, which shows the necessity of further advancements using this new dataset. The data code is available at https://github.com/ML4UWHealth/SepsisOnset_TraumaCohort.git.

</details>


### [376] [3D-Learning: Diffusion-Augmented Distributionally Robust Decision-Focused Learning](https://arxiv.org/abs/2602.02943)
*Jiaqi Wen,Lei Fan,Jianyi Yang*

Main category: cs.LG

TL;DR: The paper introduces DR-DFL and 3D-Learning to improve ML predictor robustness against OOD samples by optimizing worst-case decision performance.


<details>
  <summary>Details</summary>
Motivation: Current ML predictors in PTO pipelines degrade under OOD conditions, harming downstream tasks.

Method: Proposes 3D-Learning, a framework using diffusion models to identify worst-case distributions for robust training.

Result: 3D-Learning outperforms existing DRO and Data Augmentation methods in OOD generalization.

Conclusion: 3D-Learning balances average and worst-case performance, improving robustness in real-world tasks.

Abstract: Predict-then-Optimize (PTO) pipelines are widely employed in computing and networked systems, where Machine Learning (ML) models are used to predict critical contextual information for downstream decision-making tasks such as cloud LLM serving, data center demand response, and edge workload scheduling. However, these ML predictors are often vulnerable to out-of-distribution (OOD) samples at test time, leading to significant decision performance degradation due to large prediction errors. To address the generalization challenges under OOD conditions, we present the framework of Distributionally Robust Decision-Focused Learning (DR-DFL), which trains ML models to optimize decision performance under the worst-case distribution. Instead of relying on classical Distributionally Robust Optimization (DRO) techniques, we propose Diffusion-Augmented Distributionally Robust Decision-Focused Learning (3D-Learning), which searches for the worst-case distribution within the parameterized space of a diffusion model. By leveraging the powerful distribution modeling capabilities of diffusion models, 3D-Learning identifies worst-case distributions that remain consistent with real data, achieving a favorable balance between average and worst-case scenarios. Empirical results on an LLM resource provisioning task demonstrate that 3D-Learning outperforms existing DRO and Data Augmentation methods in OOD generalization performance.

</details>


### [377] [Variational Sparse Paired Autoencoders (vsPAIR) for Inverse Problems and Uncertainty Quantification](https://arxiv.org/abs/2602.02948)
*Jack Michael Solomon,Rishi Leburu,Matthias Chung*

Main category: cs.LG

TL;DR: The paper introduces vsPAIR, a Variational Sparse Paired Autoencoder, for solving inverse problems with interpretable uncertainty estimates.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of providing fast inference and interpretable uncertainty estimates in inverse problems.

Method: Pairs a standard VAE encoding observations with a sparse VAE encoding quantities of interest, connected via a latent mapping. Includes modifications like hard-concrete spike-and-slab relaxation and beta hyperprior.

Result: Effective performance demonstrated in blind inpainting and computed tomography, providing interpretable uncertainty.

Conclusion: vsPAIR is a capable solver for inverse problems, offering structured and interpretable uncertainty estimates.

Abstract: Inverse problems are fundamental to many scientific and engineering disciplines; they arise when one seeks to reconstruct hidden, underlying quantities from noisy measurements. Many applications demand not just point estimates but interpretable uncertainty. Providing fast inference alongside uncertainty estimates remains challenging yet desirable in numerous applications.
  We propose the Variational Sparse Paired Autoencoder (vsPAIR) to address this challenge. The architecture pairs a standard VAE encoding observations with a sparse VAE encoding quantities of interest, connected through a learned latent mapping. The variational structure enables uncertainty estimation, the paired architecture encourages interpretability by anchoring QoI representations to clean data, and sparse encodings provide structure by concentrating information into identifiable factors rather than diffusing across all dimensions. We also propose modifications to existing sparse VAE methods: a hard-concrete spike-and-slab relaxation for differentiable training and a beta hyperprior for adaptive sparsity levels. To validate the effectiveness of our proposed architecture, we conduct experiments on blind inpainting and computed tomography, demonstrating that vsPAIR is a capable inverse problem solver that can provide interpretable and structured uncertainty estimates.

</details>


### [378] [Quant VideoGen: Auto-Regressive Long Video Generation via 2-Bit KV-Cache Quantization](https://arxiv.org/abs/2602.02958)
*Haocheng Xi,Shuo Yang,Yilong Zhao,Muyang Li,Han Cai,Xingyang Li,Yujun Lin,Zhuoyang Zhang,Jintao Zhang,Xiuyu Li,Zhiying Xu,Jun Wu,Chenfeng Xu,Ion Stoica,Song Han,Kurt Keutzer*

Main category: cs.LG

TL;DR: The paper introduces Quant VideoGen (QVG), a framework to address the KV cache memory bottleneck in autoregressive video diffusion models, reducing memory usage by up to 7.0× with minimal latency overhead.


<details>
  <summary>Details</summary>
Motivation: The KV cache memory bottleneck restricts deployability and generation quality in autoregressive video diffusion models, necessitating a solution.

Method: QVG uses Semantic Aware Smoothing and Progressive Residual Quantization to leverage video redundancy and reduce quantization error.

Result: QVG reduces KV cache memory by up to 7.0× with less than 4% latency overhead, outperforming baselines in quality.

Conclusion: QVG effectively balances memory efficiency and generation quality, enabling deployment on widely available hardware.

Abstract: Despite rapid progress in autoregressive video diffusion, an emerging system algorithm bottleneck limits both deployability and generation capability: KV cache memory. In autoregressive video generation models, the KV cache grows with generation history and quickly dominates GPU memory, often exceeding 30 GB, preventing deployment on widely available hardware. More critically, constrained KV cache budgets restrict the effective working memory, directly degrading long horizon consistency in identity, layout, and motion. To address this challenge, we present Quant VideoGen (QVG), a training free KV cache quantization framework for autoregressive video diffusion models. QVG leverages video spatiotemporal redundancy through Semantic Aware Smoothing, producing low magnitude, quantization friendly residuals. It further introduces Progressive Residual Quantization, a coarse to fine multi stage scheme that reduces quantization error while enabling a smooth quality memory trade off. Across LongCat Video, HY WorldPlay, and Self Forcing benchmarks, QVG establishes a new Pareto frontier between quality and memory efficiency, reducing KV cache memory by up to 7.0 times with less than 4% end to end latency overhead while consistently outperforming existing baselines in generation quality.

</details>


### [379] [Human-Centric Traffic Signal Control for Equity: A Multi-Agent Action Branching Deep Reinforcement Learning Approach](https://arxiv.org/abs/2602.02959)
*Xiaocai Zhang,Neema Nassir,Lok Sang Chan,Milad Haghani*

Main category: cs.LG

TL;DR: MA2B-DDQN is a human-centric multi-agent DRL framework for traffic signal coordination, optimizing traveler-level equity by decomposing control tasks and penalizing delayed individuals, showing superior performance in diverse scenarios.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of vehicle-centric DRL approaches in traffic signal coordination, the paper aims to prioritize equity and efficiency for all travelers, including pedestrians and transit passengers.

Method: Proposes MA2B-DDQN, a multi-agent action-branching DQN framework that decomposes corridor control into local and global actions and uses a human-centric reward function.

Result: Outperforms existing DRL methods in reducing delayed individuals across seven traffic scenarios, demonstrating robustness with minimal variance.

Conclusion: The framework offers a scalable, fair solution adaptable to diverse urban traffic conditions, advocating for equitable traffic management.

Abstract: Coordinating traffic signals along multimodal corridors is challenging because many multi-agent deep reinforcement learning (DRL) approaches remain vehicle-centric and struggle with high-dimensional discrete action spaces. We propose MA2B-DDQN, a human-centric multi-agent action-branching double Deep Q-Network (DQN) framework that explicitly optimizes traveler-level equity. Our key contribution is an action-branching discrete control formulation that decomposes corridor control into (i) local, per-intersection actions that allocate green time between the next two phases and (ii) a single global action that selects the total duration of those phases. This decomposition enables scalable coordination under discrete control while reducing the effective complexity of joint decision-making. We also design a human-centric reward that penalizes the number of delayed individuals in the corridor, accounting for pedestrians, vehicle occupants, and transit passengers. Extensive evaluations across seven realistic traffic scenarios in Melbourne, Australia, demonstrate that our approach significantly reduces the number of impacted travelers, outperforming existing DRL and baseline methods. Experiments confirm the robustness of our model, showing minimal variance across diverse settings. This framework not only advocates for a fairer traffic signal system but also provides a scalable solution adaptable to varied urban traffic conditions.

</details>


### [380] [Q-ShiftDP: A Differentially Private Parameter-Shift Rule for Quantum Machine Learning](https://arxiv.org/abs/2602.02962)
*Hoang M. Ngo,Nhat Hoang-Xuan,Quan Nguyen,Nguyen Do,Incheol Shin,My T. Thai*

Main category: cs.LG

TL;DR: Q-ShiftDP is a novel privacy mechanism tailored for Quantum Machine Learning (QML), leveraging quantum gradient properties to outperform classical privacy methods.


<details>
  <summary>Details</summary>
Motivation: Preserving data privacy in QML is challenging, as classical methods like DP-SGD do not exploit quantum gradient properties.

Method: Introduces Q-ShiftDP, which uses inherent boundedness and stochasticity of quantum gradients (via parameter-shift rule) and combines calibrated Gaussian noise with quantum noise.

Result: Q-ShiftDP provides tighter sensitivity analysis, reduces noise, and improves privacy-utility trade-off, outperforming classical DP methods in experiments.

Conclusion: Q-ShiftDP is effective for QML privacy, harnessing quantum properties for superior performance.

Abstract: Quantum Machine Learning (QML) promises significant computational advantages, but preserving training data privacy remains challenging. Classical approaches like differentially private stochastic gradient descent (DP-SGD) add noise to gradients but fail to exploit the unique properties of quantum gradient estimation. In this work, we introduce the Differentially Private Parameter-Shift Rule (Q-ShiftDP), the first privacy mechanism tailored to QML. By leveraging the inherent boundedness and stochasticity of quantum gradients computed via the parameter-shift rule, Q-ShiftDP enables tighter sensitivity analysis and reduces noise requirements. We combine carefully calibrated Gaussian noise with intrinsic quantum noise to provide formal privacy and utility guarantees, and show that harnessing quantum noise further improves the privacy-utility trade-off. Experiments on benchmark datasets demonstrate that Q-ShiftDP consistently outperforms classical DP methods in QML.

</details>


### [381] [Co2PO: Coordinated Constrained Policy Optimization for Multi-Agent RL](https://arxiv.org/abs/2602.02970)
*Shrenik Patel,Christine Truong*

Main category: cs.LG

TL;DR: Co2PO is a communication-augmented MARL framework that enhances safety and coordination by using risk-aware communication and shared intent signals, outperforming traditional constrained methods.


<details>
  <summary>Details</summary>
Motivation: Existing constrained MARL methods often suppress exploration due to reactive constraints, leading to over-conservatism. Co2PO aims to address this by enabling proactive safety coordination.

Method: Co2PO uses a shared blackboard for intent and yield signals, guided by a hazard predictor to forecast violations. It integrates these forecasts into constrained optimization for proactive hazard navigation.

Result: Co2PO achieves higher returns and cost-compliant policies in multi-agent safety benchmarks compared to traditional constrained baselines.

Conclusion: Co2PO's approach of risk-aware communication and shared memory validates its effectiveness in balancing exploration and safety in constrained MARL.

Abstract: Constrained multi-agent reinforcement learning (MARL) faces a fundamental tension between exploration and safety-constrained optimization. Existing leading approaches, such as Lagrangian methods, typically rely on global penalties or centralized critics that react to violations after they occur, often suppressing exploration and leading to over-conservatism. We propose Co2PO, a novel MARL communication-augmented framework that enables coordination-driven safety through selective, risk-aware communication. Co2PO introduces a shared blackboard architecture for broadcasting positional intent and yield signals, governed by a learned hazard predictor that proactively forecasts potential violations over an extended temporal horizon. By integrating these forecasts into a constrained optimization objective, Co2PO allows agents to anticipate and navigate collective hazards without the performance trade-offs inherent in traditional reactive constraints. We evaluate Co2PO across a suite of complex multi-agent safety benchmarks, where it achieves higher returns compared to leading constrained baselines while converging to cost-compliant policies at deployment. Ablation studies further validate the necessity of risk-triggered communication, adaptive gating, and shared memory components.

</details>


### [382] [Why Some Models Resist Unlearning: A Linear Stability Perspective](https://arxiv.org/abs/2602.02986)
*Wei-Kai Chang,Rajiv Khanna*

Main category: cs.LG

TL;DR: The paper analyzes machine unlearning theoretically through asymptotic linear stability and data coherence, linking memorization strength to ease of forgetting. Empirical results validate the derived stability thresholds and trade-offs.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the lack of theoretical understanding in machine unlearning, focusing on how optimization dynamics and data geometry influence forgetting. It aims to provide principled insights into when and why unlearning works.

Method: The authors frame unlearning via asymptotic linear stability and introduce data coherence as a key metric, decomposing it into interactions within/between retain and forget sets. They analyze a two-layer ReLU CNN under a signal-plus-noise model and use random matrix theory tools.

Result: The study proves tight stability thresholds for unlearning and shows that weaker memorization (lower SNR) reduces coherence, making unlearning easier. Empirical tests (Hessian, CNN heatmaps) align with theoretical predictions, validating the stability frontier.

Conclusion: The work provides the first principled analysis of memorization-coherence-unlearning trade-offs, demonstrating that high SNR models resist forgetting, while low SNR models unlearn more easily. Empirical results confirm theoretical insights.

Abstract: Machine unlearning, the ability to erase the effect of specific training samples without retraining from scratch, is critical for privacy, regulation, and efficiency. However, most progress in unlearning has been empirical, with little theoretical understanding of when and why unlearning works. We tackle this gap by framing unlearning through the lens of asymptotic linear stability to capture the interaction between optimization dynamics and data geometry. The key quantity in our analysis is data coherence which is the cross sample alignment of loss surface directions near the optimum. We decompose coherence along three axes: within the retain set, within the forget set, and between them, and prove tight stability thresholds that separate convergence from divergence. To further link data properties to forgettability, we study a two layer ReLU CNN under a signal plus noise model and show that stronger memorization makes forgetting easier: when the signal to noise ratio (SNR) is lower, cross sample alignment is weaker, reducing coherence and making unlearning easier; conversely, high SNR, highly aligned models resist unlearning. For empirical verification, we show that Hessian tests and CNN heatmaps align closely with the predicted boundary, mapping the stability frontier of gradient based unlearning as a function of batching, mixing, and data/model alignment. Our analysis is grounded in random matrix theory tools and provides the first principled account of the trade offs between memorization, coherence, and unlearning.

</details>


### [383] [NLI:Non-uniform Linear Interpolation Approximation of Nonlinear Operations for Efficient LLMs Inference](https://arxiv.org/abs/2602.02988)
*Jiangyong Yu,Xiaomeng Han,Xing Hu,Chen Xu,Zhe Jiang,Dawei Yang*

Main category: cs.LG

TL;DR: Proposes Non-uniform Linear Interpolation (NLI) to efficiently approximate nonlinear functions in LLMs, reducing computational costs without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: LLMs face deployment challenges due to high memory and computational costs, especially in nonlinear layers.

Method: Introduces NLI, a calibration-free, dynamic-programming-optimal framework for approximating nonlinear functions.

Result: NLI achieves >4x computational efficiency improvement over state-of-the-art designs.

Conclusion: NLI enables seamless integration into LLMs with minimal accuracy loss, addressing nonlinear computation bottlenecks.

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of tasks, but their deployment is often constrained by substantial memory footprints and computational costs. While prior work has achieved significant progress in compressing and accelerating linear layers, nonlinear layers-such as SiLU, RMSNorm, and Softmax-still heavily depend on high-precision floating-point operations. In this paper, we propose a calibration-free, dynamic-programming-optimal, and hardware-friendly framework called Non-uniform Linear Interpolation (NLI). NLI is capable of efficiently approximating a variety of nonlinear functions, enabling seamless integration into LLMs and other deep neural networks with almost no loss in accuracy. NLI ingeniously recasts cutpoint selection as a dynamic-programming problem, achieving the globally minimal interpolation error in O(MxN2) time via Bellman's optimality principle. Based on the NLI algorithm, we also design and implement a plug-and-play universal nonlinear computation unit. Hardware experiments demonstrate that the NLI Engine achieves more than 4x improvement in computational efficiency compared to the state-of-the-art designs.

</details>


### [384] [Learning to Repair Lean Proofs from Compiler Feedback](https://arxiv.org/abs/2602.02990)
*Evan Wang,Simon Chess,Daniel Lee,Siyuan Ge,Ajit Mallavarapu,Vasily Ilin*

Main category: cs.LG

TL;DR: APRIL introduces a dataset for supervised Lean proof repair, pairing erroneous proofs with compiler feedback to train models for correcting proofs and providing explanations.


<details>
  <summary>Details</summary>
Motivation: Existing Lean datasets lack supervision for understanding and repairing proof failures, limiting neural theorem provers' ability to act on compiler feedback.

Method: APRIL provides 260,000 tuples of proof failures, diagnostics, repairs, and explanations. Language models are trained on this dataset for proof repair.

Result: A finetuned 4B-parameter model outperforms baselines in single-shot proof repair, improving accuracy and feedback-conditioned reasoning.

Conclusion: Diagnostic-conditioned supervision enhances feedback-using provers, with APRIL offering a valuable dataset for advancing proof repair.

Abstract: As neural theorem provers become increasingly agentic, the ability to interpret and act on compiler feedback is critical. However, existing Lean datasets consist almost exclusively of correct proofs, offering little supervision for understanding and repairing failures. We study Lean proof repair as a supervised learning problem: given an erroneous proof and compiler feedback, predict both a corrected proof and a natural-language diagnosis grounded in the same feedback. We introduce APRIL (Automated Proof Repair in Lean), a dataset of 260,000 supervised tuples pairing systematically generated proof failures with compiler diagnostics and aligned repair and explanation targets. Training language models on APRIL substantially improves repair accuracy and feedback-conditioned reasoning; in our single-shot repair evaluation setting, a finetuned 4B-parameter model outperforms the strongest open-source baseline. We view diagnostic-conditioned supervision as a complementary training signal for feedback-using provers. Our dataset is available at \href{https://huggingface.co/datasets/uw-math-ai/APRIL}{this link}.

</details>


### [385] [SAFE-KD: Risk-Controlled Early-Exit Distillation for Vision Backbones](https://arxiv.org/abs/2602.03043)
*Salim Khazem*

Main category: cs.LG

TL;DR: SAFE-KD is a universal multi-exit wrapper for vision backbones that uses hierarchical distillation and conformal risk control to improve inference efficiency and guarantee selective misclassification risk.


<details>
  <summary>Details</summary>
Motivation: Early-exit networks reduce inference cost but require reliable stopping mechanisms to ensure safe early exits.

Method: SAFE-KD attaches lightweight exit heads, employs Decoupled Knowledge Distillation (DKD), and enforces deep-to-shallow consistency between exits. It uses conformal risk control (CRC) to calibrate stopping thresholds.

Result: SAFE-KD improves accuracy-compute trade-offs, calibration, and robustness across datasets and architectures while providing finite-sample risk guarantees.

Conclusion: SAFE-KD effectively balances efficiency and accuracy in early-exit networks with guaranteed risk control.

Abstract: Early-exit networks reduce inference cost by allowing ``easy'' inputs to stop early, but practical deployment hinges on knowing \emph{when} early exit is safe. We introduce SAFE-KD, a universal multi-exit wrapper for modern vision backbones that couples hierarchical distillation with \emph{conformal risk control}. SAFE-KD attaches lightweight exit heads at intermediate depths, distills a strong teacher into all exits via Decoupled Knowledge Distillation (DKD), and enforces deep-to-shallow consistency between exits. At inference, we calibrate per-exit stopping thresholds on a held-out set using conformal risk control (CRC) to guarantee a user-specified \emph{selective} misclassification risk (among the samples that exit early) under exchangeability. Across multiple datasets and architectures, SAFE-KD yields improved accuracy compute trade-offs, stronger calibration, and robust performance under corruption while providing finite-sample risk guarantees.

</details>


### [386] [Adaptive Batch Sizes Using Non-Euclidean Gradient Noise Scales for Stochastic Sign and Spectral Descent](https://arxiv.org/abs/2602.03001)
*Hiroki Naganuma,Shagun Gupta,Youssef Briki,Ioannis Mitliagkas,Irina Rish,Parameswaran Raman,Hao-Jun Michael Shi*

Main category: cs.LG

TL;DR: The paper proposes adaptive batch size strategies for non-Euclidean optimizers (signSGD and specSGD) by deriving gradient noise scales aligned with their norms, reducing training steps significantly.


<details>
  <summary>Details</summary>
Motivation: Existing adaptive batch size methods assume Euclidean geometry, mismatching non-Euclidean optimizers like signSGD/specSGD.

Method: Derives gradient noise scales for signSGD/specSGD using their dual norms and proposes efficient variance estimation via mini-batch gradients in distributed systems.

Result: Achieves same validation loss as constant-batch baselines while cutting training steps by up to 66% for Signum/Muon on a 160M-parameter Llama model.

Conclusion: Non-Euclidean gradient noise scales enable efficient adaptive batch sizing for signSGD/specSGD, improving training efficiency.

Abstract: To maximize hardware utilization, modern machine learning systems typically employ large constant or manually tuned batch size schedules, relying on heuristics that are brittle and costly to tune. Existing adaptive strategies based on gradient noise scale (GNS) offer a principled alternative. However, their assumption of SGD's Euclidean geometry creates a fundamental mismatch with popular optimizers based on generalized norms, such as signSGD / Signum ($\ell_\infty$) and stochastic spectral descent (specSGD) / Muon ($\mathcal{S}_\infty$). In this work, we derive gradient noise scales for signSGD and specSGD that naturally emerge from the geometry of their respective dual norms. To practically estimate these non-Euclidean metrics, we propose an efficient variance estimation procedure that leverages the local mini-batch gradients on different ranks in distributed data-parallel systems. Our experiments demonstrate that adaptive batch size strategies using non-Euclidean GNS enable us to match the validation loss of constant-batch baselines while reducing training steps by up to 66% for Signum and Muon on a 160 million parameter Llama model.

</details>


### [387] [Neural Predictor-Corrector: Solving Homotopy Problems with Reinforcement Learning](https://arxiv.org/abs/2602.03086)
*Jiayao Mai,Bangyan Liao,Zhenjun Zhao,Yingping Zeng,Haoang Li,Javier Civera,Tailin Wu,Yi Zhou,Peidong Liu*

Main category: cs.LG

TL;DR: The paper introduces Neural Predictor-Corrector (NPC), a unified neural solver for homotopy problems, replacing manual heuristics with learned policies via reinforcement learning, achieving better efficiency and stability across tasks.


<details>
  <summary>Details</summary>
Motivation: Hand-crafted heuristics in predictor-corrector solvers are often suboptimal and task-specific, prompting the need for a unified, automated approach.

Method: NPC replaces heuristics with learned policies, framing policy selection as a sequential decision-making problem solved via reinforcement learning, and employs amortized training for generalization.

Result: NPC outperforms classical and specialized baselines in efficiency and stability across four homotopy problems.

Conclusion: Unifying homotopy methods into a neural framework enhances generalization and performance, demonstrating the potential of learned policies over manual heuristics.

Abstract: The Homotopy paradigm, a general principle for solving challenging problems, appears across diverse domains such as robust optimization, global optimization, polynomial root-finding, and sampling. Practical solvers for these problems typically follow a predictor-corrector (PC) structure, but rely on hand-crafted heuristics for step sizes and iteration termination, which are often suboptimal and task-specific. To address this, we unify these problems under a single framework, which enables the design of a general neural solver. Building on this unified view, we propose Neural Predictor-Corrector (NPC), which replaces hand-crafted heuristics with automatically learned policies. NPC formulates policy selection as a sequential decision-making problem and leverages reinforcement learning to automatically discover efficient strategies. To further enhance generalization, we introduce an amortized training mechanism, enabling one-time offline training for a class of problems and efficient online inference on new instances. Experiments on four representative homotopy problems demonstrate that our method generalizes effectively to unseen instances. It consistently outperforms classical and specialized baselines in efficiency while demonstrating superior stability across tasks, highlighting the value of unifying homotopy methods into a single neural framework.

</details>


### [388] [Causal Graph Spatial-Temporal Autoencoder for Reliable and Interpretable Process Monitoring](https://arxiv.org/abs/2602.03004)
*Xiangrui Zhang,Chunyue Song,Wei Dai,Zheng Zhang,Kaihua Gao,Furong Gao*

Main category: cs.LG

TL;DR: A novel Causal Graph Spatial-Temporal Autoencoder (CGSTAE) is proposed for industrial process monitoring, combining dynamic correlation learning with causal graph derivation and spatial-temporal reconstruction.


<details>
  <summary>Details</summary>
Motivation: The paper aims to enhance reliability and interpretability in industrial process monitoring.

Method: CGSTAE integrates a correlation graph learning module (SSAM) and a spatial-temporal encoder-decoder (GCLSTM), with a three-step causal graph learning algorithm.

Result: CGSTAE demonstrates effective monitoring and fault detection via feature and residual space statistics.

Conclusion: Validation on Tennessee Eastman and real-world processes confirms CGSTAE's effectiveness.

Abstract: To improve the reliability and interpretability of industrial process monitoring, this article proposes a Causal Graph Spatial-Temporal Autoencoder (CGSTAE). The network architecture of CGSTAE combines two components: a correlation graph structure learning module based on spatial self-attention mechanism (SSAM) and a spatial-temporal encoder-decoder module utilizing graph convolutional long-short term memory (GCLSTM). The SSAM learns correlation graphs by capturing dynamic relationships between variables, while a novel three-step causal graph structure learning algorithm is introduced to derive a causal graph from these correlation graphs. The algorithm leverages a reverse perspective of causal invariance principle to uncover the invariant causal graph from varying correlations. The spatial-temporal encoder-decoder, built with GCLSTM units, reconstructs time-series process data within a sequence-to-sequence framework. The proposed CGSTAE enables effective process monitoring and fault detection through two statistics in the feature space and residual space. Finally, we validate the effectiveness of CGSTAE in process monitoring through the Tennessee Eastman process and a real-world air separation process.

</details>


### [389] [Spectral Evolution Search: Efficient Inference-Time Scaling for Reward-Aligned Image Generation](https://arxiv.org/abs/2602.03208)
*Jinyan Ye,Zhongjie Duan,Zhiwen Li,Cen Chen,Daoyuan Chen,Yaliang Li,Yingda Chen*

Main category: cs.LG

TL;DR: Spectral Evolution Search (SES) optimizes initial noise in generative models by focusing on low-frequency perturbations, improving efficiency and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing noise optimization methods are inefficient due to negligible impact of many search directions, linked to spectral bias in generative models.

Method: Proposes SES, a gradient-free evolutionary search within a low-frequency subspace, leveraging spectral scaling prediction for perturbation impact analysis.

Result: SES significantly improves the Pareto frontier of generation quality versus computational cost, outperforming baselines.

Conclusion: SES is an efficient plug-and-play framework for optimizing initial noise in generative models, validated by theoretical and experimental results.

Abstract: Inference-time scaling offers a versatile paradigm for aligning visual generative models with downstream objectives without parameter updates. However, existing approaches that optimize the high-dimensional initial noise suffer from severe inefficiency, as many search directions exert negligible influence on the final generation. We show that this inefficiency is closely related to a spectral bias in generative dynamics: model sensitivity to initial perturbations diminishes rapidly as frequency increases. Building on this insight, we propose Spectral Evolution Search (SES), a plug-and-play framework for initial noise optimization that executes gradient-free evolutionary search within a low-frequency subspace. Theoretically, we derive the Spectral Scaling Prediction from perturbation propagation dynamics, which explains the systematic differences in the impact of perturbations across frequencies. Extensive experiments demonstrate that SES significantly advances the Pareto frontier of generation quality versus computational cost, consistently outperforming strong baselines under equivalent budgets.

</details>


### [390] [From Zero to Hero: Advancing Zero-Shot Foundation Models for Tabular Outlier Detection](https://arxiv.org/abs/2602.03018)
*Xueying Ding,Haomin Wen,Simon Klütterman,Leman Akoglu*

Main category: cs.LG

TL;DR: OUTFORMER advances outlier detection with synthetic priors and self-evolving curriculum training, achieving state-of-the-art performance without labeled outliers.


<details>
  <summary>Details</summary>
Motivation: Overcome challenges in outlier detection deployment due to lack of labeled outliers and cumbersome algorithm selection.

Method: Uses synthetic priors and self-evolving curriculum training, pretraining on synthetic data and inferring labels via in-context learning.

Result: Achieves top performance on AdBench and new benchmarks, with fast, zero-shot inference.

Conclusion: OUTFORMER enables plug-and-play outlier detection with no labeled data or additional training.

Abstract: Outlier detection (OD) is widely used in practice; but its effective deployment on new tasks is hindered by lack of labeled outliers, which makes algorithm and hyperparameter selection notoriously hard. Foundation models (FMs) have transformed ML, and OD is no exception: Shen et. al. (2025) introduced FoMo-0D, the first FM for OD, achieving remarkable performance against numerous baselines. This work introduces OUTFORMER, which advances FoMo-0D with (1) a mixture of synthetic priors and (2) self-evolving curriculum training. OUTFORMER is pretrained solely on synthetic labeled datasets and infers test labels of a new task by using its training data as in-context input. Inference is fast and zero-shot, requiring merely forward pass and no labeled outliers. Thanks to in-context learning, it requires zero additional work-no OD model training or bespoke model selection-enabling truly plug-and-play deployment. OUTFORMER achieves state-of-the-art performance on the prominent AdBench, as well as two new large-scale OD benchmarks that we introduce, comprising over 1,500 datasets, while maintaining speedy inference.

</details>


### [391] [FedKRSO: Communication and Memory Efficient Federated Fine-Tuning of Large Language Models](https://arxiv.org/abs/2602.03019)
*Guohao Yang,Tongle Wu,Yuanxiong Guo,Ying Sun,Yanmin Gong*

Main category: cs.LG

TL;DR: FedKRSO is a novel federated learning method for efficient LLM fine-tuning, reducing communication and memory costs while maintaining performance close to full fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Addressing the high costs of communication and memory in federated learning for LLM fine-tuning, especially with resource-constrained clients.

Method: Clients update models within shared low-dimension subspaces and transmit only model update accumulators, reducing overhead.

Result: FedKRSO achieves performance close to federated FFT with significantly lower communication and memory usage.

Conclusion: FedKRSO offers a practical solution for efficient federated LLM fine-tuning, suitable for resource-constrained environments.

Abstract: Fine-tuning is essential to adapt general-purpose large language models (LLMs) to domain-specific tasks. As a privacy-preserving framework to leverage decentralized data for collaborative model training, Federated Learning (FL) is gaining popularity in LLM fine-tuning, but remains challenging due to the high cost of transmitting full model parameters and computing full gradients on resource-constrained clients. While Parameter-Efficient Fine-Tuning (PEFT) methods are widely used in FL to reduce communication and memory costs, they often sacrifice model performance compared to FFT. This paper proposes FedKRSO (Federated $K$-Seed Random Subspace Optimization), a novel method that enables communication and memory efficient FFT of LLMs in federated settings. In FedKRSO, clients update the model within a shared set of random low-dimension subspaces generated by the server to save memory usage. Furthermore, instead of transmitting full model parameters in each FL round, clients send only the model update accumulators along the subspaces to the server, enabling efficient global model aggregation and dissemination. By using these strategies, FedKRSO can substantially reduce communication and memory overhead while overcoming the performance limitations of PEFT, closely approximating the performance of federated FFT. The convergence properties of FedKRSO are analyzed rigorously under general FL settings. Extensive experiments on the GLUE benchmark across diverse FL scenarios demonstrate that FedKRSO achieves both superior performance and low communication and memory overhead, paving the way towards on federated LLM fine-tuning at the resource-constrained edge.

</details>


### [392] [Consistency Deep Equilibrium Models](https://arxiv.org/abs/2602.03024)
*Junchao Lin,Zenan Ling,Jingwen Xu,Robert C. Qiu*

Main category: cs.LG

TL;DR: C-DEQs accelerates DEQ inference using consistency distillation, achieving better accuracy with fewer steps.


<details>
  <summary>Details</summary>
Motivation: DEQs suffer from high inference latency due to iterative fixed-point solvers.

Method: Leverages consistency distillation to map intermediate states directly to equilibrium, enabling few-step inference.

Result: Achieves 2-20x accuracy improvements under the same inference budget.

Conclusion: C-DEQs offer faster, flexible inference without compromising performance.

Abstract: Deep Equilibrium Models (DEQs) have emerged as a powerful paradigm in deep learning, offering the ability to model infinite-depth networks with constant memory usage. However, DEQs incur significant inference latency due to the iterative nature of fixed-point solvers. In this work, we introduce the Consistency Deep Equilibrium Model (C-DEQ), a novel framework that leverages consistency distillation to accelerate DEQ inference. We cast the DEQ iterative inference process as evolution along a fixed ODE trajectory toward the equilibrium. Along this trajectory, we train C-DEQs to consistently map intermediate states directly to the fixed point, enabling few-step inference while preserving the performance of the teacher DEQ. At the same time, it facilitates multi-step evaluation to flexibly trade computation for performance gains. Extensive experiments across various domain tasks demonstrate that C-DEQs achieves consistent 2-20$\times$ accuracy improvements over implicit DEQs under the same few-step inference budget.

</details>


### [393] [Scaling Continual Learning with Bi-Level Routing Mixture-of-Experts](https://arxiv.org/abs/2602.03473)
*Meng Lou,Yunxiang Fu,Yizhou Yu*

Main category: cs.LG

TL;DR: CaRE is a scalable continual learner using a bi-level routing mechanism (BR-MoE) to dynamically activate task-specific routers and experts, excelling in long task sequences.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of maintaining stability and plasticity in class-incremental learning (CIL) over long task sequences.

Method: Proposes BR-MoE with a bi-level routing mechanism for dynamic task-specific router and expert activation.

Result: CaRE outperforms baselines on long task sequences (100-300 tasks) and standard CIL datasets.

Conclusion: CaRE is the first scalable continual learner for very long task sequences, demonstrating superior performance.

Abstract: Continual learning, especially class-incremental learning (CIL), on the basis of a pre-trained model (PTM) has garnered substantial research interest in recent years. However, how to effectively learn both discriminative and comprehensive feature representations while maintaining stability and plasticity over very long task sequences remains an open problem. We propose CaRE, a scalable {C}ontinual Le{a}rner with efficient Bi-Level {R}outing Mixture-of-{E}xperts (BR-MoE). The core idea of BR-MoE is a bi-level routing mechanism: a router selection stage that dynamically activates relevant task-specific routers, followed by an expert routing phase that dynamically activates and aggregates experts, aiming to inject discriminative and comprehensive representations into every intermediate network layer. On the other hand, we introduce a challenging evaluation protocol for comprehensively assessing CIL methods across very long task sequences spanning hundreds of tasks. Extensive experiments show that CaRE demonstrates leading performance across a variety of datasets and task settings, including commonly used CIL datasets with classical CIL settings (e.g., 5-20 tasks). To the best of our knowledge, CaRE is the first continual learner that scales to very long task sequences (ranging from 100 to over 300 non-overlapping tasks), while outperforming all baselines by a large margin on such task sequences. Code will be publicly released at https://github.com/LMMMEng/CaRE.git.

</details>


### [394] [Robust Representation Learning in Masked Autoencoders](https://arxiv.org/abs/2602.03531)
*Anika Shrivastava,Renu Rameshan,Samar Agnihotri*

Main category: cs.LG

TL;DR: MAEs learn robust latent representations for image classification, progressively separating embeddings by class across network depth and showing persistent global attention.


<details>
  <summary>Details</summary>
Motivation: To understand why MAEs perform well in downstream classification tasks and explore their robustness to degradations like blur and occlusions.

Method: Layer-wise analysis of token embeddings, sensitivity indicators (directional alignment and head-wise retention), and comparison with ViTs.

Result: MAEs construct class-aware latent spaces and maintain global attention, achieving robust classification under degradations.

Conclusion: MAEs' robustness and class-aware representations explain their strong downstream performance.

Abstract: Masked Autoencoders (MAEs) achieve impressive performance in image classification tasks, yet the internal representations they learn remain less understood. This work started as an attempt to understand the strong downstream classification performance of MAE. In this process we discover that representations learned with the pretraining and fine-tuning, are quite robust - demonstrating a good classification performance in the presence of degradations, such as blur and occlusions. Through layer-wise analysis of token embeddings, we show that pretrained MAE progressively constructs its latent space in a class-aware manner across network depth: embeddings from different classes lie in subspaces that become increasingly separable. We further observe that MAE exhibits early and persistent global attention across encoder layers, in contrast to standard Vision Transformers (ViTs). To quantify feature robustness, we introduce two sensitivity indicators: directional alignment between clean and perturbed embeddings, and head-wise retention of active features under degradations. These studies help establish the robust classification performance of MAEs.

</details>


### [395] [Clarify Before You Draw: Proactive Agents for Robust Text-to-CAD Generation](https://arxiv.org/abs/2602.03045)
*Bo Yuan,Zelin Zhao,Petr Molodyk,Bin Hu,Yongxin Chen*

Main category: cs.LG

TL;DR: ProCAD is a proactive framework for text-to-CAD synthesis that resolves ambiguities through targeted clarification questions, improving robustness and outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Geometric descriptions in text-to-CAD systems are often under-specified or inconsistent, leading to hallucinations in fine-tuned models. ProCAD aims to proactively address these issues.

Method: ProCAD uses a clarifying agent to audit prompts and ask targeted questions, paired with a CAD coding agent trained on curated data. Both agents are fine-tuned and trained via SFT.

Result: ProCAD reduces Chamfer distance by 79.9% and lowers invalidity ratio from 4.8% to 0.9%, outperforming models like Claude Sonnet 4.5.

Conclusion: Proactive clarification enhances text-to-CAD robustness with minimal overhead, demonstrating significant improvements over existing approaches.

Abstract: Large language models have recently enabled text-to-CAD systems that synthesize parametric CAD programs (e.g., CadQuery) from natural language prompts. In practice, however, geometric descriptions can be under-specified or internally inconsistent: critical dimensions may be missing and constraints may conflict. Existing fine-tuned models tend to reactively follow user instructions and hallucinate dimensions when the text is ambiguous. To address this, we propose a proactive agentic framework for text-to-CadQuery generation, named ProCAD, that resolves specification issues before code synthesis. Our framework pairs a proactive clarifying agent, which audits the prompt and asks targeted clarification questions only when necessary to produce a self-consistent specification, with a CAD coding agent that translates the specification into an executable CadQuery program. We fine-tune the coding agent on a curated high-quality text-to-CadQuery dataset and train the clarifying agent via agentic SFT on clarification trajectories. Experiments show that proactive clarification significantly improves robustness to ambiguous prompts while keeping interaction overhead low. ProCAD outperforms frontier closed-source models, including Claude Sonnet 4.5, reducing the mean Chamfer distance by 79.9 percent and lowering the invalidity ratio from 4.8 percent to 0.9 percent. Our code and datasets will be made publicly available.

</details>


### [396] [CoBA-RL: Capability-Oriented Budget Allocation for Reinforcement Learning in LLMs](https://arxiv.org/abs/2602.03048)
*Zhiyuan Yao,Yi-Kai Zhang,Yuxin Chen,Yueqing Sun,Zishan Xu,Yu Yang,Tianhao Hu,Qi Gu,Hui Su,Xunliang Cai*

Main category: cs.LG

TL;DR: CoBA-RL introduces adaptive rollout budget allocation in RLVR for LLM reasoning, using a Capability-Oriented Value function and heap-based greedy strategy to improve resource efficiency and generalization.


<details>
  <summary>Details</summary>
Motivation: Standard RL frameworks like GRPO use uniform rollout budgets, causing inefficiency, and existing adaptive methods fail to capture dynamic learning states.

Method: Proposes CoBA-RL, which adaptively allocates budgets using a Capability-Oriented Value function and a heap-based greedy strategy.

Result: Demonstrates effective exploration-exploitation trade-off and consistent generalization improvements across benchmarks.

Conclusion: Quantifying sample training value and optimizing budget allocation are key to advancing LLM post-training efficiency.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key approach for enhancing LLM reasoning.However, standard frameworks like Group Relative Policy Optimization (GRPO) typically employ a uniform rollout budget, leading to resource inefficiency. Moreover, existing adaptive methods often rely on instance-level metrics, such as task pass rates, failing to capture the model's dynamic learning state. To address these limitations, we propose CoBA-RL, a reinforcement learning algorithm designed to adaptively allocate rollout budgets based on the model's evolving capability. Specifically, CoBA-RL utilizes a Capability-Oriented Value function to map tasks to their potential training gains and employs a heap-based greedy strategy to efficiently self-calibrate the distribution of computational resources to samples with high training value. Extensive experiments demonstrate that our approach effectively orchestrates the trade-off between exploration and exploitation, delivering consistent generalization improvements across multiple challenging benchmarks. These findings underscore that quantifying sample training value and optimizing budget allocation are pivotal for advancing LLM post-training efficiency.

</details>


### [397] [Fedcompass: Federated Clustered and Periodic Aggregation Framework for Hybrid Classical-Quantum Models](https://arxiv.org/abs/2602.03052)
*Yueheng Wang,Xing He,Zinuo Cai,Rui Zhang,Ruhui Ma,Yuan Liu,Rajkumar Buyya*

Main category: cs.LG

TL;DR: FEDCOMPASS is a layered aggregation framework for hybrid classical-quantum federated learning, improving accuracy and stability under non-IID data.


<details>
  <summary>Details</summary>
Motivation: Address performance degradation in hybrid classical-quantum federated learning caused by non-IID data.

Method: Uses spectral clustering for client grouping, cluster-wise aggregation for classical features, and circular mean aggregation with adaptive optimization for quantum parameters.

Result: Improves test accuracy by up to 10.22% and enhances convergence stability, outperforming six baselines.

Conclusion: FEDCOMPASS effectively mitigates non-IID challenges in hybrid federated learning, achieving superior performance.

Abstract: Federated learning enables collaborative model training across decentralized clients under privacy constraints. Quantum computing offers potential for alleviating computational and communication burdens in federated learning, yet hybrid classical-quantum federated learning remains susceptible to performance degradation under non-IID data. To address this,we propose FEDCOMPASS, a layered aggregation framework for hybrid classical-quantum federated learning. FEDCOMPASS employs spectral clustering to group clients by class distribution similarity and performs cluster-wise aggregation for classical feature extractors. For quantum parameters, it uses circular mean aggregation combined with adaptive optimization to ensure stable global updates. Experiments on three benchmark datasets show that FEDCOMPASS improves test accuracy by up to 10.22% and enhances convergence stability under non-IID settings, outperforming six strong federated learning baselines.

</details>


### [398] [Shortcut Features as Top Eigenfunctions of NTK: A Linear Neural Network Case and More](https://arxiv.org/abs/2602.03066)
*Jinwoo Lim,Suhyun Kim,Soo-Mook Moon*

Main category: cs.LG

TL;DR: The paper discusses shortcut learning in deep-learning models, where neural networks favor dominant features in training data, even if these features aren't generalizable. Using Neural Tangent Kernel (NTK) analysis, it shows that shortcut features correlate with larger eigenvalues in imbalanced data, persist post-training, and aren't solely due to max-margin bias.


<details>
  <summary>Details</summary>
Motivation: Shortcut learning is a persistent issue in deep learning, where models rely on dominant but non-generalizable features. The study aims to understand why this happens and how it relates to NTK properties, providing insights into feature preferences in neural networks.

Method: The study uses the Neural Tangent Kernel (NTK) framework to analyze linear neural networks. It defines features as NTK eigenfunctions and examines how shortcut features, linked to larger eigenvalues, emerge from imbalanced training data. Empirical validation extends findings to ReLU networks and ResNet-18.

Result: Shortcut features correspond to larger eigenvalues in NTK for imbalanced data. These features retain significant influence on outputs post-training due to cluster variances. The preference persists even when controlling for max-margin bias.

Conclusion: Shortcut learning in neural networks is driven by features with larger NTK eigenvalues in imbalanced datasets, and it isn't solely tied to max-margin bias. This behavior extends to more complex architectures, highlighting a fundamental challenge in deep learning.

Abstract: One of the chronic problems of deep-learning models is shortcut learning. In a case where the majority of training data are dominated by a certain feature, neural networks prefer to learn such a feature even if the feature is not generalizable outside the training set. Based on the framework of Neural Tangent Kernel (NTK), we analyzed the case of linear neural networks to derive some important properties of shortcut learning. We defined a feature of a neural network as an eigenfunction of NTK. Then, we found that shortcut features correspond to features with larger eigenvalues when the shortcuts stem from the imbalanced number of samples in the clustered distribution. We also showed that the features with larger eigenvalues still have a large influence on the neural network output even after training, due to data variances in the clusters. Such a preference for certain features remains even when a margin of a neural network output is controlled, which shows that the max-margin bias is not the only major reason for shortcut learning. These properties of linear neural networks are empirically extended for more complex neural networks as a two-layer fully-connected ReLU network and a ResNet-18.

</details>


### [399] [FlashSinkhorn: IO-Aware Entropic Optimal Transport](https://arxiv.org/abs/2602.03067)
*Felix X. -F. Ye,Xingjie Li,An Yu,Ming-Ching Chang,Linsong Chu,Davis Wertheimer*

Main category: cs.LG

TL;DR: FlashSinkhorn is an efficient GPU solver for Entropic Optimal Transport (EOT) using Sinkhorn iterations, achieving significant speedups by leveraging FlashAttention-style fusion and tiling to reduce HBM IO.


<details>
  <summary>Details</summary>
Motivation: GPU solvers for EOT are inefficient at scale, with existing methods suffering from high memory traffic or limited fusion. The goal is to improve efficiency for large-scale applications.

Method: FlashSinkhorn rewrites Sinkhorn updates as row-wise LogSumExp reductions, enabling fused Triton kernels for on-chip SRAM streaming and single-pass dual potential updates. It also includes streaming kernels for transport application.

Result: On A100 GPUs, FlashSinkhorn achieves up to $32\times$ forward-pass and $161\times$ end-to-end speedups over state-of-the-art baselines, improving scalability on OT-based tasks.

Conclusion: FlashSinkhorn offers a highly efficient, IO-aware solution for EOT, with significant performance gains and open-source availability for reproducibility.

Abstract: Entropic optimal transport (EOT) via Sinkhorn iterations is widely used in modern machine learning, yet GPU solvers remain inefficient at scale. Tensorized implementations suffer quadratic HBM traffic from dense $n\times m$ interactions, while existing online backends avoid storing dense matrices but still rely on generic tiled map-reduce reduction kernels with limited fusion. We present \textbf{FlashSinkhorn}, an IO-aware EOT solver for squared Euclidean cost that rewrites stabilized log-domain Sinkhorn updates as row-wise LogSumExp reductions of biased dot-product scores, the same normalization as transformer attention. This enables FlashAttention-style fusion and tiling: fused Triton kernels stream tiles through on-chip SRAM and update dual potentials in a single pass, substantially reducing HBM IO per iteration while retaining linear-memory operations. We further provide streaming kernels for transport application, enabling scalable first- and second-order optimization. On A100 GPUs, FlashSinkhorn achieves up to $32\times$ forward-pass and $161\times$ end-to-end speedups over state-of-the-art online baselines on point-cloud OT, improves scalability on OT-based downstream tasks. For reproducibility, we release an open-source implementation at https://github.com/ot-triton-lab/ot_triton.

</details>


### [400] [Geometry-Preserving Neural Architectures on Manifolds with Boundary](https://arxiv.org/abs/2602.03082)
*Karthik Elamvazhuthi,Shiba Biswal,Kian Rosenblum,Arushi Katyal,Tianli Qu,Grady Ma,Rishi Sonthalia*

Main category: cs.LG

TL;DR: The paper introduces a unified framework for geometry-aware neural architectures that preserve geometric structure, offering universal approximation results and demonstrating strong performance in experiments.


<details>
  <summary>Details</summary>
Motivation: Preserving geometric structure is crucial in learning tasks, especially for constrained data or dynamics on manifolds.

Method: Proposes a class of architectures with interleaved geometric updates, including projection layers and intrinsic exponential map updates, derived from discretizations of projected dynamical systems. Also explores learning projections via small-time heat-kernel limits.

Result: Establishes universal approximation results for constrained neural ODEs and shows strong performance in experiments on dynamics over S^2 and SO(3), as well as diffusion on S^{d-1}-valued features.

Conclusion: The framework effectively preserves geometric structure, enabling exact feasibility for analytic updates and demonstrating the utility of learned projections in data-based tasks.

Abstract: Preserving geometric structure is important in learning. We propose a unified class of geometry-aware architectures that interleave geometric updates between layers, where both projection layers and intrinsic exponential map updates arise as discretizations of projected dynamical systems on manifolds (with or without boundary). Within this framework, we establish universal approximation results for constrained neural ODEs. We also analyze architectures that enforce geometry only at the output, proving a separate universal approximation property that enables direct comparison to interleaved designs. When the constraint set is unknown, we learn projections via small-time heat-kernel limits, showing diffusion/flow-matching can be used as data-based projections. Experiments on dynamics over S^2 and SO(3), and diffusion on S^{d-1}-valued features demonstrate exact feasibility for analytic updates and strong performance for learned projections

</details>


### [401] [TextME: Bridging Unseen Modalities Through Text Descriptions](https://arxiv.org/abs/2602.03098)
*Soyeon Hong,Jinchan Kim,Jaegook You,Seungtaek Choi,Suha Kwak,Hyunsouk Cho*

Main category: cs.LG

TL;DR: TextME is a novel text-only framework for modality expansion, eliminating the need for costly paired datasets by leveraging LLM embeddings and zero-shot cross-modal transfer.


<details>
  <summary>Details</summary>
Motivation: Overcoming the limitations of reliance on large-scale paired datasets for multimodal representations, especially in expert-annotation-heavy domains like medical imaging and molecular analysis.

Method: Projects diverse modalities into LLM embedding space, utilizing the geometric structure of pretrained contrastive encoders for zero-shot transfer with only text descriptions.

Result: Demonstrates consistent modality gaps across various domains, enabling emergent cross-modal retrieval (e.g., audio-to-image) without explicit paired training.

Conclusion: Text-only training is a viable alternative to paired supervision for modality expansion.

Abstract: Expanding multimodal representations to novel modalities is constrained by reliance on large-scale paired datasets (e.g., text-image, text-audio, text-3D, text-molecule), which are costly and often infeasible in domains requiring expert annotation such as medical imaging and molecular analysis. We introduce TextME, the first text-only modality expansion framework, to the best of our knowledge, projecting diverse modalities into LLM embedding space as a unified anchor. Our approach exploits the geometric structure of pretrained contrastive encoders to enable zero-shot cross-modal transfer using only text descriptions, without paired supervision. We empirically validate that such consistent modality gaps exist across image, video, audio, 3D, X-ray, and molecular domains, demonstrating that text-only training can preserve substantial performance of pretrained encoders. We further show that our framework enables emergent cross-modal retrieval between modality pairs not explicitly aligned during training (e.g., audio-to-image, 3D-to-image). These results establish text-only training as a practical alternative to paired supervision for modality expansion.

</details>


### [402] [Consensus Group Relative Policy Optimization for Text Generation](https://arxiv.org/abs/2602.03102)
*Yuki Ichihara,Yuu Jinnai,Kaito Ariu,Eiji Uchibe*

Main category: cs.LG

TL;DR: C-GRPO distills MBR decoding into training using a group-relative objective, avoiding inference-time costs while matching MBR performance.


<details>
  <summary>Details</summary>
Motivation: To reduce computational costs of sample-and-rerank methods like MBR decoding by amortizing inference-time computation without needing gold references or explicit preference labels.

Method: Introduces Consensus Group Relative Policy Optimization (C-GRPO), formulating consensus utility as a group-relative objective within GRPO, requiring only a utility function and policy samples.

Result: Achieves performance comparable to MBR decoding without inference-time overhead and outperforms reference-free baselines in machine translation (WMT 2024) and text summarization (XSum).

Conclusion: C-GRPO effectively distills MBR decoding benefits into training, offering computational efficiency without compromising performance.

Abstract: Many strong decoding methods for text generation follow a sample-and-rerank paradigm: they draw multiple candidates, score each under a utility (reward) function using consensus across samples, and return the best one. Although effective, these methods incur high computational costs during inference due to repeated sampling and scoring. Prior attempts to amortize inference-time computation typically rely on gold references, teacher labels, or curated preference data, increasing dataset construction effort and the demand for high-fidelity reward models. We propose Consensus Group Relative Policy Optimization (C-GRPO), which distills Minimum Bayes Risk (MBR) decoding into training by formulating the consensus utility as a group-relative objective within GRPO. C-GRPO requires only a utility function and policy samples, without gold references or explicit preference labels. Under ideal conditions, we show that the objective function of C-GRPO is directionally aligned with the gradient of the expected-utility objective underlying MBR decoding, leading to a convergence guarantee. Experiments on machine translation (WMT 2024) and text summarization (XSum) demonstrate that C-GRPO successfully achieves performance comparable to MBR decoding without the associated inference-time overhead, while outperforming reference-free baseline methods.

</details>


### [403] [Function-Space Empirical Bayes Regularisation with Large Vision-Language Model Priors](https://arxiv.org/abs/2602.03119)
*Pengcheng Hao,Huaze Tang,Ercan Engin Kuruoglu,Wenbo Ding*

Main category: cs.LG

TL;DR: The paper introduces VLM-FS-EB, a Bayesian deep learning framework using vision-language models to create expressive functional priors, improving predictive performance and uncertainty estimation.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of designing scalable and informative priors in high-dimensional Bayesian deep learning, especially beyond Gaussian process limitations.

Method: Proposes VLM-FS-EB, leveraging vision-language models to generate context points and construct functional priors, evaluated against baselines.

Result: Demonstrates improved predictive performance and more reliable uncertainty estimates, notably in OOD detection and data-scarce scenarios.

Conclusion: VLM-FS-EB effectively enhances Bayesian deep learning by leveraging VLMs for expressive priors, achieving better results in key tasks.

Abstract: Bayesian deep learning (BDL) provides a principled framework for reliable uncertainty quantification by combining deep neural networks with Bayesian inference. A central challenge in BDL lies in the design of informative prior distributions that scale effectively to high-dimensional data. Recent functional variational inference (VI) approaches address this issue by imposing priors directly in function space; however, most existing methods rely on Gaussian process (GP) priors, whose expressiveness and generalisation capabilities become limited in high-dimensional regimes. In this work, we propose VLM-FS-EB, a novel function-space empirical Bayes regularisation framework, leveraging large vision-language models (VLMs) to generates semantically meaningful context points. These synthetic samples are then used VLMs for embeddings to construct expressive functional priors. Furthermore, the proposed method is evaluated against various baselines, and experimental results demonstrate that our method consistently improves predictive performance and yields more reliable uncertainty estimates, particularly in out-of-distribution (OOD) detection tasks and data-scarce regimes.

</details>


### [404] [Quantized Evolution Strategies: High-precision Fine-tuning of Quantized LLMs at Low-precision Cost](https://arxiv.org/abs/2602.03120)
*Yinggan Xu,Risto Miikkulainen,Xin Qiu*

Main category: cs.LG

TL;DR: QES introduces a method for fine-tuning quantized LLMs without backpropagation, using error feedback and seed replay to maintain precision and reduce memory usage.


<details>
  <summary>Details</summary>
Motivation: PTQ makes LLMs static and hard to fine-tune due to discrete parameter spaces. Existing methods like RL fail, and ES struggles with inaccurate gradients.

Method: QES combines accumulated error feedback to preserve gradients and stateless seed replay for low memory usage, enabling full-parameter fine-tuning in quantized space.

Result: QES outperforms state-of-the-art zeroth-order methods on arithmetic reasoning tasks, making quantized model fine-tuning feasible.

Conclusion: QES enables direct fine-tuning in quantized space, potentially allowing LLM scaling entirely in this space.

Abstract: Post-Training Quantization (PTQ) is essential for deploying Large Language Models (LLMs) on memory-constrained devices, yet it renders models static and difficult to fine-tune. Standard fine-tuning paradigms, including Reinforcement Learning (RL), fundamentally rely on backpropagation and high-precision weights to compute gradients. Thus they cannot be used on quantized models, where the parameter space is discrete and non-differentiable. While Evolution Strategies (ES) offer a backpropagation-free alternative, optimization of the quantized parameters can still fail due to vanishing or inaccurate gradient. This paper introduces Quantized Evolution Strategies (QES), an optimization paradigm that performs full-parameter fine-tuning directly in the quantized space. QES is based on two innovations: (1) it integrates accumulated error feedback to preserve high-precision gradient signals, and (2) it utilizes a stateless seed replay to reduce memory usage to low-precision inference levels. QES significantly outperforms the state-of-the-art zeroth-order fine-tuning method on arithmetic reasoning tasks, making direct fine-tuning for quantized models possible. It therefore opens up the possibility for scaling up LLMs entirely in the quantized space. The source code is available at https://github.com/dibbla/Quantized-Evolution-Strategies .

</details>


### [405] [Contrastive Concept-Tree Search for LLM-Assisted Algorithm Discovery](https://arxiv.org/abs/2602.03132)
*Timothee Leleu,Sudeera Gunathilaka,Federico Ghimenti,Surya Ganguli*

Main category: cs.LG

TL;DR: CCTS improves search efficiency in LLM-assisted algorithm discovery by learning contrastive concepts and avoiding misleading ones.


<details>
  <summary>Details</summary>
Motivation: Exploring how LLMs can better exploit internal representations of program spaces to enhance performance in algorithm discovery.

Method: Introduces Contrastive Concept-Tree Search (CCTS), which extracts hierarchical concepts from programs and guides search using a contrastive model.

Result: CCTS outperforms fitness-based baselines, producing interpretable concept trees and improving search efficiency.

Conclusion: CCTS enhances LLM-assisted discovery by learning to avoid harmful concepts, validated in synthetic and benchmark tasks.

Abstract: Large language Model (LLM)-assisted algorithm discovery is an iterative, black-box optimization process over programs to approximatively solve a target task, where an LLM proposes candidate programs and an external evaluator provides task feedback. Despite intense recent research on the topic and promising results, how can the LLM internal representation of the space of possible programs be maximally exploited to improve performance is an open question. Here, we introduce Contrastive Concept-Tree Search (CCTS), which extracts a hierarchical concept representation from the generated programs and learns a contrastive concept model that guides parent selection. By reweighting parents using a likelihood-ratio score between high- and low-performing solutions, CCTS biases search toward useful concept combinations and away from misleading ones, providing guidance through an explicit concept hierarchy rather than the algorithm lineage constructed by the LLM. We show that CCTS improves search efficiency over fitness-based baselines and produces interpretable, task-specific concept trees across a benchmark of open Erdős-type combinatorics problems. Our analysis indicates that the gains are driven largely by learning which concepts to avoid. We further validate these findings in a controlled synthetic algorithm-discovery environment, which reproduces qualitatively the search dynamics observed with the LLMs.

</details>


### [406] [Enhanced Parcel Arrival Forecasting for Logistic Hubs: An Ensemble Deep Learning Approach](https://arxiv.org/abs/2602.03135)
*Xinyue Pan,Yujia Xu,Benoit Montreuil*

Main category: cs.LG

TL;DR: A deep learning-based ensemble framework improves hub workload forecasting for logistics networks by leveraging historical and real-time data, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: The rise in online shopping demands more efficient logistics, requiring better workload forecasting to enhance hub operations.

Method: Proposes an ensemble framework combining historical arrival patterns and real-time parcel data for workload prediction.

Result: Empirical tests show the method outperforms traditional techniques and standalone models in forecasting accuracy.

Conclusion: The framework enhances operational efficiency in logistics hubs and should be widely adopted.

Abstract: The rapid expansion of online shopping has increased the demand for timely parcel delivery, compelling logistics service providers to enhance the efficiency, agility, and predictability of their hub networks. In order to solve the problem, we propose a novel deep learning-based ensemble framework that leverages historical arrival patterns and real-time parcel status updates to forecast upcoming workloads at logistic hubs. This approach not only facilitates the generation of short-term forecasts, but also improves the accuracy of future hub workload predictions for more strategic planning and resource management. Empirical tests of the algorithm, conducted through a case study of a major city's parcel logistics, demonstrate the ensemble method's superiority over both traditional forecasting techniques and standalone deep learning models. Our findings highlight the significant potential of this method to improve operational efficiency in logistics hubs and advocate for its broader adoption.

</details>


### [407] [SATORIS-N: Spectral Analysis based Traffic Observation Recovery via Informed Subspaces and Nuclear-norm minimization](https://arxiv.org/abs/2602.03138)
*Sampad Mohanty,Bhaskar Krishnamachari*

Main category: cs.LG

TL;DR: SATORIS-N leverages stable correlations in traffic-density matrices for accurate imputation under occlusion, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Accurate traffic-density reconstruction is essential for intelligent vehicles and V2X systems, especially with incomplete data from sensors or vehicles.

Method: Introduces SATORIS-N, combining subspace-aware SDP with nuclear norm for explicit subspace alignment, and a lightweight implicit strategy for low missing rates.

Result: Outperforms SoftImpute, IterativeSVD, statistical, and deep learning baselines, especially under high occlusion.

Conclusion: SATORIS-N is robust and generalizable for spatiotemporal settings with slowly evolving singular subspaces.

Abstract: Traffic-density matrices from different days exhibit both low rank and stable correlations in their singular-vector subspaces. Leveraging this, we introduce SATORIS-N, a framework for imputing partially observed traffic-density by informed subspace priors from neighboring days. Our contribution is a subspace-aware semidefinite programming (SDP)} formulation of nuclear norm that explicitly informs the reconstruction with prior singular-subspace information. This convex formulation jointly enforces low rank and subspace alignment, providing a single global optimum and substantially improving accuracy under medium and high occlusion. We also study a lightweight implicit subspace-alignment} strategy in which matrices from consecutive days are concatenated to encourage alignment of spatial or temporal singular directions. Although this heuristic offers modest gains when missing rates are low, the explicit SDP approach is markedly more robust when large fractions of entries are missing. Across two real-world datasets (Beijing and Shanghai), SATORIS-N consistently outperforms standard matrix-completion methods such as SoftImpute, IterativeSVD, statistical, and even deep learning baselines at high occlusion levels. The framework generalizes to other spatiotemporal settings in which singular subspaces evolve slowly over time. In the context of intelligent vehicles and vehicle-to-everything (V2X) systems, accurate traffic-density
  reconstruction enables critical applications including cooperative perception, predictive routing, and vehicle-to-infrastructure (V2I) communication optimization. When infrastructure sensors or vehicle-reported observations are incomplete - due to communication dropouts, sensor occlusions, or sparse connected vehicle penetration-reliable imputation becomes essential for safe and efficient autonomous navigation.

</details>


### [408] [What Makes a Good Example? Modeling Exemplar Selection with Neural Network Representations](https://arxiv.org/abs/2602.03144)
*Fanxiao Wani Qiu,Oscar Leong,Alexander LaTourrette*

Main category: cs.LG

TL;DR: Humans balance representativeness and diversity when teaching exemplars, with neural network models showing joint representativeness and diversity strategies best match human behavior.


<details>
  <summary>Details</summary>
Motivation: To understand the computational principles behind human exemplar selection for teaching, leveraging neural networks and subset selection criteria.

Method: Novel visual categories were embedded using pretrained vision models, and selection strategies varied in prototypicality, joint representativeness, and diversity. Participants selected exemplars, and models were compared to human choices.

Result: Joint representativeness and diversity strategies best matched human judgments, with transformer-based representations outperforming convolutional networks.

Conclusion: Dataset distillation methods in machine learning can serve as effective computational models for human teaching strategies.

Abstract: Teaching requires distilling a rich category distribution into a small set of informative exemplars. Although prior work shows that humans consider both representativeness and diversity when teaching, the computational principles underlying these tradeoffs remain unclear. We address this gap by modeling human exemplar selection using neural network feature representations and principled subset selection criteria. Novel visual categories were embedded along a one-dimensional morph continuum using pretrained vision models, and selection strategies varied in their emphasis on prototypicality, joint representativeness, and diversity. Adult participants selected one to three exemplars to teach a learner. Model-human comparisons revealed that strategies based on joint representativeness, or its combination with diversity, best captured human judgments, whereas purely prototypical or diversity-based strategies performed worse. Moreover, transformer-based representations consistently aligned more closely with human behavior than convolutional networks. These results highlight the potential utility of dataset distillation methods in machine learning as computational models for teaching.

</details>


### [409] [MemCast: Memory-Driven Time Series Forecasting with Experience-Conditioned Reasoning](https://arxiv.org/abs/2602.03164)
*Xiaoyu Tao,Mingyue Cheng,Ze Guo,Shuo Yu,Yaguo Liu,Qi Liu,Shijin Wang*

Main category: cs.LG

TL;DR: MemCast is a learning-to-memory framework for time series forecasting that organizes experience hierarchically and dynamically adapts confidence for continual evolution, outperforming previous methods.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based forecasters lack explicit experience accumulation and continual evolution, limiting their effectiveness in time series forecasting.

Method: MemCast learns experience from training data, organizes it into hierarchical memory (historical patterns, reasoning wisdom, general laws), and uses dynamic confidence adaptation for continual evolution.

Result: MemCast consistently outperforms previous methods in experiments across multiple datasets.

Conclusion: MemCast validates the effectiveness of hierarchical memory and dynamic confidence adaptation in improving time series forecasting.

Abstract: Time series forecasting (TSF) plays a critical role in decision-making for many real-world applications. Recently, LLM-based forecasters have made promising advancements. Despite their effectiveness, existing methods often lack explicit experience accumulation and continual evolution. In this work, we propose MemCast, a learning-to-memory framework that reformulates TSF as an experience-conditioned reasoning task. Specifically, we learn experience from the training set and organize it into a hierarchical memory. This is achieved by summarizing prediction results into historical patterns, distilling inference trajectories into reasoning wisdom, and inducing extracted temporal features into general laws. Furthermore, during inference, we leverage historical patterns to guide the reasoning process and utilize reasoning wisdom to select better trajectories, while general laws serve as criteria for reflective iteration. Additionally, to enable continual evolution, we design a dynamic confidence adaptation strategy that updates the confidence of individual entries without leaking the test set distribution. Extensive experiments on multiple datasets demonstrate that MemCast consistently outperforms previous methods, validating the effectiveness of our approach. Our code is available at https://github.com/Xiaoyu-Tao/MemCast-TS.

</details>


### [410] [StepScorer: Accelerating Reinforcement Learning with Step-wise Scoring and Psychological Regret Modeling](https://arxiv.org/abs/2602.03171)
*Zhe Xu*

Main category: cs.LG

TL;DR: The Psychological Regret Model (PRM) accelerates reinforcement learning by using regret-based feedback signals, improving convergence by 36% over PPO in sparse-reward environments like Lunar Lander.


<details>
  <summary>Details</summary>
Motivation: Slow convergence in reinforcement learning due to sparse rewards motivates the need for denser feedback signals to enhance learning efficiency.

Method: PRM introduces regret-based feedback after each decision step, comparing optimal and taken actions' values to transform sparse rewards into dense signals.

Result: PRM outperforms PPO by achieving stable performance 36% faster, excelling in continuous control tasks and delayed-feedback environments.

Conclusion: PRM effectively bridges behavioral economics and reinforcement learning, demonstrating practical utility in robotics, finance, and adaptive education.

Abstract: Reinforcement learning algorithms often suffer from slow convergence due to sparse reward signals, particularly in complex environments where feedback is delayed or infrequent. This paper introduces the Psychological Regret Model (PRM), a novel approach that accelerates learning by incorporating regret-based feedback signals after each decision step. Rather than waiting for terminal rewards, PRM computes a regret signal based on the difference between the expected value of the optimal action and the value of the action taken in each state. This transforms sparse rewards into dense feedback signals through a step-wise scoring framework, enabling faster convergence. We demonstrate that PRM achieves stable performance approximately 36\% faster than traditional Proximal Policy Optimization (PPO) in benchmark environments such as Lunar Lander. Our results indicate that PRM is particularly effective in continuous control tasks and environments with delayed feedback, making it suitable for real-world applications such as robotics, finance, and adaptive education where rapid policy adaptation is critical. The approach formalizes human-inspired counterfactual thinking as a computable regret signal, bridging behavioral economics and reinforcement learning.

</details>


### [411] [Adversarial construction as a potential solution to the experiment design problem in large task spaces](https://arxiv.org/abs/2602.03172)
*Prakhar Godara,Frederick Callaway,Marcelo G. Mattar*

Main category: cs.LG

TL;DR: A unified model for binary sequence prediction tasks using HMMs is proposed, with an adversarial construction approach outperforming random sampling for identifying novel behaviors.


<details>
  <summary>Details</summary>
Motivation: To develop a robust, task-general theory of human behavior by focusing on binary sequence prediction tasks parameterized by HMMs.

Method: Proposes an adversarial construction approach to identify tasks likely to elicit novel behaviors, avoiding infeasible exploration of the entire task space.

Result: Adversarial construction outperforms random sampling, making it a viable proxy for optimal experimental design in high-dimensional task spaces.

Conclusion: The adversarial construction approach is effective for exploring high-dimensional task spaces and uncovering novel human behaviors.

Abstract: Despite decades of work, we still lack a robust, task-general theory of human behavior even in the simplest domains. In this paper we tackle the generality problem head-on, by aiming to develop a unified model for all tasks embedded in a task-space. In particular we consider the space of binary sequence prediction tasks where the observations are generated by the space parameterized by hidden Markov models (HMM). As the space of tasks is large, experimental exploration of the entire space is infeasible. To solve this problem we propose the adversarial construction approach, which helps identify tasks that are most likely to elicit a qualitatively novel behavior. Our results suggest that adversarial construction significantly outperforms random sampling of environments and therefore could be used as a proxy for optimal experimental design in high-dimensional task spaces.

</details>


### [412] [Probe-then-Commit Multi-Objective Bandits: Theoretical Benefits of Limited Multi-Arm Feedback](https://arxiv.org/abs/2602.03175)
*Ming Shi*

Main category: cs.LG

TL;DR: The paper introduces an online resource-selection problem with multi-arm probing and develops an algorithm (	extsc{PtC-P-UCB}) to optimize Pareto frontiers under uncertainty, achieving improved performance bounds.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by practical scenarios like multi-radio access selection and mobile edge computing offloading, where limited multi-arm feedback is crucial but underexplored.

Method: The authors develop 	extsc{PtC-P-UCB}, an algorithm that optimizes probing and committing strategies to maximize Pareto frontier coverage and hypervolume gain.

Result: The algorithm achieves a dominated-hypervolume frontier error of $	ilde{O} (K_P d/ackslash sqrt{qT})$ and scalarized regret $	ilde{O} (L_φdackslash sqrt{(K/q)T})$, showing acceleration due to probing.

Conclusion: The work bridges gaps between classical bandits and full-information experts, offering practical solutions for multi-objective learning with limited feedback.

Abstract: We study an online resource-selection problem motivated by multi-radio access selection and mobile edge computing offloading. In each round, an agent chooses among $K$ candidate links/servers (arms) whose performance is a stochastic $d$-dimensional vector (e.g., throughput, latency, energy, reliability). The key interaction is \emph{probe-then-commit (PtC)}: the agent may probe up to $q>1$ candidates via control-plane measurements to observe their vector outcomes, but must execute exactly one candidate in the data plane. This limited multi-arm feedback regime strictly interpolates between classical bandits ($q=1$) and full-information experts ($q=K$), yet existing multi-objective learning theory largely focuses on these extremes. We develop \textsc{PtC-P-UCB}, an optimistic probe-then-commit algorithm whose technical core is frontier-aware probing under uncertainty in a Pareto mode, e.g., it selects the $q$ probes by approximately maximizing a hypervolume-inspired frontier-coverage potential and commits by marginal hypervolume gain to directly expand the attained Pareto region. We prove a dominated-hypervolume frontier error of $\tilde{O} (K_P d/\sqrt{qT})$, where $K_P$ is the Pareto-frontier size and $T$ is the horizon, and scalarized regret $\tilde{O} (L_φd\sqrt{(K/q)T})$, where $φ$ is the scalarizer. These quantify a transparent $1/\sqrt{q}$ acceleration from limited probing. We further extend to \emph{multi-modal probing}: each probe returns $M$ modalities (e.g., CSI, queue, compute telemetry), and uncertainty fusion yields variance-adaptive versions of the above bounds via an effective noise scale.

</details>


### [413] [Reinforcement Learning with Promising Tokens for Large Language Models](https://arxiv.org/abs/2602.03195)
*Jing-Cheng Pang,Liang Lu,Xian Tang,Kun Jiang,Sijie Wu,Kai Zhang,Xubin Li*

Main category: cs.LG

TL;DR: RLPT introduces a dynamic subset of promising tokens to focus RL on relevant actions, improving training stability and efficiency for LLMs.


<details>
  <summary>Details</summary>
Motivation: Standard RL for LLMs includes irrelevant tokens in the action space, distracting policy optimization. RLPT addresses this by focusing on a low-rank subspace of relevant tokens.

Method: RLPT decouples decision-making from token generation by leveraging semantic priors to identify promising tokens and masking out irrelevant ones during policy optimization.

Result: RLPT reduces gradient variance, stabilizes training, and improves sample efficiency, outperforming standard RL baselines in math, coding, and telecom reasoning tasks.

Conclusion: RLPT enhances RL for LLMs by refining the action space, demonstrating effectiveness across model sizes and RL algorithms.

Abstract: Reinforcement learning (RL) has emerged as a key paradigm for aligning and optimizing large language models (LLMs). Standard approaches treat the LLM as the policy and apply RL directly over the full vocabulary space. However, this formulation includes the massive tail of contextually irrelevant tokens in the action space, which could distract the policy from focusing on decision-making among the truly reasonable tokens. In this work, we verify that valid reasoning paths could inherently concentrate within a low-rank subspace. Based on this insight, we introduce Reinforcement Learning with Promising Tokens (RLPT), a framework that mitigates the action space issue by decoupling strategic decision-making from token generation. Specifically, RLPT leverages the semantic priors of the base model to identify a dynamic set of \emph{promising tokens} and constrains policy optimization exclusively to this refined subset via masking. Theoretical analysis and empirical results demonstrate that RLPT effectively reduces gradient variance, stabilizes the training process, and improves sample efficiency. Experiment results on math, coding, and telecom reasoning show that RLPT outperforms standard RL baselines and integrates effectively across various model sizes (4B and 8B) and RL algorithms (GRPO and DAPO).

</details>


### [414] [From Scalar Rewards to Potential Trends: Shaping Potential Landscapes for Model-Based Reinforcement Learning](https://arxiv.org/abs/2602.03201)
*Yao-Hui Li,Zeyu Wang,Xin Li,Wei Pang,Yingfang Yuan,Zhengkun Chen,Boya Zhang,Riashat Islam,Alex Lamb,Yonggang Zhang*

Main category: cs.LG

TL;DR: SLOPE is a novel MBRL framework that constructs potential landscapes for better planning in sparse reward environments, outperforming baselines across various benchmarks.


<details>
  <summary>Details</summary>
Motivation: Standard MBRL struggles in sparse reward settings due to flat reward landscapes lacking directional guidance for planning.

Method: SLOPE shifts from scalar reward prediction to constructing potential landscapes using optimistic distributional regression to amplify success signals.

Result: SLOPE outperforms leading baselines in 30+ tasks across 5 benchmarks, handling sparse, semi-sparse, and dense rewards effectively.

Conclusion: SLOPE's innovative approach to reward modeling enhances MBRL performance in sparse reward environments.

Abstract: Model-based reinforcement learning (MBRL) achieves high sample efficiency by simulating future trajectories with learned dynamics and reward models. However, its effectiveness is severely compromised in sparse reward settings. The core limitation lies in the standard paradigm of regressing ground-truth scalar rewards: in sparse environments, this yields a flat, gradient-free landscape that fails to provide directional guidance for planning. To address this challenge, we propose Shaping Landscapes with Optimistic Potential Estimates (SLOPE), a novel framework that shifts reward modeling from predicting scalars to constructing informative potential landscapes. SLOPE employs optimistic distributional regression to estimate high-confidence upper bounds, which amplifies rare success signals and ensures sufficient exploration gradients. Evaluations on 30+ tasks across 5 benchmarks demonstrate that SLOPE consistently outperforms leading baselines in fully sparse, semi-sparse, and dense rewards.

</details>


### [415] [Sparsity is Combinatorial Depth: Quantifying MoE Expressivity via Tropical Geometry](https://arxiv.org/abs/2602.03204)
*Ye Su,Huayi Tang,Zixuan Gong,Yong Liu*

Main category: cs.LG

TL;DR: The paper analyzes Mixture-of-Experts (MoE) architectures using tropical geometry, showing that Top-$k$ routing is algebraically isomorphic to a tropical polynomial. It introduces Effective Capacity and proves MoE's combinatorial resilience on low-dimensional data.


<details>
  <summary>Details</summary>
Motivation: To provide a theoretical foundation for MoE's success by linking its sparsity and routing mechanisms to tropical geometry and combinatorial depth.

Method: Uses tropical geometry to show Top-$k$ routing's isomorphism with tropical polynomials and introduces Effective Capacity under the Manifold Hypothesis.

Result: Demonstrates MoE's combinatorial resilience and high expressivity compared to dense networks, which suffer from capacity collapse.

Conclusion: The study rigorously justifies MoE's superiority by unifying discrete and continuous geometry, offering insights into conditional computation.

Abstract: While Mixture-of-Experts (MoE) architectures define the state-of-the-art, their theoretical success is often attributed to heuristic efficiency rather than geometric expressivity. In this work, we present the first analysis of MoE through the lens of tropical geometry, establishing that the Top-$k$ routing mechanism is algebraically isomorphic to the $k$-th elementary symmetric tropical polynomial. This isomorphism partitions the input space into the Normal Fan of a Hypersimplex, revealing that \textbf{sparsity is combinatorial depth} which scales geometric capacity by the binomial coefficient $\binom{N}{k}$. Moving beyond ambient bounds, we introduce the concept of \textit{Effective Capacity} under the Manifold Hypothesis. We prove that while dense networks suffer from capacity collapse on low-dimensional data, MoE architectures exhibit \textit{Combinatorial Resilience}, maintaining high expressivity via the transversality of routing cones. In this study, our framework unifies the discrete geometry of the Hypersimplex with the continuous geometry of neural functions, offering a rigorous theoretical justification for the topological supremacy of conditional computation.

</details>


### [416] [Lookahead Sample Reward Guidance for Test-Time Scaling of Diffusion Models](https://arxiv.org/abs/2602.03211)
*Yeongmin Kim,Donghyeok Shin,Byeonghu Na,Minsang Park,Richard Lee Kim,Il-Chul Moon*

Main category: cs.LG

TL;DR: The paper introduces LiDAR sampling, an efficient method to align diffusion model outputs with human intent by computing expected future rewards using marginal samples, avoiding costly neural backpropagation.


<details>
  <summary>Details</summary>
Motivation: Generated samples from diffusion models often misalign with human intent, and existing gradient guidance methods are computationally expensive.

Method: The proposed method computes expected future rewards (EFR) using marginal samples from a pre-trained diffusion model, enabling closed-form guidance without neural backpropagation. Lookahead sampling and an accurate solver further enhance efficiency.

Result: LiDAR sampling achieves significant performance improvements with fewer samples and faster computation (9.5x speedup) compared to existing gradient guidance methods.

Conclusion: LiDAR sampling is an efficient and effective solution for aligning diffusion model outputs with human intent, offering substantial computational savings and performance gains.

Abstract: Diffusion models have demonstrated strong generative performance; however, generated samples often fail to fully align with human intent. This paper studies a test-time scaling method that enables sampling from regions with higher human-aligned reward values. Existing gradient guidance methods approximate the expected future reward (EFR) at an intermediate particle $\mathbf{x}_t$ using a Taylor approximation, but this approximation at each time step incurs high computational cost due to sequential neural backpropagation. We show that the EFR at any $\mathbf{x}_t$ can be computed using only marginal samples from a pre-trained diffusion model. The proposed EFR formulation detaches the neural dependency between $\mathbf{x}_t$ and the EFR, enabling closed-form guidance computation without neural backpropagation. To further improve efficiency, we introduce lookahead sampling to collect marginal samples. For final sample generation, we use an accurate solver that guides particles toward high-reward lookahead samples. We refer to this sampling scheme as LiDAR sampling. LiDAR achieves substantial performance improvements using only three samples with a 3-step lookahead solver, exhibiting steep performance gains as lookahead accuracy and sample count increase; notably, it reaches the same GenEval performance as the latest gradient guidance method for SDXL with a 9.5x speedup.

</details>


### [417] [Topology Matters: A Cautionary Case Study of Graph SSL on Neuro-Inspired Benchmarks](https://arxiv.org/abs/2602.03217)
*May Kristine Jonson Carlon,Su Myat Noe,Haojiong Wang,Yasuo Kuniyoshi*

Main category: cs.LG

TL;DR: The paper presents a hierarchical SSL framework for brain connectivity data, revealing a mismatch between SSL objectives and topological properties, advocating for topology-aware SSL objectives.


<details>
  <summary>Details</summary>
Motivation: To understand how local interactions shape global brain organization by developing models that handle information at multiple scales.

Method: A hierarchical SSL framework learning node-, edge-, and graph-level embeddings, tested on a synthetic benchmark mimicking connectome topology.

Result: SSL models fail catastrophically due to misalignment with topological properties, outperformed by classical topology-aware methods.

Conclusion: Highlights the need for topology-aware SSL objectives in neuro-AI research to preserve structural properties like modularity.

Abstract: Understanding how local interactions give rise to global brain organization requires models that can represent information across multiple scales. We introduce a hierarchical self-supervised learning (SSL) framework that jointly learns node-, edge-, and graph-level embeddings, inspired by multimodal neuroimaging. We construct a controllable synthetic benchmark mimicking the topological properties of connectomes. Our four-stage evaluation protocol reveals a critical failure: the invariance-based SSL model is fundamentally misaligned with the benchmark's topological properties and is catastrophically outperformed by classical, topology-aware heuristics. Ablations confirm an objective mismatch: SSL objectives designed to be invariant to topological perturbations learn to ignore the very community structure that classical methods exploit. Our results expose a fundamental pitfall in applying generic graph SSL to connectome-like data. We present this framework as a cautionary case study, highlighting the need for new, topology-aware SSL objectives for neuro-AI research that explicitly reward the preservation of structure (e.g., modularity or motifs).

</details>


### [418] [BayeSQP: Bayesian Optimization through Sequential Quadratic Programming](https://arxiv.org/abs/2602.03232)
*Paul Brunzema,Sebastian Trimpe*

Main category: cs.LG

TL;DR: BayeSQP is a new algorithm combining sequential quadratic programming with Bayesian optimization, using Gaussian processes to model functions and constraints, and outperforms existing methods in high-dimensional tasks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to bridge classical optimization techniques with modern black-box optimization by leveraging Bayesian methods to improve efficiency and performance in high-dimensional settings.

Method: BayeSQP uses second-order Gaussian process surrogates to model objective and constraint functions, including gradients and Hessians, from zero-order information. It solves local subproblems as second-order cone programs and uses constrained Thompson sampling for line search.

Result: Empirical results demonstrate that BayeSQP outperforms state-of-the-art methods in high-dimensional optimization tasks.

Conclusion: The algorithm provides a principled and flexible framework, effectively combining classical and modern optimization approaches.

Abstract: We introduce BayeSQP, a novel algorithm for general black-box optimization that merges the structure of sequential quadratic programming with concepts from Bayesian optimization. BayeSQP employs second-order Gaussian process surrogates for both the objective and constraints to jointly model the function values, gradients, and Hessian from only zero-order information. At each iteration, a local subproblem is constructed using the GP posterior estimates and solved to obtain a search direction. Crucially, the formulation of the subproblem explicitly incorporates uncertainty in both the function and derivative estimates, resulting in a tractable second-order cone program for high probability improvements under model uncertainty. A subsequent one-dimensional line search via constrained Thompson sampling selects the next evaluation point. Empirical results show thatBayeSQP outperforms state-of-the-art methods in specific high-dimensional settings. Our algorithm offers a principled and flexible framework that bridges classical optimization techniques with modern approaches to black-box optimization.

</details>


### [419] [GraDE: A Graph Diffusion Estimator for Frequent Subgraph Discovery in Neural Architectures](https://arxiv.org/abs/2602.03257)
*Yikang Yang,Zhengxin Yang,Minghao Luo,Luzhou Peng,Hongxiao Li,Wanling Gao,Lei Wang,Jianfeng Zhan*

Main category: cs.LG

TL;DR: GraDE is a diffusion-guided search framework that uses a Graph Diffusion Estimator to efficiently and accurately identify frequent subgraphs in neural architectures, outperforming existing methods in both ranking accuracy and discovery capability.


<details>
  <summary>Details</summary>
Motivation: The need to find frequent subgraph patterns in neural architectures for efficiency and design optimization is unmet by current methods, which either lack computational feasibility or discovery capability.

Method: GraDE introduces a Graph Diffusion Estimator, leveraging graph diffusion models to score subgraph typicality within a learned distribution, ensuring both computational tractability and high discovery rates.

Result: GraDE shows up to 114% improvement in ranking accuracy over sampling-based methods and discovers large-scale patterns with 30x higher median frequency.

Conclusion: GraDE bridges the gap between accuracy and computational feasibility in subgraph discovery, making it a powerful tool for neural architecture analysis.

Abstract: Finding frequently occurring subgraph patterns or network motifs in neural architectures is crucial for optimizing efficiency, accelerating design, and uncovering structural insights. However, as the subgraph size increases, enumeration-based methods are perfectly accurate but computationally prohibitive, while sampling-based methods are computationally tractable but suffer from a severe decline in discovery capability. To address these challenges, this paper proposes GraDE, a diffusion-guided search framework that ensures both computational feasibility and discovery capability. The key innovation is the Graph Diffusion Estimator (GraDE), which is the first to introduce graph diffusion models to identify frequent subgraphs by scoring their typicality within the learned distribution. Comprehensive experiments demonstrate that the estimator achieves superior ranking accuracy, with up to 114\% improvement compared to sampling-based baselines. Benefiting from this, the proposed framework successfully discovers large-scale frequent patterns, achieving up to 30$\times$ higher median frequency than sampling-based methods.

</details>


### [420] [Beyond Suffixes: Token Position in GCG Adversarial Attacks on Large Language Models](https://arxiv.org/abs/2602.03265)
*Hicham Eddoubi,Umar Faruk Abdullahi,Fadi Hassan*

Main category: cs.LG

TL;DR: Exploring adversarial token placement in prompts reveals a blind spot in LLM safety evaluations, impacting attack success rates.


<details>
  <summary>Details</summary>
Motivation: To address robustness challenges in LLM safety alignment, particularly against jailbreak attacks like GCG.

Method: Analyzed GCG attacks by varying adversarial token placement (prefix vs. suffix) and position during evaluation.

Result: Found that adversarial token placement significantly affects attack success rates.

Conclusion: Highlights the need to consider adversarial token position in LLM safety evaluations.

Abstract: Large Language Models (LLMs) have seen widespread adoption across multiple domains, creating an urgent need for robust safety alignment mechanisms. However, robustness remains challenging due to jailbreak attacks that bypass alignment via adversarial prompts. In this work, we focus on the prevalent Greedy Coordinate Gradient (GCG) attack and identify a previously underexplored attack axis in jailbreak attacks typically framed as suffix-based: the placement of adversarial tokens within the prompt. Using GCG as a case study, we show that both optimizing attacks to generate prefixes instead of suffixes and varying adversarial token position during evaluation substantially influence attack success rates. Our findings highlight a critical blind spot in current safety evaluations and underline the need to account for the position of adversarial tokens in the adversarial robustness evaluation of LLMs.

</details>


### [421] [Unveiling Covert Toxicity in Multimodal Data via Toxicity Association Graphs: A Graph-Based Metric and Interpretable Detection Framework](https://arxiv.org/abs/2602.03268)
*Guanzong Wu,Zihao Zhu,Siwei Lyu,Baoyuan Wu*

Main category: cs.LG

TL;DR: The paper introduces a novel framework for detecting hidden toxicity in multimodal data using Toxicity Association Graphs (TAGs) and a new metric called Multimodal Toxicity Covertness (MTC). It validates the approach with a specialized dataset and shows superior performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: Detecting toxicity in multimodal data is challenging because harmful meanings often only emerge when modalities are combined, requiring advanced methods to uncover latent toxic implications.

Method: Proposes Toxicity Association Graphs (TAGs) to model semantic associations and introduces the MTC metric for quantifying hidden toxicity. Also creates the Covert Toxic Dataset for validation.

Result: The framework outperforms existing methods in detecting covert toxicity, offering interpretable and auditable results.

Conclusion: The contributions advance explainable multimodal toxicity detection and provide a foundation for future context-aware approaches.

Abstract: Detecting toxicity in multimodal data remains a significant challenge, as harmful meanings often lurk beneath seemingly benign individual modalities: only emerging when modalities are combined and semantic associations are activated. To address this, we propose a novel detection framework based on Toxicity Association Graphs (TAGs), which systematically model semantic associations between innocuous entities and latent toxic implications. Leveraging TAGs, we introduce the first quantifiable metric for hidden toxicity, the Multimodal Toxicity Covertness (MTC), which measures the degree of concealment in toxic multimodal expressions. By integrating our detection framework with the MTC metric, our approach enables precise identification of covert toxicity while preserving full interpretability of the decision-making process, significantly enhancing transparency in multimodal toxicity detection. To validate our method, we construct the Covert Toxic Dataset, the first benchmark specifically designed to capture high-covertness toxic multimodal instances. This dataset encodes nuanced cross-modal associations and serves as a rigorous testbed for evaluating both the proposed metric and detection framework. Extensive experiments demonstrate that our approach outperforms existing methods across both low- and high-covertness toxicity regimes, while delivering clear, interpretable, and auditable detection outcomes. Together, our contributions advance the state of the art in explainable multimodal toxicity detection and lay the foundation for future context-aware and interpretable approaches. Content Warning: This paper contains examples of toxic multimodal content that may be offensive or disturbing to some readers. Reader discretion is advised.

</details>


### [422] [BlockRR: A Unified Framework of RR-type Algorithms for Label Differential Privacy](https://arxiv.org/abs/2602.03277)
*Haixia Liu,Yi Ding*

Main category: cs.LG

TL;DR: BlockRR is a unified randomized-response mechanism for label differential privacy, generalizing existing RR-type mechanisms and proving ε-label DP. It includes a partition method and performs well in high and moderate privacy regimes.


<details>
  <summary>Details</summary>
Motivation: To address the need for a unified mechanism for label differential privacy that avoids case-by-case analysis and improves the balance between accuracy and privacy.

Method: BlockRR generalizes existing RR mechanisms under specific parameters and includes a partition method based on a weight matrix from label prior information, leveraging parallel composition for DP guarantees.

Result: BlockRR achieves better accuracy-privacy balance in high and moderate privacy regimes (ε ≤ 3.0) and matches standard RR in low-privacy regimes (ε ≥ 4.0).

Conclusion: BlockRR offers a flexible, unified solution for label differential privacy with strong theoretical guarantees and empirical performance.

Abstract: In this paper, we introduce BlockRR, a novel and unified randomized-response mechanism for label differential privacy. This framework generalizes existed RR-type mechanisms as special cases under specific parameter settings, which eliminates the need for separate, case-by-case analysis. Theoretically, we prove that BlockRR satisfies $ε$-label DP. We also design a partition method for BlockRR based on a weight matrix derived from label prior information; the parallel composition principle ensures that the composition of two such mechanisms remains $ε$-label DP. Empirically, we evaluate BlockRR on two variants of CIFAR-10 with varying degrees of class imbalance. Results show that in the high-privacy and moderate-privacy regimes ($ε\leq 3.0$), our propsed method gets a better balance between test accuaracy and the average of per-class accuracy. In the low-privacy regime ($ε\geq 4.0$), all methods reduce BlockRR to standard RR without additional performance loss.

</details>


### [423] [Universal Approximation of Continuous Functionals on Compact Subsets via Linear Measurements and Scalar Nonlinearities](https://arxiv.org/abs/2602.03290)
*Andrey Krylov,Maksim Penkin*

Main category: cs.LG

TL;DR: The paper shows that continuous functionals on compact subsets of Hilbert space products can be uniformly approximated using models with linear measurements and scalar nonlinearities, extending to Banach-valued maps.


<details>
  <summary>Details</summary>
Motivation: To justify the common design pattern in operator learning and imaging by proving universal approximation capabilities.

Method: Use models combining finitely many continuous linear measurements of inputs with continuous scalar nonlinearities.

Result: Functionals can be uniformly approximated; extended to Banach-valued maps for finite-rank approximations.

Conclusion: Validates the 'measure, apply nonlinearities, combine' approach in practical applications.

Abstract: We study universal approximation of continuous functionals on compact subsets of products of Hilbert spaces. We prove that any such functional can be uniformly approximated by models that first take finitely many continuous linear measurements of the inputs and then combine these measurements through continuous scalar nonlinearities. We also extend the approximation principle to maps with values in a Banach space, yielding finite-rank approximations. These results provide a compact-set justification for the common ``measure, apply scalar nonlinearities, then combine'' design pattern used in operator learning and imaging.

</details>


### [424] [Anomaly Detection via Mean Shift Density Enhancement](https://arxiv.org/abs/2602.03293)
*Pritam Kar,Rahul Bordoloi,Olaf Wolkenhauer,Saptarshi Bej*

Main category: cs.LG

TL;DR: MSDE is a robust unsupervised anomaly detection framework using density-driven manifold evolution, outperforming 13 baselines on diverse datasets and noise levels.


<details>
  <summary>Details</summary>
Motivation: Existing unsupervised anomaly detection methods lack robustness across anomaly types and noisy settings. MSDE addresses this gap by leveraging geometric responses to density changes.

Method: MSDE uses a weighted mean-shift procedure with adaptive density weights from a UMAP-based fuzzy neighborhood graph, scoring anomalies by displacement over iterations.

Result: MSDE achieves strong, balanced performance on AUC-ROC, AUC-PR, and Precision@n across 46 datasets and noise levels, surpassing 13 baselines.

Conclusion: Displacement-based scoring in MSDE provides a robust alternative to current state-of-the-art unsupervised anomaly detection methods.

Abstract: Unsupervised anomaly detection stands as an important problem in machine learning, with applications in financial fraud prevention, network security and medical diagnostics. Existing unsupervised anomaly detection algorithms rarely perform well across different anomaly types, often excelling only under specific structural assumptions. This lack of robustness also becomes particularly evident under noisy settings. We propose Mean Shift Density Enhancement (MSDE), a fully unsupervised framework that detects anomalies through their geometric response to density-driven manifold evolution. MSDE is based on the principle that normal samples, being well supported by local density, remain stable under iterative density enhancement, whereas anomalous samples undergo large cumulative displacements as they are attracted toward nearby density modes. To operationalize this idea, MSDE employs a weighted mean-shift procedure with adaptive, sample-specific density weights derived from a UMAP-based fuzzy neighborhood graph. Anomaly scores are defined by the total displacement accumulated across a small number of mean-shift iterations. We evaluate MSDE on the ADBench benchmark, comprising forty six real-world tabular datasets, four realistic anomaly generation mechanisms, and six noise levels. Compared to 13 established unsupervised baselines, MSDE achieves consistently strong, balanced and robust performance for AUC-ROC, AUC-PR, and Precision@n, at several noise levels and on average over several types of anomalies. These results demonstrate that displacement-based scoring provides a robust alternative to the existing state-of-the-art for unsupervised anomaly detection.

</details>


### [425] [Lipschitz Multiscale Deep Equilibrium Models: A Theoretically Guaranteed and Accelerated Approach](https://arxiv.org/abs/2602.03297)
*Naoki Sato,Hideaki Iiduka*

Main category: cs.LG

TL;DR: DEQs offer deep network representations efficiently but face high computational costs due to fixed-point iterations. This study improves convergence and reduces time by restructuring the architecture, achieving faster performance with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: DEQs outperform traditional methods in memory efficiency but suffer from high computational time due to unstable fixed-point iterations. Addressing this inefficiency motivates restructuring for guaranteed convergence.

Method: The study introduces Lipschitz multiscale DEQ, restructuring the architecture to ensure fixed-point convergence via hyperparameter adjustments.

Result: The approach achieved up to a 4.75× speed-up on CIFAR-10 with a minor accuracy drop, demonstrating improved computational efficiency.

Conclusion: Restructuring DEQs to guarantee convergence significantly reduces computational time, making them more practical despite slight accuracy trade-offs.

Abstract: Deep equilibrium models (DEQs) achieve infinitely deep network representations without stacking layers by exploring fixed points of layer transformations in neural networks. Such models constitute an innovative approach that achieves performance comparable to state-of-the-art methods in many large-scale numerical experiments, despite requiring significantly less memory. However, DEQs face the challenge of requiring vastly more computational time for training and inference than conventional methods, as they repeatedly perform fixed-point iterations with no convergence guarantee upon each input. Therefore, this study explored an approach to improve fixed-point convergence and consequently reduce computational time by restructuring the model architecture to guarantee fixed-point convergence. Our proposed approach for image classification, Lipschitz multiscale DEQ, has theoretically guaranteed fixed-point convergence for both forward and backward passes by hyperparameter adjustment, achieving up to a 4.75$\times$ speed-up in numerical experiments on CIFAR-10 at the cost of a minor drop in accuracy.

</details>


### [426] [Periodic Regularized Q-Learning](https://arxiv.org/abs/2602.03301)
*Hyukjun Yang,Han-Dong Lim,Donghwan Lee*

Main category: cs.LG

TL;DR: The paper introduces PRQ, a new Q-learning algorithm with periodic regularization to ensure stable convergence under linear function approximation.


<details>
  <summary>Details</summary>
Motivation: Q-learning lacks convergence guarantees under linear function approximation, prompting the need for regularization techniques.

Method: PRQ regularizes the projection operator to create a contraction, extending it to a sample-based RL algorithm.

Result: Theoretical analysis proves finite-time convergence for PRQ under linear function approximation.

Conclusion: PRQ effectively addresses the convergence limitation of Q-learning in RL settings with function approximation.

Abstract: In reinforcement learning (RL), Q-learning is a fundamental algorithm whose convergence is guaranteed in the tabular setting. However, this convergence guarantee does not hold under linear function approximation. To overcome this limitation, a significant line of research has introduced regularization techniques to ensure stable convergence under function approximation. In this work, we propose a new algorithm, periodic regularized Q-learning (PRQ). We first introduce regularization at the level of the projection operator and explicitly construct a regularized projected value iteration (RP-VI), subsequently extending it to a sample-based RL algorithm. By appropriately regularizing the projection operator, the resulting projected value iteration becomes a contraction. By extending this regularized projection into the stochastic setting, we establish the PRQ algorithm and provide a rigorous theoretical analysis that proves finite-time convergence guarantees for PRQ under linear function approximation.

</details>


### [427] [medR: Reward Engineering for Clinical Offline Reinforcement Learning via Tri-Drive Potential Functions](https://arxiv.org/abs/2602.03305)
*Qianyi Xu,Gousia Habib,Feng Wu,Yanrui Du,Zhihui Chen,Swapnil Mishra,Dilruk Perera,Mengling Feng*

Main category: cs.LG

TL;DR: The paper proposes an automated pipeline using Large Language Models (LLMs) to design and verify rewards for Reinforcement Learning (RL) in clinical dynamic treatment regimes, overcoming manual heuristic limitations.


<details>
  <summary>Details</summary>
Motivation: Clinical RL is hindered by the challenge of reward engineering, which often relies on manual heuristics that don't generalize well across diverse pathologies.

Method: The authors develop an automated pipeline leveraging LLMs to design reward functions using potential functions (survival, confidence, competence) and introduce quantitative metrics for evaluation.

Result: The framework automates reward design for specific diseases and significantly improves policy performance in sparse, offline clinical environments.

Conclusion: The proposed LLM-driven approach enhances the automation and effectiveness of reward engineering in clinical RL, addressing a critical bottleneck in dynamic treatment regimes.

Abstract: Reinforcement Learning (RL) offers a powerful framework for optimizing dynamic treatment regimes (DTRs). However, clinical RL is fundamentally bottlenecked by reward engineering: the challenge of defining signals that safely and effectively guide policy learning in complex, sparse offline environments. Existing approaches often rely on manual heuristics that fail to generalize across diverse pathologies. To address this, we propose an automated pipeline leveraging Large Language Models (LLMs) for offline reward design and verification. We formulate the reward function using potential functions consisted of three core components: survival, confidence, and competence. We further introduce quantitative metrics to rigorously evaluate and select the optimal reward structure prior to deployment. By integrating LLM-driven domain knowledge, our framework automates the design of reward functions for specific diseases while significantly enhancing the performance of the resulting policies.

</details>


### [428] [Entropy-Gated Selective Policy Optimization:Token-Level Gradient Allocation for Hybrid Training of Large Language Models](https://arxiv.org/abs/2602.03309)
*Yuelin Hu,Zhengxue Cheng,Wei Liu,Li Song*

Main category: cs.LG

TL;DR: EGSPO is a three-stage hybrid training method combining SFT and RL, using token-level entropy gating to modulate gradients, improving mathematical reasoning benchmarks with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: To enhance large language model training by combining supervised fine-tuning and reinforcement learning more effectively, addressing the limitations of sample-level mixing.

Method: EGSPO involves three stages: SFT expert learning, RL rollout generation with token entropy computation, and entropy-gated gradient allocation in PPO updates.

Result: EGSPO improves performance on mathematical reasoning benchmarks (3.8% on AIME, 2.9% on MATH) with only 3.4% extra computational cost.

Conclusion: EGSPO effectively balances exploration and knowledge preservation, offering a practical improvement over existing methods.

Abstract: Hybrid training methods for large language models combine supervised fine tuning (SFT) on expert demonstrations with reinforcement learning (RL) on model rollouts, typically at the sample level. We propose Entropy Gated Selective Policy Optimization (EGSPO), a three stage framework that extends sample level mixing with token level gradient modulation.
  Stage 1, SFT expert learning, establishes a reliable warm up policy using expert demonstrations with a pure SFT loss. Stage 2, RL rollout generation, samples trajectories from the current policy and computes per token predictive entropy. Stage 3, the EGSPO mechanism, applies entropy gated gradient allocation: a predictive entropy module routes high entropy tokens to full PPO updates to encourage exploration, and low entropy tokens to attenuated PPO updates to reduce variance and preserve knowledge. Critically, both branches incorporate the advantage function A_t, ensuring that incorrect trajectories receive consistent negative learning signals and preventing reinforcement of confident errors.
  EGSPO achieves consistent improvements on mathematical reasoning benchmarks, with gains of 3.8 percent on AIME and 2.9 percent on MATH over the CHORD phi baseline, while incurring only 3.4 percent additional computational overhead.

</details>


### [429] [Information-Theoretic Multi-Model Fusion for Target-Oriented Adaptive Sampling in Materials Design](https://arxiv.org/abs/2602.03319)
*Yixuan Zhang,Zhiyuan Li,Weijia He,Mian Dai,Chen Shen,Teng Long,Hongbin Zhang*

Main category: cs.LG

TL;DR: An information-theoretic framework for target-oriented adaptive sampling improves sample efficiency in high-dimensional design spaces by focusing on target-relevant directions under limited evaluation budgets.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of costly evaluations in high-dimensional, heterogeneous design spaces by making reliable progress with minimal measurements.

Method: Combines data, model beliefs, and physics priors through information budgeting, adaptive distillation, and multi-model fusion to balance exploitation and exploration.

Result: Achieves top performance in 14 materials design tasks and synthetic benchmarks, typically within 100 evaluations.

Conclusion: The framework enhances reliability and efficiency in target-oriented discovery under constrained budgets.

Abstract: Target-oriented discovery under limited evaluation budgets requires making reliable progress in high-dimensional, heterogeneous design spaces where each new measurement is costly, whether experimental or high-fidelity simulation. We present an information-theoretic framework for target-oriented adaptive sampling that reframes optimization as trajectory discovery: instead of approximating the full response surface, the method maintains and refines a low-entropy information state that concentrates search on target-relevant directions. The approach couples data, model beliefs, and physics/structure priors through dimension-aware information budgeting, adaptive bootstrapped distillation over a heterogeneous surrogate reservoir, and structure-aware candidate manifold analysis with Kalman-inspired multi-model fusion to balance consensus-driven exploitation and disagreement-driven exploration. Evaluated under a single unified protocol without dataset-specific tuning, the framework improves sample efficiency and reliability across 14 single- and multi-objective materials design tasks spanning candidate pools from $600$ to $4 \times 10^6$ and feature dimensions from $10$ to $10^3$, typically reaching top-performing regions within 100 evaluations. Complementary 20-dimensional synthetic benchmarks (Ackley, Rastrigin, Schwefel) further demonstrate robustness to rugged and multimodal landscapes.

</details>


### [430] [From Inexact Gradients to Byzantine Robustness: Acceleration and Optimization under Similarity](https://arxiv.org/abs/2602.03329)
*Renaud Gaucher,Aymeric Dieuleveut,Hadrien Hendrikx*

Main category: cs.LG

TL;DR: The paper addresses vulnerabilities in federated learning to adversarial nodes (Byzantine failures) by framing robust distributed optimization as inexact gradient oracles, enabling optimal asymptotic error and proposing accelerated convergence methods.


<details>
  <summary>Details</summary>
Motivation: Standard federated learning lacks robustness against adversarial nodes, and existing robust algorithms lack systematic analyses, hindering complex developments like accelerated methods.

Method: The work re-casts Byzantine-robust optimization as inexact gradient oracles and proposes two accelerated schemes: a Nesterov-type method and Optimization under Similarity, leveraging auxiliary loss functions.

Result: Theoretical and empirical results show reduced communication complexity and optimal asymptotic error in Byzantine settings.

Conclusion: The proposed methods advance Byzantine-robust optimization by enabling faster convergence and reduced communication overhead.

Abstract: Standard federated learning algorithms are vulnerable to adversarial nodes, a.k.a. Byzantine failures. To solve this issue, robust distributed learning algorithms have been developed, which typically replace parameter averaging by robust aggregations. While generic conditions on these aggregations exist to guarantee the convergence of (Stochastic) Gradient Descent (SGD), the analyses remain rather ad-hoc. This hinders the development of more complex robust algorithms, such as accelerated ones. In this work, we show that Byzantine-robust distributed optimization can, under standard generic assumptions, be cast as a general optimization with inexact gradient oracles (with both additive and multiplicative error terms), an active field of research.
  This allows for instance to directly show that GD on top of standard robust aggregation procedures obtains optimal asymptotic error in the Byzantine setting. Going further, we propose two optimization schemes to speed up the convergence. The first one is a Nesterov-type accelerated scheme whose proof directly derives from accelerated inexact gradient results applied to our formulation. The second one hinges on Optimization under Similarity, in which the server leverages an auxiliary loss function that approximates the global loss. Both approaches allow to drastically reduce the communication complexity compared to previous methods, as we show theoretically and empirically.

</details>


### [431] [Bayesian Conformal Prediction as a Decision Risk Problem](https://arxiv.org/abs/2602.03331)
*Fanyi Wu,Veronika Lohmanova,Samuel Kaski,Michele Caprio*

Main category: cs.LG

TL;DR: The paper introduces BCP, which uses Bayesian posterior predictive densities and Bayesian quadrature to minimize prediction set size while ensuring valid coverage, outperforming traditional methods in reliability and variability.


<details>
  <summary>Details</summary>
Motivation: To improve the reliability and efficiency of prediction sets in conformal prediction frameworks, especially under model misspecification.

Method: Uses Bayesian posterior predictive densities as non-conformity scores and Bayesian quadrature within a split conformal framework.

Result: BCP provides valid coverage guarantees, reduces run-to-run variability, and achieves better empirical coverage (81%) compared to Bayesian credible intervals (49%).

Conclusion: BCP is a robust and efficient method for generating prediction sets, particularly effective under model misspecification.

Abstract: Bayesian posterior predictive densities as non-conformity scores and Bayesian quadrature are used to estimate and minimise the expected prediction set size. Operating within a split conformal framework, BCP provides valid coverage guarantees and demonstrates reliable empirical coverage under model misspecification. Across regression and classification tasks, including distribution-shifted settings such as ImageNet-A, BCP yields prediction sets of comparable size to split conformal prediction, while exhibiting substantially lower run-to-run variability in set size. In sparse regression with nominal coverage of 80 percent, BCP achieves 81 percent empirical coverage under a misspecified prior, whereas Bayesian credible intervals under-cover at 49 percent.

</details>


### [432] [Causal Graph Learning via Distributional Invariance of Cause-Effect Relationship](https://arxiv.org/abs/2602.03353)
*Nang Hung Nguyen,Phi Le Nguyen,Thao Nguyen Truong,Trong Nghia Hoang,Masashi Sugiyama*

Main category: cs.LG

TL;DR: The paper presents a novel framework for causal graph recovery from observational data by testing the invariance of effect-cause conditional distributions across different data subsets.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the efficiency and scalability of recovering causal graphs from observational data, leveraging the invariance of conditional distributions.

Method: The method involves testing the variance of effect-cause conditional distributions across downsampled subsets to identify causal relationships, combined with a sparsity assumption for efficiency.

Result: The algorithm achieves a 25x speedup over state-of-the-art methods and shows superior or equivalent performance on large-scale datasets.

Conclusion: The framework successfully enhances causal graph recovery efficiency and scalability, making it practical for large datasets.

Abstract: This paper introduces a new framework for recovering causal graphs from observational data, leveraging the observation that the distribution of an effect, conditioned on its causes, remains invariant to changes in the prior distribution of those causes. This insight enables a direct test for potential causal relationships by checking the variance of their corresponding effect-cause conditional distributions across multiple downsampled subsets of the data. These subsets are selected to reflect different prior cause distributions, while preserving the effect-cause conditional relationships. Using this invariance test and exploiting an (empirical) sparsity of most causal graphs, we develop an algorithm that efficiently uncovers causal relationships with quadratic complexity in the number of observational variables, reducing the processing time by up to 25x compared to state-of-the-art methods. Our empirical experiments on a varied benchmark of large-scale datasets show superior or equivalent performance compared to existing works, while achieving enhanced scalability.

</details>


### [433] [Achieving Linear Speedup for Composite Federated Learning](https://arxiv.org/abs/2602.03357)
*Kun Huang,Shi Pu*

Main category: cs.LG

TL;DR: FedNMap is a normal map-based method for composite federated learning, handling nonsmooth terms and data heterogeneity with local corrections, achieving linear speedup for nonconvex losses.


<details>
  <summary>Details</summary>
Motivation: Address challenges in composite federated learning, particularly nonsmooth regularizers and data heterogeneity, by proposing a novel method.

Method: FedNMap uses a normal map-based update scheme for nonsmooth terms and local corrections for data heterogeneity.

Result: Achieves linear speedup for nonconvex losses with/without PL condition, a first for nonconvex composite federated learning.

Conclusion: FedNMap effectively handles nonsmoothness and heterogeneity, providing a scalable solution with theoretical guarantees.

Abstract: This paper proposes FedNMap, a normal map-based method for composite federated learning, where the objective consists of a smooth loss and a possibly nonsmooth regularizer. FedNMap leverages a normal map-based update scheme to handle the nonsmooth term and incorporates a local correction strategy to mitigate the impact of data heterogeneity across clients. Under standard assumptions, including smooth local losses, weak convexity of the regularizer, and bounded stochastic gradient variance, FedNMap achieves linear speedup with respect to both the number of clients $n$ and the number of local updates $Q$ for nonconvex losses, both with and without the Polyak-Łojasiewicz (PL) condition. To our knowledge, this is the first result establishing linear speedup for nonconvex composite federated learning.

</details>


### [434] [Rethinking Benign Relearning: Syntax as the Hidden Driver of Unlearning Failures](https://arxiv.org/abs/2602.03379)
*Sangyeon Yoon,Hyesoo Hong,Wonje Jeung,Albert No*

Main category: cs.LG

TL;DR: Machine unlearning struggles with benign relearning, where forgotten info resurfaces. Syntactic similarity, not topical relevance, drives this issue. Introducing syntactic diversification mitigates it effectively.


<details>
  <summary>Details</summary>
Motivation: Addressing the fragility of existing machine unlearning methods, particularly benign relearning, to improve robustness and utility.

Method: Systematic analysis reveals syntactic similarity as the primary driver of relearning. Introduces syntactic diversification by paraphrasing forget queries.

Result: Syntactic diversification suppresses benign relearning, speeds up forgetting, and reduces the trade-off between unlearning efficacy and model utility.

Conclusion: Focusing on syntactic similarity and diversification offers a more effective approach to machine unlearning than topical relevance.

Abstract: Machine unlearning aims to remove specific content from trained models while preserving overall performance. However, the phenomenon of benign relearning, in which forgotten information reemerges even from benign fine-tuning data, reveals that existing unlearning methods remain fundamentally fragile. A common explanation attributes this effect to topical relevance, but we find this account insufficient. Through systematic analysis, we demonstrate that syntactic similarity, rather than topicality, is the primary driver: across benchmarks, syntactically similar data consistently trigger recovery even without topical overlap, due to their alignment in representations and gradients with the forgotten content. Motivated by this insight, we introduce syntactic diversification, which paraphrases the original forget queries into heterogeneous structures prior to unlearning. This approach effectively suppresses benign relearning, accelerates forgetting, and substantially alleviates the trade-off between unlearning efficacy and model utility.

</details>


### [435] [Dynamic Topology Optimization for Non-IID Data in Decentralized Learning](https://arxiv.org/abs/2602.03383)
*Bart Cox,Antreas Ioannou,Jérémie Decouchant*

Main category: cs.LG

TL;DR: Morph is a topology optimization algorithm for decentralized learning that improves model accuracy and convergence in non-IID data scenarios by dynamically reshaping communication graphs based on model dissimilarity.


<details>
  <summary>Details</summary>
Motivation: Decentralized learning struggles with non-IID data and static communication topologies, limiting model accuracy. Morph aims to address these challenges.

Method: Morph uses gossip-based peer discovery and diversity-driven neighbor selection to dynamically optimize communication graphs, maintaining a fixed in-degree.

Result: Experiments on CIFAR-10 and FEMNIST show Morph outperforms baselines, achieving higher accuracy (1.12x on CIFAR-10, 1.08x on FEMNIST) and faster convergence with fewer communication rounds.

Conclusion: Morph effectively improves decentralized learning performance in non-IID settings, nearing fully connected upper bounds while requiring no global coordination.

Abstract: Decentralized learning (DL) enables a set of nodes to train a model collaboratively without central coordination, offering benefits for privacy and scalability. However, DL struggles to train a high accuracy model when the data distribution is non-independent and identically distributed (non-IID) and when the communication topology is static. To address these issues, we propose Morph, a topology optimization algorithm for DL. In Morph, nodes adaptively choose peers for model exchange based on maximum model dissimilarity. Morph maintains a fixed in-degree while dynamically reshaping the communication graph through gossip-based peer discovery and diversity-driven neighbor selection, thereby improving robustness to data heterogeneity. Experiments on CIFAR-10 and FEMNIST with up to 100 nodes show that Morph consistently outperforms static and epidemic baselines, while closely tracking the fully connected upper bound. On CIFAR-10, Morph achieves a relative improvement of 1.12x in test accuracy compared to the state-of-the-art baselines. On FEMNIST, Morph achieves an accuracy that is 1.08x higher than Epidemic Learning. Similar trends hold for 50 node deployments, where Morph narrows the gap to the fully connected upper bound within 0.5 percentage points on CIFAR-10. These results demonstrate that Morph achieves higher final accuracy, faster convergence, and more stable learning as quantified by lower inter-node variance, while requiring fewer communication rounds than baselines and no global knowledge.

</details>


### [436] [An Approximate Ascent Approach To Prove Convergence of PPO](https://arxiv.org/abs/2602.03386)
*Leif Doering,Daniel Schmidt,Moritz Melcher,Sebastian Kassing,Benedikt Wille,Tilman Aach,Simon Weissmann*

Main category: cs.LG

TL;DR: PPO's theoretical foundations are incomplete, but this paper interprets its policy update as approximated policy gradient ascent, controls surrogate gradient bias, and identifies an issue in truncated GAE.


<details>
  <summary>Details</summary>
Motivation: To address the incomplete theoretical understanding of PPO's advantages and convergence, and to identify overlooked issues in its implementation.

Method: Interpret PPO's policy update scheme as approximated policy gradient ascent, control surrogate gradient bias, and analyze truncated GAE's geometric weighting.

Result: A convergence theorem for PPO sheds light on its success. A simple weight correction in GAE improves performance in environments with strong terminal signals.

Conclusion: The paper advances PPO's theoretical understanding and offers practical improvements for its implementation.

Abstract: Proximal Policy Optimization (PPO) is among the most widely used deep reinforcement learning algorithms, yet its theoretical foundations remain incomplete. Most importantly, convergence and understanding of fundamental PPO advantages remain widely open. Under standard theory assumptions we show how PPO's policy update scheme (performing multiple epochs of minibatch updates on multi-use rollouts with a surrogate gradient) can be interpreted as approximated policy gradient ascent. We show how to control the bias accumulated by the surrogate gradients and use techniques from random reshuffling to prove a convergence theorem for PPO that sheds light on PPO's success. Additionally, we identify a previously overlooked issue in truncated Generalized Advantage Estimation commonly used in PPO. The geometric weighting scheme induces infinite mass collapse onto the longest $k$-step advantage estimator at episode boundaries. Empirical evaluations show that a simple weight correction can yield substantial improvements in environments with strong terminal signal, such as Lunar Lander.

</details>


### [437] [Chain-of-Goals Hierarchical Policy for Long-Horizon Offline Goal-Conditioned RL](https://arxiv.org/abs/2602.03389)
*Jinwoo Choi,Sang-Hyun Lee,Seung-Woo Seo*

Main category: cs.LG

TL;DR: CoGHP is a hierarchical reinforcement learning framework that autoregressively generates latent subgoals and actions for long-horizon tasks, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of coordinating multiple intermediate decisions in offline goal-conditioned reinforcement learning for complex tasks.

Method: Proposes Chain-of-Goals Hierarchical Policy (CoGHP), using autoregressive sequence modeling and an MLP-Mixer backbone to generate latent subgoals and actions.

Result: CoGHP consistently outperforms strong offline baselines in navigation and manipulation benchmarks.

Conclusion: CoGHP effectively handles long-horizon tasks by unifying hierarchical decision-making and leveraging autoregressive modeling.

Abstract: Offline goal-conditioned reinforcement learning remains challenging for long-horizon tasks. While hierarchical approaches mitigate this issue by decomposing tasks, most existing methods rely on separate high- and low-level networks and generate only a single intermediate subgoal, making them inadequate for complex tasks that require coordinating multiple intermediate decisions. To address this limitation, we draw inspiration from the chain-of-thought paradigm and propose the Chain-of-Goals Hierarchical Policy (CoGHP), a novel framework that reformulates hierarchical decision-making as autoregressive sequence modeling within a unified architecture. Given a state and a final goal, CoGHP autoregressively generates a sequence of latent subgoals followed by the primitive action, where each latent subgoal acts as a reasoning step that conditions subsequent predictions. To implement this efficiently, we pioneer the use of an MLP-Mixer backbone, which supports cross-token communication and captures structural relationships among state, goal, latent subgoals, and action. Across challenging navigation and manipulation benchmarks, CoGHP consistently outperforms strong offline baselines, demonstrating improved performance on long-horizon tasks.

</details>


### [438] [On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models](https://arxiv.org/abs/2602.03392)
*Shumin Wang,Yuexiang Xie,Wenhao Zhang,Yuchang Sun,Yanxi Chen,Yaliang Li,Yanyong Zhang*

Main category: cs.LG

TL;DR: The paper develops a theoretical framework for analyzing entropy dynamics in reinforcement fine-tuning (RFT) of LLMs, offering insights and practical methods for optimizing exploration-exploitation balance.


<details>
  <summary>Details</summary>
Motivation: To address the lack of principled understanding of entropy dynamics during RFT, aiming to improve the balance between exploration and exploitation in LLM fine-tuning.

Method: Establishes a theoretical framework starting with a discriminant expression for entropy change, leading to a first-order expression and extending to GRPO. Empirical evidence supports the conclusions.

Result: Provides insights into entropy control and offers a unified interpretation of entropy-based methods. Demonstrates effectiveness of derived entropy-discriminator clipping methods.

Conclusion: The study enhances understanding of RFT training dynamics, offering theoretical support and practical strategies for optimizing LLM fine-tuning.

Abstract: Entropy serves as a critical metric for measuring the diversity of outputs generated by large language models (LLMs), providing valuable insights into their exploration capabilities. While recent studies increasingly focus on monitoring and adjusting entropy to better balance exploration and exploitation in reinforcement fine-tuning (RFT), a principled understanding of entropy dynamics during this process is yet to be thoroughly investigated. In this paper, we establish a theoretical framework for analyzing the entropy dynamics during the RFT process, which begins with a discriminant expression that quantifies entropy change under a single logit update. This foundation enables the derivation of a first-order expression for entropy change, which can be further extended to the update formula of Group Relative Policy Optimization (GRPO). The corollaries and insights drawn from the theoretical analysis inspire the design of entropy control methods, and also offer a unified lens for interpreting various entropy-based methods in existing studies. We provide empirical evidence to support the main conclusions of our analysis and demonstrate the effectiveness of the derived entropy-discriminator clipping methods. This study yields novel insights into RFT training dynamics, providing theoretical support and practical strategies for optimizing the exploration-exploitation balance during LLM fine-tuning.

</details>


### [439] [The Label Horizon Paradox: Rethinking Supervision Targets in Financial Forecasting](https://arxiv.org/abs/2602.03395)
*Chen-Hui Song,Shuoling Liu,Liyuan Chen*

Main category: cs.LG

TL;DR: The paper questions the traditional assumption that training labels must match inference targets in deep learning for financial forecasting, introducing the 'Label Horizon Paradox' and proposing a bi-level optimization framework for better results.


<details>
  <summary>Details</summary>
Motivation: To scrutinize the design of supervision signals in financial forecasting, challenging the assumption that labels must mirror inference targets.

Method: Introduces a bi-level optimization framework to autonomously identify optimal proxy labels.

Result: Demonstrates consistent improvements over conventional baselines on large-scale financial datasets.

Conclusion: Opens new avenues for label-centric research in financial forecasting by uncovering the Label Horizon Paradox.

Abstract: While deep learning has revolutionized financial forecasting through sophisticated architectures, the design of the supervision signal itself is rarely scrutinized. We challenge the canonical assumption that training labels must strictly mirror inference targets, uncovering the Label Horizon Paradox: the optimal supervision signal often deviates from the prediction goal, shifting across intermediate horizons governed by market dynamics. We theoretically ground this phenomenon in a dynamic signal-noise trade-off, demonstrating that generalization hinges on the competition between marginal signal realization and noise accumulation. To operationalize this insight, we propose a bi-level optimization framework that autonomously identifies the optimal proxy label within a single training run. Extensive experiments on large-scale financial datasets demonstrate consistent improvements over conventional baselines, thereby opening new avenues for label-centric research in financial forecasting.

</details>


### [440] [Most Convolutional Networks Suffer from Small Adversarial Perturbations](https://arxiv.org/abs/2602.03415)
*Amit Daniely,Idan Mehalel*

Main category: cs.LG

TL;DR: Adversarial examples in random CNNs can be found very close to the input (order $‖x‖/√d$) using a single gradient descent step, extending prior work.


<details>
  <summary>Details</summary>
Motivation: To understand the minimal distance for adversarial examples in CNNs, improving upon non-optimal bounds in existing literature.

Method: Uses Fourier decomposition to bound singular values of random linear convolutional operators, enabling gradient descent-based adversarial example generation.

Result: Shows adversarial examples exist at $ℓ2$-distance $‖x‖/√d$, nearly optimal, and achievable with one gradient step.

Conclusion: Random CNNs are vulnerable to adversarial examples at minimal distances, and efficient methods like gradient descent can exploit this.

Abstract: The existence of adversarial examples is relatively understood for random fully connected neural networks, but much less so for convolutional neural networks (CNNs). The recent work [Daniely, 2025] establishes that adversarial examples can be found in CNNs, in some non-optimal distance from the input. We extend over this work and prove that adversarial examples in random CNNs with input dimension $d$ can be found already in $\ell_2$-distance of order $\lVert x \rVert /\sqrt{d}$ from the input $x$, which is essentially the nearest possible. We also show that such adversarial small perturbations can be found using a single step of gradient descent. To derive our results we use Fourier decomposition to efficiently bound the singular values of a random linear convolutional operator, which is the main ingredient of a CNN layer. This bound might be of independent interest.

</details>


### [441] [Beyond Variance: Prompt-Efficient RLVR via Rare-Event Amplification and Bidirectional Pairing](https://arxiv.org/abs/2602.03452)
*Xin Sheng,Jiaxin Li,Yujuan Pang,Ran Peng,Yong Ma*

Main category: cs.LG

TL;DR: RLVR is improved by selecting prompts that provide reliable positive anchors and explicit negative signals, enhancing sample efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Prior prompt selection methods based on training-accuracy variance led to unstable optimization and weak transfer. The study aims to improve RLVR by addressing these limitations.

Method: Proposes positive-negative pairing, sampling hard-but-solvable and easy-but-brittle prompts to balance learning signals. Introduces Weighted GRPO to amplify rare successes and penalize failures.

Result: Significant performance improvements: AIME~2025 Pass@8 increased from 16.8 to 22.2, AMC23 Pass@64 from 94.0 to 97.0, while staying competitive with large-scale RLVR.

Conclusion: The bidirectional signal approach improves RLVR's effectiveness and sample efficiency without suppressing exploration.

Abstract: Reinforcement learning with verifiable rewards (RLVR) is effective for training large language models on deterministic outcome reasoning tasks. Prior work shows RLVR works with few prompts, but prompt selection is often based only on training-accuracy variance, leading to unstable optimization directions and weaker transfer. We revisit prompt selection from a mechanism-level view and argue that an effective minibatch should provide both (i) a reliable positive anchor and (ii) explicit negative learning signals from rare failures. Based on this principle, we propose \emph{positive--negative pairing}: at each update, we sample a hard-but-solvable $q^{+}$ and an easy-but-brittle prompt $q^{-}$(high success rate but not perfect), characterized by low and high empirical success rates under multiple rollouts. We further introduce Weighted GRPO, which reweights binary outcomes at the pair level and uses group-normalized advantages to amplify rare successes on $q^{+}$ into sharp positive guidance while turning rare failures on $q^{-}$ into strong negative penalties. This bidirectional signal provides informative learning feedback for both successes and failures, improving sample efficiency without suppressing exploration. On Qwen2.5-Math-7B, a single paired minibatch per update consistently outperforms a GRPO baseline that selects two prompts via commonly used variance-based selection heuristics: AIME~2025 Pass@8 improves from 16.8 to 22.2, and AMC23 Pass@64 from 94.0 to 97.0, while remaining competitive with large-scale RLVR trained from a pool of 1209 training prompts. Similar gains are observed on Qwen2.5-Math-7B-Instruct.

</details>


### [442] [Causal Inference on Networks under Misspecified Exposure Mappings: A Partial Identification Framework](https://arxiv.org/abs/2602.03459)
*Maresa Schröder,Miruna Oprescu,Stefan Feuerriegel,Nathan Kallus*

Main category: cs.LG

TL;DR: A novel partial identification framework assesses robustness of treatment effects in networks under misspecified exposure mappings, providing sharp bounds for direct and spillover effects.


<details>
  <summary>Details</summary>
Motivation: Existing methods for estimating treatment effects in networks rely on exposure mappings, which can introduce bias if misspecified. This work aims to address this limitation by assessing robustness under misspecification.

Method: The authors propose a partial identification framework to derive sharp upper and lower bounds for direct and spillover effects under exposure mapping misspecifications. They apply this framework to three canonical exposure settings and develop orthogonal estimators for these bounds.

Result: The framework produces valid, sharp, and efficient bound estimates, which remain informative and reliable even when exposure mappings are misspecified. Experiments confirm its effectiveness.

Conclusion: The proposed framework enhances causal inference robustness in networks by bounding treatment effects under misspecified exposure mappings, offering reliable conclusions.

Abstract: Estimating treatment effects in networks is challenging, as each potential outcome depends on the treatments of all other nodes in the network. To overcome this difficulty, existing methods typically impose an exposure mapping that compresses the treatment assignments in the network into a low-dimensional summary. However, if this mapping is misspecified, standard estimators for direct and spillover effects can be severely biased. We propose a novel partial identification framework for causal inference on networks to assess the robustness of treatment effects under misspecifications of the exposure mapping. Specifically, we derive sharp upper and lower bounds on direct and spillover effects under such misspecifications. As such, our framework presents a novel application of causal sensitivity analysis to exposure mappings. We instantiate our framework for three canonical exposure settings widely used in practice: (i) weighted means of the neighborhood treatments, (ii) threshold-based exposure mappings, and (iii) truncated neighborhood interference in the presence of higher-order spillovers. Furthermore, we develop orthogonal estimators for these bounds and prove that the resulting bound estimates are valid, sharp, and efficient. Our experiments show the bounds remain informative and provide reliable conclusions under misspecification of exposure mappings.

</details>


### [443] [Soft-Radial Projection for Constrained End-to-End Learning](https://arxiv.org/abs/2602.03461)
*Philipp J. Schneider,Daniel Kuhn*

Main category: cs.LG

TL;DR: Soft-Radial Projection is introduced to avoid gradient saturation in deep learning with hard constraints, ensuring feasibility and better optimization.


<details>
  <summary>Details</summary>
Motivation: Addressing gradient saturation issues in existing constructive layers for deep learning in safety-critical systems.

Method: Proposes Soft-Radial Projection, a differentiable layer using radial mapping to maintain feasibility and full-rank Jacobians.

Result: Theoretical proof of universal approximation and empirical improvements in convergence and solution quality.

Conclusion: Soft-Radial Projection overcomes limitations of boundary-based methods, enhancing optimization in constrained deep learning.

Abstract: Integrating hard constraints into deep learning is essential for safety-critical systems. Yet existing constructive layers that project predictions onto constraint boundaries face a fundamental bottleneck: gradient saturation. By collapsing exterior points onto lower-dimensional surfaces, standard orthogonal projections induce rank-deficient Jacobians, which nullify gradients orthogonal to active constraints and hinder optimization. We introduce Soft-Radial Projection, a differentiable reparameterization layer that circumvents this issue through a radial mapping from Euclidean space into the interior of the feasible set. This construction guarantees strict feasibility while preserving a full-rank Jacobian almost everywhere, thereby preventing the optimization stalls typical of boundary-based methods. We theoretically prove that the architecture retains the universal approximation property and empirically show improved convergence behavior and solution quality over state-of-the-art optimization- and projection-based baselines.

</details>


### [444] [ScDiVa: Masked Discrete Diffusion for Joint Modeling of Single-Cell Identity and Expression](https://arxiv.org/abs/2602.03477)
*Mingxuan Wang,Cheng Chen,Gaoyang Jiang,Zijia Ren,Chuangxin Zhao,Lu Shi,Yanbiao Ma*

Main category: cs.LG

TL;DR: ScDiVa is a masked discrete diffusion model for single-cell RNA-seq data, addressing issues like artificial ordering bias and error accumulation in autoregressive methods by using a bidirectional denoiser and entropy-normalized serialization.


<details>
  <summary>Details</summary>
Motivation: Autoregressive generation imposes artificial ordering bias and suffers from error accumulation in high-dimensional, sparse, and unordered single-cell RNA-seq data.

Method: ScDiVa employs a continuous-time forward masking mechanism, bidirectional denoiser, entropy-normalized serialization, and latent anchor token. It's trained via depth-invariant time sampling and a dual denoising objective.

Result: Pre-trained on 59 million cells, ScDiVa performs well in benchmarks like batch integration, cell type annotation, and perturbation response prediction.

Conclusion: Masked discrete diffusion is a biologically coherent and effective alternative to autoregression for single-cell RNA-seq data.

Abstract: Single-cell RNA-seq profiles are high-dimensional, sparse, and unordered, causing autoregressive generation to impose an artificial ordering bias and suffer from error accumulation. To address this, we propose scDiVa, a masked discrete diffusion foundation model that aligns generation with the dropout-like corruption process by defining a continuous-time forward masking mechanism in token space. ScDiVa features a bidirectional denoiser that jointly models discrete gene identities and continuous values, utilizing entropy-normalized serialization and a latent anchor token to maximize information efficiency and preserve global cell identity. The model is trained via depth-invariant time sampling and a dual denoising objective to simulate varying sparsity levels while ensuring precise recovery of both identity and magnitude. Pre-trained on 59 million cells, scDiVa achieves strong transfer performance across major benchmarks, including batch integration, cell type annotation, and perturbation response prediction. These results suggest that masked discrete diffusion serves as a biologically coherent and effective alternative to autoregression.

</details>


### [445] [DeepDFA: Injecting Temporal Logic in Deep Learning for Sequential Subsymbolic Applications](https://arxiv.org/abs/2602.03486)
*Elena Umili,Francesco Argenziano,Roberto Capobianco*

Main category: cs.LG

TL;DR: DeepDFA integrates temporal logic (DFA/Moore Machines) into neural networks for better sequential task performance, outperforming traditional and neuro-symbolic models.


<details>
  <summary>Details</summary>
Motivation: Bridging symbolic reasoning and subsymbolic learning in sequential domains remains challenging.

Method: DeepDFA converts temporal rules into differentiable layers, tested in image sequence classification and policy learning.

Result: DeepDFA achieves state-of-the-art results in temporal knowledge integration, surpassing LSTMs, GRUs, and neuro-symbolic systems.

Conclusion: DeepDFA effectively combines symbolic and subsymbolic learning, showing promise for sequential tasks.

Abstract: Integrating logical knowledge into deep neural network training is still a hard challenge, especially for sequential or temporally extended domains involving subsymbolic observations. To address this problem, we propose DeepDFA, a neurosymbolic framework that integrates high-level temporal logic - expressed as Deterministic Finite Automata (DFA) or Moore Machines - into neural architectures. DeepDFA models temporal rules as continuous, differentiable layers, enabling symbolic knowledge injection into subsymbolic domains. We demonstrate how DeepDFA can be used in two key settings: (i) static image sequence classification, and (ii) policy learning in interactive non-Markovian environments. Across extensive experiments, DeepDFA outperforms traditional deep learning models (e.g., LSTMs, GRUs, Transformers) and novel neuro-symbolic systems, achieving state-of-the-art results in temporal knowledge integration. These results highlight the potential of DeepDFA to bridge subsymbolic learning and symbolic reasoning in sequential tasks.

</details>


### [446] [A Minimal Task Reveals Emergent Path Integration and Object-Location Binding in a Predictive Sequence Model](https://arxiv.org/abs/2602.03490)
*Linda Ariel Ventura,Victoria Bosch,Tim C Kietzmann,Sushrut Thorat*

Main category: cs.LG

TL;DR: A study explores how recurrent neural networks learn structured world models through sequential prediction, revealing mechanisms like path integration and dynamic binding.


<details>
  <summary>Details</summary>
Motivation: The paper aims to understand how predictive neural networks form internal models of objects and their relations, a key aspect of adaptive cognition.

Method: A recurrent neural network is trained to predict upcoming tokens in 2D continuous scenes using current input and saccade-like displacements.

Result: Prediction accuracy improves on novel scenes, with decoding showing path integration and dynamic binding of token identity to position. Interventional tests confirm flexible binding.

Conclusion: Sequential prediction enables the emergence of structured representations with flexible binding, providing insights into world modeling mechanisms relevant to cognition.

Abstract: Adaptive cognition requires structured internal models representing objects and their relations. Predictive neural networks are often proposed to form such "world models", yet their underlying mechanisms remain unclear. One hypothesis is that action-conditioned sequential prediction suffices for learning such world models. In this work, we investigate this possibility in a minimal in-silico setting. Sequentially sampling tokens from 2D continuous token scenes, a recurrent neural network is trained to predict the upcoming token from current input and a saccade-like displacement. On novel scenes, prediction accuracy improves across the sequence, indicating in-context learning. Decoding analyses reveal path integration and dynamic binding of token identity to position. Interventional analyses show that new bindings can be learned late in sequence and that out-of-distribution bindings can be learned. Together, these results demonstrate how structured representations that rely on flexible binding emerge to support prediction, offering a mechanistic account of sequential world modeling relevant to cognitive science.

</details>


### [447] [Least but not Last: Fine-tuning Intermediate Principal Components for Better Performance-Forgetting Trade-Offs](https://arxiv.org/abs/2602.03493)
*Alessio Quercia,Arya Bangun,Ira Assent,Hanno Scharr*

Main category: cs.LG

TL;DR: The paper analyzes trade-offs in Low-Rank Adaptation (LoRA) methods, proposing an improved initialization approach using intermediate components for better performance-forgetting balance.


<details>
  <summary>Details</summary>
Motivation: Address the inconsistent performance-forgetting trade-offs in existing LoRA methods for adapting large pre-trained models.

Method: Analyze LoRA using principal components as initialization, focusing on intermediate components for robustness.

Result: Intermediate components outperform first/last components, improving accuracy and reducing forgetting across tasks.

Conclusion: The proposed LoRA initialization method offers superior trade-offs, validated empirically in vision and NLP tasks.

Abstract: Low-Rank Adaptation (LoRA) methods have emerged as crucial techniques for adapting large pre-trained models to downstream tasks under computational and memory constraints. However, they face a fundamental challenge in balancing task-specific performance gains against catastrophic forgetting of pre-trained knowledge, where existing methods provide inconsistent recommendations. This paper presents a comprehensive analysis of the performance-forgetting trade-offs inherent in low-rank adaptation using principal components as initialization. Our investigation reveals that fine-tuning intermediate components leads to better balance and show more robustness to high learning rates than first (PiSSA) and last (MiLoRA) components in existing work. Building on these findings, we provide a practical approach for initialization of LoRA that offers superior trade-offs. We demonstrate in a thorough empirical study on a variety of computer vision and NLP tasks that our approach improves accuracy and reduces forgetting, also in continual learning scenarios.

</details>


### [448] [Reparameterization Flow Policy Optimization](https://arxiv.org/abs/2602.03501)
*Hai Zhong,Zhuoran Li,Xun Wang,Longbo Huang*

Main category: cs.LG

TL;DR: RFO introduces flow policies into RPG for model-based RL, avoiding Gaussian policy limits and improving sample efficiency without log-likelihood calculations, achieving superior performance in diverse tasks.


<details>
  <summary>Details</summary>
Motivation: Prior RPG methods were limited to Gaussian policies, missing out on advancements in generative models. Flow policies align with RPG but require stabilization and exploration enhancements.

Method: RFO integrates flow policies with RPG by backpropagating through flow generation and dynamics, adding regularization for stability and exploration, and introducing action chunking.

Result: RFO outperforms baselines, notably achieving nearly double the reward on a soft-body quadruped locomotion task.

Conclusion: RFO effectively combines flow policies with RPG, demonstrating high sample efficiency and performance across diverse tasks.

Abstract: Reparameterization Policy Gradient (RPG) has emerged as a powerful paradigm for model-based reinforcement learning, enabling high sample efficiency by backpropagating gradients through differentiable dynamics. However, prior RPG approaches have been predominantly restricted to Gaussian policies, limiting their performance and failing to leverage recent advances in generative models. In this work, we identify that flow policies, which generate actions via differentiable ODE integration, naturally align with the RPG framework, a connection not established in prior work. However, naively exploiting this synergy proves ineffective, often suffering from training instability and a lack of exploration. We propose Reparameterization Flow Policy Optimization (RFO). RFO computes policy gradients by backpropagating jointly through the flow generation process and system dynamics, unlocking high sample efficiency without requiring intractable log-likelihood calculations. RFO includes two tailored regularization terms for stability and exploration. We also propose a variant of RFO with action chunking. Extensive experiments on diverse locomotion and manipulation tasks, involving both rigid and soft bodies with state or visual inputs, demonstrate the effectiveness of RFO. Notably, on a challenging locomotion task controlling a soft-body quadruped, RFO achieves almost $2\times$ the reward of the state-of-the-art baseline.

</details>


### [449] [Explaining the Explainer: Understanding the Inner Workings of Transformer-based Symbolic Regression Models](https://arxiv.org/abs/2602.03506)
*Arco van Breda,Erman Acar*

Main category: cs.LG

TL;DR: The paper explores transformer mechanisms in symbolic regression (SR) and introduces PATCHES, an evolutionary algorithm for identifying compact circuits. It analyzes 28 circuits and validates findings using causal evaluation, highlighting mean patching as reliable.


<details>
  <summary>Details</summary>
Motivation: Transformers are effective for SR, but their internal mechanisms remain unclear. Mechanistic interpretability hasn't been applied to SR, prompting this study.

Method: PATCHES, an evolutionary circuit discovery algorithm, is introduced to identify compact and correct SR circuits. A causal evaluation framework assesses circuits.

Result: 28 circuits were isolated and analyzed. Mean patching proved reliable for correct circuits, while logit attribution and probing were less effective.

Conclusion: SR is a promising domain for mechanistic interpretability. PATCHES offers a principled approach for circuit discovery, with mean patching as a reliable tool.

Abstract: Following their success across many domains, transformers have also proven effective for symbolic regression (SR); however, the internal mechanisms underlying their generation of mathematical operators remain largely unexplored. Although mechanistic interpretability has successfully identified circuits in language and vision models, it has not yet been applied to SR. In this article, we introduce PATCHES, an evolutionary circuit discovery algorithm that identifies compact and correct circuits for SR. Using PATCHES, we isolate 28 circuits, providing the first circuit-level characterisation of an SR transformer. We validate these findings through a robust causal evaluation framework based on key notions such as faithfulness, completeness, and minimality. Our analysis shows that mean patching with performance-based evaluation most reliably isolates functionally correct circuits. In contrast, we demonstrate that direct logit attribution and probing classifiers primarily capture correlational features rather than causal ones, limiting their utility for circuit discovery. Overall, these results establish SR as a high-potential application domain for mechanistic interpretability and propose a principled methodology for circuit discovery.

</details>


### [450] [A Function-Space Stability Boundary for Generalization in Interpolating Learning Systems](https://arxiv.org/abs/2602.03514)
*Ronald Katende*

Main category: cs.LG

TL;DR: The paper investigates when algorithmic stability explains good generalization in interpolating learning systems, proposing a contractive propagation condition and stability certificate, while also showing stability isn't always the explanation.


<details>
  <summary>Details</summary>
Motivation: To understand the conditions under which algorithmic stability accounts for generalization in interpolating learning systems.

Method: Models training as a function-space trajectory, measures sensitivity to perturbations, proposes a contractive propagation condition, and derives a stability certificate.

Result: Small certificates imply stability-based generalization, but some interpolating regimes defy this explanation. Experiments show certificate growth predicts generalization differences.

Conclusion: The framework identifies when stability explains generalization and when other mechanisms are needed.

Abstract: Modern learning systems often interpolate training data while still generalizing well, yet it remains unclear when algorithmic stability explains this behavior. We model training as a function-space trajectory and measure sensitivity to single-sample perturbations along this trajectory.
  We propose a contractive propagation condition and a stability certificate obtained by unrolling the resulting recursion. A small certificate implies stability-based generalization, while we also prove that there exist interpolating regimes with small risk where such contractive sensitivity cannot hold, showing that stability is not a universal explanation.
  Experiments confirm that certificate growth predicts generalization differences across optimizers, step sizes, and dataset perturbations. The framework therefore identifies regimes where stability explains generalization and where alternative mechanisms must account for success.

</details>


### [451] [Mitigating Staleness in Asynchronous Pipeline Parallelism via Basis Rotation](https://arxiv.org/abs/2602.03515)
*Hyunji Jung,Sungbin Shin,Namhoon Lee*

Main category: cs.LG

TL;DR: The paper addresses gradient staleness in asynchronous pipeline parallelism, revealing its linear scaling with pipeline depth and proposing basis rotation to mitigate alignment issues, improving convergence and scalability.


<details>
  <summary>Details</summary>
Motivation: To resolve the inefficiency caused by gradient staleness in asynchronous pipeline parallelism, which undermines scalability and performance.

Method: The study investigates the impact of delayed gradients, identifies misalignment in Hessian eigenbasis, and proposes basis rotation to rectify it, supported by theoretical and empirical analysis.

Result: Basis rotation reduces the harmful effects of gradient staleness, achieving a 76.8% faster convergence for a 1B-parameter LLM compared to the baseline.

Conclusion: Basis rotation effectively restores scalable asynchronous training by addressing gradient staleness and alignment issues, enhancing optimization efficiency.

Abstract: Asynchronous pipeline parallelism maximizes hardware utilization by eliminating the pipeline bubbles inherent in synchronous execution, offering a path toward efficient large-scale distributed training. However, this efficiency gain can be compromised by gradient staleness, where the immediate model updates with delayed gradients introduce noise into the optimization process. Crucially, we identify a critical, yet often overlooked, pathology: this delay scales linearly with pipeline depth, fundamentally undermining the very scalability that the method originally intends to provide. In this work, we investigate this inconsistency and bridge the gap by rectifying delayed gradients through basis rotation, restoring scalable asynchronous training while maintaining performance. Specifically, we observe that the deleterious effects of delayed gradients are exacerbated when the Hessian eigenbasis is misaligned with the standard coordinate basis. We demonstrate that this misalignment prevents coordinate-wise adaptive schemes, such as Adam, from effectively leveraging curvature-aware adaptivity. This failure leads to significant oscillations in the optimization trajectory and, consequently, slower convergence. We substantiate these findings through both rigorous theoretical analysis and empirical evaluation. To address this challenge, we propose the use of basis rotation, demonstrating that it effectively mitigates the alignment issue and significantly accelerates convergence in asynchronous settings. For example, our training of a 1B-parameter LLM with basis rotation achieves the same training loss in 76.8% fewer iterations compared to the best-performing asynchronous pipeline parallel training baseline.

</details>


### [452] [Not All Negative Samples Are Equal: LLMs Learn Better from Plausible Reasoning](https://arxiv.org/abs/2602.03516)
*Zixiang Di,Jinyi Han,Shuo Zhang,Ying Liao,Zhi Li,Xiaofeng Ji,Yongqi Wang,Zheming Yang,Ming Gao,Bingdong Li,Jie Wang*

Main category: cs.LG

TL;DR: The paper proposes Plausible Negative Samples (PNS), a method to improve LLM reasoning by synthesizing high-quality negative samples that mimic correct solutions but yield incorrect answers, outperforming other methods by 2.03%.


<details>
  <summary>Details</summary>
Motivation: Existing methods treat all incorrect LLM responses as equally informative, missing the potential of high-quality negative samples for better reasoning training.

Method: PNS uses reverse RL with a composite reward (format compliance, accuracy inversion, reward model assessment, and chain-of-thought evaluation) to generate plausible negative samples.

Result: PNS improves LLM reasoning, showing a 2.03% average gain over RL-trained models across seven benchmarks.

Conclusion: PNS effectively enhances LLM reasoning by leveraging high-quality negative samples, validated as a plug-and-play solution.

Abstract: Learning from negative samples holds great promise for improving Large Language Model (LLM) reasoning capability, yet existing methods treat all incorrect responses as equally informative, overlooking the crucial role of sample quality. To address this, we propose Plausible Negative Samples (PNS), a method that synthesizes high-quality negative samples exhibiting expected format and structural coherence while ultimately yielding incorrect answers. PNS trains a dedicated model via reverse reinforcement learning (RL) guided by a composite reward combining format compliance, accuracy inversion, reward model assessment, and chain-of-thought evaluation, generating responses nearly indistinguishable from correct solutions. We further validate PNS as a plug-and-play data source for preference optimization across three backbone models on seven mathematical reasoning benchmarks. Results demonstrate that PNS consistently outperforms other negative sample synthesis methods, achieving an average improvement of 2.03% over RL-trained models.

</details>


### [453] [Rank-Learner: Orthogonal Ranking of Treatment Effects](https://arxiv.org/abs/2602.03517)
*Henri Arno,Dennis Frauen,Emil Javurek,Thomas Demeester,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: The paper introduces Rank-Learner, a two-stage method for directly ranking individuals by treatment effects without exact estimation, outperforming traditional CATE estimators.


<details>
  <summary>Details</summary>
Motivation: Addresses the gap in directly learning rankings of treatment effects, which is crucial for applications like prioritizing patients or customers, but has been overlooked.

Method: Rank-Learner optimizes a pairwise learning objective to recover the true treatment effect order, avoiding explicit CATE estimation, and is model-agnostic.

Result: Rank-Learner outperforms standard CATE estimators and non-orthogonal ranking methods in experiments, with strong theoretical guarantees.

Conclusion: Provides practitioners with a robust, orthogonal two-stage learner for ranking treatment effects efficiently.

Abstract: Many decision-making problems require ranking individuals by their treatment effects rather than estimating the exact effect magnitudes. Examples include prioritizing patients for preventive care interventions, or ranking customers by the expected incremental impact of an advertisement. Surprisingly, while causal effect estimation has received substantial attention in the literature, the problem of directly learning rankings of treatment effects has largely remained unexplored. In this paper, we introduce Rank-Learner, a novel two-stage learner that directly learns the ranking of treatment effects from observational data. We first show that naive approaches based on precise treatment effect estimation solve a harder problem than necessary for ranking, while our Rank-Learner optimizes a pairwise learning objective that recovers the true treatment effect ordering, without explicit CATE estimation. We further show that our Rank-Learner is Neyman-orthogonal and thus comes with strong theoretical guarantees, including robustness to estimation errors in the nuisance functions. In addition, our Rank-Learner is model-agnostic, and can be instantiated with arbitrary machine learning models (e.g., neural networks). We demonstrate the effectiveness of our method through extensive experiments where Rank-Learner consistently outperforms standard CATE estimators and non-orthogonal ranking methods. Overall, we provide practitioners with a new, orthogonal two-stage learner for ranking individuals by their treatment effects.

</details>


### [454] [Live or Lie: Action-Aware Capsule Multiple Instance Learning for Risk Assessment in Live Streaming Platforms](https://arxiv.org/abs/2602.03520)
*Yiran Qiao,Jing Chen,Xiang Ao,Qiwei Zhong,Yang Liu,Qing He*

Main category: cs.LG

TL;DR: AC-MIL is a novel framework for detecting coordinated malicious behaviors in live streams using action-aware capsules and MIL, outperforming baselines and offering interpretable risk segments.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of detecting sparse, coordinated malicious behaviors in live streams due to weak supervision and concealed activities.

Method: Formulates the task as MIL, using structured user-timeslot capsules as instances, and proposes AC-MIL to model individual and group behaviors through a serial-parallel architecture.

Result: AC-MIL outperforms MIL and sequential baselines on Douyin datasets, achieving state-of-the-art performance and providing interpretable capsule-level insights.

Conclusion: AC-MIL effectively assesses room-level risks in live streams with interpretable evidence, enabling actionable interventions.

Abstract: Live streaming has become a cornerstone of today's internet, enabling massive real-time social interactions. However, it faces severe risks arising from sparse, coordinated malicious behaviors among multiple participants, which are often concealed within normal activities and challenging to detect timely and accurately. In this work, we provide a pioneering study on risk assessment in live streaming rooms, characterized by weak supervision where only room-level labels are available. We formulate the task as a Multiple Instance Learning (MIL) problem, treating each room as a bag and defining structured user-timeslot capsules as instances. These capsules represent subsequences of user actions within specific time windows, encapsulating localized behavioral patterns. Based on this formulation, we propose AC-MIL, an Action-aware Capsule MIL framework that models both individual behaviors and group-level coordination patterns. AC-MIL captures multi-granular semantics and behavioral cues through a serial and parallel architecture that jointly encodes temporal dynamics and cross-user dependencies. These signals are integrated for robust room-level risk prediction, while also offering interpretable evidence at the behavior segment level. Extensive experiments on large-scale industrial datasets from Douyin demonstrate that AC-MIL significantly outperforms MIL and sequential baselines, establishing new state-of-the-art performance in room-level risk assessment for live streaming. Moreover, AC-MIL provides capsule-level interpretability, enabling identification of risky behavior segments as actionable evidence for intervention. The project page is available at: https://qiaoyran.github.io/AC-MIL/.

</details>


### [455] [WARP Logic Neural Networks](https://arxiv.org/abs/2602.03527)
*Lino Gerlach,Thore Gerlach,Liv Våge,Elliott Kauffman,Isobel Ojalvo*

Main category: cs.LG

TL;DR: WARP logic neural networks offer a gradient-based framework for efficient learning of Boolean functions, improving training and scalability.


<details>
  <summary>Details</summary>
Motivation: Existing logic neural networks face high training costs, redundancy, or reliance on approximate gradients, limiting their scalability.

Method: The paper introduces WARP, which uses gradient-based learning of logic blocks, learnable thresholding, residual initialization, and stochastic smoothing.

Result: WARP achieves faster convergence, scales to deeper architectures, and handles higher input arity logic functions better than baselines.

Conclusion: WARP provides a parameter-efficient solution for learning Boolean functions, outperforming prior methods.

Abstract: Fast and efficient AI inference is increasingly important, and recent models that directly learn low-level logic operations have achieved state-of-the-art performance. However, existing logic neural networks incur high training costs, introduce redundancy or rely on approximate gradients, which limits scalability. To overcome these limitations, we introduce WAlsh Relaxation for Probabilistic (WARP) logic neural networks -- a novel gradient-based framework that efficiently learns combinations of hardware-native logic blocks. We show that WARP yields the most parameter-efficient representation for exactly learning Boolean functions and that several prior approaches arise as restricted special cases. Training is improved by introducing learnable thresholding and residual initialization, while we bridge the gap between relaxed training and discrete logic inference through stochastic smoothing. Experiments demonstrate faster convergence than state-of-the-art baselines, while scaling effectively to deeper architectures and logic functions with higher input arity.

</details>


### [456] [Sparse Training of Neural Networks based on Multilevel Mirror Descent](https://arxiv.org/abs/2602.03535)
*Yannick Lunk,Sebastian J. Scott,Leon Bungert*

Main category: cs.LG

TL;DR: A dynamic sparse training algorithm combining Bregman iterations and adaptive freezing achieves high sparsity and accuracy with reduced FLOPs.


<details>
  <summary>Details</summary>
Motivation: To efficiently explore sparse parameter spaces while maintaining sparsity and accuracy in neural networks.

Method: Uses linearized Bregman iterations/mirror descent with alternating static and dynamic sparsity updates, embedded in a multilevel optimization framework.

Result: Produces highly sparse and accurate models, reducing FLOPs from 38% (standard Bregman) to 6% compared to SGD.

Conclusion: The method effectively balances sparsity and performance, offering theoretical and empirical improvements.

Abstract: We introduce a dynamic sparse training algorithm based on linearized Bregman iterations / mirror descent that exploits the naturally incurred sparsity by alternating between periods of static and dynamic sparsity pattern updates. The key idea is to combine sparsity-inducing Bregman iterations with adaptive freezing of the network structure to enable efficient exploration of the sparse parameter space while maintaining sparsity. We provide convergence guaranties by embedding our method in a multilevel optimization framework. Furthermore, we empirically show that our algorithm can produce highly sparse and accurate models on standard benchmarks. We also show that the theoretical number of FLOPs compared to SGD training can be reduced from 38% for standard Bregman iterations to 6% for our method while maintaining test accuracy.

</details>


### [457] [MatGPTQ: Accurate and Efficient Post-Training Matryoshka Quantization](https://arxiv.org/abs/2602.03537)
*Maximilian Kleinegger,Elvir Crnčević,Dan Alistarh*

Main category: cs.LG

TL;DR: MatGPTQ introduces a PTQ pipeline for efficient single-model multi-precision quantization, improving low-bit performance with cross-bit compensation and kernel support.


<details>
  <summary>Details</summary>
Motivation: Addressing MatQuant's reliance on costly QAT and lack of PTQ/open-source support for multi-precision quantization.

Method: Uses PTQ with multi-precision objectives, bit-slicing, and cross-bit error compensation, plus budget-aware bit-width search.

Result: Achieves high-bit accuracy and better low-bit performance, enabling practical multi-precision deployment.

Conclusion: Establishes state-of-the-art PTQ for Matryoshka quantization, making multi-precision single-checkpoint models practical.

Abstract: Matryoshka Quantization (MatQuant) is a recent quantization approach showing that a single integer-quantized model can be served across multiple precisions, by slicing the most significant bits (MSB) at inference time. This enables a single checkpoint to cover a wide range of memory and latency budgets, but renders quantization much more challenging. In particular, the initial MatQuant relies on expensive quantization-aware training (QAT) variants, rather than fast one-shot post training quantization (PTQ), and lacks open-source and kernel support. We address all of these limitations by introducing Post-Training Matryoshka Quantization (MatGPTQ), a new PTQ pipeline that produces a single parent model jointly optimized for multiple target precisions in one-shot, based on a small calibration set. MatGPTQ casts Matryoshka quantization as a multi-precision objective with bit-slicing and cross-bit error compensation, resulting in an algorithm that produces a multi-bit-width, "sliceable" model in a single pass. We also incorporate a new budget-aware search for heterogeneous per-layer bit-witdhs and provide efficient kernels that implement slicing and mixed-precision execution. Across standard LLMs and benchmarks, MatGPTQ preserves high-bit accuracy while substantially improving performance at low-bit-witdh settings. Overall, we establish a new state of the art for Matryoshka-style post-training quantization and make single-checkpoint, multi-precision deployment open and practical. Code is available at https://github.com/IST-DASLab/MatGPTQ.

</details>


### [458] [How to Train Your Resistive Network: Generalized Equilibrium Propagation and Analytical Learning](https://arxiv.org/abs/2602.03546)
*Jonathan Lin,Aman Desai,Frank Barrows,Francesco Caravelli*

Main category: cs.LG

TL;DR: The paper proposes an algorithm to calculate gradients for analog computing systems using Kirchhoff's laws and introduces Generalized Equilibrium Propagation, demonstrating its effectiveness through simulations.


<details>
  <summary>Details</summary>
Motivation: To address the energy inefficiency of digital hardware for machine learning by exploring analog computing alternatives, overcoming locality constraints in training such systems.

Method: Develops an algorithm to compute gradients analytically using Kirchhoff's laws and introduces Generalized Equilibrium Propagation, comparing it with existing methods like Equilibrium Propagation and Coupled Learning.

Result: Numerical simulations show successful training of resistor networks without needing full network replication or readout, and updates to only a subset of resistances maintain performance.

Conclusion: The proposed framework and algorithm enable efficient training of analog computing systems, offering energy savings while adhering to physical constraints.

Abstract: Machine learning is a powerful method of extracting meaning from data; unfortunately, current digital hardware is extremely energy-intensive. There is interest in an alternative analog computing implementation that could match the performance of traditional machine learning while being significantly more energy-efficient. However, it remains unclear how to train such analog computing systems while adhering to locality constraints imposed by the physical (as opposed to digital) nature of these systems. Local learning algorithms such as Equilibrium Propagation and Coupled Learning have been proposed to address this issue. In this paper, we develop an algorithm to exactly calculate gradients using a graph theoretic and analytical framework for Kirchhoff's laws. We also introduce Generalized Equilibrium Propagation, a framework encompassing a broad class of Hebbian learning algorithms, including Coupled Learning and Equilibrium Propagation, and show how our algorithm compares. We demonstrate our algorithm using numerical simulations and show that we can train resistor networks without the need for a replica or readout over all resistors, only at the output layer. We also show that under the analytical gradient approach, it is possible to update only a subset of the resistance values without a strong degradation in performance.

</details>


### [459] [NPCNet: Navigator-Driven Pseudo Text for Deep Clustering of Early Sepsis Phenotyping](https://arxiv.org/abs/2602.03562)
*Pi-Ju Tsai,Charkkri Limbud,Kuan-Fu Chen,Yi-Ju Tseng*

Main category: cs.LG

TL;DR: NPCNet, a novel deep clustering network, identifies four clinically distinct sepsis phenotypes ($α$, $β$, $γ$, $δ$) using temporal EHRs, improving treatment precision.


<details>
  <summary>Details</summary>
Motivation: Sepsis is heterogeneous, and current clustering methods often lack clinical relevance, limiting actionable insights.

Method: NPCNet integrates temporal EHRs with a target navigator to cluster sepsis patients into clinically meaningful phenotypes.

Result: Four phenotypes ($α$, $β$, $γ$, $δ$) were identified with divergent SOFA trajectories, and NPCNet differentiated patients likely to improve ($α$) from those at risk of worsening ($δ$). Early vasopressor administration benefited $α$, $β$, and $δ$ phenotypes.

Conclusion: NPCNet improves sepsis treatment precision by uncovering clinically relevant phenotypes.

Abstract: Sepsis is a heterogeneous syndrome. Identifying clinically distinct phenotypes may enable more precise treatment strategies. In recent years, many researchers have applied clustering algorithms to sepsis patients. However, the clustering process rarely incorporates clinical relevance, potentially limiting to reflect clinically distinct phenotypes. We propose NPCNet, a novel deep clustering network with a target navigator that integrates temporal Electronic Health Records (EHRs) to better align sepsis phenotypes with clinical significance. We identify four sepsis phenotypes ($α$, $β$, $γ$, and $δ$) with divergence in SOFA trajectories. Notably, while $α$ and $δ$ phenotypes both show severe conditions in the early stage, NPCNet effectively differentiates patients who are likely to improve ($α$) from those at risk of deterioration ($δ$). Furthermore, through the treatment effect analysis, we discover that $α$, $β$, and $δ$ phenotypes may benefit from early vasopressor administration. The results show that NPCNet enhances precision treatment strategies by uncovering clinically distinct phenotypes.

</details>


### [460] [CoGenCast: A Coupled Autoregressive-Flow Generative Framework for Time Series Forecasting](https://arxiv.org/abs/2602.03564)
*Yaguo Liu,Mingyue Cheng,Daoyu Wang,Xiaoyu Tao,Qi Liu*

Main category: cs.LG

TL;DR: CoGenCast combines pre-trained LLMs with flow-matching for time series forecasting, outperforming existing baselines.


<details>
  <summary>Details</summary>
Motivation: Current methods for time series forecasting either focus on semantic context (using LLMs) or stochastic dynamics (using diffusion models), but not both. CoGenCast aims to bridge this gap.

Method: The hybrid framework reconfigures pre-trained LLMs into an encoder-decoder backbone with modified attention topology. It integrates a flow-matching mechanism to model temporal dynamics.

Result: CoGenCast demonstrates superior performance on multiple benchmarks and supports multimodal forecasting and cross-domain unified training.

Conclusion: CoGenCast effectively combines the strengths of LLMs and flow-matching, offering a robust solution for time series forecasting.

Abstract: Time series forecasting can be viewed as a generative problem that requires both semantic understanding over contextual conditions and stochastic modeling of continuous temporal dynamics. Existing approaches typically rely on either autoregressive large language models (LLMs) for semantic context modeling or diffusion-like models for continuous probabilistic generation. However, neither method alone can adequately model both aspects simultaneously. In this work, we propose CoGenCast, a hybrid generative framework that couples pre-trained LLMs with flow-matching mechanism for effective time series forecasting. Specifically, we reconfigure pre-trained decoder-only LLMs into a native forecasting encoder-decoder backbone by modifying only the attention topology, enabling bidirectional context encoding and causal representation generation. Building on this, a flow-matching mechanism is further integrated to model temporal evolution, capturing continuous stochastic dynamics conditioned on the autoregressively generated representation. Notably, CoGenCast naturally supports multimodal forecasting and cross-domain unified training. Extensive experiments on multiple benchmarks show that CoGenCast consistently outperforms previous compared baselines. Code is available at https://github.com/liuyaguo/_CoGenCast.

</details>


### [461] [Riemannian Neural Optimal Transport](https://arxiv.org/abs/2602.03566)
*Alessandro Micheli,Yueqi Cao,Anthea Monod,Samir Bhatt*

Main category: cs.LG

TL;DR: The paper introduces Riemannian Neural OT (RNOT) maps to address the curse of dimensionality in neural optimal transport (OT) on manifolds, proving sub-exponential complexity and showing competitive performance.


<details>
  <summary>Details</summary>
Motivation: Existing neural OT methods are limited to Euclidean geometry, and extending them to high-dimensional Riemannian manifolds is challenging due to the curse of dimensionality.

Method: The paper proposes RNOT maps, continuous neural-network parameterizations of OT maps on manifolds, which avoid discretization and incorporate geometric structure.

Result: RNOT maps approximate Riemannian OT maps with sub-exponential complexity, outperforming discretization-based methods in scalability and performance.

Conclusion: RNOT maps provide a scalable and efficient solution for neural OT on manifolds, overcoming the limitations of discretization-based approaches.

Abstract: Computational optimal transport (OT) offers a principled framework for generative modeling. Neural OT methods, which use neural networks to learn an OT map (or potential) from data in an amortized way, can be evaluated out of sample after training, but existing approaches are tailored to Euclidean geometry. Extending neural OT to high-dimensional Riemannian manifolds remains an open challenge. In this paper, we prove that any method for OT on manifolds that produces discrete approximations of transport maps necessarily suffers from the curse of dimensionality: achieving a fixed accuracy requires a number of parameters that grows exponentially with the manifold dimension. Motivated by this limitation, we introduce Riemannian Neural OT (RNOT) maps, which are continuous neural-network parameterizations of OT maps on manifolds that avoid discretization and incorporate geometric structure by construction. Under mild regularity assumptions, we prove that RNOT maps approximate Riemannian OT maps with sub-exponential complexity in the dimension. Experiments on synthetic and real datasets demonstrate improved scalability and competitive performance relative to discretization-based baselines.

</details>


### [462] [EVE: Efficient Verification of Data Erasure through Customized Perturbation in Approximate Unlearning](https://arxiv.org/abs/2602.03567)
*Weiqi Wang,Zhiyi Tian,Chenhan Zhang,Luoyu Chen,Shui Yu*

Main category: cs.LG

TL;DR: The paper proposes EVE, a method to verify machine unlearning without needing initial training involvement, using perturbed data to observe prediction changes.


<details>
  <summary>Details</summary>
Motivation: Current machine unlearning verification methods require backdooring during initial training, which is inefficient and impractical.

Method: EVE perturbs unlearning data to ensure prediction changes, framing perturbation generation as an adversarial optimization problem.

Result: EVE outperforms state-of-the-art methods, offering efficiency and accuracy without initial training involvement.

Conclusion: EVE provides an effective and practical tool for verifying machine unlearning, with released source code.

Abstract: Verifying whether the machine unlearning process has been properly executed is critical but remains underexplored. Some existing approaches propose unlearning verification methods based on backdooring techniques. However, these methods typically require participation in the model's initial training phase to backdoor the model for later verification, which is inefficient and impractical. In this paper, we propose an efficient verification of erasure method (EVE) for verifying machine unlearning without requiring involvement in the model's initial training process. The core idea is to perturb the unlearning data to ensure the model prediction of the specified samples will change before and after unlearning with perturbed data. The unlearning users can leverage the observation of the changes as a verification signal. Specifically, the perturbations are designed with two key objectives: ensuring the unlearning effect and altering the unlearned model's prediction of target samples. We formalize the perturbation generation as an adversarial optimization problem, solving it by aligning the unlearning gradient with the gradient of boundary change for target samples. We conducted extensive experiments, and the results show that EVE can verify machine unlearning without involving the model's initial training process, unlike backdoor-based methods. Moreover, EVE significantly outperforms state-of-the-art unlearning verification methods, offering significant speedup in efficiency while enhancing verification accuracy. The source code of EVE is released at \uline{https://anonymous.4open.science/r/EVE-C143}, providing a novel tool for verification of machine unlearning.

</details>


### [463] [Asymmetric Hierarchical Anchoring for Audio-Visual Joint Representation: Resolving Information Allocation Ambiguity for Robust Cross-Modal Generalization](https://arxiv.org/abs/2602.03570)
*Bixing Wu,Yuhong Zhao,Zongli Ye,Jiachen Lian,Xiangyu Yue,Gopala Anumanchipalli*

Main category: cs.LG

TL;DR: AHA framework improves cross-modal generalization by enforcing directional information allocation and hierarchical anchoring, outperforming symmetric baselines.


<details>
  <summary>Details</summary>
Motivation: Address information allocation ambiguity and semantic-specific leakage in existing symmetric frameworks for cross-modal generalization.

Method: Proposes Asymmetric Hierarchical Anchoring (AHA) using audio RVQ hierarchical representations, GRL-based adversarial decoupler, and Local Sliding Alignment (LSA).

Result: AHA outperforms baselines on AVE and AVVP benchmarks, showing better semantic consistency and disentanglement.

Conclusion: AHA is effective for cross-modal transfer with broader applicability.

Abstract: Audio-visual joint representation learning under Cross-Modal Generalization (CMG) aims to transfer knowledge from a labeled source modality to an unlabeled target modality through a unified discrete representation space. Existing symmetric frameworks often suffer from information allocation ambiguity, where the absence of structural inductive bias leads to semantic-specific leakage across modalities. We propose Asymmetric Hierarchical Anchoring (AHA), which enforces directional information allocation by designating a structured semantic anchor within a shared hierarchy. In our instantiation, we exploit the hierarchical discrete representations induced by audio Residual Vector Quantization (RVQ) to guide video feature distillation into a shared semantic space. To ensure representational purity, we replace fragile mutual information estimators with a GRL-based adversarial decoupler that explicitly suppresses semantic leakage in modality-specific branches, and introduce Local Sliding Alignment (LSA) to encourage fine-grained temporal alignment across modalities. Extensive experiments on AVE and AVVP benchmarks demonstrate that AHA consistently outperforms symmetric baselines in cross-modal transfer. Additional analyses on talking-face disentanglement experiment further validate that the learned representations exhibit improved semantic consistency and disentanglement, indicating the broader applicability of the proposed framework.

</details>


### [464] [Optimization and Generation in Aerodynamics Inverse Design](https://arxiv.org/abs/2602.03582)
*Huaguan Chen,Ning Lin,Luxi Chen,Rui Zhang,Wenbing Huang,Chongxuan Li,Hao Sun*

Main category: cs.LG

TL;DR: The paper tackles inverse design challenges using physics-based objectives, introducing a new training loss, density-gradient optimization, and efficient covariance estimation methods, validated by experiments and simulations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the difficulty of inverse design with physics-based objectives, especially aerodynamic shape optimization, due to high-dimensional geometry and costly simulations.

Method: The authors propose a novel training loss for cost predictors, a density-gradient optimization method, and an efficient algorithm for approximate covariance estimation. They validate their approach through experiments, simulations, and physical tests.

Result: Results demonstrate consistent improvements in optimization and guided generation, validated by 2D and 3D aerodynamic benchmarks, simulations, and physical tests. Offline RL results further affirm the method's generality.

Conclusion: The concluded approach effectively improves objectives while preserving plausible shapes, unifying existing methods and offering efficient solutions for high-dimensional inverse design problems.

Abstract: Inverse design with physics-based objectives is challenging because it couples high-dimensional geometry with expensive simulations, as exemplified by aerodynamic shape optimization for drag reduction. We revisit inverse design through two canonical solutions, the optimal design point and the optimal design distribution, and relate them to optimization and guided generation. Building on this view, we propose a new training loss for cost predictors and a density-gradient optimization method that improves objectives while preserving plausible shapes. We further unify existing training-free guided generation methods. To address their inability to approximate conditional covariance in high dimensions, we develop a time- and memory-efficient algorithm for approximate covariance estimation. Experiments on a controlled 2D study and high-fidelity 3D aerodynamic benchmarks (car and aircraft), validated by OpenFOAM simulations and miniature wind-tunnel tests with 3D-printed prototypes, demonstrate consistent gains in both optimization and guided generation. Additional offline RL results further support the generality of our approach.

</details>


### [465] [APEX: Probing Neural Networks via Activation Perturbation](https://arxiv.org/abs/2602.03586)
*Tao Ren,Xiaoyu Luo,Qiongxiu Li*

Main category: cs.LG

TL;DR: APEX is a probing technique that perturbs hidden activations to analyze neural networks, revealing structural insights beyond traditional input-space methods.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of input-space analysis and parameter perturbation in accessing structural information in neural networks.

Method: Activation Perturbation for EXploration (APEX), which perturbs hidden activations while keeping inputs and parameters fixed.

Result: APEX reveals sample regularity, distinguishes structured from random models, and exposes biases like backdooring effects.

Conclusion: APEX provides a novel and effective way to explore and understand neural networks beyond input-space methods.

Abstract: Prior work on probing neural networks primarily relies on input-space analysis or parameter perturbation, both of which face fundamental limitations in accessing structural information encoded in intermediate representations. We introduce Activation Perturbation for EXploration (APEX), an inference-time probing paradigm that perturbs hidden activations while keeping both inputs and model parameters fixed. We theoretically show that activation perturbation induces a principled transition from sample-dependent to model-dependent behavior by suppressing input-specific signals and amplifying representation-level structure, and further establish that input perturbation corresponds to a constrained special case of this framework. Through representative case studies, we demonstrate the practical advantages of APEX. In the small-noise regime, APEX provides a lightweight and efficient measure of sample regularity that aligns with established metrics, while also distinguishing structured from randomly labeled models and revealing semantically coherent prediction transitions. In the large-noise regime, APEX exposes training-induced model-level biases, including a pronounced concentration of predictions on the target class in backdoored models. Overall, our results show that APEX offers an effective perspective for exploring, and understanding neural networks beyond what is accessible from input space alone.

</details>


### [466] [SAGE-5GC: Security-Aware Guidelines for Evaluating Anomaly Detection in the 5G Core Network](https://arxiv.org/abs/2602.03596)
*Cristian Manca,Christian Scano,Giorgio Piras,Fabio Brau,Maura Pintor,Battista Biggio*

Main category: cs.LG

TL;DR: The paper proposes guidelines for evaluating anomaly detectors in 5G Core networks under realistic adversarial conditions, demonstrating their vulnerability and suggesting improvements.


<details>
  <summary>Details</summary>
Motivation: Existing anomaly detection systems in 5G Core networks rely on unrealistic assumptions (e.g., IID data), ignoring adaptive attackers. This study addresses this gap by focusing on real-world deployment challenges.

Method: The authors introduce SAGE-5GC guidelines, evaluate baseline anomaly detectors, and test their robustness against adversarial attacks using feature manipulation and genetic algorithms.

Result: Adversarially crafted attacks significantly reduce detection performance, highlighting vulnerabilities in current systems.

Conclusion: Robust, security-aware evaluation methods are essential for effective anomaly detection in 5G Core networks facing adaptive threats.

Abstract: Machine learning-based anomaly detection systems are increasingly being adopted in 5G Core networks to monitor complex, high-volume traffic. However, most existing approaches are evaluated under strong assumptions that rarely hold in operational environments, notably the availability of independent and identically distributed (IID) data and the absence of adaptive attackers.In this work, we study the problem of detecting 5G attacks \textit{in the wild}, focusing on realistic deployment settings. We propose a set of Security-Aware Guidelines for Evaluating anomaly detectors in 5G Core Network (SAGE-5GC), driven by domain knowledge and consideration of potential adversarial threats. Using a realistic 5G Core dataset, we first train several anomaly detectors and assess their baseline performance against standard 5GC control-plane cyberattacks targeting PFCP-based network services.We then extend the evaluation to adversarial settings, where an attacker tries to manipulate the observable features of the network traffic to evade detection, under the constraint that the intended functionality of the malicious traffic is preserved. Starting from a selected set of controllable features, we analyze model sensitivity and adversarial robustness through randomized perturbations. Finally, we introduce a practical optimization strategy based on genetic algorithms that operates exclusively on attacker-controllable features and does not require prior knowledge of the underlying detection model. Our experimental results show that adversarially crafted attacks can substantially degrade detection performance, underscoring the need for robust, security-aware evaluation methodologies for anomaly detection in 5G networks deployed in the wild.

</details>


### [467] [Explanations Leak: Membership Inference with Differential Privacy and Active Learning Defense](https://arxiv.org/abs/2602.03611)
*Fatima Ezzeddine,Osama Zammar,Silvia Giordano,Omran Ayoub*

Main category: cs.LG

TL;DR: Counterfactual explanations (CFs) in MLaaS systems enhance transparency but also increase vulnerability to membership inference attacks (MIAs). This paper explores how CFs strengthen MIAs, proposes a defense framework using Differential Privacy and Active Learning, and evaluates the trade-off between privacy, performance, and explanation quality.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand how CFs in MLaaS systems expand the attack surface for MIAs and to develop defenses that maintain utility and explainability while mitigating privacy risks.

Method: The authors analyze how CFs enable shadow-based MIAs and propose a defense framework combining Differential Privacy (DP) and Active Learning (AL) to reduce memorization and limit data exposure.

Result: Extensive empirical evaluation reveals a trade-off between privacy leakage, predictive performance, and explanation quality, emphasizing the need for balanced MLaaS deployment.

Conclusion: The paper underscores the importance of carefully balancing transparency, utility, and privacy in explainable MLaaS systems to mitigate emerging risks without compromising performance.

Abstract: Counterfactual explanations (CFs) are increasingly integrated into Machine Learning as a Service (MLaaS) systems to improve transparency; however, ML models deployed via APIs are already vulnerable to privacy attacks such as membership inference and model extraction, and the impact of explanations on this threat landscape remains insufficiently understood. In this work, we focus on the problem of how CFs expand the attack surface of MLaaS by strengthening membership inference attacks (MIAs), and on the need to design defense mechanisms that mitigate this emerging risk without undermining utility and explainability. First, we systematically analyze how exposing CFs through query-based APIs enables more effective shadow-based MIAs. Second, we propose a defense framework that integrates Differential Privacy (DP) with Active Learning (AL) to jointly reduce memorization and limit effective training data exposure. Finally, we conduct an extensive empirical evaluation to characterize the three-way trade-off between privacy leakage, predictive performance, and explanation quality. Our findings highlight the need to carefully balance transparency, utility, and privacy in the responsible deployment of explainable MLaaS systems.

</details>


### [468] [Quantization-Aware Regularizers for Deep Neural Networks Compression](https://arxiv.org/abs/2602.03614)
*Dario Malchiodi,Mattia Ferraretto,Marco Frasca*

Main category: cs.LG

TL;DR: The paper proposes a method to integrate quantization awareness into neural network training via per-layer regularization, reducing accuracy loss while maintaining compression benefits.


<details>
  <summary>Details</summary>
Motivation: The need for efficient deployment of deep neural networks on resource-constrained devices drives the exploration of quantization-aware training methods to mitigate accuracy drops caused by traditional quantization techniques.

Method: The authors introduce per-layer regularization terms during training to naturally cluster weights, embedding quantization awareness into the optimization process. Quantization representatives are treated as network parameters and updated via backpropagation.

Result: Experiments on CIFAR-10 with AlexNet and VGG16 show the method effectively reduces accuracy loss while preserving compression potential.

Conclusion: The proposed quantization-aware training framework successfully integrates quantization into the learning phase, improving accuracy and maintaining compression benefits.

Abstract: Deep Neural Networks reached state-of-the-art performance across numerous domains, but this progress has come at the cost of increasingly large and over-parameterized models, posing serious challenges for deployment on resource-constrained devices. As a result, model compression has become essential, and -- among compression techniques -- weight quantization is largely used and particularly effective, yet it typically introduces a non-negligible accuracy drop. However, it is usually applied to already trained models, without influencing how the parameter space is explored during the learning phase. In contrast, we introduce per-layer regularization terms that drive weights to naturally form clusters during training, integrating quantization awareness directly into the optimization process. This reduces the accuracy loss typically associated with quantization methods while preserving their compression potential. Furthermore, in our framework quantization representatives become network parameters, marking, to the best of our knowledge, the first approach to embed quantization parameters directly into the backpropagation procedure. Experiments on CIFAR-10 with AlexNet and VGG16 models confirm the effectiveness of the proposed strategy.

</details>


### [469] [Ultra Fast PDE Solving via Physics Guided Few-step Diffusion](https://arxiv.org/abs/2602.03627)
*Cindy Xiangrui Kong,Yueqi Wang,Haoyang Zheng,Weijian Luo,Guang Lin*

Main category: cs.LG

TL;DR: Phys-Instruct is a physics-guided distillation framework that enhances diffusion-based PDE solvers by enabling rapid sampling and improving physical consistency, achieving faster inference and reduced errors.


<details>
  <summary>Details</summary>
Motivation: Addressing the high sampling costs and insufficient physical consistency of diffusion-based PDE solvers by integrating explicit physics constraints and compressing models for efficiency.

Method: Proposes Phys-Instruct, combining diffusion distribution matching for rapid sampling and PDE distillation guidance for physics consistency, supported by a theoretical foundation.

Result: Achieves orders-of-magnitude faster inference and reduces PDE error by over 8 times compared to state-of-the-art diffusion baselines across five benchmarks.

Conclusion: Phys-Instruct is an effective and efficient framework for ultra-fast PDE solving, also serving as a compact prior for downstream tasks.

Abstract: Diffusion-based models have demonstrated impressive accuracy and generalization in solving partial differential equations (PDEs). However, they still face significant limitations, such as high sampling costs and insufficient physical consistency, stemming from their many-step iterative sampling mechanism and lack of explicit physics constraints. To address these issues, we propose Phys-Instruct, a novel physics-guided distillation framework which not only (1) compresses a pre-trained diffusion PDE solver into a few-step generator via matching generator and prior diffusion distributions to enable rapid sampling, but also (2) enhances the physics consistency by explicitly injecting PDE knowledge through a PDE distillation guidance. Physic-Instruct is built upon a solid theoretical foundation, leading to a practical physics-constrained training objective that admits tractable gradients. Across five PDE benchmarks, Phys-Instruct achieves orders-of-magnitude faster inference while reducing PDE error by more than 8 times compared to state-of-the-art diffusion baselines. Moreover, the resulting unconditional student model functions as a compact prior, enabling efficient and physically consistent inference for various downstream conditional tasks. Our results indicate that Phys-Instruct is a novel, effective, and efficient framework for ultra-fast PDE solving powered by deep generative models.

</details>


### [470] [CTTVAE: Latent Space Structuring for Conditional Tabular Data Generation on Imbalanced Datasets](https://arxiv.org/abs/2602.03641)
*Milosh Devic,Jordan Gierschendorf,David Garson*

Main category: cs.LG

TL;DR: CTTVAE+TBS is a new framework for generating synthetic tabular data under severe class imbalance, combining a transformer-based VAE with latent space restructuring and adaptive sampling for better minority class performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of generating useful synthetic data for rare but impactful events in decision-making domains where existing models fail.

Method: Combine a Conditional Transformer-based Tabular VAE (CTTVAE) with a class-aware triplet margin loss for latent space restructuring and a training-by-sampling strategy (TBS) to focus on underrepresented groups.

Result: CTTVAE+TBS outperforms other models in downstream utility for minority classes across six benchmarks, often surpassing original imbalanced data training, while maintaining fidelity and privacy.

Conclusion: CTTVAE+TBS offers a robust solution for conditional tabular data generation, particularly beneficial in industries like healthcare and fraud detection where minority class accuracy is critical.

Abstract: Generating synthetic tabular data under severe class imbalance is essential for domains where rare but high-impact events drive decision-making. However, most generative models either overlook minority groups or fail to produce samples that are useful for downstream learning. We introduce CTTVAE, a Conditional Transformer-based Tabular Variational Autoencoder equipped with two complementary mechanisms: (i) a class-aware triplet margin loss that restructures the latent space for sharper intra-class compactness and inter-class separation, and (ii) a training-by-sampling strategy that adaptively increases exposure to underrepresented groups. Together, these components form CTTVAE+TBS, a framework that consistently yields more representative and utility-aligned samples without destabilizing training. Across six real-world benchmarks, CTTVAE+TBS achieves the strongest downstream utility on minority classes, often surpassing models trained on the original imbalanced data while maintaining competitive fidelity and bridging the gap for privacy for interpolation-based sampling methods and deep generative methods. Ablation studies further confirm that both latent structuring and targeted sampling contribute to these gains. By explicitly prioritizing downstream performance in rare categories, CTTVAE+TBS provides a robust and interpretable solution for conditional tabular data generation, with direct applicability to industries such as healthcare, fraud detection, and predictive maintenance where even small gains in minority cases can be critical.

</details>


### [471] [Reinforcement Fine-Tuning for History-Aware Dense Retriever in RAG](https://arxiv.org/abs/2602.03645)
*Yicheng Zhang,Zhen Qin,Zhaomin Wu,Wenqi Zhang,Shuiguang Deng*

Main category: cs.LG

TL;DR: The paper introduces a reinforcement learning (RL)-based approach to optimize retrieval-augmented generation (RAG) pipelines, addressing key challenges like deterministic retrieval and state aliasing.


<details>
  <summary>Details</summary>
Motivation: Existing retriever optimization methods suffer from objective mismatch with RAG goals, prompting the need for a more effective solution like RL.

Method: The proposed method replaces deterministic retrieval with stochastic sampling, formulates RAG as a Markov decision process, and incorporates retrieval history to mitigate state aliasing.

Result: Experiments show consistent improvements in RAG performance across various pipelines, datasets, and retriever scales.

Conclusion: The RL-based approach effectively enhances RAG performance by addressing fundamental challenges in retriever optimization.

Abstract: Retrieval-augmented generation (RAG) enables large language models (LLMs) to produce evidence-based responses, and its performance hinges on the matching between the retriever and LLMs. Retriever optimization has emerged as an efficient alternative to fine-tuning LLMs. However, existing solutions suffer from objective mismatch between retriever optimization and the goal of RAG pipeline. Reinforcement learning (RL) provides a promising solution to address this limitation, yet applying RL to retriever optimization introduces two fundamental challenges: 1) the deterministic retrieval is incompatible with RL formulations, and 2) state aliasing arises from query-only retrieval in multi-hop reasoning. To address these challenges, we replace deterministic retrieval with stochastic sampling and formulate RAG as a Markov decision process, making retriever optimizable by RL. Further, we incorporate retrieval history into the state at each retrieval step to mitigate state aliasing. Extensive experiments across diverse RAG pipelines, datasets, and retriever scales demonstrate consistent improvements of our approach in RAG performance.

</details>


### [472] [Sequential Group Composition: A Window into the Mechanics of Deep Learning](https://arxiv.org/abs/2602.03655)
*Giovanni Luca Marchetti,Daniel Kunin,Adele Myers,Francisco Acosta,Nina Miolane*

Main category: cs.LG

TL;DR: The paper investigates how neural networks learn structured operations like arithmetic and algorithmic computation, focusing on the sequential group composition task. It analyzes group structure, encoding, and sequence length effects, revealing how network depth and recurrence improve learning efficiency.


<details>
  <summary>Details</summary>
Motivation: Understanding how neural networks acquire the ability to perform structured operations, such as arithmetic or algorithmic tasks, by studying their behavior on the sequential group composition task.

Method: The study introduces the sequential group composition task, where networks predict the cumulative product of sequences from a finite group. It examines the roles of group structure, encoding statistics, and sequence length, comparing shallow and deep architectures.

Result: Two-layer networks learn the task one irreducible representation at a time, requiring exponential width for perfect learning. Deeper models (RNNs and multilayer networks) use associativity to achieve better scaling with logarithmic depth.

Conclusion: The sequential group composition task provides a tractable framework to study how neural networks learn structured operations, highlighting the efficiency gains of deeper architectures.

Abstract: How do neural networks trained over sequences acquire the ability to perform structured operations, such as arithmetic, geometric, and algorithmic computation? To gain insight into this question, we introduce the sequential group composition task. In this task, networks receive a sequence of elements from a finite group encoded in a real vector space and must predict their cumulative product. The task can be order-sensitive and requires a nonlinear architecture to be learned. Our analysis isolates the roles of the group structure, encoding statistics, and sequence length in shaping learning. We prove that two-layer networks learn this task one irreducible representation of the group at a time in an order determined by the Fourier statistics of the encoding. These networks can perfectly learn the task, but doing so requires a hidden width exponential in the sequence length $k$. In contrast, we show how deeper models exploit the associativity of the task to dramatically improve this scaling: recurrent neural networks compose elements sequentially in $k$ steps, while multilayer networks compose adjacent pairs in parallel in $\log k$ layers. Overall, the sequential group composition task offers a tractable window into the mechanics of deep learning.

</details>


### [473] [Equilibrium Propagation for Non-Conservative Systems](https://arxiv.org/abs/2602.03670)
*Antonino Emanuele Scurria,Dimitri Vanden Abeele,Bortolo Matteo Mognetti,Serge Massar*

Main category: cs.LG

TL;DR: The paper extends Equilibrium Propagation (EP) to nonconservative systems, including feedforward networks, introducing a framework that computes the exact gradient of the cost function.


<details>
  <summary>Details</summary>
Motivation: EP's original formulation was limited to conservative systems, but nonconservative systems are crucial in applications, necessitating an extension.

Method: The authors modify the learning-phase dynamics by adding a term proportional to the non-reciprocal part of interactions, ensuring exact gradient computation.

Result: Numerical experiments on MNIST show improved performance and faster learning compared to previous proposals.

Conclusion: The framework successfully generalizes EP to nonconservative systems while retaining its key properties.

Abstract: Equilibrium Propagation (EP) is a physics-inspired learning algorithm that uses stationary states of a dynamical system both for inference and learning. In its original formulation it is limited to conservative systems, $\textit{i.e.}$ to dynamics which derive from an energy function. Given their importance in applications, it is important to extend EP to nonconservative systems, $\textit{i.e.}$ systems with non-reciprocal interactions. Previous attempts to generalize EP to such systems failed to compute the exact gradient of the cost function. Here we propose a framework that extends EP to arbitrary nonconservative systems, including feedforward networks. We keep the key property of equilibrium propagation, namely the use of stationary states both for inference and learning. However, we modify the dynamics in the learning phase by a term proportional to the non-reciprocal part of the interaction so as to obtain the exact gradient of the cost function. This algorithm can also be derived using a variational formulation that generates the learning dynamics through an energy function defined over an augmented state space. Numerical experiments using the MNIST database show that this algorithm achieves better performance and learns faster than previous proposals.

</details>


### [474] [ContraLog: Log File Anomaly Detection with Contrastive Learning and Masked Language Modeling](https://arxiv.org/abs/2602.03678)
*Simon Dietz,Kai Klede,An Nguyen,Bjoern M Eskofier*

Main category: cs.LG

TL;DR: ContraLog proposes a parser-free, self-supervised method for log anomaly detection by predicting continuous message embeddings instead of discrete templates, outperforming benchmarks on diverse datasets.


<details>
  <summary>Details</summary>
Motivation: Existing log anomaly detection methods rely on parsers that lose valuable variable and semantic content, limiting effectiveness.

Method: ContraLog uses a message encoder for embeddings and a sequence encoder for temporal dependencies, trained via masked language modeling and contrastive learning.

Result: Experiments show ContraLog's effectiveness on complex datasets (HDFS, BGL, Thunderbird), with embeddings predictive of anomalies even without sequence context.

Conclusion: Embedding-level prediction is a promising approach for log anomaly detection, potentially applicable to other event sequences.

Abstract: Log files record computational events that reflect system state and behavior, making them a primary source of operational insights in modern computer systems. Automated anomaly detection on logs is therefore critical, yet most established methods rely on log parsers that collapse messages into discrete templates, discarding variable values and semantic content. We propose ContraLog, a parser-free and self-supervised method that reframes log anomaly detection as predicting continuous message embeddings rather than discrete template IDs. ContraLog combines a message encoder that produces rich embeddings for individual log messages with a sequence encoder to model temporal dependencies within sequences. The model is trained with a combination of masked language modeling and contrastive learning to predict masked message embeddings based on the surrounding context. Experiments on the HDFS, BGL, and Thunderbird benchmark datasets empirically demonstrate effectiveness on complex datasets with diverse log messages. Additionally, we find that message embeddings generated by ContraLog carry meaningful information and are predictive of anomalies even without sequence context. These results highlight embedding-level prediction as an approach for log anomaly detection, with potential applicability to other event sequences.

</details>


### [475] [Universal One-third Time Scaling in Learning Peaked Distributions](https://arxiv.org/abs/2602.03685)
*Yizhou Liu,Ziming Liu,Cengiz Pehlevan,Jeff Gore*

Main category: cs.LG

TL;DR: The slow power-law convergence in training LLMs stems from softmax and cross-entropy, causing vanishing gradients and a universal $1/3$ exponent in loss scaling.


<details>
  <summary>Details</summary>
Motivation: To understand the origin of slow power-law convergence in LLM training, which is computationally expensive.

Method: Analyzed toy models and empirical evaluations of LLMs to identify the role of softmax and cross-entropy in peaked probability distributions.

Result: Found that softmax and cross-entropy create vanishing losses/gradients, leading to power-law loss scaling with a universal $1/3$ exponent.

Conclusion: Provides a mechanistic explanation for neural scaling and suggests ways to improve LLM training efficiency.

Abstract: Training large language models (LLMs) is computationally expensive, partly because the loss exhibits slow power-law convergence whose origin remains debatable. Through systematic analysis of toy models and empirical evaluation of LLMs, we show that this behavior can arise intrinsically from the use of softmax and cross-entropy. When learning peaked probability distributions, e.g., next-token distributions, these components yield power-law vanishing losses and gradients, creating a fundamental optimization bottleneck. This ultimately leads to power-law time scaling of the loss with a universal exponent of $1/3$. Our results provide a mechanistic explanation for observed neural scaling and suggest new directions for improving LLM training efficiency.

</details>


### [476] [QuAIL: Quality-Aware Inertial Learning for Robust Training under Data Corruption](https://arxiv.org/abs/2602.03686)
*Mattia Sabella,Alberto Archetti,Pietro Pinoli,Matteo Matteucci,Cinzia Cappiello*

Main category: cs.LG

TL;DR: QuAIL is a quality-informed training mechanism that uses feature reliability priors to improve model performance under non-uniform corruption in tabular data.


<details>
  <summary>Details</summary>
Motivation: Tabular data often suffers from non-uniform corruption (noise, missing entries, biases), but existing robustness techniques rely on instance-wise quality annotations, which are rarely available.

Method: QuAIL introduces a learnable feature-modulation layer constrained by a quality-dependent proximal regularizer, enabling controlled adaptation to feature trustworthiness without explicit data repair.

Result: QuAIL improves performance over neural baselines across 50 datasets under random and value-dependent corruption, especially in low-data and biased settings.

Conclusion: Incorporating feature reliability into optimization is practical and effective for resilient tabular learning.

Abstract: Tabular machine learning systems are frequently trained on data affected by non-uniform corruption, including noisy measurements, missing entries, and feature-specific biases. In practice, these defects are often documented only through column-level reliability indicators rather than instance-wise quality annotations, limiting the applicability of many robustness and cleaning techniques. We present QuAIL, a quality-informed training mechanism that incorporates feature reliability priors directly into the learning process. QuAIL augments existing models with a learnable feature-modulation layer whose updates are selectively constrained by a quality-dependent proximal regularizer, thereby inducing controlled adaptation across features of varying trustworthiness. This stabilizes optimization under structured corruption without explicit data repair or sample-level reweighting. Empirical evaluation across 50 classification and regression datasets demonstrates that QuAIL consistently improves average performance over neural baselines under both random and value-dependent corruption, with especially robust behavior in low-data and systematically biased settings. These results suggest that incorporating feature reliability information directly into optimization dynamics is a practical and effective approach for resilient tabular learning.

</details>


### [477] [LLM-Inspired Pretrain-Then-Finetune for Small-Data, Large-Scale Optimization](https://arxiv.org/abs/2602.03690)
*Zishi Zhang,Jinhui Han,Ming Hu,Yijie Peng*

Main category: cs.LG

TL;DR: The paper proposes a pretrain-then-finetune approach using a custom Transformer model to tackle small-data, large-scale decision problems, combining synthetic pretraining with real-data finetuning for improved performance and error guarantees.


<details>
  <summary>Details</summary>
Motivation: Address operational decision-making challenges where firms face many decisions but limited noisy data per instance, leveraging LLM-inspired techniques.

Method: Pretrain a designed Transformer on domain-informed synthetic data, then finetune on real observations, using tailored architecture and training.

Result: Theoretical error analysis validates effectiveness, showing pretraining and finetuning's joint impact, with finetuning benefiting from economies of scale.

Conclusion: The approach successfully combines synthetic pretraining and real-data finetuning, offering scalable and effective solutions for small-data, large-scale problems.

Abstract: We consider small-data, large-scale decision problems in which a firm must make many operational decisions simultaneously (e.g., across a large product portfolio) while observing only a few, potentially noisy, data points per instance. Inspired by the success of large language models (LLMs), we propose a pretrain-then-finetune approach built on a designed Transformer model to address this challenge. The model is first pretrained on large-scale, domain-informed synthetic data that encode managerial knowledge and structural features of the decision environment, and is then fine-tuned on real observations. This new pipeline offers two complementary advantages: pretraining injects domain knowledge into the learning process and enables the training of high-capacity models using abundant synthetic data, while finetuning adapts the pretrained model to the operational environment and improves alignment with the true data-generating regime. While we have leveraged the Transformer's state-of-the-art representational capacity, particularly its attention mechanism, to efficiently extract cross-task structure, our approach is not an off-the-shelf application. Instead, it relies on problem-specific architectural design and a tailored training procedure to match the decision setting. Theoretically, we develop the first comprehensive error analysis regarding Transformer learning in relevant contexts, establishing nonasymptotic guarantees that validate the method's effectiveness. Critically, our analysis reveals how pretraining and fine-tuning jointly determine performance, with the dominant contribution governed by whichever is more favorable. In particular, finetuning exhibits an economies-of-scale effect, whereby transfer learning becomes increasingly effective as the number of instances grows.

</details>


### [478] [Data-Driven Graph Filters via Adaptive Spectral Shaping](https://arxiv.org/abs/2602.03698)
*Dylan Sandfelder,Mihai Cucuringu,Xiaowen Dong*

Main category: cs.LG

TL;DR: Adaptive Spectral Shaping (ASS) is a data-driven graph filtering framework using reusable kernels modulated by Gaussian factors. It scales via Chebyshev polynomials and offers interpretability. TASS enables few-shot transfer learning by fixing the kernel and adapting shaping parameters. ASS outperforms fixed-prototype wavelets and learned linear banks in synthetic benchmarks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop scalable, interpretable, and transferable spectral graph filters for heterogeneous graph signal processing and graph neural networks, addressing limitations of fixed-prototype wavelets and learned linear banks.

Method: ASS learns a reusable baseline spectral kernel modulated by Gaussian factors, implemented with Chebyshev polynomials for scalability. TASS transfers the baseline kernel from source to target graphs, adapting only shaping parameters for few-shot learning.

Result: ASS reduces reconstruction error compared to fixed-prototype wavelets and learned linear banks in synthetic benchmarks. TASS achieves consistent positive transfer across graph families and signal regimes.

Conclusion: ASS and TASS provide scalable, interpretable, and transferable spectral modules suitable for graph signal processing and neural networks, enhancing cross-graph generalization.

Abstract: We introduce Adaptive Spectral Shaping, a data-driven framework for graph filtering that learns a reusable baseline spectral kernel and modulates it with a small set of Gaussian factors. The resulting multi-peak, multi-scale responses allocate energy to heterogeneous regions of the Laplacian spectrum while remaining interpretable via explicit centers and bandwidths. To scale, we implement filters with Chebyshev polynomial expansions, avoiding eigendecompositions. We further propose Transferable Adaptive Spectral Shaping (TASS): the baseline kernel is learned on source graphs and, on a target graph, kept fixed while only the shaping parameters are adapted, enabling few-shot transfer under matched compute. Across controlled synthetic benchmarks spanning graph families and signal regimes, Adaptive Spectral Shaping reduces reconstruction error relative to fixed-prototype wavelets and learned linear banks, and TASS yields consistent positive transfer. The framework provides compact spectral modules that plug into graph signal processing pipelines and graph neural networks, combining scalability, interpretability, and cross-graph generalization.

</details>


### [479] [Anytime Pretraining: Horizon-Free Learning-Rate Schedules with Weight Averaging](https://arxiv.org/abs/2602.03702)
*Alexandru Meterez,Pranav Ajit Nair,Depen Morwani,Cengiz Pehlevan,Sham Kakade*

Main category: cs.LG

TL;DR: The paper proposes anytime learning schedules for large language models, focusing on weight averaging and simple step sizes as alternatives to traditional horizon-dependent schedules.


<details>
  <summary>Details</summary>
Motivation: Existing pretraining methods rely on fixed compute budgets and horizon-dependent schedules, which are unsuitable for continual or open-ended training scenarios.

Method: The authors provide theoretical analysis for overparameterized linear regression, highlighting weight averaging's role. Empirical evaluation involves testing 150M and 300M parameter models with constant and $1/\sqrt{t}$ schedules against cosine decay.

Result: Anytime schedules achieve comparable final loss to cosine decay, demonstrating their effectiveness across the full training range.

Conclusion: Weight averaging with simple step sizes offers a practical anytime alternative to cosine schedules for pretraining large language models.

Abstract: Large language models are increasingly trained in continual or open-ended settings, where the total training horizon is not known in advance. Despite this, most existing pretraining recipes are not anytime: they rely on horizon-dependent learning rate schedules and extensive tuning under a fixed compute budget. In this work, we provide a theoretical analysis demonstrating the existence of anytime learning schedules for overparameterized linear regression, and we highlight the central role of weight averaging - also known as model merging - in achieving the minimax convergence rates of stochastic gradient descent. We show that these anytime schedules polynomially decay with time, with the decay rate determined by the source and capacity conditions of the problem. Empirically, we evaluate 150M and 300M parameter language models trained at 1-32x Chinchilla scale, comparing constant learning rates with weight averaging and $1/\sqrt{t}$ schedules with weight averaging against a well-tuned cosine schedule. Across the full training range, the anytime schedules achieve comparable final loss to cosine decay. Taken together, our results suggest that weight averaging combined with simple, horizon-free step sizes offers a practical and effective anytime alternative to cosine learning rate schedules for large language model pretraining.

</details>


### [480] [Efficient Training of Boltzmann Generators Using Off-Policy Log-Dispersion Regularization](https://arxiv.org/abs/2602.03729)
*Henrik Schopmans,Christopher von Klitzing,Pascal Friederich*

Main category: cs.LG

TL;DR: Boltzmann generators improve sampling efficiency from unnormalized densities using off-policy log-dispersion regularization (LDR), enhancing performance and reducing data costs.


<details>
  <summary>Details</summary>
Motivation: Sampling from unnormalized densities is computationally challenging, and existing methods like Boltzmann generators require costly simulation data and energy evaluations.

Method: Introduces LDR, a regularization framework leveraging target energy labels to shape the energy landscape, usable with biased/unbiased datasets or variational training.

Result: LDR boosts performance and data efficiency, achieving up to 10x sample efficiency gains in benchmarks.

Conclusion: LDR offers a versatile and effective way to enhance Boltzmann generators without additional on-policy samples.

Abstract: Sampling from unnormalized probability densities is a central challenge in computational science. Boltzmann generators are generative models that enable independent sampling from the Boltzmann distribution of physical systems at a given temperature. However, their practical success depends on data-efficient training, as both simulation data and target energy evaluations are costly. To this end, we propose off-policy log-dispersion regularization (LDR), a novel regularization framework that builds on a generalization of the log-variance objective. We apply LDR in the off-policy setting in combination with standard data-based training objectives, without requiring additional on-policy samples. LDR acts as a shape regularizer of the energy landscape by leveraging additional information in the form of target energy labels. The proposed regularization framework is broadly applicable, supporting unbiased or biased simulation datasets as well as purely variational training without access to target samples. Across all benchmarks, LDR improves both final performance and data efficiency, with sample efficiency gains of up to one order of magnitude.

</details>


### [481] [Fast-MWEM: Private Data Release in Sublinear Time](https://arxiv.org/abs/2602.03732)
*Themistoklis Haris,Steve Choi,Mutiraj Laksanawisit*

Main category: cs.LG

TL;DR: The paper introduces a lazy sampling approach to improve the runtime of the Multiplicative Weights Exponential Mechanism (MWEM) from Θ(m) to Θ(√m) per iteration, enhancing scalability for private data analysis tasks like linear query release and LP solving.


<details>
  <summary>Details</summary>
Motivation: The MWEM framework's scalability is limited by its Θ(m) per-iteration runtime due to the exponential mechanism. The goal is to reduce this dependency to make MWEM more efficient for large-scale applications.

Method: The authors modify MWEM using lazy sampling with Gumbel noise and a k-Nearest Neighbor data structure to approximate scores without exhaustive scans. This accelerates the Report-Noisy-Max mechanism.

Result: The modified framework achieves Θ(√m) runtime per iteration in expectation, confirmed by experiments showing significant speedups over classic MWEM.

Conclusion: The proposed lazy sampling approach effectively reduces the runtime bottleneck of MWEM, making it more scalable for applications like private linear query release and LP solving.

Abstract: The Multiplicative Weights Exponential Mechanism (MWEM) is a fundamental iterative framework for private data analysis, with broad applications such as answering $m$ linear queries, or privately solving systems of $m$ linear constraints. However, a critical bottleneck hindering its scalability is the $Θ(m)$ time complexity required to execute the exponential mechanism in each iteration. We introduce a modification to the MWEM framework that improves the per-iteration runtime dependency to $Θ(\sqrt{m})$ in expectation. This is done via a lazy sampling approach to the Report-Noisy-Max mechanism, which we implement efficiently using Gumbel noise and a $k$-Nearest Neighbor data structure. This allows for the rapid selection of the approximate score in the exponential mechanism without an exhaustive linear scan. We apply our accelerated framework to the problems of private linear query release and solving Linear Programs (LPs) under neighboring constraint conditions and low-sensitivity assumptions. Experimental evaluation confirms that our method provides a substantial runtime improvement over classic MWEM.

</details>


### [482] [Soft Sensor for Bottom-Hole Pressure Estimation in Petroleum Wells Using Long Short-Term Memory and Transfer Learning](https://arxiv.org/abs/2602.03737)
*M. A. Fernandes,E. Gildin,M. A. Sampaio*

Main category: cs.LG

TL;DR: A machine learning-based soft sensor using LSTM outperforms traditional methods like MLP and Ridge Regression in estimating Bottom-Hole Pressure (BHP) from wellhead data, achieving under 2% MAPE.


<details>
  <summary>Details</summary>
Motivation: Monitoring BHP is crucial for optimizing production and reducing emissions, but existing Permanent Downhole Gauges (PDGs) face reliability and cost challenges.

Method: Proposes an LSTM-based soft sensor, comparing it with MLP and Ridge Regression. Introduces Transfer Learning for model adaptation across environments.

Result: Achieves MAPE consistently below 2% on real offshore datasets from Brazil's Pre-salt basin, outperforming benchmarks.

Conclusion: The LSTM-based soft sensor is a cost-effective, accurate alternative to physical sensors, applicable across diverse conditions.

Abstract: Monitoring bottom-hole variables in petroleum wells is essential for production optimization, safety, and emissions reduction. Permanent Downhole Gauges (PDGs) provide real-time pressure data but face reliability and cost issues. We propose a machine learning-based soft sensor to estimate flowing Bottom-Hole Pressure (BHP) using wellhead and topside measurements. A Long Short-Term Memory (LSTM) model is introduced and compared with Multi-Layer Perceptron (MLP) and Ridge Regression. We also pioneer Transfer Learning for adapting models across operational environments. Tested on real offshore datasets from Brazil's Pre-salt basin, the methodology achieved Mean Absolute Percentage Error (MAPE) consistently below 2\%, outperforming benchmarks. This work offers a cost-effective, accurate alternative to physical sensors, with broad applicability across diverse reservoir and flow conditions.

</details>


### [483] [Decision-oriented benchmarking to transform AI weather forecast access: Application to the Indian monsoon](https://arxiv.org/abs/2602.03767)
*Rajat Masiwal,Colin Aitken,Adam Marchakitus,Mayank Gupta,Katherine Kowal,Hamid A. Pahlavan,Tyler Yang,Y. Qiang Sun,Michael Kremer,Amir Jina,William R. Boos,Pedram Hassanzadeh*

Main category: cs.LG

TL;DR: AIWP models outperform traditional physics-based models in weather prediction with lower resource use, but current evaluations overlook local stakeholder needs. A new framework integrating meteorology, AI, and social sciences is introduced and tested on Indian monsoon forecasting, benefiting agriculture and informing government action.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the gap in evaluating AIWP models by considering local stakeholders' operational needs, particularly for vulnerable populations facing weather shocks.

Method: The method involves a decision-oriented benchmarking framework that connects meteorology, AI, and social sciences, applied to Indian monsoon forecasting with agriculturally relevant metrics.

Result: AIWP models effectively predict monsoon onset indices weeks in advance, aiding a government effort to inform millions of farmers about unusual monsoon patterns.

Conclusion: The framework offers a blueprint for leveraging AIWP models to help vulnerable populations adapt to climate variability and change.

Abstract: Artificial intelligence weather prediction (AIWP) models now often outperform traditional physics-based models on common metrics while requiring orders-of-magnitude less computing resources and time. Open-access AIWP models thus hold promise as transformational tools for helping low- and middle-income populations make decisions in the face of high-impact weather shocks. Yet, current approaches to evaluating AIWP models focus mainly on aggregated meteorological metrics without considering local stakeholders' needs in decision-oriented, operational frameworks. Here, we introduce such a framework that connects meteorology, AI, and social sciences. As an example, we apply it to the 150-year-old problem of Indian monsoon forecasting, focusing on benefits to rain-fed agriculture, which is highly susceptible to climate change. AIWP models skillfully predict an agriculturally relevant onset index at regional scales weeks in advance when evaluated out-of-sample using deterministic and probabilistic metrics. This framework informed a government-led effort in 2025 to send 38 million Indian farmers AI-based monsoon onset forecasts, which captured an unusual weeks-long pause in monsoon progression. This decision-oriented benchmarking framework provides a key component of a blueprint for harnessing the power of AIWP models to help large vulnerable populations adapt to weather shocks in the face of climate variability and change.

</details>


### [484] [UniGeM: Unifying Data Mixing and Selection via Geometric Exploration and Mining](https://arxiv.org/abs/2602.03772)
*Changhao Wang,Yunfei Yu,Xinhao Yao,Jiaolong Yang,Riccardo Cantoro,Chaobo Li,Qing Cui,Jun Zhou*

Main category: cs.LG

TL;DR: UniGeM is a unified framework for data mixing and selection in LLMs, treating data curation as a manifold approximation problem, achieving 2x data efficiency and improved performance over SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Scaling LLMs is limited by data quality, and existing methods handle data mixing and selection separately, potentially disrupting code corpora structure.

Method: UniGeM unifies mixing and selection hierarchically: Macro-Exploration learns mixing weights via stability-based clustering, while Micro-Mining filters instances based on geometric distribution.

Result: UniGeM achieves 2x data efficiency over random baselines and outperforms SOTA methods in reasoning-heavy evaluations and multilingual generalization.

Conclusion: UniGeM effectively improves LLM scaling by unifying data curation, ensuring logical consistency and better performance.

Abstract: The scaling of Large Language Models (LLMs) is increasingly limited by data quality. Most methods handle data mixing and sample selection separately, which can break the structure in code corpora. We introduce \textbf{UniGeM}, a framework that unifies mixing and selection by treating data curation as a \textit{manifold approximation} problem without training proxy models or relying on external reference datasets. UniGeM operates hierarchically: \textbf{Macro-Exploration} learns mixing weights with stability-based clustering; \textbf{Micro-Mining} filters high-quality instances by their geometric distribution to ensure logical consistency. Validated by training 8B and 16B MoE models on 100B tokens, UniGeM achieves \textbf{2.0$\times$ data efficiency} over a random baseline and further improves overall performance compared to SOTA methods in reasoning-heavy evaluations and multilingual generalization.

</details>


### [485] [Reasoning with Latent Tokens in Diffusion Language Models](https://arxiv.org/abs/2602.03769)
*Andre He,Sean Welleck,Daniel Fried*

Main category: cs.LG

TL;DR: Discrete diffusion models outperform autoregressive models in reasoning tasks but are slower at inference due to joint prediction of all tokens. Modulating latent tokens balances speed and quality.


<details>
  <summary>Details</summary>
Motivation: The paper aims to understand why discrete diffusion models outperform autoregressive models in reasoning tasks despite slower inference, focusing on the role of joint prediction of all tokens.

Method: The authors ablate joint prediction in diffusion models, revealing its importance, and introduce latent tokens to modulate inference speed and quality. They also apply latent tokens to autoregressive models via a multi-token prediction objective.

Result: Modulating latent tokens enables a smooth tradeoff between inference speed and sample quality in diffusion models. Introducing latent tokens into autoregressive models improves their performance on reasoning tasks.

Conclusion: Latent tokens, naturally arising in diffusion models, are a general mechanism for enhancing performance in tasks requiring global coherence or lookahead.

Abstract: Discrete diffusion models have recently become competitive with autoregressive models for language modeling, even outperforming them on reasoning tasks requiring planning and global coherence, but they require more computation at inference time. We trace this trade-off to a key mechanism: diffusion models are trained to jointly predict a distribution over all unknown tokens, including those that will not actually be decoded in the current step. Ablating this joint prediction yields faster inference but degrades performance, revealing that accurate prediction at the decoded position relies on joint reasoning about the distribution of undecoded tokens. We interpret these as latent tokens and introduce a method for modulating their number, demonstrating empirically that this enables a smooth tradeoff between inference speed and sample quality. Furthermore, we demonstrate that latent tokens can be introduced into autoregressive models through an auxiliary multi-token prediction objective, yielding substantial improvements on the same reasoning tasks where they have traditionally struggled. Our results suggest that latent tokens, while arising naturally in diffusion, represent a general mechanism for improving performance on tasks requiring global coherence or lookahead.

</details>


### [486] [Reward Redistribution for CVaR MDPs using a Bellman Operator on L-infinity](https://arxiv.org/abs/2602.03778)
*Aneri Muni,Vincent Taboga,Esther Derman,Pierre-Luc Bacon,Erick Delage*

Main category: cs.LG

TL;DR: The paper introduces a novel formulation for static CVaR objectives in risk-averse decision-making, addressing issues like sparse rewards in classical methods. It proposes Bellman operators with dense rewards and contraction properties, leading to efficient algorithms with convergence guarantees.


<details>
  <summary>Details</summary>
Motivation: Classical CVaR formulations suffer from sparse rewards and degenerate fixed points due to state augmentation. The authors aim to overcome these limitations to improve risk-averse decision-making.

Method: The authors propose an alternative state augmentation approach, resulting in a Bellman operator with dense rewards and contraction properties. They develop risk-averse value iteration and Q-learning algorithms using discretized augmented states.

Result: Theoretical analysis shows dense rewards and contraction properties. Empirical results confirm successful learning of CVaR-sensitive policies with effective performance-safety trade-offs.

Conclusion: The novel CVaR formulation and algorithms provide a robust framework for risk-averse decision-making, addressing shortcomings of classical methods.

Abstract: Tail-end risk measures such as static conditional value-at-risk (CVaR) are used in safety-critical applications to prevent rare, yet catastrophic events. Unlike risk-neutral objectives, the static CVaR of the return depends on entire trajectories without admitting a recursive Bellman decomposition in the underlying Markov decision process. A classical resolution relies on state augmentation with a continuous variable. However, unless restricted to a specialized class of admissible value functions, this formulation induces sparse rewards and degenerate fixed points. In this work, we propose a novel formulation of the static CVaR objective based on augmentation. Our alternative approach leads to a Bellman operator with: (1) dense per-step rewards; (2) contracting properties on the full space of bounded value functions. Building on this theoretical foundation, we develop risk-averse value iteration and model-free Q-learning algorithms that rely on discretized augmented states. We further provide convergence guarantees and approximation error bounds due to discretization. Empirical results demonstrate that our algorithms successfully learn CVaR-sensitive policies and achieve effective performance-safety trade-offs.

</details>


### [487] [Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL](https://arxiv.org/abs/2602.03773)
*Ian Wu,Yuxiao Qu,Amrith Setlur,Aviral Kumar*

Main category: cs.LG

TL;DR: RC is a decoding algorithm enabling LLMs to improve iteratively beyond training budgets, achieving superior performance on long reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Standard RL limits LLMs' adaptability to distribution shifts at test time, hindering continual improvement.

Method: Introduces RC, an iterative decoding algorithm leveraging LLMs' asymmetry in generation and summarization to build reasoning chains.

Result: A 4B model with RC improves performance from 40% to 70% on HMMT 2025, surpassing larger models.

Conclusion: RC enables LLMs to extrapolate beyond training horizons and better utilize scaffolds for test-time scaling.

Abstract: Large Language Models (LLMs) that can continually improve beyond their training budgets are able to solve increasingly difficult problems by adapting at test time, a property we refer to as extrapolation. However, standard reinforcement learning (RL) operates over fixed problem distributions and training budgets, which limits extrapolation amidst distribution shift at test time. To address this, we introduce RC, an iterative decoding algorithm that replaces standard autoregressive decoding during both training and inference. RC exploits an asymmetry between the response generation and summarization capabilities of LLMs to construct reasoning chains that consistently improve across iterations. Models trained to use RC can extrapolate and continually improve over reasoning horizons more than an order of magnitude longer than those seen during training. Empirically, training a 4B model with RC using a 16k-token training budget improves performance on HMMT 2025 from 40% to nearly 70% with 0.5m tokens at test time, outperforming both comparably sized models and many larger reasoning LLMs. Finally, we also show that models trained with RC can more effectively leverage existing scaffolds to further scale test-time performance, due to the improved summary-conditioned generation abilities learned through training.

</details>


### [488] [Enhancing Imbalanced Node Classification via Curriculum-Guided Feature Learning and Three-Stage Attention Network](https://arxiv.org/abs/2602.03808)
*Abdul Joseph Fofanah,Lian Wen,David Chen,Shaoyang Zhang*

Main category: cs.LG

TL;DR: CL3AN-GNN introduces a curriculum-guided, three-stage attention network to address imbalanced node classification in GNNs, improving performance on diverse datasets.


<details>
  <summary>Details</summary>
Motivation: Imbalanced node classification skews model performance toward common classes; CL3AN-GNN aims to mitigate this by mimicking human learning stages.

Method: Uses Engage (simple features), Enact (complex features), and Embed (consolidation) stages with curriculum-aligned attention and loss weighting.

Result: Outperforms state-of-the-art methods in accuracy, F1-score, and AUC across eight datasets, with faster convergence and better generalization.

Conclusion: CL3AN-GNN effectively addresses imbalance in GNNs through curriculum learning, validated by metrics and theoretical grounding.

Abstract: Imbalanced node classification in graph neural networks (GNNs) happens when some labels are much more common than others, which causes the model to learn unfairly and perform badly on the less common classes. To solve this problem, we propose a Curriculum-Guided Feature Learning and Three-Stage Attention Network (CL3AN-GNN), a learning network that uses a three-step attention system (Engage, Enact, Embed) similar to how humans learn. The model begins by engaging with structurally simpler features, defined as (1) local neighbourhood patterns (1-hop), (2) low-degree node attributes, and (3) class-separable node pairs identified via initial graph convolutional networks and graph attention networks (GCN and GAT) embeddings. This foundation enables stable early learning despite label skew. The Enact stage then addresses complicated aspects: (1) connections that require multiple steps, (2) edges that connect different types of nodes, and (3) nodes at the edges of minority classes by using adjustable attention weights. Finally, Embed consolidates these features via iterative message passing and curriculum-aligned loss weighting. We evaluate CL3AN-GNN on eight Open Graph Benchmark datasets spanning social, biological, and citation networks. Experiments show consistent improvements across all datasets in accuracy, F1-score, and AUC over recent state-of-the-art methods. The model's step-by-step method works well with different types of graph datasets, showing quicker results than training everything at once, better performance on new, imbalanced graphs, and clear explanations of each step using gradient stability and attention correlation learning curves. This work provides both a theoretically grounded framework for curriculum learning in GNNs and practical evidence of its effectiveness against imbalances, validated through metrics, convergence speeds, and generalisation tests.

</details>


### [489] [Inference-time Unlearning Using Conformal Prediction](https://arxiv.org/abs/2602.03787)
*Somnath Basu Roy Chowdhury,Rahul Kidambi,Avinava Dubey,David Wang,Gokhan Mergen,Amr Ahmed,Aranyak Mehta*

Main category: cs.LG

TL;DR: A framework for inference-time machine unlearning uses verifier feedback and conformal prediction to refine responses without updating model parameters, reducing error by up to 93%.


<details>
  <summary>Details</summary>
Motivation: Existing unlearning methods degrade model performance and struggle with real-world applications, especially in generative models.

Method: Uses a verifier to judge responses and iteratively refines them via conformal prediction, avoiding parameter updates.

Result: Reduces unlearning error by up to 93% compared to state-of-the-art methods.

Conclusion: The proposed inference-time unlearning framework outperforms existing methods while preserving model capabilities.

Abstract: Machine unlearning is the process of efficiently removing specific information from a trained machine learning model without retraining from scratch. Existing unlearning methods, which often provide provable guarantees, typically involve retraining a subset of model parameters based on a forget set. While these approaches show promise in certain scenarios, their underlying assumptions are often challenged in real-world applications -- particularly when applied to generative models. Furthermore, updating parameters using these unlearning procedures often degrades the general-purpose capabilities the model acquired during pre-training. Motivated by these shortcomings, this paper considers the paradigm of inference time unlearning -- wherein, the generative model is equipped with an (approximately correct) verifier that judges whether the model's response satisfies appropriate unlearning guarantees. This paper introduces a framework that iteratively refines the quality of the generated responses using feedback from the verifier without updating the model parameters. The proposed framework leverages conformal prediction to reduce computational overhead and provide distribution-free unlearning guarantees. This paper's approach significantly outperforms existing state-of-the-art methods, reducing unlearning error by up to 93% across challenging unlearning benchmarks.

</details>


### [490] [Should I use Synthetic Data for That? An Analysis of the Suitability of Synthetic Data for Data Sharing and Augmentation](https://arxiv.org/abs/2602.03791)
*Bogdan Kulynych,Theresa Stadler,Jean Louis Raisaro,Carmela Troncoso*

Main category: cs.LG

TL;DR: The paper evaluates synthetic data's effectiveness in three use cases—privacy-preserving data sharing, ML training augmentation, and statistical estimation—identifying fundamental and practical limitations that often make synthetic data unsuitable.


<details>
  <summary>Details</summary>
Motivation: To address common problems like data access, scarcity, and under-representation, synthetic data is widely proposed. The paper aims to assess its viability.

Method: Formal analysis and case studies are used to evaluate synthetic data's effectiveness in three use cases, identifying constraints and limitations.

Result: Synthetic data often fails to meet objectives due to fundamental and practical limits, making it unsuitable for many envisioned uses.

Conclusion: Decision makers must carefully assess synthetic data's fit for specific problems, as its effectiveness is constrained by identified limits.

Abstract: Recent advances in generative modelling have led many to see synthetic data as the go-to solution for a range of problems around data access, scarcity, and under-representation. In this paper, we study three prominent use cases: (1) Sharing synthetic data as a proxy for proprietary datasets to enable statistical analyses while protecting privacy, (2) Augmenting machine learning training sets with synthetic data to improve model performance, and (3) Augmenting datasets with synthetic data to reduce variance in statistical estimation. For each use case, we formalise the problem setting and study, through formal analysis and case studies, under which conditions synthetic data can achieve its intended objectives. We identify fundamental and practical limits that constrain when synthetic data can serve as an effective solution for a particular problem. Our analysis reveals that due to these limits many existing or envisioned use cases of synthetic data are a poor problem fit. Our formalisations and classification of synthetic data use cases enable decision makers to assess whether synthetic data is a suitable approach for their specific data availability problem.

</details>


### [491] [PLATE: Plasticity-Tunable Efficient Adapters for Geometry-Aware Continual Learning](https://arxiv.org/abs/2602.03846)
*Romain Cosentino*

Main category: cs.LG

TL;DR: PLATE is a continual learning method for pretrained models that avoids old-task data, leveraging geometric redundancy to control plasticity-retention trade-offs with structured low-rank updates.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of adapting foundation models without access to old-task data by exploiting geometric redundancy in pretrained networks.

Method: Utilizes redundant neurons to create protected update subspaces and restrict plasticity to subsets of neurons, parameterizing layers with frozen components $B$ and $Q$, and trainable $A$.

Result: Improved worst-case retention guarantees and reduced functional drift on old-data distributions, offering explicit control over plasticity-retention trade-offs.

Conclusion: PLATE provides an efficient, data-free continual learning solution with structured updates, validated by theoretical and empirical results.

Abstract: We develop a continual learning method for pretrained models that \emph{requires no access to old-task data}, addressing a practical barrier in foundation model adaptation where pretraining distributions are often unavailable. Our key observation is that pretrained networks exhibit substantial \emph{geometric redundancy}, and that this redundancy can be exploited in two complementary ways. First, redundant neurons provide a proxy for dominant pretraining-era feature directions, enabling the construction of approximately protected update subspaces directly from pretrained weights. Second, redundancy offers a natural bias for \emph{where} to place plasticity: by restricting updates to a subset of redundant neurons and constraining the remaining degrees of freedom, we obtain update families with reduced functional drift on the old-data distribution and improved worst-case retention guarantees. These insights lead to \textsc{PLATE} (\textbf{Pla}sticity-\textbf{T}unable \textbf{E}fficient Adapters), a continual learning method requiring no past-task data that provides explicit control over the plasticity-retention trade-off. PLATE parameterizes each layer with a structured low-rank update $ΔW = B A Q^\top$, where $B$ and $Q$ are computed once from pretrained weights and kept frozen, and only $A$ is trained on the new task. The code is available at https://github.com/SalesforceAIResearch/PLATE.

</details>


### [492] [Manifold Random Features](https://arxiv.org/abs/2602.03797)
*Ananya Parashar,Derek Long,Dwaipayan Saha,Krzysztof Choromanski*

Main category: cs.LG

TL;DR: A new method called Manifold Random Features (MRFs) is introduced for approximating bi-variate functions on manifolds, leveraging discretization and Graph Random Features (GRFs). MRFs ensure bounded and positive features, enabling accurate approximations. The method connects GRFs with continuous random features and simplifies Gaussian kernel approximation, validated by theory and experiments.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle to derive continuous approximation mechanisms for bi-variate functions on general manifolds analytically. MRFs aim to address this gap by using discretization and GRFs to provide accurate, low-variance approximations.

Method: MRFs discretize the manifold and employ Graph Random Features (GRFs) to learn continuous fields. This approach ensures bounded and positive features, facilitating accurate approximations. The method also connects discrete GRFs with continuous random features and simplifies Gaussian kernel approximation using random walks on graphs.

Result: MRFs successfully provide bounded and positive features for approximating bi-variate functions on manifolds. The method bridges the gap between discrete GRFs and continuous random features and simplifies complex mathematical computations for Gaussian kernel approximation.

Conclusion: MRFs offer a novel and effective paradigm for approximating bi-variate functions on manifolds, validated by theoretical analysis and experimental results. The method simplifies existing techniques while maintaining accuracy and low variance.

Abstract: We present a new paradigm for creating random features to approximate bi-variate functions (in particular, kernels) defined on general manifolds. This new mechanism of Manifold Random Features (MRFs) leverages discretization of the manifold and the recently introduced technique of Graph Random Features (GRFs) to learn continuous fields on manifolds. Those fields are used to find continuous approximation mechanisms that otherwise, in general scenarios, cannot be derived analytically. MRFs provide positive and bounded features, a key property for accurate, low-variance approximation. We show deep asymptotic connection between GRFs, defined on discrete graph objects, and continuous random features used for regular kernels. As a by-product of our method, we re-discover recently introduced mechanism of Gaussian kernel approximation applied in particular to improve linear-attention Transformers, considering simple random walks on graphs and by-passing original complex mathematical computations. We complement our algorithm with a rigorous theoretical analysis and verify in thorough experimental studies.

</details>


### [493] [Prediction of Critical Heat Flux in Rod Bundles Using Tube-Based Hybrid Machine Learning Models in CTF](https://arxiv.org/abs/2602.03805)
*Aidan Furlong,Robert Salko,Xingang Zhao,Xu Wu*

Main category: cs.LG

TL;DR: Machine learning models outperform traditional methods in predicting critical heat flux (CHF) in rod bundles.


<details>
  <summary>Details</summary>
Motivation: To improve CHF prediction accuracy beyond empirical correlations and lookup tables (LUTs) for full-scale reactor core simulations.

Method: Implemented a purely data-driven DNN and two hybrid bias-correction models in the CTF subchannel code, tested on Combustion Engineering 5-by-5 bundle CHF data.

Result: ML models outperformed baseline comparators (W-3 correlation, Bowring correlation, Groeneveld LUT), with the hybrid LUT model showing the best performance.

Conclusion: ML-based CHF prediction models generalize well from tube-based to rod bundle geometries, offering superior accuracy.

Abstract: The prediction of critical heat flux (CHF) using machine learning (ML) approaches has become a highly active research activity in recent years, the goal of which is to build models more accurate than current conventional approaches such as empirical correlations or lookup tables (LUTs). Previous work developed and deployed tube-based pure and hybrid ML models in the CTF subchannel code, however, full-scale reactor core simulations require the use of rod bundle geometries. Unlike isolated subchannels, rod bundles experience complex thermal hydraulic phenomena such as channel crossflow, spacer grid losses, and effects from unheated conductors. This study investigates the generalization of ML-based CHF prediction models in rod bundles after being trained on tube-based CHF data. A purely data-driven DNN and two hybrid bias-correction models were implemented in the CTF subchannel code and used to predict CHF location and magnitude in the Combustion Engineering 5-by-5 bundle CHF test series. The W-3 correlation, Bowring correlation, and Groeneveld LUT were used as baseline comparators. On average, all three ML-based approaches produced magnitude and location predictions more accurate than the baseline models, with the hybrid LUT model exhibiting the most favorable performance metrics.

</details>


### [494] [SymPlex: A Structure-Aware Transformer for Symbolic PDE Solving](https://arxiv.org/abs/2602.03816)
*Yesom Park,Annie C. Lu,Shao-Ching Huang,Qiyang Hu,Y. Sungtaek Ju,Stanley Osher*

Main category: cs.LG

TL;DR: SymPlex is a reinforcement learning framework that discovers symbolic solutions to PDEs using a Transformer-based model tailored for hierarchical symbolic dependencies, ensuring interpretable and exact results.


<details>
  <summary>Details</summary>
Motivation: The goal is to find analytical symbolic solutions for PDEs without relying on ground-truth expressions, addressing limitations of numerical and neural methods.

Method: SymPlex uses SymFormer, a Transformer with tree-relative self-attention and grammar-constrained autoregressive decoding, optimizing solutions using PDEs and boundary conditions.

Result: Empirical results show successful recovery of non-smooth and parametric PDE solutions, validated symbolically.

Conclusion: SymPlex advances symbolic PDE solving by producing interpretable solutions directly in expression space, outperforming traditional approximation methods.

Abstract: We propose SymPlex, a reinforcement learning framework for discovering analytical symbolic solutions to partial differential equations (PDEs) without access to ground-truth expressions. SymPlex formulates symbolic PDE solving as tree-structured decision-making and optimizes candidate solutions using only the PDE and its boundary conditions. At its core is SymFormer, a structure-aware Transformer that models hierarchical symbolic dependencies via tree-relative self-attention and enforces syntactic validity through grammar-constrained autoregressive decoding, overcoming the limited expressivity of sequence-based generators. Unlike numerical and neural approaches that approximate solutions in discretized or implicit function spaces, SymPlex operates directly in symbolic expression space, enabling interpretable and human-readable solutions that naturally represent non-smooth behavior and explicit parametric dependence. Empirical results demonstrate exact recovery of non-smooth and parametric PDE solutions using deep learning-based symbolic methods.

</details>


### [495] [Robust Intervention Learning from Emergency Stop Interventions](https://arxiv.org/abs/2602.03825)
*Ethan Pronovost,Khimya Khetarpal,Siddhartha Srinivasa*

Main category: cs.LG

TL;DR: The paper introduces Robust Intervention Learning (RIL) and Residual Intervention Fine-Tuning (RIFT) to improve autonomous systems by learning from noisy intervention signals while leveraging prior policies.


<details>
  <summary>Details</summary>
Motivation: Human interventions during testing of autonomous systems are noisy and incomplete but valuable for policy improvement. The paper aims to address these limitations by proposing methods robust to intervention quality.

Method: The authors propose Residual Intervention Fine-Tuning (RIFT), which treats interventions as incomplete signals and combines them with prior policies to resolve ambiguity. Theoretical analysis and experiments validate the approach.

Result: Experiments show that RIFT enables consistent policy improvement across various intervention strategies and prior policy qualities.

Conclusion: Robust intervention learning, particularly RIFT, is a promising direction for enhancing autonomous systems by effectively utilizing noisy intervention data.

Abstract: Human interventions are a common source of data in autonomous systems during testing. These interventions provide an important signal about where the current policy needs improvement, but are often noisy and incomplete. We define Robust Intervention Learning (RIL) as the problem of learning from intervention data while remaining robust to the quality and informativeness of the intervention signal. In the best case, interventions are precise and avoiding them is sufficient to solve the task, but in many realistic settings avoiding interventions is necessary but not sufficient for achieving good performance. We study robust intervention learning in the context of emergency stop interventions and propose Residual Intervention Fine-Tuning (RIFT), a residual fine-tuning algorithm that treats intervention feedback as an incomplete learning signal and explicitly combines it with a prior policy. By framing intervention learning as a fine-tuning problem, our approach leverages structure encoded in the prior policy to resolve ambiguity when intervention signals under-specify the task. We provide theoretical analysis characterizing conditions under which this formulation yields principled policy improvement, and identify regimes where intervention learning is expected to fail. Our experiments reveal that residual fine-tuning enables robust and consistent policy improvement across a range of intervention strategies and prior policy qualities, and highlight robust intervention learning as a promising direction for future work.

</details>


### [496] [Understanding and Exploiting Weight Update Sparsity for Communication-Efficient Distributed RL](https://arxiv.org/abs/2602.03839)
*Erfan Miahi,Eugene Belilovsky*

Main category: cs.LG

TL;DR: PULSE (Patch Updates via Lossless Sparse Encoding) is a lightweight method for synchronizing only modified parameters in distributed RL, reducing communication bandwidth by over 100x while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Scalability in distributed RL is bottlenecked by policy weight synchronization, especially in bandwidth-constrained settings, despite updates often modifying only a small fraction of parameters.

Method: The study investigates weight-update sparsity and introduces PULSE, a lossless method transmitting only indices and values of modified parameters.

Result: PULSE reduces communication by over 100x (14 GB to ~108 MB) and cuts required bandwidth from 20 Gbit/s to 0.2 Gbit/s, maintaining identical training dynamics.

Conclusion: PULSE enables decentralized RL training to approach centralized throughput by efficiently leveraging high update sparsity.

Abstract: Reinforcement learning (RL) is a critical component for post-training large language models (LLMs). However, in bandwidth-constrained distributed RL, scalability is often bottlenecked by the synchronization of policy weights from trainers to inference workers, particularly over commodity networks or in decentralized settings. While recent studies suggest that RL updates modify only a small fraction of model parameters, these observations are typically based on coarse checkpoint differences. We present a systematic empirical study of weight-update sparsity at both step-level and multi-step granularities, examining its evolution across training dynamics, off-policy delay, and model scale. We find that update sparsity is consistently high, frequently exceeding 99% across practically relevant settings. Leveraging this structure, we propose PULSE (Patch Updates via Lossless Sparse Encoding), a simple yet highly efficient lossless weight synchronization method that transmits only the indices and values of modified parameters. PULSE is robust to transmission errors and avoids floating-point drift inherent in additive delta schemes. In bandwidth-constrained decentralized environments, our approach achieves over 100x (14 GB to ~108 MB) communication reduction while maintaining bit-identical training dynamics and performance compared to full weight synchronization. By exploiting this structure, PULSE enables decentralized RL training to approach centralized throughput, reducing the bandwidth required for weight synchronization from 20 Gbit/s to 0.2 Gbit/s to maintain high GPU utilization.

</details>
