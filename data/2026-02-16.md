<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 66]
- [cs.CL](#cs.CL) [Total: 35]
- [cs.MA](#cs.MA) [Total: 4]
- [cs.AI](#cs.AI) [Total: 18]
- [cs.LG](#cs.LG) [Total: 93]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Thermal Imaging for Contactless Cardiorespiratory and Sudomotor Response Monitoring](https://arxiv.org/abs/2602.12361)
*Constantino Álvarez Casado,Mohammad Rahman,Sasan Sharifipour,Nhi Nguyen,Manuel Lage Cañellas,Xiaoting Wu,Miguel Bordallo López*

Main category: cs.CV

TL;DR: The paper presents a method to estimate electrodermal activity (EDA), heart rate (HR), and breathing rate (BR) from facial thermal video using signal-processing techniques.


<details>
  <summary>Details</summary>
Motivation: Thermal infrared imaging offers contactless biosignal estimation, which visible-light methods cannot achieve for EDA, a key sympathetic activation marker. This study aims to explore thermal imaging's potential for EDA, HR, and BR extraction.

Method: The signal-processing pipeline involves tracking facial regions, spatial aggregation, and separating sudomotor trends from cardiorespiratory signals. HR is estimated using OMIT decomposition, and BR is derived from nasal/cheek signals. EDA configurations are evaluated for optimal performance.

Result: The best EDA configuration achieved a mean absolute correlation of 0.40 against palm EDA. BR estimation had a mean error of 3.1 bpm, while HR estimation had 13.8 bpm MAE. Findings include signal polarity alternation and demographic effects on extraction quality.

Conclusion: The study establishes baseline performance for thermal contactless biosignal estimation and provides design guidance, highlighting the method's potential despite limitations like low frame rate.

Abstract: Thermal infrared imaging captures skin temperature changes driven by autonomic regulation and can potentially provide contactless estimation of electrodermal activity (EDA), heart rate (HR), and breathing rate (BR). While visible-light methods address HR and BR, they cannot access EDA, a standard marker of sympathetic activation. This paper characterizes the extraction of these three biosignals from facial thermal video using a signal-processing pipeline that tracks anatomical regions, applies spatial aggregation, and separates slow sudomotor trends from faster cardiorespiratory components. For HR, we apply an orthogonal matrix image transformation (OMIT) decomposition across multiple facial regions of interest (ROIs), and for BR we average nasal and cheek signals before spectral peak detection. We evaluate 288 EDA configurations and the HR/BR pipeline on 31 sessions from the public SIMULATOR STUDY 1 (SIM1) driver monitoring dataset. The best fixed EDA configuration (nose region, exponential moving average) reaches a mean absolute correlation of $0.40 \pm 0.23$ against palm EDA, with individual sessions reaching 0.89. BR estimation achieves a mean absolute error of $3.1 \pm 1.1$ bpm, while HR estimation yields $13.8 \pm 7.5$ bpm MAE, limited by the low camera frame rate (7.5 Hz). We report signal polarity alternation across sessions, short thermodynamic latency for well-tracked signals, and condition-dependent and demographic effects on extraction quality. These results provide baseline performance bounds and design guidance for thermal contactless biosignal estimation.

</details>


### [2] [LLaMo: Scaling Pretrained Language Models for Unified Motion Understanding and Generation with Continuous Autoregressive Tokens](https://arxiv.org/abs/2602.12370)
*Zekun Li,Sizhe An,Chengcheng Tang,Chuan Guo,Ivan Shugurov,Linguang Zhang,Amy Zhao,Srinath Sridhar,Lingling Tao,Abhay Mittal*

Main category: cs.CV

TL;DR: LLaMo is a unified framework leveraging pretrained LLMs for motion-language tasks, addressing issues like catastrophic forgetting and jitter artifacts with a Mixture-of-Transformers architecture and continuous latent space encoding.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in existing motion-language models, such as catastrophic forgetting and discrete tokenization artifacts, by unifying multimodal generation and understanding.

Method: Extends pretrained LLMs via a Mixture-of-Transformers architecture, encodes motion into a continuous latent space, and uses a flow-matching head for real-time streaming motion generation.

Result: Achieves high-fidelity text-to-motion generation and motion-to-text captioning, especially in zero-shot settings, with real-time performance (>30 FPS).

Conclusion: LLaMo represents a significant advancement towards a general unified motion-language large model.

Abstract: Recent progress in large models has led to significant advances in unified multimodal generation and understanding. However, the development of models that unify motion-language generation and understanding remains largely underexplored. Existing approaches often fine-tune large language models (LLMs) on paired motion-text data, which can result in catastrophic forgetting of linguistic capabilities due to the limited scale of available text-motion pairs. Furthermore, prior methods typically convert motion into discrete representations via quantization to integrate with language models, introducing substantial jitter artifacts from discrete tokenization. To address these challenges, we propose LLaMo, a unified framework that extends pretrained LLMs through a modality-specific Mixture-of-Transformers (MoT) architecture. This design inherently preserves the language understanding of the base model while enabling scalable multimodal adaptation. We encode human motion into a causal continuous latent space and maintain the next-token prediction paradigm in the decoder-only backbone through a lightweight flow-matching head, allowing for streaming motion generation in real-time (>30 FPS). Leveraging the comprehensive language understanding of pretrained LLMs and large-scale motion-text pretraining, our experiments demonstrate that LLaMo achieves high-fidelity text-to-motion generation and motion-to-text captioning in general settings, especially zero-shot motion generation, marking a significant step towards a general unified motion-language large model.

</details>


### [3] [Synthetic Image Detection with CLIP: Understanding and Assessing Predictive Cues](https://arxiv.org/abs/2602.12381)
*Marco Willi,Melanie Mathys,Michael Graber*

Main category: cs.CV

TL;DR: SynthCLIC dataset and interpretable analysis reveal CLIP-based synthetic image detectors rely on high-level photographic attributes, not artifacts, but struggle with generalization across generative models.


<details>
  <summary>Details</summary>
Motivation: The rise of near-photorealistic generative models challenges photo trustworthiness, but existing SID methods fail to generalize or perform practically. CLIP shows promise, but its detection cues are unclear.

Method: Introduces SynthCLIC dataset (real vs. diffusion-based synthetic images) and analyzes CLIP-based detectors using interpretable linear heads and concept models.

Result: CLIP detectors achieve high accuracy (0.96 mAP on GANs, 0.92 on SynthCLIC) but poor cross-model generalization (0.37 mAP). They detect photographic attributes, not artifacts.

Conclusion: CLIP-based SID is effective but requires continual updates and broader training exposure to improve robustness across diverse generative models.

Abstract: Recent generative models produce near-photorealistic images, challenging the trustworthiness of photographs. Synthetic image detection (SID) has thus become an important area of research. Prior work has highlighted how synthetic images differ from real photographs--unfortunately, SID methods often struggle to generalize to novel generative models and often perform poorly in practical settings. CLIP, a foundational vision-language model which yields semantically rich image-text embeddings, shows strong accuracy and generalization for SID. Yet, the underlying relevant cues embedded in CLIP-features remain unknown. It is unclear, whether CLIP-based detectors simply detect strong visual artifacts or exploit subtle semantic biases, both of which would render them useless in practical settings or on generative models of high quality. We introduce SynthCLIC, a paired dataset of real photographs and high-quality synthetic counterparts from recent diffusion models, designed to reduce semantic bias in SID. Using an interpretable linear head with de-correlated activations and a text-grounded concept-model, we analyze what CLIP-based detectors learn. CLIP-based linear detectors reach 0.96 mAP on a GAN-based benchmark but only 0.92 on our high-quality diffusion dataset SynthCLIC, and generalization across generator families drops to as low as 0.37 mAP. We find that the detectors primarily rely on high-level photographic attributes (e.g., minimalist style, lens flare, or depth layering), rather than overt generator-specific artifacts. CLIP-based detectors perform well overall but generalize unevenly across diverse generative architectures. This highlights the need for continual model updates and broader training exposure, while reinforcing CLIP-based approaches as a strong foundation for more universal, robust SID.

</details>


### [4] [Reproducing DragDiffusion: Interactive Point-Based Editing with Diffusion Models](https://arxiv.org/abs/2602.12393)
*Ali Subhan,Ashir Raza*

Main category: cs.CV

TL;DR: DragDiffusion is a method for interactive image editing using point-based manipulation. A reproducibility study confirms its claims but highlights sensitivity to hyperparameters like timestep and feature level.


<details>
  <summary>Details</summary>
Motivation: To validate the reproducibility and effectiveness of DragDiffusion's claims regarding accurate spatial control in image editing.

Method: Conducted reproducibility study using the authors' implementation and DragBench benchmark, reproducing ablation studies on timestep selection, fine-tuning, regularization, and feature supervision.

Result: Findings align with original work but reveal hyperparameter sensitivity, particularly to timestep and feature level. Multi-timestep optimization did not improve accuracy.

Conclusion: DragDiffusion's central claims are supported, but performance depends on specific hyperparameter settings. Code is available for further exploration.

Abstract: DragDiffusion is a diffusion-based method for interactive point-based image editing that enables users to manipulate images by directly dragging selected points. The method claims that accurate spatial control can be achieved by optimizing a single diffusion latent at an intermediate timestep, together with identity-preserving fine-tuning and spatial regularization. This work presents a reproducibility study of DragDiffusion using the authors' released implementation and the DragBench benchmark. We reproduce the main ablation studies on diffusion timestep selection, LoRA-based fine-tuning, mask regularization strength, and UNet feature supervision, and observe close agreement with the qualitative and quantitative trends reported in the original work. At the same time, our experiments show that performance is sensitive to a small number of hyperparameter assumptions, particularly the optimized timestep and the feature level used for motion supervision, while other components admit broader operating ranges. We further evaluate a multi-timestep latent optimization variant and find that it does not improve spatial accuracy while substantially increasing computational cost. Overall, our findings support the central claims of DragDiffusion while clarifying the conditions under which they are reliably reproducible. Code is available at https://github.com/AliSubhan5341/DragDiffusion-TMLR-Reproducibility-Challenge.

</details>


### [5] [What does RL improve for Visual Reasoning? A Frankenstein-Style Analysis](https://arxiv.org/abs/2602.12395)
*Xirui Li,Ming Li,Tianyi Zhou*

Main category: cs.CV

TL;DR: The paper investigates how reinforcement learning (RL) improves visual reasoning in vision-language models compared to supervised fine-tuning (IN), using a framework to dissect specific skills and attributions.


<details>
  <summary>Details</summary>
Motivation: To clarify the specific capabilities RL enhances in visual reasoning, disentangling these from benchmark conflations and understanding RL's unique contributions.

Method: Proposes a framework involving causal probing, parameter comparison, and model merging to localize, characterize, and test RL-induced improvements.

Result: RL primarily refines mid-to-late transformer layers, improving vision-to-reasoning alignment and reasoning performance, which are transferable and necessary for gains.

Conclusion: RL's contribution is systematic refinement of mid-to-late computation, not uniform perception enhancement, highlighting the need for deeper evaluation beyond benchmarks.

Abstract: Reinforcement learning (RL) with verifiable rewards has become a standard post-training stage for boosting visual reasoning in vision-language models, yet it remains unclear what capabilities RL actually improves compared with supervised fine-tuning as cold-start initialization (IN). End-to-end benchmark gains conflate multiple factors, making it difficult to attribute improvements to specific skills. To bridge the gap, we propose a Frankenstein-style analysis framework including: (i) functional localization via causal probing; (ii) update characterization via parameter comparison; and (iii) transferability test via model merging. Instead, RL induces a consistent inference-time shift primarily in mid-to-late layers, and these mid-to-late refinements are both transferable (via merging) and necessary (via freezing) for RL gains. Overall, our results suggest that RL's reliable contribution in visual reasoning is not a uniform enhancement of visual perception, but a systematic refinement of mid-to-late transformer computation that improves vision-to-reasoning alignment and reasoning performance, highlighting the limitations of benchmark-only evaluation for understanding multimodal reasoning improvements.

</details>


### [6] [ZeroDiff++: Substantial Unseen Visual-semantic Correlation in Zero-shot Learning](https://arxiv.org/abs/2602.12401)
*Zihan Ye,Shreyank N Gowda,Kaile Du,Weijian Luo,Ling Shao*

Main category: cs.CV

TL;DR: ZeroDiff++ is a diffusion-based generative framework for Zero-shot Learning (ZSL) that enhances visual-semantic correlations and mitigates spurious correlations by leveraging diffusion augmentation, supervised contrastive representations, and multi-view discriminators. It also introduces test-time adaptation and generation techniques, achieving robust performance with scarce training data.


<details>
  <summary>Details</summary>
Motivation: The paper addresses spurious visual-semantic correlations in existing generative ZSL methods, exacerbated by scarce seen class samples and unadaptive generators, leading to features disconnected from real test samples.

Method: ZeroDiff++ employs diffusion augmentation, supervised contrastive representations, and multi-view discriminators with Wasserstein mutual learning during training. It introduces Diffusion-based Test time Adaptation (DiffTTA) and Diffusion-based Test time Generation (DiffGen) to adapt the generator and produce partially synthesized features.

Result: Extensive experiments on three ZSL benchmarks show ZeroDiff++ significantly outperforms existing methods and maintains robust performance even with scarce training data.

Conclusion: ZeroDiff++ effectively enhances visual-semantic correlations and mitigates spurious correlations in ZSL, demonstrating superior performance and robustness.

Abstract: Zero-shot Learning (ZSL) enables classifiers to recognize classes unseen during training, commonly via generative two stage methods: (1) learn visual semantic correlations from seen classes; (2) synthesize unseen class features from semantics to train classifiers. In this paper, we identify spurious visual semantic correlations in existing generative ZSL worsened by scarce seen class samples and introduce two metrics to quantify spuriousness for seen and unseen classes. Furthermore, we point out a more critical bottleneck: existing unadaptive fully noised generators produce features disconnected from real test samples, which also leads to the spurious correlation. To enhance the visual-semantic correlations on both seen and unseen classes, we propose ZeroDiff++, a diffusion-based generative framework. In training, ZeroDiff++ uses (i) diffusion augmentation to produce diverse noised samples, (ii) supervised contrastive (SC) representations for instance level semantics, and (iii) multi view discriminators with Wasserstein mutual learning to assess generated features. At generation time, we introduce (iv) Diffusion-based Test time Adaptation (DiffTTA) to adapt the generator using pseudo label reconstruction, and (v) Diffusion-based Test time Generation (DiffGen) to trace the diffusion denoising path and produce partially synthesized features that connect real and generated data, and mitigates data scarcity further. Extensive experiments on three ZSL benchmarks demonstrate that ZeroDiff++ not only achieves significant improvements over existing ZSL methods but also maintains robust performance even with scarce training data. Code would be available.

</details>


### [7] [MonoLoss: A Training Objective for Interpretable Monosemantic Representations](https://arxiv.org/abs/2602.12403)
*Ali Nasiri-Sarvi,Anh Tien Nguyen,Hassan Rivaz,Dimitris Samaras,Mahdi S. Hosseini*

Main category: cs.CV

TL;DR: The paper introduces Monosemanticity Loss (MonoLoss), a training objective that improves monosemanticity in sparse autoencoders (SAEs) by efficiently computing MonoScore and enhancing interpretability and performance.


<details>
  <summary>Details</summary>
Motivation: Standard SAE training objectives weakly encourage monosemantic decompositions, and existing metrics are inefficient. The goal is to improve monosemanticity efficiently.

Method: The paper derives a single-pass algorithm for MonoScore and introduces MonoLoss, a plug-in objective to reward semantically consistent activations.

Result: MonoLoss achieves significant speedups (1200x evaluation, 159x training), improves MonoScore and class purity, and boosts accuracy (e.g., 0.6% on ImageNet-1K).

Conclusion: MonoLoss enhances monosemanticity in SAEs efficiently and improves model interpretability and performance across various datasets and architectures.

Abstract: Sparse autoencoders (SAEs) decompose polysemantic neural representations, where neurons respond to multiple unrelated concepts, into monosemantic features that capture single, interpretable concepts. However, standard training objectives only weakly encourage this decomposition, and existing monosemanticity metrics require pairwise comparisons across all dataset samples, making them inefficient during training and evaluation. We study a recent MonoScore metric and derive a single-pass algorithm that computes exactly the same quantity, but with a cost that grows linearly, rather than quadratically, with the number of dataset images. On OpenImagesV7, we achieve up to a 1200x speedup wall-clock speedup in evaluation and 159x during training, while adding only ~4% per-epoch overhead. This allows us to treat MonoScore as a training signal: we introduce the Monosemanticity Loss (MonoLoss), a plug-in objective that directly rewards semantically consistent activations for learning interpretable monosemantic representations. Across SAEs trained on CLIP, SigLIP2, and pretrained ViT features, using BatchTopK, TopK, and JumpReLU SAEs, MonoLoss increases MonoScore for most latents. MonoLoss also consistently improves class purity (the fraction of a latent's activating images belonging to its dominant class) across all encoder and SAE combinations, with the largest gain raising baseline purity from 0.152 to 0.723. Used as an auxiliary regularizer during ResNet-50 and CLIP-ViT-B/32 finetuning, MonoLoss yields up to 0.6\% accuracy gains on ImageNet-1K and monosemantic activating patterns on standard benchmark datasets. The code is publicly available at https://github.com/AtlasAnalyticsLab/MonoLoss.

</details>


### [8] [Prototype-driven fusion of pathology and spatial transcriptomics for interpretable survival prediction](https://arxiv.org/abs/2602.12441)
*Lihe Liu,Xiaoxi Pan,Yinyin Yuan,Lulu Shang*

Main category: cs.CV

TL;DR: PathoSpatial integrates whole slide images (WSIs) and spatial transcriptomics (ST) for interpretable prognostic modeling using multi-level prototypes and cross-modal learning.


<details>
  <summary>Details</summary>
Motivation: Combining complementary spatial signals from WSIs and ST for prognosis lacks principled fusion strategies, especially at scale.

Method: PathoSpatial uses task-guided prototype learning within a multi-level experts architecture to fuse WSIs and ST data.

Result: PathoSpatial outperforms unimodal and multimodal methods on breast cancer survival endpoints and offers interpretable insights.

Conclusion: PathoSpatial demonstrates scalable, interpretable multimodal learning for spatial omics-pathology fusion.

Abstract: Whole slide images (WSIs) enable weakly supervised prognostic modeling via multiple instance learning (MIL). Spatial transcriptomics (ST) preserves in situ gene expression, providing a spatial molecular context that complements morphology. As paired WSI-ST cohorts scale to population level, leveraging their complementary spatial signals for prognosis becomes crucial; however, principled cross-modal fusion strategies remain limited for this paradigm. To this end, we introduce PathoSpatial, an interpretable end-to-end framework integrating co-registered WSIs and ST to learn spatially informed prognostic representations. PathoSpatial uses task-guided prototype learning within a multi-level experts architecture, adaptively orchestrating unsupervised within-modality discovery with supervised cross-modal aggregation. By design, PathoSpatial substantially strengthens interpretability while maintaining discriminative ability. We evaluate PathoSpatial on a triple-negative breast cancer cohort with paired ST and WSIs. PathoSpatial delivers strong and consistent performance across five survival endpoints, achieving superior or comparable performance to leading unimodal and multimodal methods. PathoSpatial inherently enables post-hoc prototype interpretation and molecular risk decomposition, providing quantitative, biologically grounded explanations, highlighting candidate prognostic factors. We present PathoSpatial as a proof-of-concept for scalable and interpretable multimodal learning for spatial omics-pathology fusion.

</details>


### [9] [Semantic-aware Adversarial Fine-tuning for CLIP](https://arxiv.org/abs/2602.12461)
*Jiacheng Zhang,Jinhao Li,Hanxun Huang,Sarah M. Erfani,Benjamin I. P. Rubinstein,Feng Liu*

Main category: cs.CV

TL;DR: SAFT improves CLIP's adversarial robustness by generating semantic-aware adversarial examples using refined textual descriptions, outperforming existing methods across 16 datasets.


<details>
  <summary>Details</summary>
Motivation: Current adversarial fine-tuning methods for CLIP rely on cosine similarity with hand-crafted templates, which may not capture semantic richness, leading to less robust models.

Method: Proposes Semantic-ensemble Attack to generate semantic-aware adversarial examples by minimizing similarity between images and refined textual descriptions. Introduces SAFT for fine-tuning CLIP's image encoder with these examples.

Result: SAFT significantly enhances zero-shot adversarial robustness, outperforming existing methods across 16 datasets.

Conclusion: SAFT addresses limitations of cosine similarity-based adversarial examples, demonstrating superior robustness through semantic-aware fine-tuning.

Abstract: Recent studies have shown that CLIP model's adversarial robustness in zero-shot classification tasks can be enhanced by adversarially fine-tuning its image encoder with adversarial examples (AEs), which are generated by minimizing the cosine similarity between images and a hand-crafted template (e.g., ''A photo of a {label}''). However, it has been shown that the cosine similarity between a single image and a single hand-crafted template is insufficient to measure the similarity for image-text pairs. Building on this, in this paper, we find that the AEs generated using cosine similarity may fail to fool CLIP when the similarity metric is replaced with semantically enriched alternatives, making the image encoder fine-tuned with these AEs less robust. To overcome this issue, we first propose a semantic-ensemble attack to generate semantic-aware AEs by minimizing the average similarity between the original image and an ensemble of refined textual descriptions. These descriptions are initially generated by a foundation model to capture core semantic features beyond hand-crafted templates and are then refined to reduce hallucinations. To this end, we propose Semantic-aware Adversarial Fine-Tuning (SAFT), which fine-tunes CLIP's image encoder with semantic-aware AEs. Extensive experiments show that SAFT outperforms current methods, achieving substantial improvements in zero-shot adversarial robustness across 16 datasets. Our code is available at: https://github.com/tmlr-group/SAFT.

</details>


### [10] [A Lightweight and Explainable DenseNet-121 Framework for Grape Leaf Disease Classification](https://arxiv.org/abs/2602.12484)
*Md. Ehsanul Haque,Md. Saymon Hosen Polash,Rakib Hasan Ovi,Aminul Kader Bulbul,Md Kamrul Siam,Tamim Hasan Saykat*

Main category: cs.CV

TL;DR: The study proposes an optimized DenseNet 121 model for precise and interpretable grape leaf disease classification, outperforming baseline CNNs with high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Grape diseases significantly impact production and quality, but current automated methods are computationally costly and lack interpretability.

Method: Uses Optimized DenseNet 121 with domain-specific preprocessing and Grad-CAM for interpretability, comparing it with ResNet18, VGG16, AlexNet, and SqueezeNet.

Result: Achieves 99.27% accuracy, 99.28% F1 score, 99.71% specificity, and 98.86% Kappa with 9-second inference time. Cross-validation shows 99.12% mean accuracy.

Conclusion: The framework is scalable, precise, and computationally inexpensive, suitable for real-time grape leaf disease detection.

Abstract: Grapes are among the most economically and culturally significant fruits on a global scale, and table grapes and wine are produced in significant quantities in Europe and Asia. The production and quality of grapes are significantly impacted by grape diseases such as Bacterial Rot, Downy Mildew, and Powdery Mildew. Consequently, the sustainable management of a vineyard necessitates the early and precise identification of these diseases. Current automated methods, particularly those that are based on the YOLO framework, are often computationally costly and lack interpretability that makes them unsuitable for real-world scenarios. This study proposes grape leaf disease classification using Optimized DenseNet 121. Domain-specific preprocessing and extensive connectivity reveal disease-relevant characteristics, including veins, edges, and lesions. An extensive comparison with baseline CNN models, including ResNet18, VGG16, AlexNet, and SqueezeNet, demonstrates that the proposed model exhibits superior performance. It achieves an accuracy of 99.27%, an F1 score of 99.28%, a specificity of 99.71%, and a Kappa of 98.86%, with an inference time of 9 seconds. The cross-validation findings show a mean accuracy of 99.12%, indicating strength and generalizability across all classes. We also employ Grad-CAM to highlight disease-related regions to guarantee the model is highlighting physiologically relevant aspects and increase transparency and confidence. Model optimization reduces processing requirements for real-time deployment, while transfer learning ensures consistency on smaller and unbalanced samples. An effective architecture, domain-specific preprocessing, and interpretable outputs make the proposed framework scalable, precise, and computationally inexpensive for detecting grape leaf diseases.

</details>


### [11] [Human-Like Coarse Object Representations in Vision Models](https://arxiv.org/abs/2602.12486)
*Andrey Gizdov,Andrea Procopio,Yichen Li,Daniel Harari,Tomer Ullman*

Main category: cs.CV

TL;DR: The paper explores whether segmentation models acquire human-like coarse, volumetric object representations for intuitive physics. It finds that alignment with human behavior peaks at an intermediate model granularity.


<details>
  <summary>Details</summary>
Motivation: Humans use coarse, volumetric representations for intuitive physics, while segmentation models focus on pixel-accurate masks. The study investigates if and when these models align with human-like representations.

Method: The study uses a time-to-collision (TTC) behavioral paradigm, introduces a comparison pipeline and alignment metric, and varies model training time, size, and capacity via pruning.

Result: Alignment with human behavior follows an inverse U-shaped curve: small or briefly trained models under-segment, large or fully trained models over-segment, and intermediate models best match human behavior.

Conclusion: Human-like coarse representations emerge from resource constraints, not bespoke biases. Early checkpoints, modest architectures, or light pruning can elicit physics-efficient representations.

Abstract: Humans appear to represent objects for intuitive physics with coarse, volumetric bodies'' that smooth concavities - trading fine visual details for efficient physical predictions - yet their internal structure is largely unknown. Segmentation models, in contrast, optimize pixel-accurate masks that may misalign with such bodies. We ask whether and when these models nonetheless acquire human-like bodies. Using a time-to-collision (TTC) behavioral paradigm, we introduce a comparison pipeline and alignment metric, then vary model training time, size, and effective capacity via pruning. Across all manipulations, alignment with human behavior follows an inverse U-shaped curve: small/briefly trained/pruned models under-segment into blobs; large/fully trained models over-segment with boundary wiggles; and an intermediate ideal body granularity'' best matches humans. This suggests human-like coarse bodies emerge from resource constraints rather than bespoke biases, and points to simple knobs - early checkpoints, modest architectures, light pruning - for eliciting physics-efficient representations. We situate these results within resource-rational accounts balancing recognition detail against physical affordances.

</details>


### [12] [Insertion Network for Image Sequence Correspondence](https://arxiv.org/abs/2602.12489)
*Dingjie Su,Weixiang Hong,Benoit M. Dawant,Bennett A. Landman*

Main category: cs.CV

TL;DR: A novel method for matching 2D image sequences improves slice-level navigation in 3D volumes by leveraging contextual info and attention, reducing localization errors from 8.4mm to 5.4mm.


<details>
  <summary>Details</summary>
Motivation: To enhance slice-level navigation in 3D volumes for applications like diagnostics and registration by addressing limitations of independent slice analysis in current methods.

Method: Trains a network to insert slices into sequences using contextual encoding and a slice-to-slice attention mechanism, avoiding independent predictions.

Result: Reduces slice localization errors from 8.4 mm to 5.4 mm compared to state-of-the-art body part regression.

Conclusion: The method outperforms existing techniques by utilizing sequence context, offering significant accuracy improvements.

Abstract: We propose a novel method for establishing correspondence between two sequences of 2D images. One particular application of this technique is slice-level content navigation, where the goal is to localize specific 2D slices within a 3D volume or determine the anatomical coverage of a 3D scan based on its 2D slices. This serves as an important preprocessing step for various diagnostic tasks, as well as for automatic registration and segmentation pipelines. Our approach builds sequence correspondence by training a network to learn how to insert a slice from one sequence into the appropriate position in another. This is achieved by encoding contextual representations of each slice and modeling the insertion process using a slice-to-slice attention mechanism. We apply this method to localize manually labeled key slices in body CT scans and compare its performance to the current state-of-the-art alternative known as body part regression, which predicts anatomical position scores for individual slices. Unlike body part regression, which treats each slice independently, our method leverages contextual information from the entire sequence. Experimental results show that the insertion network reduces slice localization errors in supervised settings from 8.4 mm to 5.4 mm, demonstrating a substantial improvement in accuracy.

</details>


### [13] [Layer-Specific Fine-Tuning for Improved Negation Handling in Medical Vision-Language Models](https://arxiv.org/abs/2602.12498)
*Ali Abbasi,Mehdi Taghipour,Rahmatollah Beheshti*

Main category: cs.CV

TL;DR: The paper identifies VLMs' difficulty in distinguishing negated medical statements and introduces a benchmark, dataset, and method (NAST) to improve negation sensitivity without harming general performance.


<details>
  <summary>Details</summary>
Motivation: VLMs often fail to differentiate affirmative from negated clinical statements, posing risks in medical contexts.

Method: Developed a radiology benchmark and dataset for negation. Proposed NAST, using causal tracing to modulate gradient updates per layer's contribution to negation processing.

Result: NAST improved negation discrimination without degrading general vision-language alignment.

Conclusion: Causal interpretability aids targeted model adaptation in medical VLMs, enhancing negation sensitivity safely.

Abstract: Negation is a fundamental linguistic operation in clinical reporting, yet vision-language models (VLMs) frequently fail to distinguish affirmative from negated medical statements. To systematically characterize this limitation, we introduce a radiology-specific diagnostic benchmark that evaluates polarity sensitivity under controlled clinical conditions, revealing that common medical VLMs consistently confuse negated and non-negated findings. To enable learning beyond simple condition absence, we further construct a contextual clinical negation dataset that encodes structured claims and supports attribute-level negations involving location and severity. Building on these resources, we propose Negation-Aware Selective Training (NAST), an interpretability-guided adaptation method that uses causal tracing effects (CTEs) to modulate layer-wise gradient updates during fine-tuning. Rather than applying uniform learning rates, NAST scales each layer's update according to its causal contribution to negation processing, transforming mechanistic interpretability signals into a principled optimization rule. Experiments demonstrate improved discrimination of affirmative and negated clinical statements without degrading general vision-language alignment, highlighting the value of causal interpretability for targeted model adaptation in safety-critical medical settings. Code and resources are available at https://github.com/healthylaife/NAST.

</details>


### [14] [Matching of SAR and optical images based on transformation to shared modality](https://arxiv.org/abs/2602.12515)
*Alexey Borisov,Evgeny Myasnikov,Vladislav Myasnikov*

Main category: cs.CV

TL;DR: A new method for matching optical and SAR images by transforming them into a shared modality, enabling the use of pre-trained models like RoMa without retraining, outperforming existing approaches.


<details>
  <summary>Details</summary>
Motivation: Precise co-registration of optical and SAR images is challenging due to their differing physical principles, necessitating a novel matching approach.

Method: Transform images to a common modality with equal channels, similarity, and non-degeneracy, then apply the RoMa model for matching.

Result: Evaluation on the MultiSenGE dataset shows superior matching quality and versatility over traditional methods.

Conclusion: The proposed approach improves matching quality and leverages pre-trained models effectively for optical-SAR image registration.

Abstract: Significant differences in optical images and Synthetic Aperture Radar (SAR) images are caused by fundamental differences in the physical principles underlying their acquisition by Earth remote sensing platforms. These differences make precise image matching (co-registration) of these two types of images difficult. In this paper, we propose a new approach to image matching of optical and SAR images, which is based on transforming the images to a new modality. The new image modality is common to both optical and SAR images and satisfies the following conditions. First, the transformed images must have an equal pre-defined number of channels. Second, the transformed and co-registered images must be as similar as possible. Third, the transformed images must be non-degenerate, meaning they must preserve the significant features of the original images. To further match images transformed to this shared modality, we train the RoMa image matching model, which is one of the leading solutions for matching of regular digital photographs. We evaluated the proposed approach on the publicly available MultiSenGE dataset containing both optical and SAR images. We demonstrated its superiority over alternative approaches based on image translation between original modalities and various feature matching algorithms. The proposed solution not only provides better quality of matching, but is also more versatile. It enables the use of ready-made RoMa and DeDoDe models, pre-trained for regular images, without retraining for a new modality, while maintaining high-quality matching of optical and SAR images.

</details>


### [15] [LiDAR-Anchored Collaborative Distillation for Robust 2D Representations](https://arxiv.org/abs/2602.12524)
*Wonjun Jo,Hyunwoo Ha,Kim Ji-Yeon,Hawook Jeong,Tae-Hyun Oh*

Main category: cs.CV

TL;DR: A novel self-supervised method, Collaborative Distillation, uses 3D LiDAR to enhance 2D image encoders' robustness in adverse conditions while maintaining their original capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing 2D image encoders struggle with noisy and adverse weather conditions, limiting their practical use in robust visual perception tasks.

Method: Collaborative Distillation leverages 3D LiDAR as self-supervision to improve 2D encoders, enhancing robustness and retaining original performance.

Result: Outperforms competitors in diverse downstream tasks, shows strong generalization, and improves 3D awareness.

Conclusion: The method is practical and adaptable for real-world scenarios, addressing limitations of current 2D encoders.

Abstract: As deep learning continues to advance, self-supervised learning has made considerable strides. It allows 2D image encoders to extract useful features for various downstream tasks, including those related to vision-based systems. Nevertheless, pre-trained 2D image encoders fall short in conducting the task under noisy and adverse weather conditions beyond clear daytime scenes, which require for robust visual perception. To address these issues, we propose a novel self-supervised approach, \textbf{Collaborative Distillation}, which leverages 3D LiDAR as self-supervision to improve robustness to noisy and adverse weather conditions in 2D image encoders while retaining their original capabilities. Our method outperforms competing methods in various downstream tasks across diverse conditions and exhibits strong generalization ability. In addition, our method also improves 3D awareness stemming from LiDAR's characteristics. This advancement highlights our method's practicality and adaptability in real-world scenarios.

</details>


### [16] [Self-Supervised JEPA-based World Models for LiDAR Occupancy Completion and Forecasting](https://arxiv.org/abs/2602.12540)
*Haoran Zhu,Anna Choromanska*

Main category: cs.CV

TL;DR: AD-LiST-JEPA is a self-supervised world model for autonomous driving using LiDAR data, improving occupancy completion and forecasting tasks.


<details>
  <summary>Details</summary>
Motivation: Autonomous driving needs scalable world models for long-term planning, achievable via self-supervised learning without human annotations.

Method: Proposes AD-LiST-JEPA, leveraging JEPA framework to predict spatiotemporal evolution from LiDAR data.

Result: Experiments show improved performance in occupancy completion and forecasting with pretrained encoders.

Conclusion: AD-LiST-JEPA demonstrates effectiveness in self-supervised world model learning for autonomous driving.

Abstract: Autonomous driving, as an agent operating in the physical world, requires the fundamental capability to build \textit{world models} that capture how the environment evolves spatiotemporally in order to support long-term planning. At the same time, scalability demands learning such models in a self-supervised manner; \textit{joint-embedding predictive architecture (JEPA)} enables learning world models via leveraging large volumes of unlabeled data without relying on expensive human annotations. In this paper, we propose \textbf{AD-LiST-JEPA}, a self-supervised world model for autonomous driving that predicts future spatiotemporal evolution from LiDAR data using a JEPA framework. We evaluate the quality of the learned representations through a downstream LiDAR-based occupancy completion and forecasting (OCF) task, which jointly assesses perception and prediction. Proof of concept experiments show better OCF performance with pretrained encoder after JEPA-based world model learning.

</details>


### [17] [PLLM: Pseudo-Labeling Large Language Models for CAD Program Synthesis](https://arxiv.org/abs/2602.12561)
*Yuanbo Li,Dule Shu,Yanying Chen,Matt Klenk,Daniel Ritchie*

Main category: cs.CV

TL;DR: PLLM is a self-training framework for CAD program synthesis from unlabeled 3D shapes, improving geometric fidelity and program diversity without paired shape-program data.


<details>
  <summary>Details</summary>
Motivation: Existing CAD program synthesis methods rely on supervised training with paired data, which is often unavailable.

Method: PLLM iteratively samples candidate programs, selects high-fidelity executions, and augments programs to create synthetic program-shape pairs for fine-tuning.

Result: The framework shows consistent improvements in geometric fidelity and program diversity when tested on the ABC dataset.

Conclusion: PLLM successfully addresses the lack of paired data by leveraging self-training, enabling better CAD program synthesis.

Abstract: Recovering Computer-Aided Design (CAD) programs from 3D geometries is a widely studied problem. Recent advances in large language models (LLMs) have enabled progress in CAD program synthesis, but existing methods rely on supervised training with paired shape-program data, which is often unavailable. We introduce PLLM, a self-training framework for CAD program synthesis from unlabeled 3D shapes. Given a pre-trained CAD-capable LLM and a shape dataset, PLLM iteratively samples candidate programs, selects high-fidelity executions, and augments programs to construct synthetic program-shape pairs for fine-tuning. We experiment on adapting CAD-Recode from DeepCAD to the unlabeled ABC dataset show consistent improvements in geometric fidelity and program diversity.

</details>


### [18] [The Constant Eye: Benchmarking and Bridging Appearance Robustness in Autonomous Driving](https://arxiv.org/abs/2602.12563)
*Jiabao Wang,Hongyu Zhou,Yuanbo Yang,Jiahao Shao,Yiyi Liao*

Main category: cs.CV

TL;DR: The paper addresses the fragility of autonomous driving algorithms under OOD conditions by distinguishing appearance-based shifts from structural changes. It introduces navdream, a benchmark isolating appearance impacts, and proposes a universal perception interface using DINOv3 for robust performance across planning paradigms.


<details>
  <summary>Details</summary>
Motivation: Current autonomous driving algorithms struggle with OOD conditions, but it's unclear whether failures stem from appearance shifts (e.g., weather) or structural changes. The paper aims to decouple these factors and improve robustness.

Method: The authors develop navdream, a benchmark using generative pixel-aligned style transfer to isolate appearance impacts. They propose a universal perception interface leveraging DINOv3 for appearance-invariant features, compatible with various planning models.

Result: Experiments show significant degradation of existing planners under OOD appearance conditions. The proposed interface maintains robust performance across extreme appearance shifts without fine-tuning.

Conclusion: By decoupling appearance and structural factors, the paper provides a benchmark and a plug-and-play solution (DINOv3-based interface) to enhance autonomous driving robustness under OOD conditions.

Abstract: Despite rapid progress, autonomous driving algorithms remain notoriously fragile under Out-of-Distribution (OOD) conditions. We identify a critical decoupling failure in current research: the lack of distinction between appearance-based shifts, such as weather and lighting, and structural scene changes. This leaves a fundamental question unanswered: Is the planner failing because of complex road geometry, or simply because it is raining? To resolve this, we establish navdream, a high-fidelity robustness benchmark leveraging generative pixel-aligned style transfer. By creating a visual stress test with negligible geometric deviation, we isolate the impact of appearance on driving performance. Our evaluation reveals that existing planning algorithms often show significant degradation under OOD appearance conditions, even when the underlying scene structure remains consistent. To bridge this gap, we propose a universal perception interface leveraging a frozen visual foundation model (DINOv3). By extracting appearance-invariant features as a stable interface for the planner, we achieve exceptional zero-shot generalization across diverse planning paradigms, including regression-based, diffusion-based, and scoring-based models. Our plug-and-play solution maintains consistent performance across extreme appearance shifts without requiring further fine-tuning. The benchmark and code will be made available.

</details>


### [19] [Language-Guided Invariance Probing of Vision-Language Models](https://arxiv.org/abs/2511.13494)
*Jae Joong Lee*

Main category: cs.CV

TL;DR: LGIP benchmark evaluates VLMs' reliability to linguistic perturbations, revealing strengths and weaknesses unseen by standard metrics.


<details>
  <summary>Details</summary>
Motivation: Assess VLMs' robustness to linguistic changes (paraphrases and semantic flips) for better understanding their real-world reliability.

Method: Introduce LGIP benchmark using 40k MS COCO images with paraphrased/flipped captions, measuring invariance and sensitivity.

Result: EVA02-CLIP and OpenCLIP perform well, while SigLIP models show higher errors and preference for flipped captions.

Conclusion: LGIP offers a critical diagnostic tool for VLMs' linguistic robustness beyond traditional accuracy measures.

Abstract: Recent vision-language models (VLMs) such as CLIP, OpenCLIP, EVA02-CLIP and SigLIP achieve strong zero-shot performance, but it is unclear how reliably they respond to controlled linguistic perturbations. We introduce Language-Guided Invariance Probing (LGIP), a benchmark that measures (i) invariance to meaning-preserving paraphrases and (ii) sensitivity to meaning-changing semantic flips in image-text matching. Using 40k MS COCO images with five human captions each, we automatically generate paraphrases and rule-based flips that alter object category, color or count, and summarize model behavior with an invariance error, a semantic sensitivity gap and a positive-rate statistic.
  Across nine VLMs, EVA02-CLIP and large OpenCLIP variants lie on a favorable invariance-sensitivity frontier, combining low paraphrase-induced variance with consistently higher scores for original captions than for their flipped counterparts. In contrast, SigLIP and SigLIP2 show much larger invariance error and often prefer flipped captions to the human descriptions, especially for object and color edits. These failures are largely invisible to standard retrieval metrics, indicating that LGIP provides a model-agnostic diagnostic for the linguistic robustness of VLMs beyond conventional accuracy scores.

</details>


### [20] [Unbiased Gradient Estimation for Event Binning via Functional Backpropagation](https://arxiv.org/abs/2602.12590)
*Jinze Chen,Wei Zhai,Han Han,Tiankai Ma,Yang Cao,Bin Li,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: The paper introduces a framework for unbiased gradient estimation in event-based vision by synthesizing weak derivatives during backpropagation, improving learning efficiency and performance on tasks like egomotion estimation and SLAM.


<details>
  <summary>Details</summary>
Motivation: Current methods for processing event-based vision either rely on discontinuous binning functions (limiting gradients) or suffer from biased gradient estimation, reducing learning efficiency.

Method: The proposed framework uses integration by parts to lift target functions to functionals, enabling unbiased gradient estimation via weak derivatives while maintaining forward output.

Result: The method achieves 3.2% lower RMS error in egomotion estimation, 9.4% lower EPE in optical flow, and 5.1% lower RMS error in SLAM, with faster convergence.

Conclusion: The framework effectively addresses gradient estimation challenges in event-based vision, enhancing performance across multiple tasks.

Abstract: Event-based vision encodes dynamic scenes as asynchronous spatio-temporal spikes called events. To leverage conventional image processing pipelines, events are typically binned into frames. However, binning functions are discontinuous, which truncates gradients at the frame level and forces most event-based algorithms to rely solely on frame-based features. Attempts to directly learn from raw events avoid this restriction but instead suffer from biased gradient estimation due to the discontinuities of the binning operation, ultimately limiting their learning efficiency. To address this challenge, we propose a novel framework for unbiased gradient estimation of arbitrary binning functions by synthesizing weak derivatives during backpropagation while keeping the forward output unchanged. The key idea is to exploit integration by parts: lifting the target functions to functionals yields an integral form of the derivative of the binning function during backpropagation, where the cotangent function naturally arises. By reconstructing this cotangent function from the sampled cotangent vector, we compute weak derivatives that provably match long-range finite differences of both smooth and non-smooth targets. Experimentally, our method improves simple optimization-based egomotion estimation with 3.2\% lower RMS error and 1.57$\times$ faster convergence. On complex downstream tasks, we achieve 9.4\% lower EPE in self-supervised optical flow, and 5.1\% lower RMS error in SLAM, demonstrating broad benefits for event-based visual perception. Source code can be found at https://github.com/chjz1024/EventFBP.

</details>


### [21] [QuEPT: Quantized Elastic Precision Transformers with One-Shot Calibration for Multi-Bit Switching](https://arxiv.org/abs/2602.12609)
*Ke Xu,Yixin Wang,Zhongcheng Li,Hao Cui,Jinshui Hu,Xingyi Zhang*

Main category: cs.CV

TL;DR: QuEPT proposes an efficient post-training quantization method for large language models, enabling dynamic adaptation to various bit-widths without repeated optimization.


<details>
  <summary>Details</summary>
Motivation: The need for efficient quantization methods for large language models, given the high storage and optimization costs of the Transformer architecture, especially in elastic precision quantization scenarios.

Method: QuEPT uses block-wise multi-bit error reconstruction with one-shot calibration, cascading low-rank adapters for dynamic bit-width adaptation, and introduces MB-ToMe for token feature fusion and MB-CLoRA for bit-width correlation.

Result: QuEPT achieves comparable or better performance than state-of-the-art post-training quantization methods.

Conclusion: The proposed QuEPT method effectively addresses the challenges of elastic precision quantization for large language models, offering dynamic adaptation and improved performance.

Abstract: Elastic precision quantization enables multi-bit deployment via a single optimization pass, fitting diverse quantization scenarios.Yet, the high storage and optimization costs associated with the Transformer architecture, research on elastic quantization remains limited, particularly for large language models.This paper proposes QuEPT, an efficient post-training scheme that reconstructs block-wise multi-bit errors with one-shot calibration on a small data slice. It can dynamically adapt to various predefined bit-widths by cascading different low-rank adapters, and supports real-time switching between uniform quantization and mixed precision quantization without repeated optimization. To enhance accuracy and robustness, we introduce Multi-Bit Token Merging (MB-ToMe) to dynamically fuse token features across different bit-widths, improving robustness during bit-width switching. Additionally, we propose Multi-Bit Cascaded Low-Rank adapters (MB-CLoRA) to strengthen correlations between bit-width groups, further improve the overall performance of QuEPT. Extensive experiments demonstrate that QuEPT achieves comparable or better performance to existing state-of-the-art post-training quantization methods.Our code is available at https://github.com/xuke225/QuEPT

</details>


### [22] [Vision Token Reduction via Attention-Driven Self-Compression for Efficient Multimodal Large Language Models](https://arxiv.org/abs/2602.12618)
*Omer Faruk Deniz,Ruiyu Mao,Ruochen Li,Yapeng Tian,Latifur Khan*

Main category: cs.CV

TL;DR: ADSC reduces FLOPs and KV-cache memory significantly while preserving model performance by leveraging LLM's attention for token compression.


<details>
  <summary>Details</summary>
Motivation: Existing pruning methods are limited by diverse designs or heuristic incompatibilities with FlashAttention, prompting a need for a more efficient and general approach.

Method: ADSC progressively reduces vision tokens using the LLM's attention mechanism, applying uniform downsampling at selected layers without auxiliary modules or attention modification.

Result: Achieves 53.7% reduction in FLOPs and 56.7% in peak KV-cache memory while maintaining 98.2% of original performance, outperforming prior methods.

Conclusion: ADSC is robust, efficient, and broadly applicable, offering superior performance under high compression ratios compared to heuristic-based techniques.

Abstract: Multimodal Large Language Models (MLLMs) incur significant computational cost from processing numerous vision tokens through all LLM layers. Prior pruning methods operate either before the LLM, limiting generality due to diverse encoder-projector designs or within the LLM using heuristics that are incompatible with FlashAttention. We take a different approach: rather than identifying unimportant tokens, we treat the LLM itself as the optimal guide for compression. Observing that deeper layers naturally transmit vision-to-text information, we introduce Attention-Driven Self-Compression (ADSC), a simple, broadly applicable method that progressively reduces vision tokens using only the LLM's attention mechanism. Our method applies uniform token downsampling at selected layers, forming bottlenecks that encourage the model to reorganize and compress information into the remaining tokens. It requires no score computation, auxiliary modules, or attention modification, and remains fully compatible with FlashAttention. Applied to LLaVA-1.5, ADSC reduces FLOPs by 53.7% and peak KV-cache memory by 56.7%, while preserving 98.2% of the original model performance. Across multiple benchmarks, it outperforms prior pruning approaches in both efficiency and accuracy. Crucially, under high compression ratios, our method remains robust while heuristic-based techniques degrade sharply.

</details>


### [23] [ImageRAGTurbo: Towards One-step Text-to-Image Generation with Retrieval-Augmented Diffusion Models](https://arxiv.org/abs/2602.12640)
*Peijie Qiu,Hariharan Ramshankar,Arnau Ramisa,René Vidal,Amit Kumar K C,Vamsi Salaka,Rahul Bhagat*

Main category: cs.CV

TL;DR: ImageRAGTurbo enhances few-step diffusion models for text-to-image generation by using retrieval-augmented finetuning, improving speed and quality without additional latency.


<details>
  <summary>Details</summary>
Motivation: Current few-step diffusion models reduce sampling steps but often compromise image quality and prompt alignment, while requiring expensive training. ImageRAGTurbo aims to address these limitations.

Method: The approach retrieves relevant text-image pairs to condition generation, uses retrieved content to edit the UNet denoiser's latent space, and adds a trainable adapter for better blending.

Result: The method achieves high-fidelity images with reduced steps, improving prompt fidelity and maintaining low latency compared to existing approaches.

Conclusion: ImageRAGTurbo effectively balances speed and quality in text-to-image generation, leveraging retrieval augmentation and latent space adaptation.

Abstract: Diffusion models have emerged as the leading approach for text-to-image generation. However, their iterative sampling process, which gradually morphs random noise into coherent images, introduces significant latency that limits their applicability. While recent few-step diffusion models reduce the number of sampling steps to as few as one to four steps, they often compromise image quality and prompt alignment, especially in one-step generation. Additionally, these models require computationally expensive training procedures. To address these limitations, we propose ImageRAGTurbo, a novel approach to efficiently finetune few-step diffusion models via retrieval augmentation. Given a text prompt, we retrieve relevant text-image pairs from a database and use them to condition the generation process. We argue that such retrieved examples provide rich contextual information to the UNet denoiser that helps reduce the number of denoising steps without compromising image quality. Indeed, our initial investigations show that using the retrieved content to edit the denoiser's latent space ($\mathcal{H}$-space) without additional finetuning already improves prompt fidelity. To further improve the quality of the generated images, we augment the UNet denoiser with a trainable adapter in the $\mathcal{H}$-space, which efficiently blends the retrieved content with the target prompt using a cross-attention mechanism. Experimental results on fast text-to-image generation demonstrate that our approach produces high-fidelity images without compromising latency compared to existing methods.

</details>


### [24] [Multi-Task Learning with Additive U-Net for Image Denoising and Classification](https://arxiv.org/abs/2602.12649)
*Vikram Lakkavalli,Neelam Sinha*

Main category: cs.CV

TL;DR: Additive U-Net (AddUNet) replaces concatenative skips with gated additive fusion in U-Nets for image denoising and MTL, improving training stability and task-aware feature redistribution without added complexity.


<details>
  <summary>Details</summary>
Motivation: To enhance U-Net architectures for image denoising and MTL by stabilizing joint optimization and improving task-aware feature flow without increasing model size.

Method: Introduces AddUNet, which uses gated additive fusion for skip connections instead of concatenation, regularizing encoder-decoder information flow.

Result: Competitive denoising performance, robust MTL outcomes, and systematic task-aware skip weight redistribution (shallow skips favor reconstruction, deeper ones support discrimination).

Conclusion: Additive skip fusion effectively regularizes U-Nets for stable MTL, demonstrating implicit task decoupling and scalability without added complexity.

Abstract: We investigate additive skip fusion in U-Net architectures for image denoising and denoising-centric multi-task learning (MTL). By replacing concatenative skips with gated additive fusion, the proposed Additive U-Net (AddUNet) constrains shortcut capacity while preserving fixed feature dimensionality across depth. This structural regularization induces controlled encoder-decoder information flow and stabilizes joint optimization. Across single-task denoising and joint denoising-classification settings, AddUNet achieves competitive reconstruction performance with improved training stability. In MTL, learned skip weights exhibit systematic task-aware redistribution: shallow skips favor reconstruction, while deeper features support discrimination. Notably, reconstruction remains robust even under limited classification capacity, indicating implicit task decoupling through additive fusion. These findings show that simple constraints on skip connections act as an effective architectural regularizer for stable and scalable multi-task learning without increasing model complexity.

</details>


### [25] [CBEN -- A Multimodal Machine Learning Dataset for Cloud Robust Remote Sensing Image Understanding](https://arxiv.org/abs/2602.12652)
*Marco Stricker,Masakazu Iwamura,Koichi Kise*

Main category: cs.CV

TL;DR: The paper discusses the challenge of cloudy optical satellite imagery in remote sensing, proposing cloud-robust methods by combining optical and radar data. It introduces CloudyBigEarthNet (CBEN) to address the exclusion of cloudy images in training, showing performance drops in existing methods and improvements when adapted.


<details>
  <summary>Details</summary>
Motivation: Clouds distort optical satellite imagery, limiting time-sensitive applications like disaster response. Existing cloud removal methods are flawed, so the paper aims to develop cloud-robust techniques using combined optical and radar data.

Method: The authors assembled CBEN, a dataset of paired optical and radar images with cloud occlusion, to train and evaluate methods. They tested state-of-the-art approaches on cloudy images and adapted them for cloudy optical data.

Result: State-of-the-art methods trained on clear-sky data showed performance drops of 23-33 percentage points on cloudy images. Adapting these methods improved performance by 17.2-28.7 percentage points on cloudy test cases.

Conclusion: Combining optical and radar data and including cloudy scenarios in training enhances cloud robustness. CBEN provides a valuable resource for developing and evaluating such methods.

Abstract: Clouds are a common phenomenon that distorts optical satellite imagery, which poses a challenge for remote sensing. However, in the literature cloudless analysis is often performed where cloudy images are excluded from machine learning datasets and methods. Such an approach cannot be applied to time sensitive applications, e.g., during natural disasters. A possible solution is to apply cloud removal as a preprocessing step to ensure that cloudfree solutions are not failing under such conditions. But cloud removal methods are still actively researched and suffer from drawbacks, such as generated visual artifacts. Therefore, it is desirable to develop cloud robust methods that are less affected by cloudy weather. Cloud robust methods can be achieved by combining optical data with radar, a modality unaffected by clouds. While many datasets for machine learning combine optical and radar data, most researchers exclude cloudy images. We identify this exclusion from machine learning training and evaluation as a limitation that reduces applicability to cloudy scenarios. To investigate this, we assembled a dataset, named CloudyBigEarthNet (CBEN), of paired optical and radar images with cloud occlusion for training and evaluation. Using average precision (AP) as the evaluation metric, we show that state-of-the-art methods trained on combined clear-sky optical and radar imagery suffer performance drops of 23-33 percentage points when evaluated on cloudy images. We then adapt these methods to cloudy optical data during training, achieving relative improvement of 17.2-28.7 percentage points on cloudy test cases compared with the original approaches. Code and dataset are publicly available at: https://github.com/mstricker13/CBEN

</details>


### [26] [IndicFairFace: Balanced Indian Face Dataset for Auditing and Mitigating Geographical Bias in Vision-Language Models](https://arxiv.org/abs/2602.12659)
*Aarish Shah Mohsin,Mohammed Tayyab Ilyas Khan,Mohammad Nadeem,Shahab Saquib Sohail,Erik Cambria,Jiechao Gao*

Main category: cs.CV

TL;DR: IndicFairFace addresses intra-national geographical bias in VLMs for India by introducing a balanced dataset and reducing bias without significantly harming performance.


<details>
  <summary>Details</summary>
Motivation: Current fairness-aware datasets oversimplify Indian diversity, ignoring its vast geographical and representational variations.

Method: IndicFairFace includes 14,400 images balanced across India's states and genders, sourced ethically. Debiasing is done via Iterative Nullspace Projection.

Result: Debiasing reduces geographical bias in VLMs with minimal impact (<1.5% drop) on retrieval accuracy.

Conclusion: IndicFairFace sets a benchmark for studying geographical bias in VLMs for India, addressing prior oversimplifications.

Abstract: Vision-Language Models (VLMs) are known to inherit and amplify societal biases from their web-scale training data with Indian being particularly misrepresented. Existing fairness-aware datasets have significantly improved demographic balance across global race and gender groups, yet they continue to treat Indian as a single monolithic category. The oversimplification ignores the vast intra-national diversity across 28 states and 8 Union Territories of India and leads to representational and geographical bias. To address the limitation, we present IndicFairFace, a novel and balanced face dataset comprising 14,400 images representing geographical diversity of India. Images were sourced ethically from Wikimedia Commons and open-license web repositories and uniformly balanced across states and gender. Using IndicFairFace, we quantify intra-national geographical bias in prominent CLIP-based VLMs and reduce it using post-hoc Iterative Nullspace Projection debiasing approach. We also show that the adopted debiasing approach does not adversely impact the existing embedding space as the average drop in retrieval accuracy on benchmark datasets is less than 1.5 percent. Our work establishes IndicFairFace as the first benchmark to study geographical bias in VLMs for the Indian context.

</details>


### [27] [Motion Prior Distillation in Time Reversal Sampling for Generative Inbetweening](https://arxiv.org/abs/2602.12679)
*Wooseok Jeon,Seunghyun Shin,Dongmin Shin,Hae-Gon Jeon*

Main category: cs.CV

TL;DR: The paper introduces Motion Prior Distillation (MPD), a technique to improve image-to-video diffusion models by addressing temporal discontinuities and visual artifacts caused by misaligned forward and backward paths during inference.


<details>
  <summary>Details</summary>
Motivation: To resolve the issue of temporal discontinuities and visual artifacts in existing inference-time sampling strategies for image-to-video diffusion models.

Method: Proposes Motion Prior Distillation (MPD), which suppresses bidirectional mismatch by distilling the motion residual of the forward path into the backward path.

Result: MPD yields more temporally coherent inbetweening results by leveraging the forward motion prior and avoids denoising ambiguities.

Conclusion: The method effectively improves temporal coherence in generative inbetweening, validated through quantitative benchmarks and user studies.

Abstract: Recent progress in image-to-video (I2V) diffusion models has significantly advanced the field of generative inbetweening, which aims to generate semantically plausible frames between two keyframes. In particular, inference-time sampling strategies, which leverage the generative priors of large-scale pre-trained I2V models without additional training, have become increasingly popular. However, existing inference-time sampling, either fusing forward and backward paths in parallel or alternating them sequentially, often suffers from temporal discontinuities and undesirable visual artifacts due to the misalignment between the two generated paths. This is because each path follows the motion prior induced by its own conditioning frame. In this work, we propose Motion Prior Distillation (MPD), a simple yet effective inference-time distillation technique that suppresses bidirectional mismatch by distilling the motion residual of the forward path into the backward path. Our method can deliberately avoid denoising the end-conditioned path which causes the ambiguity of the path, and yield more temporally coherent inbetweening results with the forward motion prior. We not only perform quantitative evaluations on standard benchmarks, but also conduct extensive user studies to demonstrate the effectiveness of our approach in practical scenarios.

</details>


### [28] [Channel-Aware Probing for Multi-Channel Imaging](https://arxiv.org/abs/2602.12696)
*Umar Marikkar,Syed Sameed Husain,Muhammad Awais,Sara Atito*

Main category: cs.CV

TL;DR: CAP improves probing performance for Multi-Channel Imaging data by leveraging inter-channel diversity with Independent Feature Encoding and Decoupled Pooling.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with variable channel configurations in MCI datasets, and probing with frozen encoders is underexplored.

Method: Channel-Aware Probing (CAP) uses Independent Feature Encoding (IFE) and Decoupled Pooling (DCP) to handle inter-channel diversity.

Result: CAP consistently outperforms default probing, matches training from scratch, and narrows the gap to full fine-tuning.

Conclusion: CAP effectively leverages MCI's inter-channel diversity for better downstream task performance.

Abstract: Training and evaluating vision encoders on Multi-Channel Imaging (MCI) data remains challenging as channel configurations vary across datasets, preventing fixed-channel training and limiting reuse of pre-trained encoders on new channel settings. Prior work trains MCI encoders but typically evaluates them via full fine-tuning, leaving probing with frozen pre-trained encoders comparatively underexplored. Existing studies that perform probing largely focus on improving representations, rather than how to best leverage fixed representations for downstream tasks. Although the latter problem has been studied in other domains, directly transferring those strategies to MCI yields weak results, even worse than training from scratch. We therefore propose Channel-Aware Probing (CAP), which exploits the intrinsic inter-channel diversity in MCI datasets by controlling feature flow at both the encoder and probe levels. CAP uses Independent Feature Encoding (IFE) to encode each channel separately, and Decoupled Pooling (DCP) to pool within channels before aggregating across channels. Across three MCI benchmarks, CAP consistently improves probing performance over the default probing protocol, matches fine-tuning from scratch, and largely reduces the gap to full fine-tuning from the same MCI pre-trained checkpoints. Code can be found in https://github.com/umarikkar/CAP.

</details>


### [29] [VimRAG: Navigating Massive Visual Context in Retrieval-Augmented Generation via Multimodal Memory Graph](https://arxiv.org/abs/2602.12735)
*Qiuchen Wang,Shihang Wang,Yu Zeng,Qiang Zhang,Fanrui Zhang,Zhuoning Guo,Bosi Zhang,Wenxuan Huang,Lin Chen,Zehui Chen,Pengjun Xie,Ruixue Ding*

Main category: cs.CV

TL;DR: VimRAG introduces a multimodal retrieval-augmented reasoning framework using dynamic graphs to handle long-context tasks involving visual data, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional RAG methods struggle with long-context tasks involving token-heavy visual data, necessitating a more efficient multimodal approach.

Method: VimRAG models reasoning as a dynamic graph, uses Graph-Modulated Visual Memory Encoding, and employs Graph-Guided Policy Optimization for fine-grained credit assignment.

Result: VimRAG achieves state-of-the-art performance across diverse multimodal RAG benchmarks.

Conclusion: The framework effectively addresses the limitations of traditional RAG methods, offering a robust solution for multimodal reasoning.

Abstract: Effectively retrieving, reasoning, and understanding multimodal information remains a critical challenge for agentic systems. Traditional Retrieval-augmented Generation (RAG) methods rely on linear interaction histories, which struggle to handle long-context tasks, especially those involving information-sparse yet token-heavy visual data in iterative reasoning scenarios. To bridge this gap, we introduce VimRAG, a framework tailored for multimodal Retrieval-augmented Reasoning across text, images, and videos. Inspired by our systematic study, we model the reasoning process as a dynamic directed acyclic graph that structures the agent states and retrieved multimodal evidence. Building upon this structured memory, we introduce a Graph-Modulated Visual Memory Encoding mechanism, with which the significance of memory nodes is evaluated via their topological position, allowing the model to dynamically allocate high-resolution tokens to pivotal evidence while compressing or discarding trivial clues. To implement this paradigm, we propose a Graph-Guided Policy Optimization strategy. This strategy disentangles step-wise validity from trajectory-level rewards by pruning memory nodes associated with redundant actions, thereby facilitating fine-grained credit assignment. Extensive experiments demonstrate that VimRAG consistently achieves state-of-the-art performance on diverse multimodal RAG benchmarks. The code is available at https://github.com/Alibaba-NLP/VRAG.

</details>


### [30] [SPRig: Self-Supervised Pose-Invariant Rigging from Mesh Sequences](https://arxiv.org/abs/2602.12740)
*Ruipeng Wang,Langkun Zhong,Miaowei Wang*

Main category: cs.CV

TL;DR: SPRig is a fine-tuning framework that ensures cross-frame consistency for pose-invariant rigging in sequential data, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Traditional rigging methods fail for sequential data without a canonical rest pose, causing inconsistencies.

Method: SPRig uses cross-frame consistency losses on existing models.

Result: Achieves SOTA temporal stability and reduces artifacts significantly.

Conclusion: SPRig is effective for rigging challenging sequences.

Abstract: State-of-the-art rigging methods assume a canonical rest pose--an assumption that fails for sequential data (e.g., animal motion capture or AIGC/video-derived mesh sequences) that lack the T-pose. Applied frame-by-frame, these methods are not pose-invariant and produce topological inconsistencies across frames. Thus We propose SPRig, a general fine-tuning framework that enforces cross-frame consistency losses to learn pose-invariant rigs on top of existing models. We validate our approach on rigging using a new permutation-invariant stability protocol. Experiments demonstrate SOTA temporal stability: our method produces coherent rigs from challenging sequences and dramatically reduces the artifacts that plague baseline methods. The code will be released publicly upon acceptance.

</details>


### [31] [Synthetic Craquelure Generation for Unsupervised Painting Restoration](https://arxiv.org/abs/2602.12742)
*Jana Cuch-Guillén,Antonio Agudo,Raül Pérez-Gonzalo*

Main category: cs.CV

TL;DR: Proposes an annotation-free framework for preserving cultural heritage paintings by simulating craquelure patterns and combining morphological detection with learning-based refinement.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of identifying and restoring fine craquelure patterns without pixel-level annotations in cultural heritage preservation.

Method: Uses a synthetic craquelure generator with Bézier trajectories, couples morphological detection with SegFormer-LoRA refinement, and employs detector-guided strategies for training. Inpainting is done via Anisotropic Diffusion.

Result: Outperforms state-of-the-art restoration models in zero-shot settings while preserving original brushwork.

Conclusion: The framework effectively restores craquelure patterns without annotations and maintains artistic integrity.

Abstract: Cultural heritage preservation increasingly demands non-invasive digital methods for painting restoration, yet identifying and restoring fine craquelure patterns from complex brushstrokes remains challenging due to scarce pixel-level annotations. We propose a fully annotation-free framework driven by a domain-specific synthetic craquelure generator, which simulates realistic branching and tapered fissure geometry using Bézier trajectories. Our approach couples a classical morphological detector with a learning-based refinement module: a SegFormer backbone adapted via Low-Rank Adaptation (LoRA). Uniquely, we employ a detector-guided strategy, injecting the morphological map as an input spatial prior, while a masked hybrid loss and logit adjustment constrain the training to focus specifically on refining candidate crack regions. The refined masks subsequently guide an Anisotropic Diffusion inpainting stage to reconstruct missing content. Experimental results demonstrate that our pipeline significantly outperforms state-of-the-art photographic restoration models in zero-shot settings, while faithfully preserving the original paint brushwork.

</details>


### [32] [ReBA-Pred-Net: Weakly-Supervised Regional Brain Age Prediction on MRI](https://arxiv.org/abs/2602.12751)
*Shuai Shao,Yan Wang,Shu Jiang,Shiyuan Zhao,Xinzhe Luo,Di Yang,Jiangtao Wang,Yutong Bai,Jianguo Zhang*

Main category: cs.CV

TL;DR: The paper introduces ReBA-Pred-Net, a Teacher-Student framework for regional brain age estimation, validated using indirect metrics like HCS and NDC.


<details>
  <summary>Details</summary>
Motivation: Prior work focuses on whole brain age (WBA), which lacks regional specificity needed for disease characterization and aging research. Robust regional brain age (ReBA) estimation is crucial but not yet widely generalizable.

Method: The proposed ReBA-Pred-Net uses a Teacher-Student framework with a clinical-prior consistency constraint to ensure reliable ReBA estimates.

Result: Experiments validate the method's statistical (HCS) and factual (NDC) consistency across multiple backbones.

Conclusion: ReBA-Pred-Net provides a reliable and generalizable solution for fine-grained brain age estimation, addressing limitations of WBA.

Abstract: Brain age has become a prominent biomarker of brain health. Yet most prior work targets whole brain age (WBA), a coarse paradigm that struggles to support tasks such as disease characterization and research on development and aging patterns, because relevant changes are typically region-selective rather than brain-wide. Therefore, robust regional brain age (ReBA) estimation is critical, yet a widely generalizable model has yet to be established. In this paper, we propose the Regional Brain Age Prediction Network (ReBA-Pred-Net), a Teacher-Student framework designed for fine-grained brain age estimation. The Teacher produces soft ReBA to guide the Student to yield reliable ReBA estimates with a clinical-prior consistency constraint (regions within the same function should change similarly). For rigorous evaluation, we introduce two indirect metrics: Healthy Control Similarity (HCS), which assesses statistical consistency by testing whether regional brain-age-gap (ReBA minus chronological age) distributions align between training and unseen HC; and Neuro Disease Correlation (NDC), which assesses factual consistency by checking whether clinically confirmed patients show elevated brain-age-gap in disease-associated regions. Experiments across multiple backbones demonstrate the statistical and factual validity of our method.

</details>


### [33] [Towards reconstructing experimental sparse-view X-ray CT data with diffusion models](https://arxiv.org/abs/2602.12755)
*Nelas J. Thomsen,Xinyuan Wang,Felix Lucka,Ezgi Demircan-Tureyen*

Main category: cs.CV

TL;DR: Diffusion-based image generators show potential for solving inverse problems like sparse-view CT, but performance gaps exist between synthetic and experimental data due to domain and forward model mismatches.


<details>
  <summary>Details</summary>
Motivation: The study explores whether diffusion priors, effective on synthetic data, can be successfully applied to experimental CT data, considering challenges like domain shift and forward model mismatch.

Method: The authors trained diffusion priors on synthetic data with varying degrees of domain shift towards a physical phantom. They tested these priors on sparse-view CT data of increasing difficulty using a Decomposed Diffusion Sampling scheme.

Result: Domain shift's impact is nuanced: severe mismatch causes issues, but diverse priors outperform narrow ones. Forward model mismatch introduces artifacts, mitigated by annealed likelihood schedules.

Conclusion: While promising, diffusion priors' performance doesn't seamlessly translate to experimental data, emphasizing the need for real-world validation in future research.

Abstract: Diffusion-based image generators are promising priors for ill-posed inverse problems like sparse-view X-ray Computed Tomography (CT). As most studies consider synthetic data, it is not clear whether training data mismatch (``domain shift'') or forward model mismatch complicate their successful application to experimental data. We measured CT data from a physical phantom resembling the synthetic Shepp-Logan phantom and trained diffusion priors on synthetic image data sets with different degrees of domain shift towards it. Then, we employed the priors in a Decomposed Diffusion Sampling scheme on sparse-view CT data sets with increasing difficulty leading to the experimental data. Our results reveal that domain shift plays a nuanced role: while severe mismatch causes model collapse and hallucinations, diverse priors outperform well-matched but narrow priors. Forward model mismatch pulls the image samples away from the prior manifold, which causes artifacts but can be mitigated with annealed likelihood schedules that also increase computational efficiency. Overall, we demonstrate that performance gains do not immediately translate from synthetic to experimental data, and future development must validate against real-world benchmarks.

</details>


### [34] [Towards complete digital twins in cultural heritage with ART3mis 3D artifacts annotator](https://arxiv.org/abs/2602.12761)
*Dimitrios Karamatskos,Vasileios Arampatzakis,Vasileios Sevetlidis,Stavros Nousias,Athanasios Kalogeras,Christos Koulamas,Aris Lalos,George Pavlidis*

Main category: cs.CV

TL;DR: ART3mis is a web-based tool for annotating 3D artifacts, designed for cultural heritage professionals without technical expertise, ensuring interoperability and ease of use.


<details>
  <summary>Details</summary>
Motivation: Cultural heritage professionals need advanced tools beyond basic 3D visualization for annotating and sharing metadata on artifacts, but existing solutions lack generalization and interoperability.

Method: The paper introduces ART3mis, a user-friendly, web-based tool compliant with the W3C Web Annotation Data Model, enabling interactive annotation and metadata attachment for 3D objects.

Result: ART3mis addresses the limitations of existing tools by providing a general-purpose solution that supports communication, distribution, and reuse of annotated 3D artifact data.

Conclusion: ART3mis successfully bridges the gap for cultural heritage professionals, offering a practical and interoperable solution for annotating and managing 3D digital artifacts.

Abstract: Archaeologists, as well as specialists and practitioners in cultural heritage, require applications with additional functions, such as the annotation and attachment of metadata to specific regions of the 3D digital artifacts, to go beyond the simplistic three-dimensional (3D) visualization. Different strategies addressed this issue, most of which are excellent in their particular area of application, but their capacity is limited to their design's purpose; they lack generalization and interoperability. This paper introduces ART3mis, a general-purpose, user-friendly, feature-rich, interactive web-based textual annotation tool for 3D objects. Moreover, it enables the communication, distribution, and reuse of information as it complies with the W3C Web Annotation Data Model. It is primarily designed to help cultural heritage conservators, restorers, and curators who lack technical expertise in 3D imaging and graphics, handle, segment, and annotate 3D digital replicas of artifacts with ease.

</details>


### [35] [PixelRush: Ultra-Fast, Training-Free High-Resolution Image Generation via One-step Diffusion](https://arxiv.org/abs/2602.12769)
*Hong-Phuc Lai,Phong Nguyen,Anh Tran*

Main category: cs.CV

TL;DR: PixelRush introduces a tuning-free framework for high-resolution text-to-image generation, significantly speeding up the process while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Pre-trained diffusion models struggle with high-resolution generation due to computational inefficiencies in existing methods.

Method: PixelRush utilizes patch-based inference with seamless blending and noise injection to enable efficient, low-step denoising.

Result: PixelRush achieves a 10× to 35× speedup, generating 4K images in ~20 seconds with superior visual fidelity.

Conclusion: PixelRush offers a practical and efficient solution for high-resolution text-to-image generation without compromising quality.

Abstract: Pre-trained diffusion models excel at generating high-quality images but remain inherently limited by their native training resolution. Recent training-free approaches have attempted to overcome this constraint by introducing interventions during the denoising process; however, these methods incur substantial computational overhead, often requiring more than five minutes to produce a single 4K image. In this paper, we present PixelRush, the first tuning-free framework for practical high-resolution text-to-image generation. Our method builds upon the established patch-based inference paradigm but eliminates the need for multiple inversion and regeneration cycles. Instead, PixelRush enables efficient patch-based denoising within a low-step regime. To address artifacts introduced by patch blending in few-step generation, we propose a seamless blending strategy. Furthermore, we mitigate over-smoothing effects through a noise injection mechanism. PixelRush delivers exceptional efficiency, generating 4K images in approximately 20 seconds representing a 10$\times$ to 35$\times$ speedup over state-of-the-art methods while maintaining superior visual fidelity. Extensive experiments validate both the performance gains and the quality of outputs achieved by our approach.

</details>


### [36] [Bootstrapping MLLM for Weakly-Supervised Class-Agnostic Object Counting](https://arxiv.org/abs/2602.12774)
*Xiaowen Zhang,Zijie Yue,Yong Luo,Cairong Zhao,Qijun Chen,Miaojing Shi*

Main category: cs.CV

TL;DR: WS-COC proposes a weakly-supervised, MLLM-driven framework for class-agnostic object counting, achieving results comparable to fully-supervised methods while reducing annotation costs.


<details>
  <summary>Details</summary>
Motivation: Current weakly-supervised counting methods are limited to single categories. WS-COC aims to overcome this limitation by leveraging MLLMs for class-agnostic counting without costly annotations.

Method: WS-COC uses three strategies: divide-and-discern dialogue tuning to refine count ranges, compare-and-rank optimization for relative count ranking, and global-and-local fusion for dense scenes.

Result: Extensive experiments show WS-COC matches or surpasses fully-supervised methods on benchmarks like FSC-147 and ShanghaiTech.

Conclusion: WS-COC effectively reduces annotation costs and achieves strong performance in class-agnostic object counting, demonstrating the potential of weakly-supervised approaches.

Abstract: Object counting is a fundamental task in computer vision, with broad applicability in many real-world scenarios. Fully-supervised counting methods require costly point-level annotations per object. Few weakly-supervised methods leverage only image-level object counts as supervision and achieve fairly promising results. They are, however, often limited to counting a single category, e.g. person. In this paper, we propose WS-COC, the first MLLM-driven weakly-supervised framework for class-agnostic object counting. Instead of directly fine-tuning MLLMs to predict object counts, which can be challenging due to the modality gap, we incorporate three simple yet effective strategies to bootstrap the counting paradigm in both training and testing: First, a divide-and-discern dialogue tuning strategy is proposed to guide the MLLM to determine whether the object count falls within a specific range and progressively break down the range through multi-round dialogue. Second, a compare-and-rank count optimization strategy is introduced to train the MLLM to optimize the relative ranking of multiple images according to their object counts. Third, a global-and-local counting enhancement strategy aggregates and fuses local and global count predictions to improve counting performance in dense scenes. Extensive experiments on FSC-147, CARPK, PUCPR+, and ShanghaiTech show that WS-COC matches or even surpasses many state-of-art fully-supervised methods while significantly reducing annotation costs. Code is available at https://github.com/viscom-tongji/WS-COC.

</details>


### [37] [Thinking Like a Radiologist: A Dataset for Anatomy-Guided Interleaved Vision Language Reasoning in Chest X-ray Interpretation](https://arxiv.org/abs/2602.12843)
*Yichen Zhao,Zelin Peng,Piao Yang,Xiaokang Yang,Wei Shen*

Main category: cs.CV

TL;DR: The paper introduces MMRad-IVL-22K, a dataset for interleaved visual-language reasoning in chest X-ray interpretation, showing improved accuracy and report quality over text-only methods.


<details>
  <summary>Details</summary>
Motivation: Current LVLMs for radiology rely on text-only reasoning, losing rich visual details. The work aims to mirror radiologists' interleaved visual and language reasoning.

Method: Develops MMRad-IVL-22K, a dataset with 21,994 diagnostic traces for multimodal CoT reasoning, and benchmarks it against existing LVLMs.

Result: Multimodal CoT outperforms text-only CoT (e.g., 6% higher RadGraph score) and fine-tuned models on MMRad-IVL-22K surpass general and medical LVLMs.

Conclusion: Interleaved vision-language evidence is crucial for reliable medical AI, demonstrated by MMRad-IVL-22K's success.

Abstract: Radiological diagnosis is a perceptual process in which careful visual inspection and language reasoning are repeatedly interleaved. Most medical large vision language models (LVLMs) perform visual inspection only once and then rely on text-only chain-of-thought (CoT) reasoning, which operates purely in the linguistic space and is prone to hallucination. Recent methods attempt to mitigate this issue by introducing visually related coordinates, such as bounding boxes. However, these remain a pseudo-visual solution: coordinates are still text and fail to preserve rich visual details like texture and density. Motivated by the interleaved nature of radiological diagnosis, we introduce MMRad-IVL-22K, the first large-scale dataset designed for natively interleaved visual language reasoning in chest X-ray interpretation. MMRad-IVL-22K reflects a repeated cycle of reasoning and visual inspection workflow of radiologists, in which visual rationales complement textual descriptions and ground each step of the reasoning process. MMRad-IVL-22K comprises 21,994 diagnostic traces, enabling systematic scanning across 35 anatomical regions. Experimental results on advanced closed-source LVLMs demonstrate that report generation guided by multimodal CoT significantly outperforms that guided by text-only CoT in clinical accuracy and report quality (e.g., 6\% increase in the RadGraph metric), confirming that high-fidelity interleaved vision language evidence is a non-substitutable component of reliable medical AI. Furthermore, benchmarking across seven state-of-the-art open-source LVLMs demonstrates that models fine-tuned on MMRad-IVL-22K achieve superior reasoning consistency and report quality compared with both general-purpose and medical-specific LVLMs. The project page is available at https://github.com/qiuzyc/thinking_like_a_radiologist.

</details>


### [38] [RoadscapesQA: A Multitask, Multimodal Dataset for Visual Question Answering on Indian Roads](https://arxiv.org/abs/2602.12877)
*Vijayasri Iyer,Maahin Rathinagiriswaran,Jyothikamalesh S*

Main category: cs.CV

TL;DR: Roadscapes is a multimodal dataset for autonomous driving, featuring 9,000 diverse Indian road scene images with verified annotations and QA pairs for scene understanding tasks.


<details>
  <summary>Details</summary>
Motivation: Enhancing autonomous driving systems by improving their ability to interpret complex and unstructured road scenes, particularly in diverse Indian environments.

Method: Created a dataset with manually verified bounding boxes and used rule-based heuristics to generate QA pairs for tasks like object grounding and reasoning.

Result: Roadscapes includes diverse scenes (urban, rural, day, night) and provides baselines for vision-language models in image QA tasks.

Conclusion: The dataset advances research in visual scene understanding for autonomous driving, offering scalable solutions for interpreting unstructured environments.

Abstract: Understanding road scenes is essential for autonomous driving, as it enables systems to interpret visual surroundings to aid in effective decision-making. We present Roadscapes, a multitask multimodal dataset consisting of upto 9,000 images captured in diverse Indian driving environments, accompanied by manually verified bounding boxes. To facilitate scalable scene understanding, we employ rule-based heuristics to infer various scene attributes, which are subsequently used to generate question-answer (QA) pairs for tasks such as object grounding, reasoning, and scene understanding. The dataset includes a variety of scenes from urban and rural India, encompassing highways, service roads, village paths, and congested city streets, captured in both daytime and nighttime settings. Roadscapes has been curated to advance research on visual scene understanding in unstructured environments. In this paper, we describe the data collection and annotation process, present key dataset statistics, and provide initial baselines for image QA tasks using vision-language models.

</details>


### [39] [RADAR: Revealing Asymmetric Development of Abilities in MLLM Pre-training](https://arxiv.org/abs/2602.12892)
*Yunshuang Nie,Bingqian Lin,Minzhe Niu,Kun Xiang,Jianhua Han,Guowei Huang,Xingyue Quan,Hang Xu,Bokui Chen,Xiaodan Liang*

Main category: cs.CV

TL;DR: RADAR is an evaluation framework for MLLMs, introducing Soft Discrimination Score and Multi-Modal Mixture Benchmark to assess perception and reasoning abilities without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Current methods lack efficient evaluation frameworks, relying on costly fine-tuning or misaligned benchmarks. RADAR addresses these gaps.

Method: RADAR uses Soft Discrimination Score to track abilities and a Multi-Modal Mixture Benchmark (15K+ samples) for 0-shot evaluation.

Result: It reveals asymmetric development of MLLMs' perceptual and reasoning abilities across factors like data volume and model size.

Conclusion: RADAR provides a decomposed perspective on bottlenecks, enabling targeted improvements in MLLM pre-training.

Abstract: Pre-trained Multi-modal Large Language Models (MLLMs) provide a knowledge-rich foundation for post-training by leveraging their inherent perception and reasoning capabilities to solve complex tasks. However, the lack of an efficient evaluation framework impedes the diagnosis of their performance bottlenecks. Current evaluation primarily relies on testing after supervised fine-tuning, which introduces laborious additional training and autoregressive decoding costs. Meanwhile, common pre-training metrics cannot quantify a model's perception and reasoning abilities in a disentangled manner. Furthermore, existing evaluation benchmarks are typically limited in scale or misaligned with pre-training objectives. Thus, we propose RADAR, an efficient ability-centric evaluation framework for Revealing Asymmetric Development of Abilities in MLLM pRe-training. RADAR involves two key components: (1) Soft Discrimination Score, a novel metric for robustly tracking ability development without fine-tuning, based on quantifying nuanced gradations of the model preference for the correct answer over distractors; and (2) Multi-Modal Mixture Benchmark, a new 15K+ sample benchmark for comprehensively evaluating pre-trained MLLMs' perception and reasoning abilities in a 0-shot manner, where we unify authoritative benchmark datasets and carefully collect new datasets, extending the evaluation scope and addressing the critical gaps in current benchmarks. With RADAR, we comprehensively reveal the asymmetric development of perceptual and reasoning capabilities in pretrained MLLMs across diverse factors, including data volume, model size, and pretraining strategy. Our RADAR underscores the need for a decomposed perspective on pre-training ability bottlenecks, informing targeted interventions to advance MLLMs efficiently. Our code is publicly available at https://github.com/Nieysh/RADAR.

</details>


### [40] [Robustness of Object Detection of Autonomous Vehicles in Adverse Weather Conditions](https://arxiv.org/abs/2602.12902)
*Fox Pettersen,Hong Zhu*

Main category: cs.CV

TL;DR: The paper proposes a method to evaluate object detection models in autonomous vehicles under adverse weather and lighting conditions using synthetic data and AFFC metrics, finding Faster R-CNN most robust.


<details>
  <summary>Details</summary>
Motivation: To ensure public safety by determining safe operational thresholds for autonomous vehicles in adverse conditions.

Method: Uses data augmentation to simulate adverse conditions and measures robustness via AFFC across four object detection models.

Result: Faster R-CNN was most robust (71.9% AFFC), while YOLO variants averaged 43%. Training with synthetic data improves robustness but can lead to diminishing returns.

Conclusion: The method is feasible and effective for evaluating model robustness in adverse conditions, with potential improvements via targeted training.

Abstract: As self-driving technology advances toward widespread adoption, determining safe operational thresholds across varying environmental conditions becomes critical for public safety. This paper proposes a method for evaluating the robustness of object detection ML models in autonomous vehicles under adverse weather conditions. It employs data augmentation operators to generate synthetic data that simulates different severance degrees of the adverse operation conditions at progressive intensity levels to find the lowest intensity of the adverse conditions at which the object detection model fails. The robustness of the object detection model is measured by the average first failure coefficients (AFFC) over the input images in the benchmark. The paper reports an experiment with four object detection models: YOLOv5s, YOLOv11s, Faster R-CNN, and Detectron2, utilising seven data augmentation operators that simulate weather conditions fog, rain, and snow, and lighting conditions of dark, bright, flaring, and shadow. The experiment data show that the method is feasible, effective, and efficient to evaluate and compare the robustness of object detection models in various adverse operation conditions. In particular, the Faster R-CNN model achieved the highest robustness with an overall average AFFC of 71.9% over all seven adverse conditions, while YOLO variants showed the AFFC values of 43%. The method is also applied to assess the impact of model training that targets adverse operation conditions using synthetic data on model robustness. It is observed that such training can improve robustness in adverse conditions but may suffer from diminishing returns and forgetting phenomena (i.e., decline in robustness) if overtrained.

</details>


### [41] [Adaptive Scaling with Geometric and Visual Continuity of completed 3D objects](https://arxiv.org/abs/2602.12905)
*Jelle Vermandere,Maarten Bassier,Maarten Vergauwen*

Main category: cs.CV

TL;DR: A part-aware scaling framework enables flexible manipulation of static SDFs by segmenting parts, defining scaling zones, and applying smooth interpolation for deformation, outperforming global scaling methods.


<details>
  <summary>Details</summary>
Motivation: Static SDFs limit flexibility in object manipulation for applications like indoor redesign and digital content creation.

Method: The framework segments parts, defines scaling zones, uses smooth interpolation for SDFs and colors, and employs repetition-based strategies for large deformations.

Result: Experiments show the method effectively avoids structural distortions and preserves repetitive patterns, outperforming global scaling.

Conclusion: The proposed framework successfully transforms rigid SDFs into editable objects, enhancing flexibility for practical applications.

Abstract: Object completion networks typically produce static Signed Distance Fields (SDFs) that faithfully reconstruct geometry but cannot be rescaled or deformed without introducing structural distortions. This limitation restricts their use in applications requiring flexible object manipulation, such as indoor redesign, simulation, and digital content creation. We introduce a part-aware scaling framework that transforms these static completed SDFs into editable, structurally coherent objects. Starting from SDFs and Texture Fields generated by state-of-the-art completion models, our method performs automatic part segmentation, defines user-controlled scaling zones, and applies smooth interpolation of SDFs, color, and part indices to enable proportional and artifact-free deformation. We further incorporate a repetition-based strategy to handle large-scale deformations while preserving repeating geometric patterns. Experiments on Matterport3D and ShapeNet objects show that our method overcomes the inherent rigidity of completed SDFs and is visually more appealing than global and naive selective scaling, particularly for complex shapes and repetitive structures.

</details>


### [42] [Reliable Thinking with Images](https://arxiv.org/abs/2602.12916)
*Haobin Li,Yutong Yang,Yijie Lin,Dai Xiang,Mouxing Yang,Xi Peng*

Main category: cs.CV

TL;DR: TWI enhances MLLMs' reasoning with visual cues, but noisy thinking (NT) degrades performance. RTWI mitigates NT by estimating cue reliability and filtering errors.


<details>
  <summary>Details</summary>
Motivation: Existing TWI assumes perfect image-text CoTs, but noisy thinking (NT) arises from imperfect visual cue mining and reasoning, harming MLLM performance.

Method: Propose RTWI, which estimates visual and textual CoT reliability in a text-centric way, using robust filtering and voting to avoid NT.

Result: RTWI outperforms baselines on seven benchmarks, proving effective against NT.

Conclusion: RTWI successfully addresses NT in TWI, improving MLLM robustness by filtering unreliable visual and textual cues.

Abstract: As a multimodal extension of Chain-of-Thought (CoT), Thinking with Images (TWI) has recently emerged as a promising avenue to enhance the reasoning capability of Multi-modal Large Language Models (MLLMs), which generates interleaved CoT by incorporating visual cues into the textual reasoning process. However, the success of existing TWI methods heavily relies on the assumption that interleaved image-text CoTs are faultless, which is easily violated in real-world scenarios due to the complexity of multimodal understanding. In this paper, we reveal and study a highly-practical yet under-explored problem in TWI, termed Noisy Thinking (NT). Specifically, NT refers to the imperfect visual cues mining and answer reasoning process. As the saying goes, ``One mistake leads to another'', erroneous interleaved CoT would cause error accumulation, thus significantly degrading the performance of MLLMs. To solve the NT problem, we propose a novel method dubbed Reliable Thinking with Images (RTWI). In brief, RTWI estimates the reliability of visual cues and textual CoT in a unified text-centric manner and accordingly employs robust filtering and voting modules to prevent NT from contaminating the final answer. Extensive experiments on seven benchmarks verify the effectiveness of RTWI against NT.

</details>


### [43] [EPRBench: A High-Quality Benchmark Dataset for Event Stream Based Visual Place Recognition](https://arxiv.org/abs/2602.12919)
*Xiao Wang,Xingxing Xiong,Jinfeng Gao,Xufeng Lou,Bo Jiang,Si-bao Chen,Yaowei Wang,Yonghong Tian*

Main category: cs.CV

TL;DR: EPRBench is a new benchmark for event stream-based Visual Place Recognition (VPR), featuring 10K sequences and 65K frames, with LLM-generated descriptions and multi-modal fusion for improved accuracy and explainability.


<details>
  <summary>Details</summary>
Motivation: Addressing the instability of conventional cameras in challenging conditions like low illumination and high-speed motion, and filling the gap in dedicated datasets for event stream-based VPR.

Method: Introduces EPRBench with diverse event sequences, uses LLMs for scene descriptions, and proposes a multi-modal fusion framework combining textual guidance, token selection, and feature fusion.

Result: Benchmarked 15 VPR algorithms, achieving highly accurate place recognition with interpretable reasoning.

Conclusion: EPRBench and the proposed framework advance event-based VPR, offering a robust dataset and interpretable solutions.

Abstract: Event stream-based Visual Place Recognition (VPR) is an emerging research direction that offers a compelling solution to the instability of conventional visible-light cameras under challenging conditions such as low illumination, overexposure, and high-speed motion. Recognizing the current scarcity of dedicated datasets in this domain, we introduce EPRBench, a high-quality benchmark specifically designed for event stream-based VPR. EPRBench comprises 10K event sequences and 65K event frames, collected using both handheld and vehicle-mounted setups to comprehensively capture real-world challenges across diverse viewpoints, weather conditions, and lighting scenarios. To support semantic-aware and language-integrated VPR research, we provide LLM-generated scene descriptions, subsequently refined through human annotation, establishing a solid foundation for integrating LLMs into event-based perception pipelines. To facilitate systematic evaluation, we implement and benchmark 15 state-of-the-art VPR algorithms on EPRBench, offering a strong baseline for future algorithmic comparisons. Furthermore, we propose a novel multi-modal fusion paradigm for VPR: leveraging LLMs to generate textual scene descriptions from raw event streams, which then guide spatially attentive token selection, cross-modal feature fusion, and multi-scale representation learning. This framework not only achieves highly accurate place recognition but also produces interpretable reasoning processes alongside its predictions, significantly enhancing model transparency and explainability. The dataset and source code will be released on https://github.com/Event-AHU/Neuromorphic_ReID

</details>


### [44] [Beyond Benchmarks of IUGC: Rethinking Requirements of Deep Learning Methods for Intrapartum Ultrasound Biometry from Fetal Ultrasound Videos](https://arxiv.org/abs/2602.12922)
*Jieyun Bai,Zihao Zhou,Yitong Tang,Jie Gan,Zhuonan Liang,Jianan Fan,Lisa B. Mcguire,Jillian L. Clarke,Weidong Cai,Jacaueline Spurway,Yubo Tang,Shiye Wang,Wenda Shen,Wangwang Yu,Yihao Li,Philippe Zhang,Weili Jiang,Yongjie Li,Salem Muhsin Ali Binqahal Al Nasim,Arsen Abzhanov,Numan Saeed,Mohammad Yaqub,Zunhui Xian,Hongxing Lin,Libin Lan,Jayroop Ramesh,Valentin Bacher,Mark Eid,Hoda Kalabizadeh,Christian Rupprecht,Ana I. L. Namburete,Pak-Hei Yeung,Madeleine K. Wyburd,Nicola K. Dinsdale,Assanali Serikbey,Jiankai Li,Sung-Liang Chen,Zicheng Hu,Nana Liu,Yian Deng,Wei Hu,Cong Tan,Wenfeng Zhang,Mai Tuyet Nhi,Gregor Koehler,Rapheal Stock,Klaus Maier-Hein,Marawan Elbatel,Xiaomeng Li,Saad Slimani,Victor M. Campello,Benard Ohene-Botwe,Isaac Khobo,Yuxin Huang,Zhenyan Han,Hongying Hou,Di Qiu,Zheng Zheng,Gongning Luo,Dong Ni,Yaosheng Lu,Karim Lekadir,Shuo Li*

Main category: cs.CV

TL;DR: The Intrapartum Ultrasound Grand Challenge (IUGC) aims to tackle the shortage of trained sonographers in low-resource settings by developing a multi-task automatic measurement framework for intrapartum biometry, supported by the largest multi-center ultrasound video dataset to date.


<details>
  <summary>Details</summary>
Motivation: High maternal and neonatal mortality during intrapartum in low- and middle-income countries due to limited ultrasound access necessitates automated solutions.

Method: The IUGC introduces a multi-task framework combining plane classification, segmentation, and biometry, using a dataset of 774 videos (68,106 frames) from three hospitals.

Result: Encouraging performance was achieved, but the field is still nascent, requiring further research for clinical deployment.

Conclusion: The challenge highlights potential solutions and future research directions, with publicly released data and benchmark solutions to advance intrapartum ultrasound biometry.

Abstract: A substantial proportion (45\%) of maternal deaths, neonatal deaths, and stillbirths occur during the intrapartum phase, with a particularly high burden in low- and middle-income countries. Intrapartum biometry plays a critical role in monitoring labor progression; however, the routine use of ultrasound in resource-limited settings is hindered by a shortage of trained sonographers. To address this challenge, the Intrapartum Ultrasound Grand Challenge (IUGC), co-hosted with MICCAI 2024, was launched. The IUGC introduces a clinically oriented multi-task automatic measurement framework that integrates standard plane classification, fetal head-pubic symphysis segmentation, and biometry, enabling algorithms to exploit complementary task information for more accurate estimation. Furthermore, the challenge releases the largest multi-center intrapartum ultrasound video dataset to date, comprising 774 videos (68,106 frames) collected from three hospitals, providing a robust foundation for model training and evaluation. In this study, we present a comprehensive overview of the challenge design, review the submissions from eight participating teams, and analyze their methods from five perspectives: preprocessing, data augmentation, learning strategy, model architecture, and post-processing. In addition, we perform a systematic analysis of the benchmark results to identify key bottlenecks, explore potential solutions, and highlight open challenges for future research. Although encouraging performance has been achieved, our findings indicate that the field remains at an early stage, and further in-depth investigation is required before large-scale clinical deployment. All benchmark solutions and the complete dataset have been publicly released to facilitate reproducible research and promote continued advances in automatic intrapartum ultrasound biometry.

</details>


### [45] [Human-Aligned MLLM Judges for Fine-Grained Image Editing Evaluation: A Benchmark, Framework, and Analysis](https://arxiv.org/abs/2602.13028)
*Runzhou Liu,Hailey Weingord,Sejal Mittal,Prakhar Dungarwal,Anusha Nandula,Bo Ni,Samyadeep Basu,Hongjie Chen,Nesreen K. Ahmed,Li Li,Jiayi Zhang,Koustava Goswami,Subhojyoti Mukherjee,Branislav Kveton,Puneet Mathur,Franck Dernoncourt,Yue Zhao,Yu Wang,Ryan A. Rossi,Zhengzhong Tu,Hongru Du*

Main category: cs.CV

TL;DR: The paper introduces a fine-grained MLLM-based evaluation framework for image editing, outperforming traditional metrics by aligning closely with human judgments.


<details>
  <summary>Details</summary>
Motivation: Traditional image editing metrics are coarse, lack interpretability, and often fail to capture human-perceived quality, controllability, and instruction fidelity.

Method: Proposes a MLLM-as-a-Judge framework decomposing evaluation into twelve fine-grained factors, supported by a human-validated benchmark.

Result: MLLM judges align closely with human evaluations, outperforming traditional metrics in capturing nuanced aspects of image editing.

Conclusion: The framework provides a scalable, reliable, and intuitive method for evaluating and improving image editing models.

Abstract: Evaluating image editing models remains challenging due to the coarse granularity and limited interpretability of traditional metrics, which often fail to capture aspects important to human perception and intent. Such metrics frequently reward visually plausible outputs while overlooking controllability, edit localization, and faithfulness to user instructions. In this work, we introduce a fine-grained Multimodal Large Language Model (MLLM)-as-a-Judge framework for image editing that decomposes common evaluation notions into twelve fine-grained interpretable factors spanning image preservation, edit quality, and instruction fidelity. Building on this formulation, we present a new human-validated benchmark that integrates human judgments, MLLM-based evaluations, model outputs, and traditional metrics across diverse image editing tasks. Through extensive human studies, we show that the proposed MLLM judges align closely with human evaluations at a fine granularity, supporting their use as reliable and scalable evaluators. We further demonstrate that traditional image editing metrics are often poor proxies for these factors, failing to distinguish over-edited or semantically imprecise outputs, whereas our judges provide more intuitive and informative assessments in both offline and online settings. Together, this work introduces a benchmark, a principled factorization, and empirical evidence positioning fine-grained MLLM judges as a practical foundation for studying, comparing, and improving image editing approaches.

</details>


### [46] [Deep-Learning Atlas Registration for Melanoma Brain Metastases: Preserving Pathology While Enabling Cohort-Level Analyses](https://arxiv.org/abs/2602.12933)
*Nanna E. Wielenberg,Ilinca Popp,Oliver Blanck,Lucas Zander,Jan C. Peeken,Stephanie E. Combs,Anca-Ligia Grosu,Dimos Baltas,Tobias Fechter*

Main category: cs.CV

TL;DR: A deep-learning-based deformable registration framework aligns pathological brains to a common atlas, handling metastatic lesions without masks or preprocessing. It achieved high accuracy and preserved metastatic volumes, revealing spatial patterns in melanoma brain metastases (MBM).


<details>
  <summary>Details</summary>
Motivation: MBM lesions are spatially heterogeneous and complicate cohort-level analyses due to anatomical variability and differing MRI protocols. A standardized method for registration is needed.

Method: The framework uses a forward-model similarity metric with distance-transformed labels and volume-preserving regularization. Performance was measured using DSC, HD, ASSD, and Jacobian metrics.

Result: High registration accuracy (DSC 0.89-0.92, HD 6.79-7.60 mm, ASSD 0.63-0.77 mm) was achieved. Spatial analysis showed MBM over-representation in cortex/putamen and under-representation in white matter.

Conclusion: The method enables robust atlas registration without lesion masks, confirming MBM spatial predilections and supporting reproducible multi-centre research.

Abstract: Melanoma brain metastases (MBM) are common and spatially heterogeneous lesions, complicating cohort-level analyses due to anatomical variability and differing MRI protocols. We propose a fully differentiable, deep-learning-based deformable registration framework that aligns individual pathological brains to a common atlas while preserving metastatic tissue without requiring lesion masks or preprocessing.
  Missing anatomical correspondences caused by metastases are handled through a forward-model similarity metric based on distance-transformed anatomical labels, combined with a volume-preserving regularization term to ensure deformation plausibility. Registration performance was evaluated using Dice coefficient (DSC), Hausdorff distance (HD), average symmetric surface distance (ASSD), and Jacobian-based measures. The method was applied to 209 MBM patients from three centres, enabling standardized mapping of metastases to anatomical, arterial, and perfusion atlases.
  The framework achieved high registration accuracy across datasets (DSC 0.89-0.92, HD 6.79-7.60 mm, ASSD 0.63-0.77 mm) while preserving metastatic volumes. Spatial analysis demonstrated significant over-representation of MBM in the cerebral cortex and putamen, under-representation in white matter, and consistent localization near the gray-white matter junction. No arterial territory showed increased metastasis frequency after volume correction.
  This approach enables robust atlas registration of pathological brain MRI without lesion masks and supports reproducible multi-centre analyses. Applied to MBM, it confirms and refines known spatial predilections, particularly preferential seeding near the gray-white matter junction and cortical regions. The publicly available implementation facilitates reproducible research and extension to other brain tumours and neurological pathologies.

</details>


### [47] [Unleashing MLLMs on the Edge: A Unified Framework for Cross-Modal ReID via Adaptive SVD Distillation](https://arxiv.org/abs/2602.12936)
*Hongbo Jiang,Jie Li,Xinqi Cai,Tianyu Xie,Yunhang Shen,Pingyang Dai,Liujuan Cao*

Main category: cs.CV

TL;DR: MLLMEmbed-ReID leverages MLLMs for unified cross-modal re-identification (CM-ReID), using cloud-edge architecture and novel distillation for edge deployment.


<details>
  <summary>Details</summary>
Motivation: To address fragmentation in CM-ReID by unifying modalities and enabling efficient edge deployment using MLLMs.

Method: Adapts MLLM into a cloud model with instruction-based prompting and LoRA-SFT training, then distills knowledge to an edge model using Principal Component Mapping and Feature Relation losses.

Result: Achieves state-of-the-art performance on CM-ReID benchmarks for both cloud and edge models.

Conclusion: MLLMEmbed-ReID provides a unified, efficient solution for deploying MLLM-level intelligence on edge devices.

Abstract: Practical cloud-edge deployment of Cross-Modal Re-identification (CM-ReID) faces challenges due to maintaining a fragmented ecosystem of specialized cloud models for diverse modalities. While Multi-Modal Large Language Models (MLLMs) offer strong unification potential, existing approaches fail to adapt them into a single end-to-end backbone and lack effective knowledge distillation strategies for edge deployment. To address these limitations, we propose MLLMEmbed-ReID, a unified framework based on a powerful cloud-edge architecture. First, we adapt a foundational MLLM into a state-of-the-art cloud model. We leverage instruction-based prompting to guide the MLLM in generating a unified embedding space across RGB, infrared, sketch, and text modalities. This model is then trained efficiently with a hierarchical Low-Rank Adaptation finetuning (LoRA-SFT) strategy, optimized under a holistic cross-modal alignment objective. Second, to deploy its knowledge onto an edge-native student, we introduce a novel distillation strategy motivated by the low-rank property in the teacher's feature space. To prioritize essential information, this method employs a Principal Component Mapping loss, while relational structures are preserved via a Feature Relation loss. Our lightweight edge-based model achieves state-of-the-art performance on multiple visual CM-ReID benchmarks, while its cloud-based counterpart excels across all CM-ReID benchmarks. The MLLMEmbed-ReID framework thus presents a complete and effective solution for deploying unified MLLM-level intelligence on resource-constrained devices. The code and models will be open-sourced soon.

</details>


### [48] [Training-Free Acceleration for Document Parsing Vision-Language Model with Hierarchical Speculative Decoding](https://arxiv.org/abs/2602.12957)
*Wenhui Liao,Hongliang Li,Pengyu Xie,Xinyu Cai,Yufan Shen,Yi Xin,Qi Qin,Shenglong Ye,Tianbin Li,Ming Hu,Junjun He,Yihao Liu,Wenhai Wang,Min Dou,Bin Fu,Botian Shi,Yu Qiao,Lianwen Jin*

Main category: cs.CV

TL;DR: Proposal of a training-free, efficient acceleration method for document parsing using speculative decoding and parallel region-based decoding to reduce inference latency in VLM-based models.


<details>
  <summary>Details</summary>
Motivation: Existing VLM-based document parsing models suffer from high inference latency due to auto-regressive generation of long token sequences for long-form documents.

Method: Uses a lightweight draft model to predict token batches and a VLM for parallel verification. Pages are partitioned into regions for parallel decoding.

Result: Achieves 2.42x to 4.89x acceleration on benchmarks like OmniDocBench, with no loss in accuracy.

Conclusion: The method effectively reduces latency in document parsing tasks, maintaining accuracy and improving efficiency.

Abstract: Document parsing is a fundamental task in multimodal understanding, supporting a wide range of downstream applications such as information extraction and intelligent document analysis. Benefiting from strong semantic modeling and robust generalization, VLM-based end-to-end approaches have emerged as the mainstream paradigm in recent years. However, these models often suffer from substantial inference latency, as they must auto-regressively generate long token sequences when processing long-form documents. In this work, motivated by the extremely long outputs and complex layout structures commonly found in document parsing, we propose a training-free and highly efficient acceleration method. Inspired by speculative decoding, we employ a lightweight document parsing pipeline as a draft model to predict batches of future tokens, while the more accurate VLM verifies these draft predictions in parallel. Moreover, we further exploit the layout-structured nature of documents by partitioning each page into independent regions, enabling parallel decoding of each region using the same draft-verify strategy. The final predictions are then assembled according to the natural reading order. Experimental results demonstrate the effectiveness of our approach: on the general-purpose OmniDocBench, our method provides a 2.42x lossless acceleration for the dots.ocr model, and achieves up to 4.89x acceleration on long-document parsing tasks. We will release our code to facilitate reproducibility and future research.

</details>


### [49] [Detecting Object Tracking Failure via Sequential Hypothesis Testing](https://arxiv.org/abs/2602.12983)
*Alejandro Monroy Muñoz,Rajeev Verma,Alexander Timans*

Main category: cs.CV

TL;DR: The paper proposes a sequential hypothesis test (e-process) for real-time object tracking to identify failures reliably while controlling false alerts, applicable across models without extra training.


<details>
  <summary>Details</summary>
Motivation: Existing tracking systems lack formal safety assurances for reliability, relying on heuristic confidence measures. This work aims to provide statistically grounded failure detection.

Method: The approach frames tracking as a sequential hypothesis test, accumulating evidence over time to detect failures. It uses e-processes to limit false alerts and operates model-agnostically with supervised/unsupervised variants.

Result: The method effectively identifies tracking failures with controlled false alert rates, demonstrated on two tracking models across four benchmarks. It is lightweight and requires no additional training.

Conclusion: Sequential testing offers a statistically efficient solution for embedding safety assurances in real-time tracking systems, applicable broadly without model-specific adjustments.

Abstract: Real-time online object tracking in videos constitutes a core task in computer vision, with wide-ranging applications including video surveillance, motion capture, and robotics. Deployed tracking systems usually lack formal safety assurances to convey when tracking is reliable and when it may fail, at best relying on heuristic measures of model confidence to raise alerts. To obtain such assurances we propose interpreting object tracking as a sequential hypothesis test, wherein evidence for or against tracking failures is gradually accumulated over time. Leveraging recent advancements in the field, our sequential test (formalized as an e-process) quickly identifies when tracking failures set in whilst provably containing false alerts at a desired rate, and thus limiting potentially costly re-calibration or intervention steps. The approach is computationally light-weight, requires no extra training or fine-tuning, and is in principle model-agnostic. We propose both supervised and unsupervised variants by leveraging either ground-truth or solely internal tracking information, and demonstrate its effectiveness for two established tracking models across four video benchmarks. As such, sequential testing can offer a statistically grounded and efficient mechanism to incorporate safety assurances into real-time tracking systems.

</details>


### [50] [MASAR: Motion-Appearance Synergy Refinement for Joint Detection and Trajectory Forecasting](https://arxiv.org/abs/2602.13003)
*Mohammed Amine Bencheikh Lehocine,Julian Schmidt,Frank Moosmann,Dikshant Gupta,Fabian Flohr*

Main category: cs.CV

TL;DR: MASAR is a differentiable framework combining 3D detection and trajectory forecasting, leveraging long-term temporal dependencies for improved performance.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of traditional autonomous driving systems that separate perception and prediction, and enhance synergy between appearance and motion cues.

Method: Proposes MASAR, a transformer-based framework with an object-centric spatio-temporal mechanism to jointly encode appearance and motion features.

Result: Achieves over 20% improvement in minADE and minFDE on nuScenes dataset while maintaining strong detection performance.

Conclusion: MASAR effectively integrates perception and prediction, outperforming traditional and recent end-to-end approaches.

Abstract: Classical autonomous driving systems connect perception and prediction modules via hand-crafted bounding-box interfaces, limiting information flow and propagating errors to downstream tasks. Recent research aims to develop end-to-end models that jointly address perception and prediction; however, they often fail to fully exploit the synergy between appearance and motion cues, relying mainly on short-term visual features. We follow the idea of "looking backward to look forward", and propose MASAR, a novel fully differentiable framework for joint 3D detection and trajectory forecasting compatible with any transformer-based 3D detector. MASAR employs an object-centric spatio-temporal mechanism that jointly encodes appearance and motion features. By predicting past trajectories and refining them using guidance from appearance cues, MASAR captures long-term temporal dependencies that enhance future trajectory forecasting. Experiments conducted on the nuScenes dataset demonstrate MASAR's effectiveness, showing improvements of over 20% in minADE and minFDE while maintaining robust detection performance. Code and models are available at https://github.com/aminmed/MASAR.

</details>


### [51] [Towards Universal Video MLLMs with Attribute-Structured and Quality-Verified Instructions](https://arxiv.org/abs/2602.13013)
*Yunheng Li,Hengrui Zhang,Meng-Hao Guo,Wenzhao Gao,Shaoyong Jia,Shaohui Jiao,Qibin Hou,Ming-Ming Cheng*

Main category: cs.CV

TL;DR: The paper introduces ASID-1M, ASID-Verify, and ASID-Captioner to improve fine-grained audiovisual instruction annotations and video understanding, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing models are limited by poorly annotated video-instruction data, lacking fine-grained organization and reliability.

Method: Proposes ASID-1M (structured annotations), ASID-Verify (data curation pipeline), and ASID-Captioner (trained model).

Result: ASID-Captioner outperforms benchmarks in caption quality, hallucination reduction, and instruction following, competing with Gemini-3-Pro.

Conclusion: The introduced tools and model significantly enhance fine-grained video understanding and set new benchmarks.

Abstract: Universal video understanding requires modeling fine-grained visual and audio information over time in diverse real-world scenarios. However, the performance of existing models is primarily constrained by video-instruction data that represents complex audiovisual content as single, incomplete descriptions, lacking fine-grained organization and reliable annotation. To address this, we introduce: (i) ASID-1M, an open-source collection of one million structured, fine-grained audiovisual instruction annotations with single- and multi-attribute supervision; (ii) ASID-Verify, a scalable data curation pipeline for annotation, with automatic verification and refinement that enforces semantic and temporal consistency between descriptions and the corresponding audiovisual content; and (iii) ASID-Captioner, a video understanding model trained via Supervised Fine-Tuning (SFT) on the ASID-1M. Experiments across seven benchmarks covering audiovisual captioning, attribute-wise captioning, caption-based QA, and caption-based temporal grounding show that ASID-Captioner improves fine-grained caption quality while reducing hallucinations and improving instruction following. It achieves state-of-the-art performance among open-source models and is competitive with Gemini-3-Pro.

</details>


### [52] [CoPE-VideoLM: Codec Primitives For Efficient Video Language Models](https://arxiv.org/abs/2602.13191)
*Sayan Deb Sarkar,Rémi Pautrat,Ondrej Miksik,Marc Pollefeys,Iro Armeni,Mahdi Rad,Mihai Dusmanu*

Main category: cs.CV

TL;DR: VideoLMs often miss details due to sparse keyframe sampling and high computational costs. The proposed method uses video codec primitives (motion vectors, residuals) and lightweight transformers to reduce overhead while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Current VideoLMs face challenges with sparse temporal coverage from keyframe sampling and high computational costs from full-image encoding, limiting their effectiveness.

Method: Leverages video codec primitives (motion vectors, residuals) for redundancy and sparsity. Introduces lightweight transformer encoders aligned with image embeddings via pre-training.

Result: Reduces time-to-first-token by 86% and token usage by 93%. Maintains or exceeds performance on 14 video understanding benchmarks.

Conclusion: The approach efficiently addresses VideoLM limitations, improving speed and resource usage without sacrificing accuracy.

Abstract: Video Language Models (VideoLMs) empower AI systems to understand temporal dynamics in videos. To fit to the maximum context window constraint, current methods use keyframe sampling which can miss both macro-level events and micro-level details due to the sparse temporal coverage. Furthermore, processing full images and their tokens for each frame incurs substantial computational overhead. To address these limitations, we propose to leverage video codec primitives (specifically motion vectors and residuals) which natively encode video redundancy and sparsity without requiring expensive full-image encoding for most frames. To this end, we introduce lightweight transformer-based encoders that aggregate codec primitives and align their representations with image encoder embeddings through a pre-training strategy that accelerates convergence during end-to-end fine-tuning. Our approach reduces the time-to-first-token by up to $86\%$ and token usage by up to $93\%$ compared to standard VideoLMs. Moreover, by varying the keyframe and codec primitive densities we are able to maintain or exceed performance on $14$ diverse video understanding benchmarks spanning general question answering, temporal reasoning, long-form understanding, and spatial scene understanding.

</details>


### [53] [Multimodal Classification via Total Correlation Maximization](https://arxiv.org/abs/2602.13015)
*Feng Yu,Xiangyu Wu,Yang Yang,Jianfeng Lu*

Main category: cs.CV

TL;DR: The paper proposes TCMax, a method for multimodal classification that maximizes total correlation between features and labels to mitigate modality competition, outperforming existing approaches.


<details>
  <summary>Details</summary>
Motivation: Multimodal learning often overfits certain modalities, neglecting others, leading to worse performance than unimodal learning. The paper aims to address this imbalance and modality competition from an information-theoretic perspective.

Method: Introduces Total Correlation Neural Estimation (TCNE) to maximize the total correlation between multimodal features and labels. Proposes TCMax, a loss function that optimizes this correlation via variational bound optimization, without requiring hyperparameters.

Result: TCMax outperforms state-of-the-art joint and unimodal learning methods in extensive experiments.

Conclusion: The paper successfully bridges the gap between joint and unimodal learning by leveraging total correlation, providing a robust solution to modality competition in multimodal systems.

Abstract: Multimodal learning integrates data from diverse sensors to effectively harness information from different modalities. However, recent studies reveal that joint learning often overfits certain modalities while neglecting others, leading to performance inferior to that of unimodal learning. Although previous efforts have sought to balance modal contributions or combine joint and unimodal learning, thereby mitigating the degradation of weaker modalities with promising outcomes, few have examined the relationship between joint and unimodal learning from an information-theoretic perspective. In this paper, we theoretically analyze modality competition and propose a method for multimodal classification by maximizing the total correlation between multimodal features and labels. By maximizing this objective, our approach alleviates modality competition while capturing inter-modal interactions via feature alignment. Building on Mutual Information Neural Estimation (MINE), we introduce Total Correlation Neural Estimation (TCNE) to derive a lower bound for total correlation. Subsequently, we present TCMax, a hyperparameter-free loss function that maximizes total correlation through variational bound optimization. Extensive experiments demonstrate that TCMax outperforms state-of-the-art joint and unimodal learning approaches. Our code is available at https://github.com/hubaak/TCMax.

</details>


### [54] [DynaGuide: A Generalizable Dynamic Guidance Framework for Unsupervised Semantic Segmentation](https://arxiv.org/abs/2602.13020)
*Boujemaa Guermazi,Riadh Ksantini,Naimul Khan*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Unsupervised image segmentation is a critical task in computer vision. It enables dense scene understanding without human annotations, which is especially valuable in domains where labelled data is scarce. However, existing methods often struggle to reconcile global semantic structure with fine-grained boundary accuracy. This paper introduces DynaGuide, an adaptive segmentation framework that addresses these challenges through a novel dual-guidance strategy and dynamic loss optimization. Building on our previous work, DynaSeg, DynaGuide combines global pseudo-labels from zero-shot models such as DiffSeg or SegFormer with local boundary refinement using a lightweight CNN trained from scratch. This synergy allows the model to correct coarse or noisy global predictions and produce high-precision segmentations. At the heart of DynaGuide is a multi-component loss that dynamically balances feature similarity, Huber-smoothed spatial continuity, including diagonal relationships, and semantic alignment with the global pseudo-labels. Unlike prior approaches, DynaGuide trains entirely without ground-truth labels in the target domain and supports plug-and-play integration of diverse guidance sources. Extensive experiments on BSD500, PASCAL VOC2012, and COCO demonstrate that DynaGuide achieves state-of-the-art performance, improving mIoU by 17.5% on BSD500, 3.1% on PASCAL VOC2012, and 11.66% on COCO. With its modular design, strong generalization, and minimal computational footprint, DynaGuide offers a scalable and practical solution for unsupervised segmentation in real-world settings. Code available at: https://github.com/RyersonMultimediaLab/DynaGuide

</details>


### [55] [Learning Image-based Tree Crown Segmentation from Enhanced Lidar-based Pseudo-labels](https://arxiv.org/abs/2602.13022)
*Julius Pesonen,Stefan Rua,Josef Taher,Niko Koivumäki,Xiaowei Yu,Eija Honkavaara*

Main category: cs.CV

TL;DR: A method using deep learning and ALS-derived pseudo-labels enhanced by SAM 2 achieves superior tree crown segmentation in aerial imagery without manual annotation.


<details>
  <summary>Details</summary>
Motivation: Automated tree crown mapping is vital for urban and forest management but challenging due to image textures and overlaps.

Method: Train deep learning models with pseudo-labels from ALS data, enhanced by SAM 2, to segment trees in RGB/multispectral images.

Result: The method outperforms general-domain models by avoiding manual annotation and leveraging enhanced pseudo-labels.

Conclusion: ALS-derived labels enhanced by SAM 2 provide cost-effective, high-quality training data for domain-specific tree crown segmentation.

Abstract: Mapping individual tree crowns is essential for tasks such as maintaining urban tree inventories and monitoring forest health, which help us understand and care for our environment. However, automatically separating the crowns from each other in aerial imagery is challenging due to factors such as the texture and partial tree crown overlaps. In this study, we present a method to train deep learning models that segment and separate individual trees from RGB and multispectral images, using pseudo-labels derived from aerial laser scanning (ALS) data. Our study shows that the ALS-derived pseudo-labels can be enhanced using a zero-shot instance segmentation model, Segment Anything Model 2 (SAM 2). Our method offers a way to obtain domain-specific training annotations for optical image-based models without any manual annotation cost, leading to segmentation models which outperform any available models which have been targeted for general domain deployment on the same task.

</details>


### [56] [FedHENet: A Frugal Federated Learning Framework for Heterogeneous Environments](https://arxiv.org/abs/2602.13024)
*Alejandro Dopico-Castro,Oscar Fontenla-Romero,Bertha Guijarro-Berdiñas,Amparo Alonso-Betanzos,Iván Pérez Digón*

Main category: cs.CV

TL;DR: FedHENet is a federated learning method for image classification that avoids costly local fine-tuning by using a fixed feature extractor and learning only a single output layer, achieving competitive accuracy, stability, and energy efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional federated learning methods involve expensive iterative optimization and risk privacy via shared gradients. FedHENet aims to address these issues by simplifying the training process and enhancing privacy.

Method: FedHENet uses a pre-trained feature extractor and learns a single output layer analytically aggregated using homomorphic encryption, avoiding iterative optimization.

Result: FedHENet achieves competitive accuracy, superior stability, and up to 70% better energy efficiency compared to iterative FL baselines.

Conclusion: FedHENet offers a privacy-compliant, energy-efficient, and hyperparameter-free alternative to traditional federated learning methods.

Abstract: Federated Learning (FL) enables collaborative training without centralizing data, essential for privacy compliance in real-world scenarios involving sensitive visual information. Most FL approaches rely on expensive, iterative deep network optimization, which still risks privacy via shared gradients. In this work, we propose FedHENet, extending the FedHEONN framework to image classification. By using a fixed, pre-trained feature extractor and learning only a single output layer, we avoid costly local fine-tuning. This layer is learned by analytically aggregating client knowledge in a single round of communication using homomorphic encryption (HE). Experiments show that FedHENet achieves competitive accuracy compared to iterative FL baselines while demonstrating superior stability performance and up to 70\% better energy efficiency. Crucially, our method is hyperparameter-free, removing the carbon footprint associated with hyperparameter tuning in standard FL. Code available in https://github.com/AlejandroDopico2/FedHENet/

</details>


### [57] [Implicit-Scale 3D Reconstruction for Multi-Food Volume Estimation from Monocular Images](https://arxiv.org/abs/2602.13041)
*Yuhao Chen,Gautham Vinod,Siddeshwar Raghavan,Talha Ibn Mahmud,Bruce Coburn,Jinge Ma,Fengqing Zhu,Jiangpeng He*

Main category: cs.CV

TL;DR: Implicit-Scale 3D Reconstruction from Monocular Multi-Food Images is a benchmark dataset for food portion estimation via geometric reasoning in realistic dining scenarios, outperforming appearance-based methods.


<details>
  <summary>Details</summary>
Motivation: Existing dietary assessment methods lack geometric reasoning and struggle with scale ambiguity. This benchmark aims to address these limitations by focusing on implicit-scale 3D reconstruction.

Method: The benchmark introduces a dataset with multi-food scenes, occlusions, and contextual objects (plates, utensils) to infer scale implicitly. Geometry-based reconstruction methods are evaluated.

Result: Geometry-based methods outperform vision-language baselines, achieving 0.21 MAPE in volume estimation and 5.7 L1 Chamfer Distance in geometric accuracy.

Conclusion: Geometry-based reconstruction improves accuracy and robustness in food portion estimation, validating its effectiveness for real-world dining scenarios.

Abstract: We present Implicit-Scale 3D Reconstruction from Monocular Multi-Food Images, a benchmark dataset designed to advance geometry-based food portion estimation in realistic dining scenarios. Existing dietary assessment methods largely rely on single-image analysis or appearance-based inference, including recent vision-language models, which lack explicit geometric reasoning and are sensitive to scale ambiguity. This benchmark reframes food portion estimation as an implicit-scale 3D reconstruction problem under monocular observations. To reflect real-world conditions, explicit physical references and metric annotations are removed; instead, contextual objects such as plates and utensils are provided, requiring algorithms to infer scale from implicit cues and prior knowledge. The dataset emphasizes multi-food scenes with diverse object geometries, frequent occlusions, and complex spatial arrangements. The benchmark was adopted as a challenge at the MetaFood 2025 Workshop, where multiple teams proposed reconstruction-based solutions. Experimental results show that while strong vision--language baselines achieve competitive performance, geometry-based reconstruction methods provide both improved accuracy and greater robustness, with the top-performing approach achieving 0.21 MAPE in volume estimation and 5.7 L1 Chamfer Distance in geometric accuracy.

</details>


### [58] [Curriculum-DPO++: Direct Preference Optimization via Data and Model Curricula for Text-to-Image Generation](https://arxiv.org/abs/2602.13055)
*Florinel-Alin Croitoru,Vlad Hondru,Radu Tudor Ionescu,Nicu Sebe,Mubarak Shah*

Main category: cs.CV

TL;DR: Curriculum-DPO++ enhances DPO by adding a model-level curriculum to dynamically increase learning capacity during training, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods like RLHF and DPO don't account for varying preference-learning difficulties, leading to suboptimal optimization.

Method: Combines data-level and model-level curricula, dynamically unfreezing layers and increasing LoRA rank dimensions during training.

Result: Outperforms Curriculum-DPO and other methods on nine benchmarks in text alignment, aesthetics, and human preference.

Conclusion: Curriculum-DPO++ improves preference optimization by adapting learning capacity dynamically, achieving better performance.

Abstract: Direct Preference Optimization (DPO) has been proposed as an effective and efficient alternative to reinforcement learning from human feedback (RLHF). However, neither RLHF nor DPO take into account the fact that learning certain preferences is more difficult than learning other preferences, rendering the optimization process suboptimal. To address this gap in text-to-image generation, we recently proposed Curriculum-DPO, a method that organizes image pairs by difficulty. In this paper, we introduce Curriculum-DPO++, an enhanced method that combines the original data-level curriculum with a novel model-level curriculum. More precisely, we propose to dynamically increase the learning capacity of the denoising network as training advances. We implement this capacity increase via two mechanisms. First, we initialize the model with only a subset of the trainable layers used in the original Curriculum-DPO. As training progresses, we sequentially unfreeze layers until the configuration matches the full baseline architecture. Second, as the fine-tuning is based on Low-Rank Adaptation (LoRA), we implement a progressive schedule for the dimension of the low-rank matrices. Instead of maintaining a fixed capacity, we initialize the low-rank matrices with a dimension significantly smaller than that of the baseline. As training proceeds, we incrementally increase their rank, allowing the capacity to grow until it converges to the same rank value as in Curriculum-DPO. Furthermore, we propose an alternative ranking strategy to the one employed by Curriculum-DPO. Finally, we compare Curriculum-DPO++ against Curriculum-DPO and other state-of-the-art preference optimization approaches on nine benchmarks, outperforming the competing methods in terms of text alignment, aesthetics and human preference. Our code is available at https://github.com/CroitoruAlin/Curriculum-DPO.

</details>


### [59] [A Calibrated Memorization Index (MI) for Detecting Training Data Leakage in Generative MRI Models](https://arxiv.org/abs/2602.13066)
*Yash Deo,Yan Jia,Toni Lassila,Victoria J Hodge,Alejandro F Frang,Chenghao Qian,Siyuan Kang,Ibrahim Habli*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Image generative models are known to duplicate images from the training data as part of their outputs, which can lead to privacy concerns when used for medical image generation. We propose a calibrated per-sample metric for detecting memorization and duplication of training data. Our metric uses image features extracted using an MRI foundation model, aggregates multi-layer whitened nearest-neighbor similarities, and maps them to a bounded \emph{Overfit/Novelty Index} (ONI) and \emph{Memorization Index} (MI) scores. Across three MRI datasets with controlled duplication percentages and typical image augmentations, our metric robustly detects duplication and provides more consistent metric values across datasets. At the sample level, our metric achieves near-perfect detection of duplicates.

</details>


### [60] [SIEFormer: Spectral-Interpretable and -Enhanced Transformer for Generalized Category Discovery](https://arxiv.org/abs/2602.13067)
*Chunming Li,Shidong Wang,Tong Xin,Haofeng Zhang*

Main category: cs.CV

TL;DR: SIEFormer enhances Vision Transformers using spectral analysis, improving feature adaptability for Generalized Category Discovery (GCD) tasks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to reinterpret and enhance the attention mechanism in Vision Transformers by incorporating spectral analysis, addressing challenges in GCD tasks where adaptability and feature modeling are critical.

Method: SIEFormer consists of two branches: an implicit branch using graph Laplacians and a Band-adaptive Filter layer for local token correlations, and an explicit branch with a Maneuverable Filtering Layer (MFL) that modulates features in the frequency domain via Fourier transforms.

Result: The approach achieves state-of-the-art performance on multiple image recognition datasets, validated by ablation studies and visualizations.

Conclusion: SIEFormer successfully integrates spectral perspectives into Vision Transformers, demonstrating superior feature adaptability and performance in GCD tasks.

Abstract: This paper presents a novel approach, Spectral-Interpretable and -Enhanced Transformer (SIEFormer), which leverages spectral analysis to reinterpret the attention mechanism within Vision Transformer (ViT) and enhance feature adaptability, with particular emphasis on challenging Generalized Category Discovery (GCD) tasks. The proposed SIEFormer is composed of two main branches, each corresponding to an implicit and explicit spectral perspective of the ViT, enabling joint optimization. The implicit branch realizes the use of different types of graph Laplacians to model the local structure correlations of tokens, along with a novel Band-adaptive Filter (BaF) layer that can flexibly perform both band-pass and band-reject filtering. The explicit branch, on the other hand, introduces a Maneuverable Filtering Layer (MFL) that learns global dependencies among tokens by applying the Fourier transform to the input ``value" features, modulating the transformed signal with a set of learnable parameters in the frequency domain, and then performing an inverse Fourier transform to obtain the enhanced features. Extensive experiments reveal state-of-the-art performance on multiple image recognition datasets, reaffirming the superiority of our approach through ablation studies and visualizations.

</details>


### [61] [Universal Transformation of One-Class Classifiers for Unsupervised Anomaly Detection](https://arxiv.org/abs/2602.13091)
*Declan McIntosh,Alexandra Branzan Albu*

Main category: cs.CV

TL;DR: A dataset folding method transforms one-class classifiers into unsupervised anomaly detectors by leveraging weak assumptions about anomalies, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Anomaly detection is vital for real-world applications but suffers from reliance on training data purity. The paper addresses susceptibility to label noise.

Method: Proposes dataset folding, using multiple independently trained one-class classifiers to filter anomalies based on assumptions of rarity and heterogeneity.

Result: Achieves state-of-the-art unsupervised anomaly detection on MVTec AD, ViSA, and MVTec Loco AD datasets without modifying underlying detectors.

Conclusion: The method bridges one-class and unsupervised anomaly detection, enabling direct transfer of future improvements.

Abstract: Detecting anomalies in images and video is an essential task for multiple real-world problems, including industrial inspection, computer-assisted diagnosis, and environmental monitoring. Anomaly detection is typically formulated as a one-class classification problem, where the training data consists solely of nominal values, leaving methods built on this assumption susceptible to training label noise. We present a dataset folding method that transforms an arbitrary one-class classifier-based anomaly detector into a fully unsupervised method. This is achieved by making a set of key weak assumptions: that anomalies are uncommon in the training dataset and generally heterogeneous. These assumptions enable us to utilize multiple independently trained instances of a one-class classifier to filter the training dataset for anomalies. This transformation requires no modifications to the underlying anomaly detector; the only changes are algorithmically selected data subsets used for training. We demonstrate that our method can transform a wide variety of one-class classifier anomaly detectors for both images and videos into unsupervised ones. Our method creates the first unsupervised logical anomaly detectors by transforming existing methods. We also demonstrate that our method achieves state-of-the-art performance for unsupervised anomaly detection on the MVTec AD, ViSA, and MVTec Loco AD datasets. As improvements to one-class classifiers are made, our method directly transfers those improvements to the unsupervised domain, linking the domains.

</details>


### [62] [Realistic Face Reconstruction from Facial Embeddings via Diffusion Models](https://arxiv.org/abs/2602.13168)
*Dong Han,Yong Li,Joachim Denzler*

Main category: cs.CV

TL;DR: The paper proposes Face Embedding Mapping (FEM), a framework using Kolmogorov-Arnold Network (KAN) and an Identity-Preserving diffusion model to reconstruct high-resolution faces from embeddings, testing privacy risks in FR and PPFR systems.


<details>
  <summary>Details</summary>
Motivation: To address the lack of studies verifying privacy risks in PPFR systems by reconstructing realistic face images from embeddings.

Method: FEM leverages KAN and a pre-trained Identity-Preserving diffusion model to perform embedding-to-face attacks on FR and PPFR systems.

Result: Reconstructed faces can access other FR systems, and FEM handles partial/protected embeddings robustly.

Conclusion: FEM serves as a tool to evaluate privacy leakage risks in FR/PPFR systems.

Abstract: With the advancement of face recognition (FR) systems, privacy-preserving face recognition (PPFR) systems have gained popularity for their accurate recognition, enhanced facial privacy protection, and robustness to various attacks. However, there are limited studies to further verify privacy risks by reconstructing realistic high-resolution face images from embeddings of these systems, especially for PPFR. In this work, we propose the face embedding mapping (FEM), a general framework that explores Kolmogorov-Arnold Network (KAN) for conducting the embedding-to-face attack by leveraging pre-trained Identity-Preserving diffusion model against state-of-the-art (SOTA) FR and PPFR systems. Based on extensive experiments, we verify that reconstructed faces can be used for accessing other real-word FR systems. Besides, the proposed method shows the robustness in reconstructing faces from the partial and protected face embeddings. Moreover, FEM can be utilized as a tool for evaluating safety of FR and PPFR systems in terms of privacy leakage. All images used in this work are from public datasets.

</details>


### [63] [LongStream: Long-Sequence Streaming Autoregressive Visual Geometry](https://arxiv.org/abs/2602.13172)
*Chong Cheng,Xianda Chen,Tao Xie,Wei Yin,Weiqiang Ren,Qian Zhang,Xiaoyuang Guo,Hao Wang*

Main category: cs.CV

TL;DR: LongStream introduces a novel approach for stable, long-sequence 3D reconstruction by addressing pose anchoring, scale drift, and Transformer cache issues.


<details>
  <summary>Details</summary>
Motivation: Existing autoregressive models struggle with long sequences due to attention decay, scale drift, and extrapolation errors.

Method: LongStream replaces first-frame anchoring with keyframe-relative poses, introduces orthogonal scale learning, and resolves Transformer cache issues via cache-consistent training and periodic refresh.

Result: LongStream achieves state-of-the-art performance, enabling stable, metric-scale reconstruction over kilometer-scale sequences at 18 FPS.

Conclusion: LongStream effectively addresses long-sequence 3D reconstruction challenges, offering robust performance for large-scale scenes.

Abstract: Long-sequence streaming 3D reconstruction remains a significant open challenge. Existing autoregressive models often fail when processing long sequences. They typically anchor poses to the first frame, which leads to attention decay, scale drift, and extrapolation errors. We introduce LongStream, a novel gauge-decoupled streaming visual geometry model for metric-scale scene reconstruction across thousands of frames. Our approach is threefold. First, we discard the first-frame anchor and predict keyframe-relative poses. This reformulates long-range extrapolation into a constant-difficulty local task. Second, we introduce orthogonal scale learning. This method fully disentangles geometry from scale estimation to suppress drift. Finally, we solve Transformer cache issues such as attention-sink reliance and long-term KV-cache contamination. We propose cache-consistent training combined with periodic cache refresh. This approach suppresses attention degradation over ultra-long sequences and reduces the gap between training and inference. Experiments show LongStream achieves state-of-the-art performance. It delivers stable, metric-scale reconstruction over kilometer-scale sequences at 18 FPS. Project Page: https://3dagentworld.github.io/longstream/

</details>


### [64] [Monocular Markerless Motion Capture Enables Quantitative Assessment of Upper Extremity Reachable Workspace](https://arxiv.org/abs/2602.13176)
*Seth Donahue,J. D. Peiffer,R. Tyler Richardson,Yishan Zhong,Shaun Q. Y. Tan,Benoit Marteau,Stephanie R. Russo,May D. Wang,R. James Cotton,Ross Chafetz*

Main category: cs.CV

TL;DR: The paper validates a single-camera AI-driven method for assessing Upper Extremity Reachable Workspace (UERW), showing strong agreement with marker-based systems, especially with frontal camera views.


<details>
  <summary>Details</summary>
Motivation: To provide a clinically accessible and less technically complex method for quantifying UERW using AI-driven monocular motion capture, reducing barriers to adoption in clinical settings.

Method: Nine unimpaired adults performed a standardized UERW task while movements were captured using a marker-based system and eight FLIR cameras. Monocular video analysis was performed on frontal and offset camera views for comparison.

Result: The frontal camera view showed minimal bias ($0.61 \pm 0.12\%$) compared to the marker-based system, while the offset view underestimated workspace reached ($-5.66 \pm 0.45\%$).

Conclusion: The frontal monocular camera configuration is feasible for UERW assessment, particularly for anterior workspace evaluation, demonstrating clinical potential for simplified, single-camera assessments.

Abstract: To validate a clinically accessible approach for quantifying the Upper Extremity Reachable Workspace (UERW) using a single (monocular) camera and Artificial Intelligence (AI)-driven Markerless Motion Capture (MMC) for biomechanical analysis. Objective assessment and validation of these techniques for specific clinically oriented tasks are crucial for their adoption in clinical motion analysis. AI-driven monocular MMC reduces the barriers to adoption in the clinic and has the potential to reduce the overhead for analysis of this common clinical assessment. Nine adult participants with no impairments performed the standardized UERW task, which entails reaching targets distributed across a virtual sphere centered on the torso, with targets displayed in a VR headset. Movements were simultaneously captured using a marker-based motion capture system and a set of eight FLIR cameras. We performed monocular video analysis on two of these video camera views to compare a frontal and offset camera configurations. The frontal camera orientation demonstrated strong agreement with the marker-based reference, exhibiting a minimal mean bias of $0.61 \pm 0.12$ \% reachspace reached per octanct (mean $\pm$ standard deviation). In contrast, the offset camera view underestimated the percent workspace reached ($-5.66 \pm 0.45$ \% reachspace reached). Conclusion: The findings support the feasibility of a frontal monocular camera configuration for UERW assessment, particularly for anterior workspace evaluation where agreement with marker-based motion capture was highest. The overall performance demonstrates clinical potential for practical, single-camera assessments. This study provides the first validation of monocular MMC system for the assessment of the UERW task. By reducing technical complexity, this approach enables broader implementation of quantitative upper extremity mobility assessment.

</details>


### [65] [FlexAM: Flexible Appearance-Motion Decomposition for Versatile Video Generation Control](https://arxiv.org/abs/2602.13185)
*Mingzhi Sheng,Zekai Gu,Peng Li,Cheng Lin,Hao-Xiang Guo,Ying-Cong Chen,Yuan Liu*

Main category: cs.CV

TL;DR: FlexAM is a unified framework for video generation that disentangles appearance and motion using a novel 3D control signal, achieving superior performance in diverse tasks.


<details>
  <summary>Details</summary>
Motivation: Effective and generalizable control in video generation is challenging due to reliance on ambiguous or task-specific signals. Disentangling appearance and motion is proposed as a robust solution.

Method: FlexAM introduces a novel 3D control signal represented as a point cloud, featuring multi-frequency and depth-aware positional encoding, along with flexible control signals.

Result: Extensive experiments show FlexAM excels in tasks like I2V/V2V editing, camera control, and spatial object editing.

Conclusion: FlexAM provides a scalable and effective solution for video generation by disentangling appearance and motion, outperforming existing methods.

Abstract: Effective and generalizable control in video generation remains a significant challenge. While many methods rely on ambiguous or task-specific signals, we argue that a fundamental disentanglement of "appearance" and "motion" provides a more robust and scalable pathway. We propose FlexAM, a unified framework built upon a novel 3D control signal. This signal represents video dynamics as a point cloud, introducing three key enhancements: multi-frequency positional encoding to distinguish fine-grained motion, depth-aware positional encoding, and a flexible control signal for balancing precision and generative quality. This representation allows FlexAM to effectively disentangle appearance and motion, enabling a wide range of tasks including I2V/V2V editing, camera control, and spatial object editing. Extensive experiments demonstrate that FlexAM achieves superior performance across all evaluated tasks.

</details>


### [66] [Conversational Image Segmentation: Grounding Abstract Concepts with Scalable Supervision](https://arxiv.org/abs/2602.13195)
*Aadarsh Sahoo,Georgia Gkioxari*

Main category: cs.CV

TL;DR: Conversational Image Segmentation (CIS) introduces ConverSeg, a benchmark for grounding abstract concepts into pixel-accurate masks, addressing gaps in functional and physical reasoning ignored by prior work.


<details>
  <summary>Details</summary>
Motivation: Prior work overlooks functional and physical reasoning in conversational queries, focusing only on categorical and spatial aspects. CIS aims to bridge this gap.

Method: ConverSeg-Net combines segmentation priors with language understanding, using an AI-powered data engine to generate unsupervised prompt-mask pairs.

Result: ConverSeg-Net outperforms existing models on CIS tasks while maintaining strong performance on traditional benchmarks.

Conclusion: CIS and ConverSeg-Net advance conversational image segmentation by integrating diverse reasoning facets, demonstrating significant improvements over current methods.

Abstract: Conversational image segmentation grounds abstract, intent-driven concepts into pixel-accurate masks. Prior work on referring image grounding focuses on categorical and spatial queries (e.g., "left-most apple") and overlooks functional and physical reasoning (e.g., "where can I safely store the knife?"). We address this gap and introduce Conversational Image Segmentation (CIS) and ConverSeg, a benchmark spanning entities, spatial relations, intent, affordances, functions, safety, and physical reasoning. We also present ConverSeg-Net, which fuses strong segmentation priors with language understanding, and an AI-powered data engine that generates prompt-mask pairs without human supervision. We show that current language-guided segmentation models are inadequate for CIS, while ConverSeg-Net trained on our data engine achieves significant gains on ConverSeg and maintains strong performance on existing language-guided segmentation benchmarks. Project webpage: https://glab-caltech.github.io/converseg/

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [67] [A Lightweight LLM Framework for Disaster Humanitarian Information Classification](https://arxiv.org/abs/2602.12284)
*Han Jinzhen,Kim Jisung,Yang Jong Soo,Yun Hong Sik*

Main category: cs.CL

TL;DR: The paper develops a lightweight framework for classifying disaster tweets using parameter-efficient fine-tuning, achieving high accuracy with minimal resource usage.


<details>
  <summary>Details</summary>
Motivation: The need for timely humanitarian information classification from social media during disasters, constrained by limited resources in emergency settings.

Method: Parameter-efficient fine-tuning (LoRA, QLoRA) and retrieval-augmented generation (RAG) on Llama 3.1 8B, tested on a unified dataset integrating HumAID.

Result: LoRA achieved 79.62% accuracy (+37.79% over zero-shot), QLoRA retained 99.4% performance at half memory cost, and RAG degraded performance due to label noise.

Conclusion: The framework provides a practical, reproducible solution for crisis intelligence systems with limited computational resources.

Abstract: Timely classification of humanitarian information from social media is critical for effective disaster response. However, deploying large language models (LLMs) for this task faces challenges in resource-constrained emergency settings. This paper develops a lightweight, cost-effective framework for disaster tweet classification using parameter-efficient fine-tuning. We construct a unified experimental corpus by integrating and normalizing the HumAID dataset (76,484 tweets across 19 disaster events) into a dual-task benchmark: humanitarian information categorization and event type identification. Through systematic evaluation of prompting strategies, LoRA fine-tuning, and retrieval-augmented generation (RAG) on Llama 3.1 8B, we demonstrate that: (1) LoRA achieves 79.62% humanitarian classification accuracy (+37.79% over zero-shot) while training only ~2% of parameters; (2) QLoRA enables efficient deployment with 99.4% of LoRA performance at 50% memory cost; (3) contrary to common assumptions, RAG strategies degrade fine-tuned model performance due to label noise from retrieved examples. These findings establish a practical, reproducible pipeline for building reliable crisis intelligence systems with limited computational resources.

</details>


### [68] [From Biased Chatbots to Biased Agents: Examining Role Assignment Effects on LLM Agent Robustness](https://arxiv.org/abs/2602.12285)
*Linbo Cao,Lihao Sun,Yang Yue*

Main category: cs.CL

TL;DR: Persona assignments in LLM agents can degrade task performance by introducing biases, raising concerns for safe deployment.


<details>
  <summary>Details</summary>
Motivation: To explore how demographic-based persona assignments affect LLM agents' task performance and reliability, given their real-world impacts.

Method: A systematic case study evaluating widely deployed models on agentic benchmarks across strategic reasoning, planning, and technical operations.

Result: Performance degradation up to 26.2% due to task-irrelevant persona cues, observed across task types and model architectures.

Conclusion: Persona assignments introduce implicit biases and behavioral volatility, posing risks for robust LLM agent deployment.

Abstract: Large Language Models (LLMs) are increasingly deployed as autonomous agents capable of actions with real-world impacts beyond text generation. While persona-induced biases in text generation are well documented, their effects on agent task performance remain largely unexplored, even though such effects pose more direct operational risks. In this work, we present the first systematic case study showing that demographic-based persona assignments can alter LLM agents' behavior and degrade performance across diverse domains. Evaluating widely deployed models on agentic benchmarks spanning strategic reasoning, planning, and technical operations, we uncover substantial performance variations - up to 26.2% degradation, driven by task-irrelevant persona cues. These shifts appear across task types and model architectures, indicating that persona conditioning and simple prompt injections can distort an agent's decision-making reliability. Our findings reveal an overlooked vulnerability in current LLM agentic systems: persona assignments can introduce implicit biases and increase behavioral volatility, raising concerns for the safe and robust deployment of LLM agents.

</details>


### [69] [Retrieval-Augmented Self-Taught Reasoning Model with Adaptive Chain-of-Thought for ASR Named Entity Correction](https://arxiv.org/abs/2602.12287)
*Junjie An,Jingguang Tian,Tianyi Wang,Yu Gao,Xiaofeng Mou,Yi Xu*

Main category: cs.CL

TL;DR: A novel retrieval-augmented generation framework improves named entity correction in ASR by combining rephrasing, phonetic-level retrieval, and adaptive reasoning, achieving significant error rate reductions.


<details>
  <summary>Details</summary>
Motivation: Domain-specific named entities in ASR systems often lead to misrecognitions and downstream failures, requiring advanced correction methods that fully utilize LLMs' reasoning capabilities.

Method: The proposed framework includes a rephrasing language model for named entity recognition with phonetic-level candidate retrieval and a self-taught reasoning model (A-STAR) that adapts its reasoning depth based on task difficulty.

Result: Experiments on AISHELL-1 and Homophone datasets show error rate reductions of 17.96% and 34.42%, respectively, compared to a strong baseline.

Conclusion: The method effectively corrects named entity errors in ASR by leveraging retrieval-augmented generation and adaptive reasoning, outperforming existing approaches.

Abstract: End-to-end automatic speech recognition (ASR) systems frequently misrecognize domain-specific phrases like named entities, which can cause catastrophic failures in downstream tasks. A new family of named entity correction methods based on large language models (LLMs) has recently emerged. However, these approaches have yet to fully exploit the sophisticated reasoning capabilities inherent to LLMs. To bridge this gap, we propose a novel retrieval-augmented generation framework for correcting named entity errors in ASR. Our approach consists of two key components: (1) a rephrasing language model (RLM) for named entity recognition, followed by candidate retrieval using a phonetic-level edit distance; and (2) a novel self-taught reasoning model with adaptive chain-of-thought (A-STAR) that dynamically adjusts the depth of its reasoning based on task difficulty. Experiments on the AISHELL-1 and Homophone datasets demonstrate the effectiveness of our method, which achieves relative reductions in the named entity character error rate of 17.96\% and 34.42\%, respectively, compared to a strong baseline.

</details>


### [70] [Grandes Modelos de Linguagem Multimodais (MLLMs): Da Teoria à Prática](https://arxiv.org/abs/2602.12302)
*Neemias da Silva,Júlio C. W. Scholz,John Harrison,Marina Borges,Paulo Ávila,Frances A Santos,Myriam Delgado,Rodrigo Minetto,Thiago H Silva*

Main category: cs.CL

TL;DR: The chapter covers Multimodal Large Language Models (MLLMs), their fundamentals, practical techniques, and challenges, with supplementary material available online.


<details>
  <summary>Details</summary>
Motivation: Advance AI by integrating language models with multimodal perception (image, audio).

Method: Explores preprocessing, prompt engineering, and building multimodal pipelines using LangChain and LangGraph.

Result: Provides practical techniques and supplementary resources for MLLMs.

Conclusion: Discusses challenges and future trends in MLLMs.

Abstract: Multimodal Large Language Models (MLLMs) combine the natural language understanding and generation capabilities of LLMs with perception skills in modalities such as image and audio, representing a key advancement in contemporary AI. This chapter presents the main fundamentals of MLLMs and emblematic models. Practical techniques for preprocessing, prompt engineering, and building multimodal pipelines with LangChain and LangGraph are also explored. For further practical study, supplementary material is publicly available online: https://github.com/neemiasbsilva/MLLMs-Teoria-e-Pratica. Finally, the chapter discusses the challenges and highlights promising trends.

</details>


### [71] [RankLLM: Weighted Ranking of LLMs by Quantifying Question Difficulty](https://arxiv.org/abs/2602.12424)
*Ziqian Zhang,Xingjian Hu,Yue Huang,Kai Zhang,Ruoxi Chen,Yixin Liu,Qingsong Wen,Kaidi Xu,Xiangliang Zhang,Neil Zhenqiang Gong,Lichao Sun*

Main category: cs.CL

TL;DR: RankLLM is a framework that evaluates LLMs by quantifying question difficulty and model competency, outperforming existing benchmarks with high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack differentiation in question difficulty, limiting their ability to effectively assess and compare LLM capabilities.

Method: RankLLM introduces bidirectional score propagation between models and questions, assigning competency scores to models and difficulty scores to questions based on performance.

Result: RankLLM achieves 90% agreement with human judgments, outperforms baselines like IRT, and demonstrates stability, fast convergence, and computational efficiency.

Conclusion: RankLLM provides a practical, difficulty-aware solution for large-scale LLM evaluation, enabling more nuanced comparisons of model capabilities.

Abstract: Benchmarks establish a standardized evaluation framework to systematically assess the performance of large language models (LLMs), facilitating objective comparisons and driving advancements in the field. However, existing benchmarks fail to differentiate question difficulty, limiting their ability to effectively distinguish models' capabilities. To address this limitation, we propose RankLLM, a novel framework designed to quantify both question difficulty and model competency. RankLLM introduces difficulty as the primary criterion for differentiation, enabling a more fine-grained evaluation of LLM capabilities. RankLLM's core mechanism facilitates bidirectional score propagation between models and questions. The core intuition of RankLLM is that a model earns a competency score when it correctly answers a question, while a question's difficulty score increases when it challenges a model. Using this framework, we evaluate 30 models on 35,550 questions across multiple domains. RankLLM achieves 90% agreement with human judgments and consistently outperforms strong baselines such as IRT. It also exhibits strong stability, fast convergence, and high computational efficiency, making it a practical solution for large-scale, difficulty-aware LLM evaluation.

</details>


### [72] [RBCorr: Response Bias Correction in Language Models](https://arxiv.org/abs/2602.12445)
*Om Bhatt,Anna A. Ivanova*

Main category: cs.CL

TL;DR: The paper proposes $	exttt{RBCorr}$, a simple method to correct response biases in language models, demonstrating its effectiveness across various models and question types.


<details>
  <summary>Details</summary>
Motivation: Language models exhibit response biases in fixed-response questions, necessitating low-cost correction methods to improve performance and evaluation accuracy.

Method: The authors introduce $	exttt{RBCorr}$, a response bias correction strategy, tested on 12 open-weight LMs using yes-no, entailment, and multiple-choice questions.

Result: Response bias is widespread pre-correction; $	exttt{RBCorr}$ effectively eliminates bias and enhances model performance. Its success depends on model, dataset, and prompt format.

Conclusion: $	exttt{RBCorr}$ is a practical solution to mitigate bias in smaller LMs, aligning benchmark performance with their true capabilities.

Abstract: Language models (LMs) are known to be prone to response biases, which present as option preference biases in fixed-response questions. It is therefore imperative to develop low-cost and effective response bias correction methods to improve LM performance and enable more accurate evaluations of model abilities. Here, we propose a simple response bias correction strategy ($\texttt{RBCorr}$) and test it on 12 open-weight language models using yes-no, entailment, and multiple choice questions. We show that response bias is prevalent in LMs pre-correction and that $\texttt{RBCorr}$ effectively eliminates bias and boosts model performance. We also explore the generalizability of bias behavior across models, datasets, and prompt formats, showing that LogProbs-based correction is highly dependent on all three of these aspects. Overall, $\texttt{RBCorr}$ is an easy-to-use method that can boost the performance of smaller LMs and ensure that LM performance on closed-response benchmarks aligns more closely with their true capabilities.

</details>


### [73] [Discovering Semantic Latent Structures in Psychological Scales: A Response-Free Pathway to Efficient Simplification](https://arxiv.org/abs/2602.12575)
*Bo Wang,Yuxuan Zhang,Yueqin Hu,Hanchao Hou,Kaiping Peng,Shiguang Ni*

Main category: cs.CL

TL;DR: TL;DR: A topic-modeling framework leverages NLP to refine psychological scales by analyzing semantic structure, reducing scale length by 60.5% while maintaining psychometric validity.


<details>
  <summary>Details</summary>
Motivation: Traditional scale refinement methods rely on response data, needing large samples and facing cross-cultural issues. Semantic analysis offers a response-free alternative.

Method: Uses contextual embeddings and density-based clustering to group items by latent semantic factors. Class-based term weighting and membership criteria select representative items.

Result: The method recovered factor-like groupings aligned with constructs, reduced scales by 60.5%, and preserved psychometric properties like internal consistency and correlation.

Conclusion: Semantic structure provides a viable response-free approach for scale refinement, formalized in a practical tool for easy adoption.

Abstract: Psychological scale refinement traditionally relies on response-based methods such as factor analysis, item response theory, and network psychometrics to optimize item composition. Although rigorous, these approaches require large samples and may be constrained by data availability and cross-cultural comparability. Recent advances in natural language processing suggest that the semantic structure of questionnaire items may encode latent construct organization, offering a complementary response-free perspective. We introduce a topic-modeling framework that operationalizes semantic latent structure for scale simplification. Items are encoded using contextual sentence embeddings and grouped via density-based clustering to discover latent semantic factors without predefining their number. Class-based term weighting derives interpretable topic representations that approximate constructs and enable merging of semantically adjacent clusters. Representative items are selected using membership criteria within an integrated reduction pipeline. We benchmarked the framework across DASS, IPIP, and EPOCH, evaluating structural recovery, internal consistency, factor congruence, correlation preservation, and reduction efficiency. The proposed method recovered coherent factor-like groupings aligned with established constructs. Selected items reduced scale length by 60.5% on average while maintaining psychometric adequacy. Simplified scales showed high concordance with original factor structures and preserved inter-factor correlations, indicating that semantic latent organization provides a response-free approximation of measurement structure. Our framework formalizes semantic structure as an inspectable front-end for scale construction and reduction. To facilitate adoption, we provide a visualization-supported tool enabling one-click semantic analysis and structured simplification.

</details>


### [74] [Unleashing Low-Bit Inference on Ascend NPUs: A Comprehensive Evaluation of HiFloat Formats](https://arxiv.org/abs/2602.12635)
*Pengxiang Zhao,Hui-Ling Zhen,Xing Li,Han Bao,Weizhe Lin,Zhiyuan Yang,Ziwei Yu,Xin Wang,Mingxuan Yuan,Xianzhi Yu,Zhenhua Dong*

Main category: cs.CL

TL;DR: HiFloat (HiF8 and HiF4) is a family of floating-point formats optimized for Ascend NPUs, outperforming INT8 for high-variance data and avoiding accuracy issues in 4-bit regimes.


<details>
  <summary>Details</summary>
Motivation: To address the need for efficient LLM inference on NPUs by leveraging low-bit floating-point formats.

Method: Evaluation of HiFloat formats (HiF8 and HiF4) across weight-activation and KV-cache tasks, comparing them with INT8 and other formats.

Result: HiFloat outperforms INT8 for high-variance data, avoids accuracy collapse in 4-bit regimes, and integrates seamlessly with quantization frameworks.

Conclusion: HiFloat offers a high-efficiency solution for LLM inference on NPUs, combining precision and compatibility with existing frameworks.

Abstract: As LLMs scale, low-bit floating-point formats like MXFP and NVFP4 offer new opportunities for precision and efficiency. In this work, we evaluate HiFloat (HiF8 and HiF4), a family of formats tailored for Ascend NPUs. Through rigorous comparison across weight-activation and KV-cache tasks, we provide three key insights: (1) INT8 suits narrow-range data, while floating-point formats excel with high-variance data; (2) in 4-bit regimes, HiF4's hierarchical scaling prevents the accuracy collapse seen in integer formats; and (3) HiFloat is fully compatible with state-of-the-art post-training quantization frameworks. Overall, HiFloat provides a solution for high-efficiency LLM inference on NPUs.

</details>


### [75] [CLASE: A Hybrid Method for Chinese Legalese Stylistic Evaluation](https://arxiv.org/abs/2602.12639)
*Yiran Rex Ma,Yuxiao Ye,Huiyuan Xie*

Main category: cs.CL

TL;DR: CLASE is a hybrid evaluation method combining linguistic features and LLM-as-a-judge scores to assess stylistic quality in legal texts, outperforming traditional metrics and aligning better with human judgment.


<details>
  <summary>Details</summary>
Motivation: Legal texts generated by LLMs often lack stylistic adherence to legal conventions, and existing evaluation methods fail to reliably measure stylistic fidelity without conflating semantic accuracy or suffering from opacity.

Method: CLASE integrates linguistic feature-based scores and LLM-as-a-judge scores, learned from contrastive pairs of authentic legal documents and LLM-restored versions, capturing both surface-level and implicit stylistic norms.

Result: Experiments on 200 Chinese legal documents show CLASE aligns substantially better with human judgments than traditional metrics or pure LLM-as-a-judge methods, providing interpretable score breakdowns and improvement suggestions.

Conclusion: CLASE offers a scalable, transparent, and practical solution for evaluating stylistic quality in legal text generation, addressing limitations of existing methods.

Abstract: Legal text generated by large language models (LLMs) can usually achieve reasonable factual accuracy, but it frequently fails to adhere to the specialised stylistic norms and linguistic conventions of legal writing. In order to improve stylistic quality, a crucial first step is to establish a reliable evaluation method. However, having legal experts manually develop such a metric is impractical, as the implicit stylistic requirements in legal writing practice are difficult to formalise into explicit rubrics. Meanwhile, existing automatic evaluation methods also fall short: reference-based metrics conflate semantic accuracy with stylistic fidelity, and LLM-as-a-judge evaluations suffer from opacity and inconsistency. To address these challenges, we introduce CLASE (Chinese LegAlese Stylistic Evaluation), a hybrid evaluation method that focuses on the stylistic performance of legal text. The method incorporates a hybrid scoring mechanism that combines 1) linguistic feature-based scores and 2) experience-guided LLM-as-a-judge scores. Both the feature coefficients and the LLM scoring experiences are learned from contrastive pairs of authentic legal documents and their LLM-restored counterparts. This hybrid design captures both surface-level features and implicit stylistic norms in a transparent, reference-free manner. Experiments on 200 Chinese legal documents show that CLASE achieves substantially higher alignment with human judgments than traditional metrics and pure LLM-as-a-judge methods. Beyond improved alignment, CLASE provides interpretable score breakdowns and suggestions for improvements, offering a scalable and practical solution for professional stylistic evaluation in legal text generation (Code and data for CLASE is available at: https://github.com/rexera/CLASE).

</details>


### [76] [Beyond Normalization: Rethinking the Partition Function as a Difficulty Scheduler for RLVR](https://arxiv.org/abs/2602.12642)
*Dohyung Kim,Minbeom Kim,Jeonghye Kim,Sangmook Lee,Sojeong Rhee,Kyomin Jung*

Main category: cs.CL

TL;DR: PACED-RL improves sample efficiency in RL training for LLMs by reusing partition function information as accuracy estimates, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Existing RL methods enhance LLM reasoning but reduce output diversity. GFlowNets address this but underutilize the partition function, which can signal per-prompt accuracy.

Method: PACED-RL leverages the partition function as accuracy estimates to prioritize prompts and uses error-prioritized replay, integrating these into GFlowNet training.

Result: PACED-RL outperforms GRPO and prior GFlowNet approaches across benchmarks, demonstrating improved sample efficiency.

Conclusion: PACED-RL offers a promising direction for efficient distribution-matching training in LLMs by effectively reusing existing optimization information.

Abstract: Reward-maximizing RL methods enhance the reasoning performance of LLMs, but often reduce the diversity among outputs. Recent works address this issue by adopting GFlowNets, training LLMs to match a target distribution while jointly learning its partition function. In contrast to prior works that treat this partition function solely as a normalizer, we reinterpret it as a per-prompt expected-reward (i.e., online accuracy) signal, leveraging this unused information to improve sample efficiency. Specifically, we first establish a theoretical relationship between the partition function and per-prompt accuracy estimates. Building on this key insight, we propose Partition Function-Guided RL (PACED-RL), a post-training framework that leverages accuracy estimates to prioritize informative question prompts during training, and further improves sample efficiency through an accuracy estimate error-prioritized replay. Crucially, both components reuse information already produced during GFlowNet training, effectively amortizing the compute overhead into the existing optimization process. Extensive experiments across diverse benchmarks demonstrate strong performance improvements over GRPO and prior GFlowNet approaches, highlighting PACED-RL as a promising direction for a more sample efficient distribution-matching training for LLMs.

</details>


### [77] [Learning Ordinal Probabilistic Reward from Preferences](https://arxiv.org/abs/2602.12660)
*Longze Chen,Lu Wang,Renke Shan,Ze Gong,Run Luo,Jiaming Li,Jing Luo,Qiyao Wang,Min Yang*

Main category: cs.CL

TL;DR: The paper introduces Probabilistic Reward Model (PRM) to overcome limitations of existing GRMs and DRMs, proposing its discrete variant OPRM and training strategy RgFT, improving accuracy by 2.9%∼7.4%.


<details>
  <summary>Details</summary>
Motivation: Existing reward models (GRMs and DRMs) have limitations: GRMs need costly supervision, and DRMs lack probabilistic interpretation. The authors aim to address these issues.

Method: They introduce PRM, modeling reward as a random variable, and its discrete variant OPRM. RgFT training strategy incorporates quality-level annotations for better performance.

Result: Experiments show OPRM improves accuracy by 2.9%∼7.4% over prior models, capturing both relative rankings and absolute quality.

Conclusion: PRM and OPRM offer a robust, data-efficient solution for reward modeling, outperforming existing methods.

Abstract: Reward models are crucial for aligning large language models (LLMs) with human values and intentions. Existing approaches follow either Generative (GRMs) or Discriminative (DRMs) paradigms, yet both suffer from limitations: GRMs typically demand costly point-wise supervision, while DRMs produce uncalibrated relative scores that lack probabilistic interpretation. To address these challenges, we introduce a novel reward modeling paradigm: Probabilistic Reward Model (PRM). Instead of modeling reward as a deterministic scalar, our approach treats it as a random variable, learning a full probability distribution for the quality of each response. To make this paradigm practical, we present its closed-form, discrete realization: the Ordinal Probabilistic Reward Model (OPRM), which discretizes the quality score into a finite set of ordinal ratings. Building on OPRM, we propose a data-efficient training strategy called Region Flooding Tuning (RgFT). It enables rewards to better reflect absolute text quality by incorporating quality-level annotations, which guide the model to concentrate the probability mass within corresponding rating sub-regions. Experiments on various reward model benchmarks show that our method improves accuracy by $\textbf{2.9%}\sim\textbf{7.4%}$ compared to prior reward models, demonstrating strong performance and data efficiency. Analysis of the score distribution provides evidence that our method captures not only relative rankings but also absolute quality.

</details>


### [78] [$\mathcal{X}$-KD: General Experiential Knowledge Distillation for Large Language Models](https://arxiv.org/abs/2602.12674)
*Yuang Cai,Yuyu Yuan*

Main category: cs.CL

TL;DR: Proposes Experiential Knowledge Distillation ($$\\mathcal{X}$-KD), a framework for LLMs to learn from teachers' original environments, outperforming baselines in tasks like summarization and translation.


<details>
  <summary>Details</summary>
Motivation: Existing KD approaches imitate teacher behavior but ignore the teacher's original learning environment, limiting student learning potential.

Method: Introduces $$\\mathcal{X}$-KD using AVRIL to model teacher rewards and distill policies, ensuring student-teacher consistency.

Result: Outperforms baselines in summarization, translation, and reasoning, with better performance-diversity trade-offs and data efficiency.

Conclusion: $$\\mathcal{X}$-KD is a simple, flexible framework enhancing KD effectiveness by leveraging teachers' original learning environments.

Abstract: Knowledge Distillation (KD) for Large Language Models (LLMs) has become increasingly important as models grow in size and complexity. While existing distillation approaches focus on imitating teacher behavior, they often overlook the original learning environment that shaped the teacher's knowledge. Inspired by the experiential learning theory and inverse reinforcement learning, we propose Experiential Knowledge Distillation ($\mathcal{X}$-KD), a novel and general framework that enables student models to learn in the teacher's original learning environment. $\mathcal{X}$-KD adopts the Approximated Variational Reward Imitation Learning (AVRIL) framework to jointly model the teacher's original reward function and perform policy distillation, encouraging consistency between the student policy and the original reward function. Our derivation demonstrates that $\mathcal{X}$-KD follows the supervised learning framework and applies to both sequence-level and divergence-based distillation methods, underlining the simplicity and flexibility of our approach. Empirical results show that $\mathcal{X}$-KD outperforms the generalized KD and MiniLLM baselines on abstractive summarization, machine translation, and arithmetic reasoning tasks. Additionally, $\mathcal{X}$-KD achieves better performance-diversity trade-off and data efficiency than baseline KD approaches.

</details>


### [79] [MedXIAOHE: A Comprehensive Recipe for Building Medical MLLMs](https://arxiv.org/abs/2602.12705)
*Baorong Shi,Bo Cui,Boyuan Jiang,Deli Yu,Fang Qian,Haihua Yang,Huichao Wang,Jiale Chen,Jianfei Pan,Jieqiong Cao,Jinghao Lin,Kai Wu,Lin Yang,Shengsheng Yao,Tao Chen,Xiaojun Xiao,Xiaozhong Ji,Xu Wang,Yijun He,Zhixiong Yang*

Main category: cs.CL

TL;DR: MedXIAOHE is a medical vision-language foundation model that excels in general-purpose medical understanding and reasoning, outperforming other models through entity-aware pretraining and reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: The paper aims to advance medical AI by addressing the challenges of general-purpose medical understanding, long-tail knowledge gaps, and expert-level reasoning in clinical applications.

Method: The model uses an entity-aware continual pretraining framework to organize heterogeneous medical corpora and incorporates reinforcement learning for medical reasoning and tool-augmented training.

Result: MedXIAOHE achieves state-of-the-art performance across medical benchmarks, surpassing closed-source multimodal systems, and demonstrates improved reliability in real-world use.

Conclusion: The paper highlights MedXIAOHE's advancements in medical AI and aims to inspire further research with its practical design choices and evaluation framework.

Abstract: We present MedXIAOHE, a medical vision-language foundation model designed to advance general-purpose medical understanding and reasoning in real-world clinical applications. MedXIAOHE achieves state-of-the-art performance across diverse medical benchmarks and surpasses leading closed-source multimodal systems on multiple capabilities. To achieve this, we propose an entity-aware continual pretraining framework that organizes heterogeneous medical corpora to broaden knowledge coverage and reduce long-tail gaps (e.g., rare diseases). For medical expert-level reasoning and interaction, MedXIAOHE incorporates diverse medical reasoning patterns via reinforcement learning and tool-augmented agentic training, enabling multi-step diagnostic reasoning with verifiable decision traces. To improve reliability in real-world use, MedXIAOHE integrates user-preference rubrics, evidence-grounded reasoning, and low-hallucination long-form report generation, with improved adherence to medical instructions. We release this report to document our practical design choices, scaling insights, and evaluation framework, hoping to inspire further research.

</details>


### [80] [ReFilter: Improving Robustness of Retrieval-Augmented Generation via Gated Filter](https://arxiv.org/abs/2602.12709)
*Yixin Chen,Ying Xiong,Shangyu Wu,Xiangrui Ke,Nan Guan,Chun Jason Xue*

Main category: cs.CL

TL;DR: ReFilter is a latent-based fusion framework enhancing retrieval-augmented generation by token-level filtering and fusion, outperforming existing methods across QA benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing fusion methods struggle with scalability and inefficiency as retrieval scales increase, necessitating a more robust solution.

Method: ReFilter employs a context encoder, gated filter, and token fusion module for token-level filtering and integration into LLM hidden states.

Result: ReFilter excels in general-domain QA benchmarks and zero-shot biomedical QA, achieving 70.01% accuracy without domain fine-tuning.

Conclusion: ReFilter offers scalable and efficient fusion for retrieval-augmented generation, with strong performance across diverse benchmarks.

Abstract: Retrieval-augmented generation (RAG) has become a dominant paradigm for grounding large language models (LLMs) with external evidence in knowledge-intensive question answering. A core design choice is how to fuse retrieved samples into the LLMs, where existing internal fusion approaches broadly fall into query-based fusion, parametric fusion, and latent-based fusion. Despite their effectiveness at modest retrieval scales, these methods often fail to scale gracefully as the number of retrieved candidates k increases: Larger k improves evidence coverage, yet realistic top-k retrieval inevitably contains irrelevant or redundant content and increases the inference cost.
  To address these limitations, we propose ReFilter, a novel latent-based fusion framework that performs token-level filtering and fusion. ReFilter consists of three key components: a context encoder for encoding context features, a gated filter for weighting each token, and a token fusion module for integrating the weighted token feature into the LLM's hidden states. Our experiments across four general-domain QA benchmarks show that ReFilter consistently achieves the best average performance under both in-domain adaptation and out-of-domain transfer. ReFilter further generalizes to five biomedical QA benchmarks in zero-shot transfer without domain fine-tuning, reaching 70.01% average accuracy with Qwen2.5-14B-Instruct.

</details>


### [81] [Lamer-SSL: Layer-aware Mixture of LoRA Experts for Continual Multilingual Expansion of Self-supervised Models without Forgetting](https://arxiv.org/abs/2602.12746)
*Jing Xu,Minglin Wu,Xueyuan Chen,Xixin Wu,Helen Meng*

Main category: cs.CL

TL;DR: Lamer-SSL introduces a parameter-efficient framework combining a Layer-Aware MixturE of LoRA Experts (Lamer) module with replay to enhance generalization and reduce forgetting in self-supervised speech models.


<details>
  <summary>Details</summary>
Motivation: Self-supervised speech models struggle with generalization to new languages and exhibit forgetting during continual training.

Method: Integrates a Lamer module for shared/specific representation balance and layer-aware expert allocation, plus a replay strategy for knowledge retention.

Result: Improves ASR and LID performance on new languages while maintaining prior language performance with minimal trainable parameters (2.14%).

Conclusion: Lamer-SSL effectively extends self-supervised models' language adaptability and mitigates forgetting efficiently.

Abstract: Despite their impressive performance, self-supervised speech models often struggle to generalize to new languages and tend to forget previously acquired knowledge during continual training. To address this, we propose Lamer-SSL, a parameter-efficient framework that integrates a Layer-Aware MixturE of LoRA Experts (Lamer) module with a replay strategy. The Lamer module enables flexible balancing between shared and language-specific representations, while layer-aware expert allocation assigns more experts to deeper layers where semantic information is richer. Meanwhile, the replay strategy retains prior knowledge using minimal data, mitigating forgetting during continual training. Experiments on automatic speech recognition (ASR) and language identification (LID) demonstrate that Lamer-SSL extends self-supervised models to new languages effectively while maintaining strong performance on previously learned languages with only 2.14% parameters being trainable.

</details>


### [82] [Towards a Diagnostic and Predictive Evaluation Methodology for Sequence Labeling Tasks](https://arxiv.org/abs/2602.12759)
*Elena Alvarez-Mellado,Julio Gonzalo*

Main category: cs.CL

TL;DR: The paper proposes a novel evaluation methodology for sequence labeling tasks, focusing on error analysis to provide actionable insights and predict model performance across distributions.


<details>
  <summary>Details</summary>
Motivation: Standard NLP evaluations lack actionable insights and fail to predict performance on external data, prompting a need for a more informative approach.

Method: The method involves creating small, linguistically motivated test sets that exhaustively cover span attributes (e.g., shape, length, casing) instead of relying on large datasets.

Result: The methodology offers diagnostic, actionable, and predictive results, achieving a median correlation of 0.85 for predicting model performance on external datasets.

Conclusion: This approach improves evaluation by providing deeper insights into model weaknesses and better performance prediction.

Abstract: Standard evaluation in NLP typically indicates that system A is better on average than system B, but it provides little info on how to improve performance and, what is worse, it should not come as a surprise if B ends up being better than A on outside data. We propose an evaluation methodology for sequence labeling tasks grounded on error analysis that provides both quantitative and qualitative information on where systems must be improved and predicts how models will perform on a different distribution. The key is to create test sets that, contrary to common practice, do not rely on gathering large amounts of real-world in-distribution scraped data, but consists in handcrafting a small set of linguistically motivated examples that exhaustively cover the range of span attributes (such as shape, length, casing, sentence position, etc.) a system may encounter in the wild. We demonstrate this methodology on a benchmark for anglicism identification in Spanish. Our methodology provides results that are diagnostic (because they help identify systematic weaknesses in performance), actionable (because they can inform which model is better suited for a given scenario) and predictive: our method predicts model performance on external datasets with a median correlation of 0.85.

</details>


### [83] [Aspect-Based Sentiment Analysis for Future Tourism Experiences: A BERT-MoE Framework for Persian User Reviews](https://arxiv.org/abs/2602.12778)
*Hamidreza Kazemi Taskooh,Taha Zare Harofte*

Main category: cs.CL

TL;DR: The study introduces a hybrid BERT-based model for Persian ABSA in tourism, improving efficiency and accuracy while reducing power consumption.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges of low-resource languages like Persian in ABSA, particularly for tourism domain reviews.

Method: Proposes a hybrid BERT model with Top-K routing & auxiliary losses for sentiment classification, multi-label aspect extraction, and integrated ABSA.

Result: Achieves a 90.6% weighted F1-score for ABSA, outperforms baselines, and reduces GPU power consumption by 39%.

Conclusion: The model supports sustainable AI deployment and advances Persian ABSA, with a released dataset for future research.

Abstract: This study advances aspect-based sentiment analysis (ABSA) for Persian-language user reviews in the tourism domain, addressing challenges of low-resource languages. We propose a hybrid BERT-based model with Top-K routing and auxiliary losses to mitigate routing collapse and improve efficiency. The pipeline includes: (1) overall sentiment classification using BERT on 9,558 labeled reviews, (2) multi-label aspect extraction for six tourism-related aspects (host, price, location, amenities, cleanliness, connectivity), and (3) integrated ABSA with dynamic routing. The dataset consists of 58,473 preprocessed reviews from the Iranian accommodation platform Jabama, manually annotated for aspects and sentiments. The proposed model achieves a weighted F1-score of 90.6% for ABSA, outperforming baseline BERT (89.25%) and a standard hybrid approach (85.7%). Key efficiency gains include a 39% reduction in GPU power consumption compared to dense BERT, supporting sustainable AI deployment in alignment with UN SDGs 9 and 12. Analysis reveals high mention rates for cleanliness and amenities as critical aspects. This is the first ABSA study focused on Persian tourism reviews, and we release the annotated dataset to facilitate future multilingual NLP research in tourism.

</details>


### [84] [RAT-Bench: A Comprehensive Benchmark for Text Anonymization](https://arxiv.org/abs/2602.12806)
*Nataša Krčo,Zexi Yao,Matthieu Meeus,Yves-Alexandre de Montjoye*

Main category: cs.CL

TL;DR: The paper introduces RAT-Bench, a benchmark for evaluating text anonymization tools on re-identification risk, testing NER- and LLM-based methods. Findings show LLM-based anonymizers offer better privacy-utility trade-offs across languages but with higher computational costs.


<details>
  <summary>Details</summary>
Motivation: To assess the effectiveness of current text anonymization tools in preventing re-identification, given the increasing use of personal data in LLMs.

Method: Develops RAT-Bench using synthetic text with diverse identifiers, evaluating anonymization tools based on re-identification risk inferred by an LLM attacker.

Result: LLM-based anonymizers perform better in privacy-utility trade-offs but are computationally expensive. Performance varies, especially with non-standard direct identifiers and indirect identifiers.

Conclusion: Recommends improvements for future anonymization tools and releases the benchmark to encourage expansion, particularly to other regions.

Abstract: Data containing personal information is increasingly used to train, fine-tune, or query Large Language Models (LLMs). Text is typically scrubbed of identifying information prior to use, often with tools such as Microsoft's Presidio or Anthropic's PII purifier. These tools have traditionally been evaluated on their ability to remove specific identifiers (e.g., names), yet their effectiveness at preventing re-identification remains unclear. We introduce RAT-Bench, a comprehensive benchmark for text anonymization tools based on re-identification risk. Using U.S. demographic statistics, we generate synthetic text containing various direct and indirect identifiers across domains, languages, and difficulty levels. We evaluate a range of NER- and LLM-based text anonymization tools and, based on the attributes an LLM-based attacker is able to correctly infer from the anonymized text, we report the risk of re-identification in the U.S. population, while properly accounting for the disparate impact of identifiers. We find that, while capabilities vary widely, even the best tools are far from perfect in particular when direct identifiers are not written in standard ways and when indirect identifiers enable re-identification. Overall we find LLM-based anonymizers, including new iterative anonymizers, to provide a better privacy-utility trade-off albeit at a higher computational cost. Importantly, we also find them to work well across languages. We conclude with recommendations for future anonymization tools and will release the benchmark and encourage community efforts to expand it, in particular to other geographies.

</details>


### [85] [Left-right asymmetry in predicting brain activity from LLMs' representations emerges with their formal linguistic competence](https://arxiv.org/abs/2602.12811)
*Laurent Bonnasse-Gahot,Christophe Pallier*

Main category: cs.CL

TL;DR: The study explores why LLMs' ability to predict brain activity improves more in the left hemisphere than the right during training, linking this asymmetry to formal linguistic competence rather than arithmetic or world knowledge tasks.


<details>
  <summary>Details</summary>
Motivation: To understand why the left-right asymmetry in brain activity prediction emerges during LLM training and identify the specific competencies driving this asymmetry.

Method: Using the OLMo-2 7B language model at various training checkpoints and fMRI data, the study compares brain score asymmetry with performance on linguistic and non-linguistic benchmarks, extending the analysis to Pythia models and French.

Result: The left-right asymmetry co-emerges with formal linguistic abilities (e.g., grammar judgment and text production) but not with arithmetic, Dyck language tasks, or world knowledge tasks.

Conclusion: The left-right asymmetry in brain predictivity aligns with LLMs' progress in formal linguistic competence, not other cognitive tasks.

Abstract: When humans and large language models (LLMs) process the same text, activations in the LLMs correlate with brain activity measured, e.g., with functional magnetic resonance imaging (fMRI). Moreover, it has been shown that, as the training of an LLM progresses, the performance in predicting brain activity from its internal activations improves more in the left hemisphere than in the right one. The aim of the present work is to understand which kind of competence acquired by the LLMs underlies the emergence of this left-right asymmetry. Using the OLMo-2 7B language model at various training checkpoints and fMRI data from English participants, we compare the evolution of the left-right asymmetry in brain scores alongside performance on several benchmarks. We observe that the asymmetry co-emerges with the formal linguistic abilities of the LLM. These abilities are demonstrated in two ways: by the model's capacity to assign a higher probability to an acceptable sentence than to a grammatically unacceptable one within a minimal contrasting pair, or its ability to produce well-formed text. On the opposite, the left-right asymmetry does not correlate with the performance on arithmetic or Dyck language tasks; nor with text-based tasks involving world knowledge and reasoning. We generalize these results to another family of LLMs (Pythia) and another language, namely French. Our observations indicate that the left-right asymmetry in brain predictivity matches the progress in formal linguistic competence (knowledge of linguistic patterns).

</details>


### [86] [BaziQA-Benchmark: Evaluating Symbolic and Temporally Compositional Reasoning in Large Language Models](https://arxiv.org/abs/2602.12889)
*Jiangxi Chen,Qian Liu*

Main category: cs.CL

TL;DR: BaziQA-Benchmark is a standardized benchmark derived from 200 professionally curated problems to evaluate symbolic and temporally compositional reasoning in large language models objectively.


<details>
  <summary>Details</summary>
Motivation: To provide a reliable and controlled evaluation framework for assessing reasoning capabilities in large language models, addressing gaps in anecdotal or prompt-driven evaluations.

Method: The benchmark uses 200 multiple-choice problems from the Global Fortune-teller Competition (2021--2025), evaluated under a multi-turn setting. A lightweight Structured Reasoning Protocol is introduced to constrain inference order.

Result: Models consistently outperform chance but show significant sensitivity to temporal composition and reasoning order, with systematic failures in precise temporal localization and multi-condition symbolic judgments.

Conclusion: BaziQA-Benchmark offers a robust tool for evaluating reasoning in language models, revealing their limitations and sensitivities in symbolic and temporally compositional tasks.

Abstract: We present BaziQA-Benchmark, a standardized benchmark for evaluating symbolic and temporally compositional reasoning in large language models. The benchmark is derived from 200 professionally curated, multiple-choice problems from the Global Fortune-teller Competition (2021--2025), where each instance requires structured inference over a fixed symbolic chart and interacting temporal conditions. Unlike anecdotal or prompt-driven evaluations, BaziQA-Benchmark enables objective scoring and controlled comparison across years, domains, and model families. We evaluate contemporary language models under a multi-turn setting and analyze performance variation across temporal difficulty, reasoning domains, and inference protocols.To further probe reasoning behavior, we introduce a lightweight Structured Reasoning Protocol that constrains inference order without adding domain knowledge. Results show that models consistently outperform chance but remain far from saturation, exhibiting pronounced sensitivity to temporal composition and reasoning order, as well as systematic failures on precise temporal localization and multi-condition symbolic judgments.

</details>


### [87] [ViMedCSS: A Vietnamese Medical Code-Switching Speech Dataset & Benchmark](https://arxiv.org/abs/2602.12911)
*Tung X. Nguyen,Nhu Vo,Giang-Son Nguyen,Duy Mai Hoang,Chien Dinh Huynh,Inigo Jauregi Unanue,Massimo Piccardi,Wray Buntine,Dung D. Le*

Main category: cs.CL

TL;DR: The paper introduces ViMedCSS, a Vietnamese Medical Code-Switching Speech dataset, to address ASR challenges in recognizing English medical terms within Vietnamese speech. It evaluates ASR models and fine-tuning strategies, finding that combining Vietnamese optimization and multilingual pretraining yields the best results.


<details>
  <summary>Details</summary>
Motivation: Code-switching in Vietnamese medical communication poses challenges for ASR systems, especially for low-resource languages. Existing systems struggle with recognizing English medical terms in Vietnamese sentences, and there is no benchmark for this issue.

Method: The authors construct the ViMedCSS dataset (34 hours, 16,576 utterances) containing English medical terms within Vietnamese sentences. They evaluate state-of-the-art ASR models and test fine-tuning strategies to improve recognition.

Result: Vietnamese-optimized models perform better on general segments, while multilingual pretraining helps with English terms. Combining both approaches achieves the best balance.

Conclusion: This work establishes the first benchmark for Vietnamese medical code-switching and provides insights into effective domain adaptation for low-resource, multilingual ASR systems.

Abstract: Code-switching (CS), which is when Vietnamese speech uses English words like drug names or procedures, is a common phenomenon in Vietnamese medical communication. This creates challenges for Automatic Speech Recognition (ASR) systems, especially in low-resource languages like Vietnamese. Current most ASR systems struggle to recognize correctly English medical terms within Vietnamese sentences, and no benchmark addresses this challenge. In this paper, we construct a 34-hour \textbf{Vi}etnamese \textbf{Med}ical \textbf{C}ode-\textbf{S}witching \textbf{S}peech dataset (ViMedCSS) containing 16,576 utterances. Each utterance includes at least one English medical term drawn from a curated bilingual lexicon covering five medical topics. Using this dataset, we evaluate several state-of-the-art ASR models and examine different specific fine-tuning strategies for improving medical term recognition to investigate the best approach to solve in the dataset. Experimental results show that Vietnamese-optimized models perform better on general segments, while multilingual pretraining helps capture English insertions. The combination of both approaches yields the best balance between overall and code-switched accuracy. This work provides the first benchmark for Vietnamese medical code-switching and offers insights into effective domain adaptation for low-resource, multilingual ASR systems.

</details>


### [88] [When Words Don't Mean What They Say: Figurative Understanding in Bengali Idioms](https://arxiv.org/abs/2602.12921)
*Adib Sakhawat,Shamim Ara Parveen,Md Ruhul Amin,Shamim Al Mahmud,Md Saiful Islam,Tahera Khatun*

Main category: cs.CL

TL;DR: A new large-scale Bengali idiom dataset with detailed annotations is introduced to evaluate LLMs' figurative language understanding, revealing significant performance gaps compared to humans.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of figurative language understanding in LLMs, especially for low-resource languages like Bengali.

Method: Creation of a 10,361 Bengali idiom dataset annotated under a 19-field schema and evaluation of 30 state-of-the-art LLMs.

Result: No LLM surpassed 50% accuracy, compared to 83.4% human performance, highlighting limitations in cross-linguistic and cultural reasoning.

Conclusion: The dataset and benchmark provide foundational infrastructure for improving figurative language understanding in LLMs for low-resource languages.

Abstract: Figurative language understanding remains a significant challenge for Large Language Models (LLMs), especially for low-resource languages. To address this, we introduce a new idiom dataset, a large-scale, culturally-grounded corpus of 10,361 Bengali idioms. Each idiom is annotated under a comprehensive 19-field schema, established and refined through a deliberative expert consensus process, that captures its semantic, syntactic, cultural, and religious dimensions, providing a rich, structured resource for computational linguistics. To establish a robust benchmark for Bangla figurative language understanding, we evaluate 30 state-of-the-art multilingual and instruction-tuned LLMs on the task of inferring figurative meaning. Our results reveal a critical performance gap, with no model surpassing 50% accuracy, a stark contrast to significantly higher human performance (83.4%). This underscores the limitations of existing models in cross-linguistic and cultural reasoning. By releasing the new idiom dataset and benchmark, we provide foundational infrastructure for advancing figurative language understanding and cultural grounding in LLMs for Bengali and other low-resource languages.

</details>


### [89] [Curriculum Learning and Pseudo-Labeling Improve the Generalization of Multi-Label Arabic Dialect Identification Models](https://arxiv.org/abs/2602.12937)
*Ali Mekky,Mohamed El Zeftawy,Lara Hassan,Amr Keleg,Preslav Nakov*

Main category: cs.CL

TL;DR: The paper argues for treating Arabic Dialect Identification (ADI) as multi-label classification, introduces a GPT-4o annotated dataset, and achieves SOTA performance with a BERT-based model.


<details>
  <summary>Details</summary>
Motivation: Current ADI datasets are single-label, limiting multi-label classification. The paper addresses this gap by repurposing single-label data and creating a multi-label resource.

Method: Constructs a multi-label dataset using GPT-4o and binary classifiers, trains a BERT-based model with curriculum learning based on dialect complexity.

Result: LAHJATBERT achieves macro F1 of 0.69, outperforming previous systems (0.55).

Conclusion: The approach successfully addresses multi-label ADI challenges, setting a new benchmark.

Abstract: Being modeled as a single-label classification task for a long time, recent work has argued that Arabic Dialect Identification (ADI) should be framed as a multi-label classification task. However, ADI remains constrained by the availability of single-label datasets, with no large-scale multi-label resources available for training. By analyzing models trained on single-label ADI data, we show that the main difficulty in repurposing such datasets for Multi-Label Arabic Dialect Identification (MLADI) lies in the selection of negative samples, as many sentences treated as negative could be acceptable in multiple dialects. To address these issues, we construct a multi-label dataset by generating automatic multi-label annotations using GPT-4o and binary dialect acceptability classifiers, with aggregation guided by the Arabic Level of Dialectness (ALDi). Afterward, we train a BERT-based multi-label classifier using curriculum learning strategies aligned with dialectal complexity and label cardinality. On the MLADI leaderboard, our best-performing LAHJATBERT model achieves a macro F1 of 0.69, compared to 0.55 for the strongest previously reported system. Code and data are available at https://mohamedalaa9.github.io/lahjatbert/.

</details>


### [90] [ProbeLLM: Automating Principled Diagnosis of LLM Failures](https://arxiv.org/abs/2602.12966)
*Yue Huang,Zhengzhe Jiang,Yuchen Ma,Yu Jiang,Xiangqi Wang,Yujun Zhou,Yuexing Hao,Kehan Guo,Pin-Yu Chen,Stefan Feuerriegel,Xiangliang Zhang*

Main category: cs.CL

TL;DR: ProbeLLM is an automated probing framework for LLMs that structures failure discovery into interpretable modes using hierarchical Monte Carlo Tree Search and verified test cases.


<details>
  <summary>Details</summary>
Motivation: Existing methods for probing LLMs often identify isolated failures without systematic exploration or insight into underlying weaknesses.

Method: ProbeLLM uses hierarchical Monte Carlo Tree Search to balance exploration and refinement, grounded in verifiable test cases and tool-augmented verification.

Result: It uncovers broader, cleaner, and finer-grained failure landscapes compared to static benchmarks and prior automated methods.

Conclusion: ProbeLLM advances weakness discovery from individual cases to structured modes, improving LLM evaluation.

Abstract: Understanding how and why large language models (LLMs) fail is becoming a central challenge as models rapidly evolve and static evaluations fall behind. While automated probing has been enabled by dynamic test generation, existing approaches often discover isolated failure cases, lack principled control over exploration, and provide limited insight into the underlying structure of model weaknesses. We propose ProbeLLM, a benchmark-agnostic automated probing framework that elevates weakness discovery from individual failures to structured failure modes. ProbeLLM formulates probing as a hierarchical Monte Carlo Tree Search, explicitly allocating limited probing budgets between global exploration of new failure regions and local refinement of recurring error patterns. By restricting probing to verifiable test cases and leveraging tool-augmented generation and verification, ProbeLLM grounds failure discovery in reliable evidence. Discovered failures are further consolidated into interpretable failure modes via failure-aware embeddings and boundary-aware induction. Across diverse benchmarks and LLMs, ProbeLLM reveals substantially broader, cleaner, and more fine-grained failure landscapes than static benchmarks and prior automated methods, supporting a shift from case-centric evaluation toward principled weakness discovery.

</details>


### [91] [SciAgentGym: Benchmarking Multi-Step Scientific Tool-use in LLM Agents](https://arxiv.org/abs/2602.12984)
*Yujiong Shen,Yajie Yang,Zhiheng Xi,Binze Hu,Huayu Sha,Jiazheng Zhang,Qiyuan Peng,Junlin Shang,Jixuan Huang,Yutao Fan,Jingqi Tong,Shihan Dou,Ming Zhang,Lei Bai,Zhenfei Yin,Tao Gui,Xingjun Ma,Qi Zhang,Xuanjing Huang,Yu-Gang Jiang*

Main category: cs.CL

TL;DR: SciAgentGym introduces a scalable environment with domain-specific tools, SciAgentBench evaluates agent capabilities, and SciForge improves training via dependency graphs, showing promising results with SciAgent-8B.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks lack focus on agents' ability to orchestrate tools for scientific workflows, highlighting the need for better evaluation and training methods.

Method: Introduces SciAgentGym (1,780 tools), SciAgentBench (tiered evaluation), and SciForge (data synthesis using dependency graphs) to enhance agentic tool-use capabilities.

Result: SciAgent-8B outperforms larger models like Qwen3-VL-235B-Instruct, with success rates dropping from 60.6% to 30.9% for complex workflows without enhancements.

Conclusion: SciForge and SciAgentGym demonstrate potential for advancing autonomous scientific agents through improved tool-use training and evaluation.

Abstract: Scientific reasoning inherently demands integrating sophisticated toolkits to navigate domain-specific knowledge. Yet, current benchmarks largely overlook agents' ability to orchestrate tools for such rigorous workflows. To bridge this gap, we introduce SciAgentGym, a scalable interactive environment featuring 1,780 domain-specific tools across four natural science disciplines, supported by a robust execution infrastructure. Complementing this, we present SciAgentBench, a tiered evaluation suite designed to stress-test agentic capabilities from elementary actions to long-horizon workflows. Our evaluation identifies a critical bottleneck: state-of-the-art models struggle with complex scientific tool-use. Even for a leading model like GPT-5, success rates drop sharply from 60.6% to 30.9% as interaction horizons extend, primarily due to failures in multi-step workflow execution. To address this, we propose SciForge, a data synthesis method that models the tool action space as a dependency graph to generate logic-aware training trajectories. By fine-tuning on these trajectories, our SciAgent-8B outperforms the significantly larger Qwen3-VL-235B-Instruct while exhibiting positive cross-domain transfer of scientific tool-use capabilities. These results underscore the promising potential of next-generation autonomous scientific agents.

</details>


### [92] [Evaluating the Homogeneity of Keyphrase Prediction Models](https://arxiv.org/abs/2602.12989)
*Maël Houbre,Florian Boudin,Beatrice Daille*

Main category: cs.CL

TL;DR: The paper compares keyphrase extraction and generation models, finding that extraction methods are competitive with generative models and that generating absent keyphrases can negatively impact prediction homogeneity.


<details>
  <summary>Details</summary>
Motivation: To evaluate the homogeneity of keyphrase prediction models and determine if generative models' ability to predict absent keyphrases improves homogeneity.

Method: Introduces a method to assess homogeneity in keyphrase prediction models and compares generative models with extraction methods.

Result: Keyphrase extraction methods are competitive with generative models, and generating absent keyphrases can reduce homogeneity.

Conclusion: The study challenges assumptions about generative models' advantages and highlights unexpected findings about homogeneity.

Abstract: Keyphrases which are useful in several NLP and IR applications are either extracted from text or predicted by generative models. Contrarily to keyphrase extraction approaches, keyphrase generation models can predict keyphrases that do not appear in a document's text called `absent keyphrases`. This ability means that keyphrase generation models can associate a document to a notion that is not explicitly mentioned in its text. Intuitively, this suggests that for two documents treating the same subjects, a keyphrase generation model is more likely to be homogeneous in their indexing i.e. predict the same keyphrase for both documents, regardless of those keyphrases appearing in their respective text or not; something a keyphrase extraction model would fail to do. Yet, homogeneity of keyphrase prediction models is not covered by current benchmarks. In this work, we introduce a method to evaluate the homogeneity of keyphrase prediction models and study if absent keyphrase generation capabilities actually help the model to be more homogeneous. To our surprise, we show that keyphrase extraction methods are competitive with generative models, and that the ability to generate absent keyphrases can actually have a negative impact on homogeneity. Our data, code and prompts are available on huggingface and github.

</details>


### [93] [Know More, Know Clearer: A Meta-Cognitive Framework for Knowledge Augmentation in Large Language Models](https://arxiv.org/abs/2602.12996)
*Hao Chen,Ye He,Yuchun Fan,Yukun Yan,Zhenghao Liu,Qingfu Zhu,Maosong Sun,Wanxiang Che*

Main category: cs.CL

TL;DR: A meta-cognitive framework for reliable knowledge augmentation in LLMs addresses knowledge-confidence gaps by differentiating mastered, confused, and missing knowledge regions, ensuring cognitive consistency and outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Existing methods assume model performance equals internal knowledge, ignoring knowledge-confidence gaps causing overconfident errors or uncertain truths. This work aims to bridge these gaps.

Method: The proposed meta-cognitive framework partitions knowledge into mastered, confused, and missing regions using cognitive signals and introduces a cognitive consistency mechanism to align certainty with accuracy.

Result: The framework consistently outperforms baselines, enhancing knowledge capabilities and improving the distinction between known and unknown information.

Conclusion: The framework successfully bridges knowledge-confidence gaps in LLMs, ensuring reliable knowledge augmentation and fostering better cognitive behaviors.

Abstract: Knowledge augmentation has significantly enhanced the performance of Large Language Models (LLMs) in knowledge-intensive tasks. However, existing methods typically operate on the simplistic premise that model performance equates with internal knowledge, overlooking the knowledge-confidence gaps that lead to overconfident errors or uncertain truths. To bridge this gap, we propose a novel meta-cognitive framework for reliable knowledge augmentation via differentiated intervention and alignment. Our approach leverages internal cognitive signals to partition the knowledge space into mastered, confused, and missing regions, guiding targeted knowledge expansion. Furthermore, we introduce a cognitive consistency mechanism to synchronize subjective certainty with objective accuracy, ensuring calibrated knowledge boundaries. Extensive experiments demonstrate the our framework consistently outperforms strong baselines, validating its rationality in not only enhancing knowledge capabilities but also fostering cognitive behaviors that better distinguish knowns from unknowns.

</details>


### [94] [Can we trust AI to detect healthy multilingual English speakers among the cognitively impaired cohort in the UK? An investigation using real-world conversational speech](https://arxiv.org/abs/2602.13047)
*Madhurananda Pahar,Caitlin Illingworth,Dorota Braun,Bahman Mirheidari,Lise Sproson,Daniel Blackburn,Heidi Christensen*

Main category: cs.CL

TL;DR: AI models exhibit bias against multilingual speakers from ethnic minorities in detecting cognitive decline, misclassifying them more often, especially those with South Yorkshire accents. Current tools are unreliable for diagnostics in these populations.


<details>
  <summary>Details</summary>
Motivation: To investigate trustworthiness and bias in AI models detecting cognitive decline among multilingual English speakers from ethnic minorities, ensuring clinical benefits.

Method: Monolingual participants were recruited nationally in the UK, while multilingual speakers were enrolled from Sheffield and Bradford community centres. AI models were tested using acoustic and linguistic features.

Result: AI models showed bias against multilingual speakers, especially in memory, fluency, and reading tasks, with higher misclassification rates. Bias worsened when trained on public datasets like DementiaBank.

Conclusion: Existing AI tools are unreliable for diagnosing cognitive decline in multilingual ethnic minorities due to bias. Future work aims to develop more generalizable, bias-mitigated models.

Abstract: Conversational speech often reveals early signs of cognitive decline, such as dementia and MCI. In the UK, one in four people belongs to an ethnic minority, and dementia prevalence is expected to rise most rapidly among Black and Asian communities. This study examines the trustworthiness of AI models, specifically the presence of bias, in detecting healthy multilingual English speakers among the cognitively impaired cohort, to make these tools clinically beneficial. For experiments, monolingual participants were recruited nationally (UK), and multilingual speakers were enrolled from four community centres in Sheffield and Bradford. In addition to a non-native English accent, multilinguals spoke Somali, Chinese, or South Asian languages, who were further divided into two Yorkshire accents (West and South) to challenge the efficiency of the AI tools thoroughly. Although ASR systems showed no significant bias across groups, classification and regression models using acoustic and linguistic features exhibited bias against multilingual speakers, particularly in memory, fluency, and reading tasks. This bias was more pronounced when models were trained on the publicly available DementiaBank dataset. Moreover, multilinguals were more likely to be misclassified as having cognitive decline. This study is the first of its kind to discover that, despite their strong overall performance, current AI models show bias against multilingual individuals from ethnic minority backgrounds in the UK, and they are also more likely to misclassify speakers with a certain accent (South Yorkshire) as living with a more severe cognitive decline. In this pilot study, we conclude that the existing AI tools are therefore not yet reliable for diagnostic use in these populations, and we aim to address this in future work by developing more generalisable, bias-mitigated models.

</details>


### [95] [TraceBack: Multi-Agent Decomposition for Fine-Grained Table Attribution](https://arxiv.org/abs/2602.13059)
*Tejas Anvekar,Junha Park,Rajat Jha,Devanshu Gupta,Poojah Ganesan,Puneeth Mathur,Vivek Gupta*

Main category: cs.CL

TL;DR: TraceBack is a modular multi-agent framework for scalable, cell-level attribution in table QA, outperforming baselines and introducing FairScore for evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing table QA systems lack fine-grained attribution, limiting trust in high-stakes settings.

Method: TraceBack prunes tables, decomposes questions, aligns answers with supporting cells, and uses CITEBench for evaluation.

Result: TraceBack outperforms baselines, and FairScore closely tracks human judgments.

Conclusion: TraceBack enables interpretable and scalable evaluation of table-based QA.

Abstract: Question answering (QA) over structured tables requires not only accurate answers but also transparency about which cells support them. Existing table QA systems rarely provide fine-grained attribution, so even correct answers often lack verifiable grounding, limiting trust in high-stakes settings. We address this with TraceBack, a modular multi-agent framework for scalable, cell-level attribution in single-table QA. TraceBack prunes tables to relevant rows and columns, decomposes questions into semantically coherent sub-questions, and aligns each answer span with its supporting cells, capturing both explicit and implicit evidence used in intermediate reasoning steps. To enable systematic evaluation, we release CITEBench, a benchmark with phrase-to-cell annotations drawn from ToTTo, FetaQA, and AITQA. We further propose FairScore, a reference-less metric that compares atomic facts derived from predicted cells and answers to estimate attribution precision and recall without human cell labels. Experiments show that TraceBack substantially outperforms strong baselines across datasets and granularities, while FairScore closely tracks human judgments and preserves relative method rankings, supporting interpretable and scalable evaluation of table-based QA.

</details>


### [96] [Exploring a New Competency Modeling Process with Large Language Models](https://arxiv.org/abs/2602.13084)
*Silin Du,Manqing Xin,Raymond Jia Wang*

Main category: cs.CL

TL;DR: The paper introduces a competency modeling process using LLMs to automate and enhance traditional expert-driven methods, demonstrating strong validity and consistency.


<details>
  <summary>Details</summary>
Motivation: Traditional competency modeling is costly, prone to errors, and lacks reproducibility, prompting the need for a data-driven, automated solution.

Method: The study leverages LLMs to extract behavioral and psychological data from text, maps them to competency libraries, and uses a learnable parameter for adaptive integration of information sources.

Result: Empirical results show strong predictive validity, cross-library consistency, and structural robustness in a real-world software outsourcing company.

Conclusion: The framework shifts competency modeling from a qualitative, expert-dependent process to a transparent, data-driven, and evaluable analytical method.

Abstract: Competency modeling is widely used in human resource management to select, develop, and evaluate talent. However, traditional expert-driven approaches rely heavily on manual analysis of large volumes of interview transcripts, making them costly and prone to randomness, ambiguity, and limited reproducibility. This study proposes a new competency modeling process built on large language models (LLMs). Instead of merely automating isolated steps, we reconstruct the workflow by decomposing expert practices into structured computational components. Specifically, we leverage LLMs to extract behavioral and psychological descriptions from raw textual data and map them to predefined competency libraries through embedding-based similarity. We further introduce a learnable parameter that adaptively integrates different information sources, enabling the model to determine the relative importance of behavioral and psychological signals. To address the long-standing challenge of validation, we develop an offline evaluation procedure that allows systematic model selection without requiring additional large-scale data collection. Empirical results from a real-world implementation in a software outsourcing company demonstrate strong predictive validity, cross-library consistency, and structural robustness. Overall, our framework transforms competency modeling from a largely qualitative and expert-dependent practice into a transparent, data-driven, and evaluable analytical process.

</details>


### [97] [Towards interpretable models for language proficiency assessment: Predicting the CEFR level of Estonian learner texts](https://arxiv.org/abs/2602.13102)
*Kais Allkivi*

Main category: cs.CL

TL;DR: The study uses NLP to classify Estonian learner writings (A2-C1) by focusing on linguistic features for explainable models, achieving high accuracy and noting increased complexity over time.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in research combining NLP-based analysis of learner language for automated assessment and feedback, while providing insights into second language development.

Method: Analyzed linguistic properties (lexical, morphological, surface, error features) to train classification models, comparing models with and without pre-selected features.

Result: Pre-selected features yielded similar accuracy (around 0.9) but reduced classification variation. Accuracy remained high (0.8) over a 7-10-year period.

Conclusion: The approach is effective for automated language testing and has been integrated into an open-source learning environment, demonstrating practical applicability.

Abstract: Using NLP to analyze authentic learner language helps to build automated assessment and feedback tools. It also offers new and extensive insights into the development of second language production. However, there is a lack of research explicitly combining these aspects. This study aimed to classify Estonian proficiency examination writings (levels A2-C1), assuming that careful feature selection can lead to more explainable and generalizable machine learning models for language testing. Various linguistic properties of the training data were analyzed to identify relevant proficiency predictors associated with increasing complexity and correctness, rather than the writing task. Such lexical, morphological, surface, and error features were used to train classification models, which were compared to models that also allowed for other features. The pre-selected features yielded a similar test accuracy but reduced variation in the classification of different text types. The best classifiers achieved an accuracy of around 0.9. Additional evaluation on an earlier exam sample revealed that the writings have become more complex over a 7-10-year period, while accuracy still reached 0.8 with some feature sets. The results have been implemented in the writing evaluation module of an Estonian open-source language learning environment.

</details>


### [98] [SCOPE: Selective Conformal Optimized Pairwise LLM Judging](https://arxiv.org/abs/2602.13110)
*Sher Badshah,Ali Emami,Hassan Sajjad*

Main category: cs.CL

TL;DR: SCOPE introduces a calibrated framework with statistical guarantees for LLM-based pairwise evaluation, using BPE to improve uncertainty and reliability.


<details>
  <summary>Details</summary>
Motivation: Addressing miscalibration and biases in LLM judges by ensuring error control and coverage in pairwise evaluations.

Method: Proposed SCOPE with BPE for bias-neutral uncertainty, calibrating thresholds for controlled error rates.

Result: Achieves target risk levels (e.g., 0.10) and high coverage (up to 0.98) across benchmarks and judge scales.

Conclusion: SCOPE with BPE enables reliable, high-coverage LLM evaluations, outperforming naive baselines.

Abstract: Large language models (LLMs) are increasingly used as judges to replace costly human preference labels in pairwise evaluation. Despite their practicality, LLM judges remain prone to miscalibration and systematic biases. This paper proposes SCOPE (Selective Conformal Optimized Pairwise Evaluation), a framework for selective pairwise judging with finite-sample statistical guarantees. Under exchangeability, SCOPE calibrates an acceptance threshold such that the error rate among non-abstained judgments is at most a user-specified level $α$. To provide SCOPE with a bias-neutral uncertainty signal, we introduce Bidirectional Preference Entropy (BPE), which queries the judge under both response positions, aggregates the implied preference probabilities to enforce invariance to response order, and converts the aggregated probability into an entropy-based uncertainty score. Across MT-Bench, RewardBench, and Chatbot Arena, BPE improves uncertainty quality over standard confidence proxies, providing a stronger selection signal that enables SCOPE to consistently meet the target risk level while retaining good coverage across judge scales. In particular, at $α= 0.10$, \textsc{Scope} consistently satisfies the risk bound across all benchmarks and judge scales (empirical risk $\approx 0.097$ to $0.099$), while retaining substantial coverage, reaching $0.89$ on RewardBench with Qwen-14B and $0.98$ on RewardBench with Qwen-32B. Compared to naïve baselines, \textsc{Scope} accepts up to $2.4\times$ more judgments on MT-Bench with Qwen-7B under the same target risk constraint, demonstrating that BPE enables reliable and high-coverage LLM-based evaluation.

</details>


### [99] [From sunblock to softblock: Analyzing the correlates of neology in published writing and on social media](https://arxiv.org/abs/2602.13123)
*Maria Ryskina,Matthew R. Gormley,Kyle Mahowald,David R. Mortensen,Taylor Berg-Kirkpatrick,Vivek Kulkarni*

Main category: cs.CL

TL;DR: The paper examines how evolutionary pressures shape languages differently across contexts, comparing word emergence in historical texts and Twitter posts using contextual embeddings.


<details>
  <summary>Details</summary>
Motivation: To understand if factors influencing word emergence (neology) in historical texts also apply to social media (Twitter), despite differing constraints.

Method: Extends prior methodology using static and contextual embeddings, applying it to a Twitter corpus for comparison with historical texts.

Result: Findings mostly hold across domains, but topic popularity growth may contribute less to neology on Twitter than in published writing.

Conclusion: Differences in neologism formation mechanisms between domains likely explain the variation in topic popularity's impact.

Abstract: Living languages are shaped by a host of conflicting internal and external evolutionary pressures. While some of these pressures are universal across languages and cultures, others differ depending on the social and conversational context: language use in newspapers is subject to very different constraints than language use on social media. Prior distributional semantic work on English word emergence (neology) identified two factors correlated with creation of new words by analyzing a corpus consisting primarily of historical published texts (Ryskina et al., 2020, arXiv:2001.07740). Extending this methodology to contextual embeddings in addition to static ones and applying it to a new corpus of Twitter posts, we show that the same findings hold for both domains, though the topic popularity growth factor may contribute less to neology on Twitter than in published writing. We hypothesize that this difference can be explained by the two domains favouring different neologism formation mechanisms.

</details>


### [100] [OpenLID-v3: Improving the Precision of Closely Related Language Identification -- An Experience Report](https://arxiv.org/abs/2602.13139)
*Mariia Fedorova,Nikolay Arefyev,Maja Buljan,Jindřich Helcl,Stephan Oepen,Egil Rønningstad,Yves Scherrer*

Main category: cs.CL

TL;DR: OpenLID-v3 improves language identification by adding training data, merging language variants, and labeling noise, outperforming GlotLID in benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing LID tools struggle with closely related languages and noise, contaminating datasets, especially for low-resource languages.

Method: Extended OpenLID with more training data, merged language variants, added a noise label, and evaluated against GlotLID.

Result: Ensemble approaches improved precision but reduced coverage for low-resource languages.

Conclusion: OpenLID-v3 enhances LID accuracy and addresses noise, with availability on Hugging Face.

Abstract: Language identification (LID) is an essential step in building high-quality multilingual datasets from web data. Existing LID tools (such as OpenLID or GlotLID) often struggle to identify closely related languages and to distinguish valid natural language from noise, which contaminates language-specific subsets, especially for low-resource languages. In this work we extend the OpenLID classifier by adding more training data, merging problematic language variant clusters, and introducing a special label for marking noise. We call this extended system OpenLID-v3 and evaluate it against GlotLID on multiple benchmarks. During development, we focus on three groups of closely related languages (Bosnian, Croatian, and Serbian; Romance varieties of Northern Italy and Southern France; and Scandinavian languages) and contribute new evaluation datasets where existing ones are inadequate. We find that ensemble approaches improve precision but also substantially reduce coverage for low-resource languages. OpenLID-v3 is available on https://huggingface.co/HPLT/OpenLID-v3.

</details>


### [101] [Semantic Chunking and the Entropy of Natural Language](https://arxiv.org/abs/2602.13194)
*Weishun Zhong,Doron Sivan,Tankut Can,Mikhail Katkov,Misha Tsodyks*

Main category: cs.CL

TL;DR: The paper introduces a statistical model to explain the redundancy in English text, estimating its entropy rate and linking it to semantic complexity.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand and quantify the redundancy in English text by developing a first-principles statistical model that captures its multi-scale structure.

Method: The method involves a self-similar segmentation of text into semantically coherent chunks hierarchically, enabling analytical treatment and entropy rate prediction.

Result: Numerical experiments show the model quantitatively captures real text structure, predicting an entropy rate matching printed English, and revealing it increases with semantic complexity.

Conclusion: The conclusion highlights that the model successfully explains redundancy in English, linking entropy rate to semantic complexity, with implications for understanding natural language structure.

Abstract: The entropy rate of printed English is famously estimated to be about one bit per character, a benchmark that modern large language models (LLMs) have only recently approached. This entropy rate implies that English contains nearly 80 percent redundancy relative to the five bits per character expected for random text. We introduce a statistical model that attempts to capture the intricate multi-scale structure of natural language, providing a first-principles account of this redundancy level. Our model describes a procedure of self-similarly segmenting text into semantically coherent chunks down to the single-word level. The semantic structure of the text can then be hierarchically decomposed, allowing for analytical treatment. Numerical experiments with modern LLMs and open datasets suggest that our model quantitatively captures the structure of real texts at different levels of the semantic hierarchy. The entropy rate predicted by our model agrees with the estimated entropy rate of printed English. Moreover, our theory further reveals that the entropy rate of natural language is not fixed but should increase systematically with the semantic complexity of corpora, which are captured by the only free parameter in our model.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [102] [Provably Convergent Actor-Critic in Risk-averse MARL](https://arxiv.org/abs/2602.12386)
*Yizhou Zhang,Eric Mazumdar*

Main category: cs.MA

TL;DR: The paper introduces Risk-averse Quantal response Equilibria (RQE) for infinite-horizon general-sum Markov games, proposing a two-timescale Actor-Critic algorithm with global convergence guarantees and improved performance over risk-neutral baselines.


<details>
  <summary>Details</summary>
Motivation: To address the computational intractability of learning stationary policies in infinite-horizon general-sum Markov games, the authors explore RQE, a solution concept incorporating risk aversion and bounded rationality.

Method: They propose a two-timescale Actor-Critic algorithm with a fast-timescale actor and slow-timescale critic, leveraging RQE's regularity conditions.

Result: The algorithm achieves global convergence with finite-sample guarantees and demonstrates superior convergence empirically compared to risk-neutral baselines.

Conclusion: RQE and the proposed algorithm offer a practical and theoretically grounded approach to learning stationary policies in challenging multi-agent reinforcement learning settings.

Abstract: Learning stationary policies in infinite-horizon general-sum Markov games (MGs) remains a fundamental open problem in Multi-Agent Reinforcement Learning (MARL). While stationary strategies are preferred for their practicality, computing stationary forms of classic game-theoretic equilibria is computationally intractable -- a stark contrast to the comparative ease of solving single-agent RL or zero-sum games. To bridge this gap, we study Risk-averse Quantal response Equilibria (RQE), a solution concept rooted in behavioral game theory that incorporates risk aversion and bounded rationality. We demonstrate that RQE possesses strong regularity conditions that make it uniquely amenable to learning in MGs. We propose a novel two-timescale Actor-Critic algorithm characterized by a fast-timescale actor and a slow-timescale critic. Leveraging the regularity of RQE, we prove that this approach achieves global convergence with finite-sample guarantees. We empirically validate our algorithm in several environments to demonstrate superior convergence properties compared to risk-neutral baselines.

</details>


### [103] [Agent Skills for Large Language Models: Architecture, Acquisition, Security, and the Path Forward](https://arxiv.org/abs/2602.12430)
*Renjun Xu,Yang Yan*

Main category: cs.MA

TL;DR: The paper surveys the shift to modular, skill-equipped agents, organizing the landscape into architecture, acquisition, deployment, and security, proposing a trust framework and identifying open challenges.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore the shift from monolithic language models to modular, skill-equipped agents, highlighting the dynamic capability extension enabled by agent skills without retraining. It formalizes this paradigm and provides a comprehensive survey of the rapidly evolving agent skills landscape.

Method: The survey organizes the field along four axes: (i) architectural foundations, (ii) skill acquisition, (iii) deployment at scale, and (iv) security. It examines SKILL.md, progressive context loading, reinforcement learning, autonomous skill discovery, compositional skill synthesis, deployment stacks, and security frameworks.

Result: The paper identifies vulnerabilities in community-contributed skills (26.1\%) and proposes the Skill Trust and Lifecycle Governance Framework. It also presents seven open challenges and a research agenda for trustworthy, self-improving skill ecosystems.

Conclusion: The work focuses on the emerging skill abstraction layer and its implications for the next generation of agentic systems, distinguishing itself from prior surveys by its specific focus on agent skills.

Abstract: The transition from monolithic language models to modular, skill-equipped agents marks a defining shift in how large language models (LLMs) are deployed in practice. Rather than encoding all procedural knowledge within model weights, agent skills -- composable packages of instructions, code, and resources that agents load on demand -- enable dynamic capability extension without retraining. It is formalized in a paradigm of progressive disclosure, portable skill definitions, and integration with the Model Context Protocol (MCP). This survey provides a comprehensive treatment of the agent skills landscape, as it has rapidly evolved during the last few months. We organize the field along four axes: (i) architectural foundations, examining the SKILL.md specification, progressive context loading, and the complementary roles of skills and MCP; (ii) skill acquisition, covering reinforcement learning with skill libraries (SAGE), autonomous skill discovery (SEAgent), and compositional skill synthesis; (iii) deployment at scale, including the computer-use agent (CUA) stack, GUI grounding advances, and benchmark progress on OSWorld and SWE-bench; and (iv) security, where recent empirical analyses reveal that 26.1\% of community-contributed skills contain vulnerabilities, motivating our proposed Skill Trust and Lifecycle Governance Framework -- a four-tier, gate-based permission model that maps skill provenance to graduated deployment capabilities. We identify seven open challenges -- from cross-platform skill portability to capability-based permission models -- and propose a research agenda for realizing trustworthy, self-improving skill ecosystems. Unlike prior surveys that broadly cover LLM agents or tool use, this work focuses specifically on the emerging skill abstraction layer and its implications for the next generation of agentic systems. Project repo: https://github.com/scienceaix/agentskills.

</details>


### [104] [Theory of Mind Guided Strategy Adaptation for Zero-Shot Coordination](https://arxiv.org/abs/2602.12458)
*Andrew Ni,Simon Stepputtis,Stefanos Nikolaidis,Michael Lewis,Katia P. Sycara,Woojun Kim*

Main category: cs.MA

TL;DR: Proposes an adaptive ensemble agent for zero-shot coordination in multi-agent reinforcement learning, outperforming single best-response baselines.


<details>
  <summary>Details</summary>
Motivation: Addresses the limitation of static, generalist policies in zero-shot coordination by enabling agents to adaptively infer teammate intent and select specialized policies.

Method: Introduces an adaptive ensemble agent using Theory-of-Mind-based best-response selection to choose policies from a trained ensemble, evaluated in Overcooked under various observability settings.

Result: Empirical results show superior performance over single best-response baselines in zero-shot coordination.

Conclusion: The adaptive ensemble approach effectively enhances synergy and adaptability in zero-shot coordination scenarios.

Abstract: A central challenge in multi-agent reinforcement learning is enabling agents to adapt to previously unseen teammates in a zero-shot fashion. Prior work in zero-shot coordination often follows a two-stage process, first generating a diverse training pool of partner agents, and then training a best-response agent to collaborate effectively with the entire training pool. While many previous works have achieved strong performance by devising better ways to diversify the partner agent pool, there has been less emphasis on how to leverage this pool to build an adaptive agent. One limitation is that the best-response agent may converge to a static, generalist policy that performs reasonably well across diverse teammates, rather than learning a more adaptive, specialist policy that can better adapt to teammates and achieve higher synergy. To address this, we propose an adaptive ensemble agent that uses Theory-of-Mind-based best-response selection to first infer its teammate's intentions and then select the most suitable policy from a policy ensemble. We conduct experiments in the Overcooked environment to evaluate zero-shot coordination performance under both fully and partially observable settings. The empirical results demonstrate the superiority of our method over a single best-response baseline.

</details>


### [105] [Building Large-Scale Drone Defenses from Small-Team Strategies](https://arxiv.org/abs/2602.12502)
*Grant Douglas,Stephen Franklin,Claudia Szabo,Mingyu Guo*

Main category: cs.MA

TL;DR: The paper proposes a modular framework to scale small-team drone defense strategies to larger swarms efficiently using dynamic programming and iterative refinement.


<details>
  <summary>Details</summary>
Motivation: The need to defend against large adversarial drone swarms requires scalable coordination methods beyond traditional multi-agent optimization.

Method: The framework integrates proven small-team strategies as modular components, dynamically assembles them into large teams using DP decomposition, and iteratively refines the pool of components based on large-team outcomes.

Result: Experiments show the approach scales effectively, preserving performance and uncovering cooperative behaviors unattainable through direct optimization.

Conclusion: The proposed modular and iterative framework successfully scales drone defense strategies, demonstrating its effectiveness in large-scale scenarios.

Abstract: Defending against large adversarial drone swarms requires coordination methods that scale effectively beyond conventional multi-agent optimisation. In this paper, we propose to scale strategies proven effective in small defender teams by integrating them as modular components of larger forces using our proposed framework. A dynamic programming (DP) decomposition assembles these components into large teams in polynomial time, enabling efficient construction of scalable defenses without exhaustive evaluation. Because a unit that is strong in isolation may not remain strong when combined, we sample across multiple small-team candidates. Our framework iterates between evaluating large-team outcomes and refining the pool of modular components, allowing convergence on increasingly effective strategies. Experiments demonstrate that this partitioning approach scales to substantially larger scenarios while preserving effectiveness and revealing cooperative behaviours that direct optimisation cannot reliably discover.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [106] [GT-HarmBench: Benchmarking AI Safety Risks Through the Lens of Game Theory](https://arxiv.org/abs/2602.12316)
*Pepijn Cobben,Xuanqiang Angelo Huang,Thao Amelia Pham,Isabel Dahlgren,Terry Jingchen Zhang,Zhijing Jin*

Main category: cs.AI

TL;DR: GT-HarmBench introduces a benchmark for evaluating AI safety in multi-agent environments, revealing frequent harmful outcomes despite interventions.


<details>
  <summary>Details</summary>
Motivation: To address the gap in understanding multi-agent risks like coordination failure and conflict, which are overlooked in single-agent safety benchmarks.

Method: Used GT-HarmBench, featuring 2,009 scenarios based on game-theoretic structures (e.g., Prisoner's Dilemma), and tested 15 frontier models.

Result: Agents acted socially beneficial in only 62% of cases, with game-theoretic interventions improving outcomes by up to 18%.

Conclusion: The benchmark highlights reliability gaps and offers a standardized tool for studying AI alignment in multi-agent settings.

Abstract: Frontier AI systems are increasingly capable and deployed in high-stakes multi-agent environments. However, existing AI safety benchmarks largely evaluate single agents, leaving multi-agent risks such as coordination failure and conflict poorly understood. We introduce GT-HarmBench, a benchmark of 2,009 high-stakes scenarios spanning game-theoretic structures such as the Prisoner's Dilemma, Stag Hunt and Chicken. Scenarios are drawn from realistic AI risk contexts in the MIT AI Risk Repository. Across 15 frontier models, agents choose socially beneficial actions in only 62% of cases, frequently leading to harmful outcomes. We measure sensitivity to game-theoretic prompt framing and ordering, and analyze reasoning patterns driving failures. We further show that game-theoretic interventions improve socially beneficial outcomes by up to 18%. Our results highlight substantial reliability gaps and provide a broad standardized testbed for studying alignment in multi-agent environments. The benchmark and code are available at https://github.com/causalNLP/gt-harmbench.

</details>


### [107] [A Theoretical Framework for Adaptive Utility-Weighted Benchmarking](https://arxiv.org/abs/2602.12356)
*Philip Waggoner*

Main category: cs.AI

TL;DR: The paper proposes a theoretical framework to reconceptualize benchmarking in AI as a multilayer, adaptive network integrating evaluation metrics, model components, and stakeholder priorities.


<details>
  <summary>Details</summary>
Motivation: Traditional benchmarking methods lack consideration of sociotechnical contexts and diverse stakeholder priorities, limiting their ability to evaluate AI systems holistically.

Method: Introduces a multilayer network framework with weighted interactions, conjoint-derived utilities, and human-in-the-loop updates to embed human tradeoffs dynamically.

Result: Generalizes classical leaderboards, enabling context-aware, stable, and interpretable benchmarks that better align with human priorities.

Conclusion: The framework advances AI evaluation by making it more accountable and aligned with societal needs, paving the way for robust benchmarking tools.

Abstract: Benchmarking has long served as a foundational practice in machine learning and, increasingly, in modern AI systems such as large language models, where shared tasks, metrics, and leaderboards offer a common basis for measuring progress and comparing approaches. As AI systems are deployed in more varied and consequential settings, though, there is growing value in complementing these established practices with a more holistic conceptualization of what evaluation should represent. Of note, recognizing the sociotechnical contexts in which these systems operate invites an opportunity for a deeper view of how multiple stakeholders and their unique priorities might inform what we consider meaningful or desirable model behavior. This paper introduces a theoretical framework that reconceptualizes benchmarking as a multilayer, adaptive network linking evaluation metrics, model components, and stakeholder groups through weighted interactions. Using conjoint-derived utilities and a human-in-the-loop update rule, we formalize how human tradeoffs can be embedded into benchmark structure and how benchmarks can evolve dynamically while preserving stability and interpretability. The resulting formulation generalizes classical leaderboards as a special case and provides a foundation for building evaluation protocols that are more context aware, resulting in new robust tools for analyzing the structural properties of benchmarks, which opens a path toward more accountable and human-aligned evaluation.

</details>


### [108] [Evolving Beyond Snapshots: Harmonizing Structure and Sequence via Entity State Tuning for Temporal Knowledge Graph Forecasting](https://arxiv.org/abs/2602.12389)
*Siyuan Li,Yunjia Wu,Yiyong Xiao,Pingyang Huang,Peize Li,Ruitong Liu,Yan Wen,Te Sun,Fangyi Pei*

Main category: cs.AI

TL;DR: EST is a framework for TKG forecasting that uses persistent entity states to improve long-term dependency modeling, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the episodic amnesia and decay of long-term dependencies in stateless TKG forecasting methods.

Method: Proposes Entity State Tuning (EST), which uses a global state buffer, topology-aware state perceiver, unified temporal context module, and dual-track evolution mechanism.

Result: EST consistently improves diverse backbones and achieves state-of-the-art performance on benchmarks.

Conclusion: State persistence is crucial for long-horizon TKG forecasting, and EST effectively addresses this need.

Abstract: Temporal knowledge graph (TKG) forecasting requires predicting future facts by jointly modeling structural dependencies within each snapshot and temporal evolution across snapshots. However, most existing methods are stateless: they recompute entity representations at each timestamp from a limited query window, leading to episodic amnesia and rapid decay of long-term dependencies. To address this limitation, we propose Entity State Tuning (EST), an encoder-agnostic framework that endows TKG forecasters with persistent and continuously evolving entity states. EST maintains a global state buffer and progressively aligns structural evidence with sequential signals via a closed-loop design. Specifically, a topology-aware state perceiver first injects entity-state priors into structural encoding. Then, a unified temporal context module aggregates the state-enhanced events with a pluggable sequence backbone. Subsequently, a dual-track evolution mechanism writes the updated context back to the global entity state memory, balancing plasticity against stability. Experiments on multiple benchmarks show that EST consistently improves diverse backbones and achieves state-of-the-art performance, highlighting the importance of state persistence for long-horizon TKG forecasting. The code is published at https://github.com/yuanwuyuan9/Evolving-Beyond-Snapshots

</details>


### [109] [Scaling Web Agent Training through Automatic Data Generation and Fine-grained Evaluation](https://arxiv.org/abs/2602.12544)
*Lajanugen Logeswaran,Jaekyeom Kim,Sungryull Sohn,Creighton Glasscock,Honglak Lee*

Main category: cs.AI

TL;DR: The paper introduces a scalable pipeline for generating high-quality training data for web agents, using a novel constraint-based evaluation framework to assess progress in task completion. Evaluated on BookingArena, their distilled model outperforms open-source and competes with commercial systems.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in identifying high-quality training data for web agents, especially evaluating progress in task completion.

Method: A constraint-based evaluation framework assesses progress towards task completion, enabling the use of partially successful trajectories.

Result: The distilled student model outperforms open-source approaches and matches or exceeds commercial systems on BookingArena.

Conclusion: The work provides a solution for creating diverse web interaction datasets and a systematic evaluation method for complex web tasks.

Abstract: We present a scalable pipeline for automatically generating high-quality training data for web agents. In particular, a major challenge in identifying high-quality training instances is trajectory evaluation - quantifying how much progress was made towards task completion. We introduce a novel constraint-based evaluation framework that provides fine-grained assessment of progress towards task completion. This enables us to leverage partially successful trajectories, which significantly expands the amount of usable training data. We evaluate our method on a new benchmark we propose called BookingArena, which consists of complex booking tasks across 20 popular websites, and demonstrate that our distilled student model outperforms open-source approaches and matches or exceeds commercial systems, while being a significantly smaller model. Our work addresses the challenge of efficiently creating diverse, realistic web interaction datasets and provides a systematic evaluation methodology for complex structured web tasks.

</details>


### [110] [To Mix or To Merge: Toward Multi-Domain Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2602.12566)
*Haoqing Wang,Xiang Long,Ziheng Li,Yilong Xu,Tingguang Li,Yehui Tang*

Main category: cs.AI

TL;DR: The paper explores Reinforcement Learning with Verifiable Rewards (RLVR) in LLMs, comparing mixed multi-task RLVR and separate RLVR followed by model merging across domains like math and coding. It finds minimal interference and synergistic effects in reasoning-intensive domains.


<details>
  <summary>Details</summary>
Motivation: To understand the collaboration of RLVR across different domains and compare training paradigms (mixed multi-task RLVR vs. separate RLVR with merging) for multi-domain expert-level performance.

Method: Used multiple high-level tasks (math, coding, science, instruction following) with open-source datasets for qualitative and quantitative experiments. Analyzed mutual interference and synergy.

Result: RLVR across domains shows few mutual interferences, with reasoning-intensive domains exhibiting synergistic effects. Internal mechanisms were analyzed via weight space geometry, prediction behavior, and information constraints.

Conclusion: The study advances understanding of RLVR paradigms, suggesting mixed multi-task RLVR is viable for multi-domain expert models, especially in reasoning-intensive tasks.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) plays a key role in stimulating the explicit reasoning capability of Large Language Models (LLMs). We can achieve expert-level performance in some specific domains via RLVR, such as coding or math. When a general multi-domain expert-level model is required, we need to carefully consider the collaboration of RLVR across different domains. The current state-of-the-art models mainly employ two different training paradigms for multi-domain RLVR: mixed multi-task RLVR and separate RLVR followed by model merging. However, most of the works did not provide a detailed comparison and analysis about these paradigms. To this end, we choose multiple commonly used high-level tasks (e.g., math, coding, science, and instruction following) as our target domains and design extensive qualitative and quantitative experiments using open-source datasets. We find the RLVR across domains exhibits few mutual interferences, and reasoning-intensive domains demonstrate mutually synergistic effects. Furthermore, we analyze the internal mechanisms of mutual gains from the perspectives of weight space geometry, model prediction behavior, and information constraints. This project is named as M2RL that means Mixed multi-task training or separate training followed by model Merging for Reinforcement Learning, and the homepage is at https://github.com/mosAI25/M2RL

</details>


### [111] [Can I Have Your Order? Monte-Carlo Tree Search for Slot Filling Ordering in Diffusion Language Models](https://arxiv.org/abs/2602.12586)
*Joshua Ong Jun Leang,Yu Zhao,Mihaela Cătălina Stoian,Wenda Li,Shay B. Cohen,Eleonora Giunchiglia*

Main category: cs.AI

TL;DR: McDiffuSE improves slot infilling in Masked Diffusion Models using MCTS, outperforming baselines and highlighting the need for non-sequential generation and larger exploration constants.


<details>
  <summary>Details</summary>
Motivation: Performance in plan-and-infill decoding for mathematical and code reasoning is highly sensitive to slot infilling order, causing output variance.

Method: McDiffuSE optimizes infilling orders via Monte Carlo Tree Search (MCTS), using look-ahead simulations to evaluate partial completions.

Result: McDiffuSE achieves average improvements of 3.2% over autoregressive baselines and 8.0% over baseline plan-and-infill, with notable gains on MBPP and MATH500.

Conclusion: MCTS-based planning enhances generation quality in MDMs; non-sequential generation and larger exploration constants are key.

Abstract: While plan-and-infill decoding in Masked Diffusion Models (MDMs) shows promise for mathematical and code reasoning, performance remains highly sensitive to slot infilling order, often yielding substantial output variance. We introduce McDiffuSE, a framework that formulates slot selection as decision making and optimises infilling orders through Monte Carlo Tree Search (MCTS). McDiffuSE uses look-ahead simulations to evaluate partial completions before commitment, systematically exploring the combinatorial space of generation orders. Experiments show an average improvement of 3.2% over autoregressive baselines and 8.0% over baseline plan-and-infill, with notable gains of 19.5% on MBPP and 4.9% on MATH500. Our analysis reveals that while McDiffuSE predominantly follows sequential ordering, incorporating non-sequential generation is essential for maximising performance. We observe that larger exploration constants, rather than increased simulations, are necessary to overcome model confidence biases and discover effective orderings. These findings establish MCTS-based planning as an effective approach for enhancing generation quality in MDMs.

</details>


### [112] [GeoAgent: Learning to Geolocate Everywhere with Reinforced Geographic Characteristics](https://arxiv.org/abs/2602.12617)
*Modi Jin,Yiming Zhang,Boyuan Sun,Dingwen Zhang,MingMing Cheng,Qibin Hou*

Main category: cs.AI

TL;DR: GeoAgent is a model that excels in fine-grained address reasoning by leveraging expert-annotated data and innovative rewards for geographic accuracy and consistency, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To address limitations in RL-based methods for geolocation tasks, particularly their reliance on AI-generated CoT data and lack of geographic alignment.

Method: Introduces GeoSeek, a dataset with expert-annotated CoT data, and employs geo-similarity and consistency rewards during training.

Result: GeoAgent surpasses current methods and general VLLMs in performance and human-aligned reasoning across multiple grains.

Conclusion: GeoAgent effectively bridges gaps in geolocation reasoning by integrating expert data and geographic-specific training strategies.

Abstract: This paper presents GeoAgent, a model capable of reasoning closely with humans and deriving fine-grained address conclusions. Previous RL-based methods have achieved breakthroughs in performance and interpretability but still remain concerns because of their reliance on AI-generated chain-of-thought (CoT) data and training strategies, which conflict with geographic characteristics. To address these issues, we first introduce GeoSeek, a new geolocation dataset comprising CoT data annotated by geographic experts and professional players. We further thoroughly explore the inherent characteristics of geographic tasks and propose a geo-similarity reward and a consistency reward assessed by a consistency agent to assist training. This encourages the model to converge towards correct answers from a geographic perspective while ensuring the integrity and consistency of its reasoning process. Experimental results show that GeoAgent outperforms existing methods and a series of general VLLMs across multiple grains, while generating reasoning that closely aligns with humans.

</details>


### [113] [AI Agents for Inventory Control: Human-LLM-OR Complementarity](https://arxiv.org/abs/2602.12631)
*Jackie Baek,Yaopeng Fu,Will Ma,Tianyi Peng*

Main category: cs.AI

TL;DR: Combining OR algorithms, LLMs, and human input improves inventory control performance compared to using any method alone.


<details>
  <summary>Details</summary>
Motivation: Traditional OR algorithms struggle with demand shifts and lack of context, while LLMs offer flexibility but need integration with existing methods.

Method: The study uses InventoryBench, a benchmark with synthetic and real-world demand data, and conducts a classroom experiment with human-AI collaboration.

Result: OR-augmented LLMs outperform standalone methods, and human-AI teams achieve higher profits than humans or AI alone.

Conclusion: Integrating OR, LLMs, and humans is complementary and beneficial, with substantial individual-level improvements observed.

Abstract: Inventory control is a fundamental operations problem in which ordering decisions are traditionally guided by theoretically grounded operations research (OR) algorithms. However, such algorithms often rely on rigid modeling assumptions and can perform poorly when demand distributions shift or relevant contextual information is unavailable. Recent advances in large language models (LLMs) have generated interest in AI agents that can reason flexibly and incorporate rich contextual signals, but it remains unclear how best to incorporate LLM-based methods into traditional decision-making pipelines.
  We study how OR algorithms, LLMs, and humans can interact and complement each other in a multi-period inventory control setting. We construct InventoryBench, a benchmark of over 1,000 inventory instances spanning both synthetic and real-world demand data, designed to stress-test decision rules under demand shifts, seasonality, and uncertain lead times. Through this benchmark, we find that OR-augmented LLM methods outperform either method in isolation, suggesting that these methods are complementary rather than substitutes.
  We further investigate the role of humans through a controlled classroom experiment that embeds LLM recommendations into a human-in-the-loop decision pipeline. Contrary to prior findings that human-AI collaboration can degrade performance, we show that, on average, human-AI teams achieve higher profits than either humans or AI agents operating alone. Beyond this population-level finding, we formalize an individual-level complementarity effect and derive a distribution-free lower bound on the fraction of individuals who benefit from AI collaboration; empirically, we find this fraction to be substantial.

</details>


### [114] [Think Fast and Slow: Step-Level Cognitive Depth Adaptation for LLM Agents](https://arxiv.org/abs/2602.12662)
*Ruihan Yang,Fanghua Ye,Xiang We,Ruoqing Zhao,Kang Luo,Xinbo Xu,Bo Zhao,Ruotian Ma,Shanyi Wang,Zhaopeng Tu,Xiaolong Li,Deqing Yang,Linus*

Main category: cs.AI

TL;DR: CogRouter is a framework enabling LLMs to dynamically adapt cognitive depth for efficient multi-turn decision-making, outperforming existing models with fewer tokens.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based agents use fixed cognitive patterns, which are inefficient for tasks with varying cognitive demands. CogRouter addresses this by dynamically adjusting cognitive depth.

Method: CogRouter employs ACT-R theory for four hierarchical cognitive levels and uses Cognition-aware Supervised Fine-tuning (CoSFT) and Cognition-aware Policy Optimization (CoPO) for training.

Result: CogRouter achieves an 82.3% success rate, outperforming GPT-4o, OpenAI-o3, and GRPO while using 62% fewer tokens.

Conclusion: The framework effectively balances cognitive effort and efficiency, setting a new benchmark for LLM-based agents.

Abstract: Large language models (LLMs) are increasingly deployed as autonomous agents for multi-turn decision-making tasks. However, current agents typically rely on fixed cognitive patterns: non-thinking models generate immediate responses, while thinking models engage in deep reasoning uniformly. This rigidity is inefficient for long-horizon tasks, where cognitive demands vary significantly from step to step, with some requiring strategic planning and others only routine execution. In this paper, we introduce CogRouter, a framework that trains agents to dynamically adapt cognitive depth at each step. Grounded in ACT-R theory, we design four hierarchical cognitive levels ranging from instinctive responses to strategic planning. Our two-stage training approach includes Cognition-aware Supervised Fine-tuning (CoSFT) to instill stable level-specific patterns, and Cognition-aware Policy Optimization (CoPO) for step-level credit assignment via confidence-aware advantage reweighting. The key insight is that appropriate cognitive depth should maximize the confidence of the resulting action. Experiments on ALFWorld and ScienceWorld demonstrate that CogRouter achieves state-of-the-art performance with superior efficiency. With Qwen2.5-7B, it reaches an 82.3% success rate, outperforming GPT-4o (+40.3%), OpenAI-o3 (+18.3%), and GRPO (+14.0%), while using 62% fewer tokens.

</details>


### [115] [Evaluating Robustness of Reasoning Models on Parameterized Logical Problems](https://arxiv.org/abs/2602.12665)
*Naïm Es-sebbani,Esteban Marquer,Yakoub Salhi,Zied Bouraoui*

Main category: cs.AI

TL;DR: A diagnostic benchmark for 2-SAT is introduced, isolating structural phenomena in LLM-based reasoners and revealing brittleness not captured by standard benchmarks.


<details>
  <summary>Details</summary>
Motivation: Standard SAT benchmarks conflate surface difficulty with structural phenomena, limiting their effectiveness in evaluating LLM-based reasoners.

Method: The benchmark uses parameterized families of structured 2-CNF formulas to isolate distinct competencies and failure modes, evaluating decision accuracy and assignment validity.

Result: LLM-based reasoners show sharp performance transitions under targeted structural interventions, uncovering brittleness regimes.

Conclusion: The benchmark provides a nuanced evaluation of LLM-based reasoners, revealing hidden failure modes and sensitivities.

Abstract: Logic provides a controlled testbed for evaluating LLM-based reasoners, yet standard SAT-style benchmarks often conflate surface difficulty (length, wording, clause order) with the structural phenomena that actually determine satisfiability. We introduce a diagnostic benchmark for 2-SAT built from parameterized families of structured 2--CNF formulas, where satisfiability is characterized by the implication graph and can be tuned along interpretable axes. Our generators isolate distinct competencies and failure modes: (i) contradiction-cycle UNSAT cores with controllable size and imbalance, (ii) SAT instances with a prescribed fraction of free variables to control solution multiplicity, (iii) planted backbones that modulate propagation, (iv) late bridge clauses that couple otherwise monotone regions to probe sensitivity to ordering and revision, and (v) symmetry/duplication variants that test abstraction under renaming and redundant structure. We evaluate LLM-based reasoners on decision accuracy and assignment validity, and quantify robustness under semantics-preserving perturbations such as clause reordering, filler clauses, and variable renaming. Across models, we observe sharp performance transitions under targeted structural interventions even when surface statistics are held fixed, revealing brittleness regimes that are invisible to aggregate SAT accuracy.

</details>


### [116] [SkillsBench: Benchmarking How Well Agent Skills Work Across Diverse Tasks](https://arxiv.org/abs/2602.12670)
*Xiangyi Li,Wenbo Chen,Yimin Liu,Shenghan Zheng,Xiaokun Chen,Yifeng He,Yubo Li,Bingran You,Haotian Shen,Jiankai Sun,Shuyi Wang,Qunhong Zeng,Di Wang,Xuandong Zhao,Yuanli Wang,Roey Ben Chaim,Zonglin Di,Yipeng Gao,Junwei He,Yizhuo He,Liqiang Jing,Luyang Kong,Xin Lan,Jiachen Li,Songlin Li,Yijiang Li,Yueqian Lin,Xinyi Liu,Xuanqing Liu,Haoran Lyu,Ze Ma,Bowei Wang,Runhui Wang,Tianyu Wang,Wengao Ye,Yue Zhang,Hanwen Xing,Yiqi Xue,Steven Dillmann,Han-chung Lee*

Main category: cs.AI

TL;DR: SkillsBench benchmarks LLM agents' performance with curated and self-generated Skills across 86 tasks, showing curated Skills improve results but effects vary by domain and task.


<details>
  <summary>Details</summary>
Motivation: There's no standard way to measure the effectiveness of Agent Skills, so SkillsBench was created to evaluate their impact.

Method: SkillsBench evaluates 86 tasks across 11 domains under three conditions: no Skills, curated Skills, and self-generated Skills, testing 7 agent-model configurations over 7,308 trajectories.

Result: Curated Skills improve average pass rates by 16.2pp, but effects vary by domain (e.g., +4.5pp for Software Engineering, +51.9pp for Healthcare). Self-generated Skills show no benefit. Smaller models with Skills can match larger ones without.

Conclusion: Curated Skills enhance LLM agent performance, but their impact is domain-specific. Self-generated Skills are ineffective, and focused Skills outperform comprehensive documentation.

Abstract: Agent Skills are structured packages of procedural knowledge that augment LLM agents at inference time. Despite rapid adoption, there is no standard way to measure whether they actually help. We present SkillsBench, a benchmark of 86 tasks across 11 domains paired with curated Skills and deterministic verifiers. Each task is evaluated under three conditions: no Skills, curated Skills, and self-generated Skills. We test 7 agent-model configurations over 7,308 trajectories. Curated Skills raise average pass rate by 16.2 percentage points(pp), but effects vary widely by domain (+4.5pp for Software Engineering to +51.9pp for Healthcare) and 16 of 84 tasks show negative deltas. Self-generated Skills provide no benefit on average, showing that models cannot reliably author the procedural knowledge they benefit from consuming. Focused Skills with 2--3 modules outperform comprehensive documentation, and smaller models with Skills can match larger models without them.

</details>


### [117] [X-SYS: A Reference Architecture for Interactive Explanation Systems](https://arxiv.org/abs/2602.12748)
*Tobias Labarta,Nhi Hoang,Maximilian Dreyer,Jim Berend,Oleg Hein,Jackie Ma,Wojciech Samek,Sebastian Lapuschkin*

Main category: cs.AI

TL;DR: The paper highlights challenges in deploying explainable AI (XAI) as interactive systems, proposing X-SYS, a reference architecture for such systems. It focuses on four quality attributes (STAR) and five components to decouple UI and backend evolution.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the gap between theoretical XAI methods and their practical deployment as interactive systems, which must handle repeated queries, evolving models, and governance constraints.

Method: The authors introduce X-SYS, a reference architecture organizing interactive explanation systems around STAR attributes (scalability, traceability, responsiveness, adaptability) and five components (XUI Services, Explanation Services, Model Services, Data Services, Orchestration and Governance).

Result: X-SYS is implemented via SemanticLens, demonstrating how contract-based boundaries, offline/online separation, and state management achieve the desired system qualities.

Conclusion: The work provides a reusable blueprint and concrete example (SemanticLens) for building interactive XAI systems under operational constraints, bridging theory and practice.

Abstract: The explainable AI (XAI) research community has proposed numerous technical methods, yet deploying explainability as systems remains challenging: Interactive explanation systems require both suitable algorithms and system capabilities that maintain explanation usability across repeated queries, evolving models and data, and governance constraints. We argue that operationalizing XAI requires treating explainability as an information systems problem where user interaction demands induce specific system requirements. We introduce X-SYS, a reference architecture for interactive explanation systems, that guides (X)AI researchers, developers and practitioners in connecting interactive explanation user interfaces (XUI) with system capabilities. X-SYS organizes around four quality attributes named STAR (scalability, traceability, responsiveness, and adaptability), and specifies a five-component decomposition (XUI Services, Explanation Services, Model Services, Data Services, Orchestration and Governance). It maps interaction patterns to system capabilities to decouple user interface evolution from backend computation. We implement X-SYS through SemanticLens, a system for semantic search and activation steering in vision-language models. SemanticLens demonstrates how contract-based service boundaries enable independent evolution, offline/online separation ensures responsiveness, and persistent state management supports traceability. Together, this work provides a reusable blueprint and concrete instantiation for interactive explanation systems supporting end-to-end design under operational constraints.

</details>


### [118] [WebClipper: Efficient Evolution of Web Agents with Graph-based Trajectory Pruning](https://arxiv.org/abs/2602.12852)
*Junjie Wang,Zequn Xie,Dan Yang,Jie Feng,Yue Shen,Duolin Sun,Meixiu Long,Yihan Jiao,Zhehao Tan,Jian Wang,Peng Wei,Jinjie Gu*

Main category: cs.AI

TL;DR: WebClipper compresses web agent trajectories by pruning redundant steps, improving efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing inefficiencies in web agents, such as cyclic reasoning and unproductive branches, to enhance search efficiency.

Method: Models search as a state graph, optimizes trajectories via DAG mining, and introduces F-AE Score for performance measurement.

Result: Reduces tool-call rounds by 20% while maintaining accuracy, validated through experiments.

Conclusion: WebClipper effectively balances efficiency and accuracy in web agent design.

Abstract: Deep Research systems based on web agents have shown strong potential in solving complex information-seeking tasks, yet their search efficiency remains underexplored. We observe that many state-of-the-art open-source web agents rely on long tool-call trajectories with cyclic reasoning loops and exploration of unproductive branches. To address this, we propose WebClipper, a framework that compresses web agent trajectories via graph-based pruning. Concretely, we model the agent's search process as a state graph and cast trajectory optimization as a minimum-necessary Directed Acyclic Graph (DAG) mining problem, yielding pruned trajectories that preserve essential reasoning while eliminating redundant steps. Continued training on these refined trajectories enables the agent to evolve toward more efficient search patterns and reduces tool-call rounds by about 20% while improving accuracy. Furthermore, we introduce a new metric called F-AE Score to measure the model's overall performance in balancing accuracy and efficiency. Experiments demonstrate that WebClipper compresses tool-call rounds under excellent performance, providing practical insight into balancing effectiveness and efficiency in web agent design.

</details>


### [119] [BrowseComp-$V^3$: A Visual, Vertical, and Verifiable Benchmark for Multimodal Browsing Agents](https://arxiv.org/abs/2602.12876)
*Huanyao Zhang,Jiepeng Zhou,Bo Li,Bowen Zhou,Yanzhe Dan,Haishan Lu,Zhiyong Cao,Jiaoyang Chen,Yuqian Han,Zinan Sheng,Zhengwei Tao,Hao Liang,Jialong Wu,Yang Shi,Yuanpeng He,Jiaye Lin,Qintong Zhang,Guochen Yan,Runhao Zhao,Zhengpin Li,Xiaohan Yu,Lang Mei,Chong Chen,Wentao Zhang,Bin Cui*

Main category: cs.AI

TL;DR: BrowseComp-$V^3$ is a new benchmark for evaluating multimodal deep search capabilities, featuring complex tasks and expert-validated evaluation. It reveals gaps in current model performance.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack complexity and granularity for assessing multimodal browsing and deep search capabilities.

Method: Introduced BrowseComp-$V^3$ with 300 challenging questions requiring multi-hop reasoning. Proposed OmniSeeker, a unified browsing agent framework.

Result: State-of-the-art models achieved only 36% accuracy, highlighting bottlenecks in multimodal integration and perception.

Conclusion: A significant gap exists between current models and robust real-world multimodal deep search capabilities.

Abstract: Multimodal large language models (MLLMs), equipped with increasingly advanced planning and tool-use capabilities, are evolving into autonomous agents capable of performing multimodal web browsing and deep search in open-world environments. However, existing benchmarks for multimodal browsing remain limited in task complexity, evidence accessibility, and evaluation granularity, hindering comprehensive and reproducible assessments of deep search capabilities. To address these limitations, we introduce BrowseComp-$V^3$, a novel benchmark consisting of 300 carefully curated and challenging questions spanning diverse domains. The benchmark emphasizes deep, multi-level, and cross-modal multi-hop reasoning, where critical evidence is interleaved across textual and visual modalities within and across web pages. All supporting evidence is strictly required to be publicly searchable, ensuring fairness and reproducibility. Beyond final-answer accuracy, we incorporate an expert-validated, subgoal-driven process evaluation mechanism that enables fine-grained analysis of intermediate reasoning behaviors and systematic characterization of capability boundaries. In addition, we propose OmniSeeker, a unified multimodal browsing agent framework integrating diverse web search and visual perception tools. Comprehensive experiments demonstrate that even state-of-the-art models achieve only 36% accuracy on our benchmark, revealing critical bottlenecks in multimodal information integration and fine-grained perception. Our results highlight a fundamental gap between current model capabilities and robust multimodal deep search in real-world settings.

</details>


### [120] [Information-theoretic analysis of world models in optimal reward maximizers](https://arxiv.org/abs/2602.12963)
*Alfred Harwood,Jose Faustino,Alex Altair*

Main category: cs.AI

TL;DR: The paper quantifies the information an optimal policy reveals about its environment, proving it's exactly $n \log m$ bits for deterministic policies in a controlled Markov process.


<details>
  <summary>Details</summary>
Motivation: To understand how much internal representation of the world is needed for optimal behavior in AI systems.

Method: The study analyzes a Controlled Markov Process (CMP) with uniform priors over transition dynamics, focusing on deterministic policies optimal for non-constant rewards.

Result: The mutual information between the environment and optimal policy is proven to be $n \log m$ bits, applicable across various reward objectives.

Conclusion: The findings establish an information-theoretic lower bound on the implicit world model required for achieving optimal behavior.

Abstract: An important question in the field of AI is the extent to which successful behaviour requires an internal representation of the world. In this work, we quantify the amount of information an optimal policy provides about the underlying environment. We consider a Controlled Markov Process (CMP) with $n$ states and $m$ actions, assuming a uniform prior over the space of possible transition dynamics. We prove that observing a deterministic policy that is optimal for any non-constant reward function then conveys exactly $n \log m$ bits of information about the environment. Specifically, we show that the mutual information between the environment and the optimal policy is $n \log m$ bits. This bound holds across a broad class of objectives, including finite-horizon, infinite-horizon discounted, and time-averaged reward maximization. These findings provide a precise information-theoretic lower bound on the "implicit world model'' necessary for optimality.

</details>


### [121] [Consistency of Large Reasoning Models Under Multi-Turn Attacks](https://arxiv.org/abs/2602.13093)
*Yubo Li,Ramayya Krishnan,Rema Padman*

Main category: cs.AI

TL;DR: Reasoning models outperform baselines under adversarial attacks but show vulnerabilities. Five failure modes are identified, and confidence-based defenses fail due to overconfidence.


<details>
  <summary>Details</summary>
Motivation: To explore the robustness of large reasoning models under multi-turn adversarial pressure.

Method: Evaluate nine frontier reasoning models under adversarial attacks, analyze failure modes, and test Confidence-Aware Response Generation (CARG).

Result: Reasoning models outperform baselines but exhibit vulnerabilities. CARG fails due to overconfidence; random confidence embedding works better.

Conclusion: Reasoning capabilities don't ensure adversarial robustness, and confidence-based defenses need redesign for reasoning models.

Abstract: Large reasoning models with reasoning capabilities achieve state-of-the-art performance on complex tasks, but their robustness under multi-turn adversarial pressure remains underexplored. We evaluate nine frontier reasoning models under adversarial attacks. Our findings reveal that reasoning confers meaningful but incomplete robustness: most reasoning models studied significantly outperform instruction-tuned baselines, yet all exhibit distinct vulnerability profiles, with misleading suggestions universally effective and social pressure showing model-specific efficacy. Through trajectory analysis, we identify five failure modes (Self-Doubt, Social Conformity, Suggestion Hijacking, Emotional Susceptibility, and Reasoning Fatigue) with the first two accounting for 50% of failures. We further demonstrate that Confidence-Aware Response Generation (CARG), effective for standard LLMs, fails for reasoning models due to overconfidence induced by extended reasoning traces; counterintuitively, random confidence embedding outperforms targeted extraction. Our results highlight that reasoning capabilities do not automatically confer adversarial robustness and that confidence-based defenses require fundamental redesign for reasoning models.

</details>


### [122] [Constrained Assumption-Based Argumentation Frameworks](https://arxiv.org/abs/2602.13135)
*Emanuele De Angelis,Fabio Fioravanti,Maria Chiara Meo,Alberto Pettorossi,Maurizio Proietti,Francesca Toni*

Main category: cs.AI

TL;DR: The paper introduces Constrained ABA (CABA), extending Assumption-based Argumentation (ABA) by allowing constrained variables in arguments and attacks, overcoming previous limitations to ground arguments.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the representational limitation of ABA frameworks, which were restricted to ground arguments and attacks built from propositional atoms.

Method: The authors propose Constrained ABA (CABA), where components and arguments can include constrained variables over infinite domains, and define non-ground semantics for CABA.

Result: The new semantics conservatively generalize standard ABA semantics, preserving their properties while expanding applicability.

Conclusion: CABA successfully lifts the ground argument restriction in ABA, offering a more expressive framework with broader applicability.

Abstract: Assumption-based Argumentation (ABA) is a well-established form of structured argumentation. ABA frameworks with an underlying atomic language are widely studied, but their applicability is limited by a representational restriction to ground (variable-free) arguments and attacks built from propositional atoms. In this paper, we lift this restriction and propose a novel notion of constrained ABA (CABA), whose components, as well as arguments built from them, may include constrained variables, ranging over possibly infinite domains. We define non-ground semantics for CABA, in terms of various notions of non-ground attacks. We show that the new semantics conservatively generalise standard ABA semantics.

</details>


### [123] [Optimal Take-off under Fuzzy Clearances](https://arxiv.org/abs/2602.13166)
*Hugo Henry,Arthur Tsai,Kelly Cohen*

Main category: cs.AI

TL;DR: A hybrid obstacle avoidance architecture combining Optimal Control and Fuzzy Rule Based System (FRBS) is proposed for unmanned aircraft, aiming for adaptive constraint handling. The method shows promise but faces solver compatibility issues.


<details>
  <summary>Details</summary>
Motivation: Addresses limitations of classical optimal control under uncertainty and the need for interpretable decision-making in safety-critical aviation systems.

Method: Integrates a three-stage Takagi-Sugeno-Kang fuzzy layer with optimal control, using FAA and EASA guidelines for constraint modulation. Solutions are computed using FALCON and IPOPT.

Result: Proof of concept demonstrates feasible computation times (2.3s/iteration), but solver incompatibility prevents proper constraint enforcement.

Conclusion: Future work includes solver validation, optimization of fuzzy functions, and extension to higher-fidelity models and stochastic environments.

Abstract: This paper presents a hybrid obstacle avoidance architecture that integrates Optimal Control under clearance with a Fuzzy Rule Based System (FRBS) to enable adaptive constraint handling for unmanned aircraft. Motivated by the limitations of classical optimal control under uncertainty and the need for interpretable decision making in safety critical aviation systems, we design a three stage Takagi Sugeno Kang fuzzy layer that modulates constraint radii, urgency levels, and activation decisions based on regulatory separation minima and airworthiness guidelines from FAA and EASA. These fuzzy-derived clearances are then incorporated as soft constraints into an optimal control problem solved using the FALCON toolbox and IPOPT. The framework aims to reduce unnecessary recomputations by selectively activating obstacle avoidance updates while maintaining compliance with aviation procedures. A proof of concept implementation using a simplified aircraft model demonstrates that the approach can generate optimal trajectories with computation times of 2,3 seconds per iteration in a single threaded MATLAB environment, suggesting feasibility for near real time applications. However, our experiments revealed a critical software incompatibility in the latest versions of FALCON and IPOPT, in which the Lagrangian penalty term remained identically zero, preventing proper constraint enforcement. This behavior was consistent across scenarios and indicates a solver toolbox regression rather than a modeling flaw. Future work includes validating this effect by reverting to earlier software versions, optimizing the fuzzy membership functions using evolutionary methods, and extending the system to higher fidelity aircraft models and stochastic obstacle environments.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [124] [OptiML: An End-to-End Framework for Program Synthesis and CUDA Kernel Optimization](https://arxiv.org/abs/2602.12305)
*Arijit Bhattacharjee,Heng Ping,Son Vu Le,Paul Bogdan,Nesreen K. Ahmed,Ali Jannesari*

Main category: cs.LG

TL;DR: OptiML is a framework that automatically generates optimized CUDA kernels from natural language or input code, using a two-stage process of proposal generation and search-based refinement, validated by hardware-aware rewards.


<details>
  <summary>Details</summary>
Motivation: The challenge of achieving high-performance CUDA kernels due to combinatorial optimization spaces and noisy hardware feedback motivates the development of OptiML.

Method: OptiML combines a Mixture-of-Thoughts generator (OptiML-G) for initial proposals and a search-based optimizer (OptiML-X) using Monte Carlo Tree Search, guided by hardware-aware rewards.

Result: OptiML consistently improves performance over strong baselines and produces interpretable optimization trajectories.

Conclusion: OptiML effectively addresses the challenges of CUDA kernel optimization through a systematic, verifiable, and interpretable approach.

Abstract: Generating high-performance CUDA kernels remains challenging due to the need to navigate a combinatorial space of low-level transformations under noisy and expensive hardware feedback. Although large language models can synthesize functionally correct CUDA code, achieving competitive performance requires systematic exploration and verification of optimization choices. We present OptiML, an end-to-end framework that maps either natural-language intent or input CUDA code to performance-optimized CUDA kernels by formulating kernel optimization as search under verification. OptiML consists of two decoupled stages. When the input is natural language, a Mixture-of-Thoughts generator (OptiML-G) acts as a proposal policy over kernel implementation strategies, producing an initial executable program. A search-based optimizer (OptiML-X) then refines either synthesized or user-provided kernels using Monte Carlo Tree Search over LLM-driven edits, guided by a hardware-aware reward derived from profiler feedback. Each candidate transformation is compiled, verified, and profiled with Nsight Compute, and evaluated by a composite objective that combines runtime with hardware bottleneck proxies and guardrails against regressions. We evaluate OptiML in both synthesis-and-optimize and optimization-only settings on a diverse suite of CUDA kernels. Results show that OptiML consistently discovers verified performance improvements over strong LLM baselines and produces interpretable optimization trajectories grounded in profiler evidence.

</details>


### [125] [Abstractive Red-Teaming of Language Model Character](https://arxiv.org/abs/2602.12318)
*Nate Rahn,Allison Qi,Avery Griffin,Jonathan Michala,Henry Sleight,Erik Jones*

Main category: cs.LG

TL;DR: This paper introduces abstractive red-teaming to identify query categories that cause language models to violate character specifications, using efficient algorithms like reinforcement learning and iterative synthesis.


<details>
  <summary>Details</summary>
Motivation: To preemptively detect and categorize queries that lead to character violations in language models before large-scale deployment.

Method: Two algorithms are proposed: reinforcement learning on a category generator LLM and iterative synthesis using a strong LLM to find high-scoring query categories.

Result: The algorithms outperform baselines across 12 principles and 7 target models, uncovering problematic categories like predicting the future or recommending illegal items.

Conclusion: The approach represents significant progress in pre-deployment auditing for language model character adherence.

Abstract: We want language model assistants to conform to a character specification, which asserts how the model should act across diverse user interactions. While models typically follow these character specifications, they can occasionally violate them in large-scale deployments. In this work, we aim to identify types of queries that are likely to produce such character violations at deployment, using much less than deployment-level compute. To do this, we introduce abstractive red-teaming, where we search for natural-language query categories, e.g. "The query is in Chinese. The query asks about family roles," that routinely elicit violations. These categories abstract over the many possible variants of a query which could appear in the wild. We introduce two algorithms for efficient category search against a character-trait-specific reward model: one based on reinforcement learning on a category generator LLM, and another which leverages a strong LLM to iteratively synthesize categories from high-scoring queries. Across a 12-principle character specification and 7 target models, we find that our algorithms consistently outperform baselines, and generate qualitatively interesting categories; for example, queries which ask Llama-3.1-8B-Instruct to predict the future lead to responses saying that AI will dominate humanity, and queries that ask GPT-4.1-Mini for essential prison survival items lead to enthusiastic recommendation of illegal weapons. Overall, we believe our results represent an important step towards realistic pre-deployment auditing of language model character.

</details>


### [126] [The Appeal and Reality of Recycling LoRAs with Adaptive Merging](https://arxiv.org/abs/2602.12323)
*Haokun Liu,Gyung Hyun Je,Marco Ciccone,Zhenlin Xu,Prasanth YSS,Colin Raffel*

Main category: cs.LG

TL;DR: The paper explores adaptive merging of LoRA modules from a large pool of user-contributed LoRAs, finding limited benefits over training new LoRAs but suggesting regularization effects and potential positive transfer with highly relevant LoRAs.


<details>
  <summary>Details</summary>
Motivation: To address the gap in recycling LoRAs from public repositories and evaluate the effectiveness of adaptive merging methods.

Method: Empirical study involving adaptive and non-adaptive merging methods, including a new method via design space search, using 1,000 LoRAs from Llama 3.1 8B-Instruct.

Result: Adaptive merging improves over the base model but offers little advantage over training new LoRAs. Random LoRAs perform similarly, hinting at regularization effects. Positive transfer confirmed with highly relevant LoRAs.

Conclusion: Adaptive merging may work via regularization, not cross-task transfer, unless highly relevant LoRAs are available. Findings challenge previous assumptions.

Abstract: The widespread availability of fine-tuned LoRA modules for open pre-trained models has led to an interest in methods that can adaptively merge LoRAs to improve performance. These methods typically include some way of selecting LoRAs from a pool and tune merging coefficients based on a task-specific dataset. While adaptive merging methods have demonstrated improvements in some settings, no past work has attempted to recycle LoRAs found "in the wild" on model repositories like the Hugging Face Hub. To address this gap, we consider recycling from a pool of nearly 1,000 user-contributed LoRAs trained from the Llama 3.1 8B-Instruct language model. Our empirical study includes a range of adaptive and non-adaptive merging methods in addition to a new method designed via a wide search over the methodological design space. We demonstrate that adaptive merging methods can improve performance over the base model but provide limited benefit over training a new LoRA on the same data used to set merging coefficients. We additionally find not only that the specific choice of LoRAs to merge has little importance, but that using LoRAs with randomly initialized parameter values yields similar performance. This raises the possibility that adaptive merging from recycled LoRAs primarily works via some kind of regularization effect, rather than by enabling positive cross-task transfer. To better understand why past work has proven successful, we confirm that positive transfer is indeed possible when there are highly relevant LoRAs in the pool. We release the model checkpoints and code online.

</details>


### [127] [Bench-MFG: A Benchmark Suite for Learning in Stationary Mean Field Games](https://arxiv.org/abs/2602.12517)
*Lorenzo Magnino,Jiacheng Shen,Matthieu Geist,Olivier Pietquin,Mathieu Laurière*

Main category: cs.LG

TL;DR: The paper introduces Bench-MFG, a standardized benchmark suite for evaluating algorithms at the intersection of Mean Field Games and Reinforcement Learning, addressing the lack of uniform testing environments.


<details>
  <summary>Details</summary>
Motivation: Current MFG and RL research lacks standardized evaluation protocols, leading to fragmented and simplistic testing environments. This makes it hard to assess algorithm robustness and generalization.

Method: The authors propose Bench-MFG, a benchmark suite with a taxonomy of problem classes and prototypical environments. They also introduce MF-Garnets for generating random MFG instances and benchmark various learning algorithms, including MF-PSO.

Result: Extensive empirical results are provided, demonstrating the effectiveness of the benchmark suite and the proposed algorithms. Guidelines for future experimental comparisons are also suggested.

Conclusion: Bench-MFG fills a critical gap in MFG and RL research by providing a standardized framework for evaluating algorithms, promoting reproducibility and robust comparisons.

Abstract: The intersection of Mean Field Games (MFGs) and Reinforcement Learning (RL) has fostered a growing family of algorithms designed to solve large-scale multi-agent systems. However, the field currently lacks a standardized evaluation protocol, forcing researchers to rely on bespoke, isolated, and often simplistic environments. This fragmentation makes it difficult to assess the robustness, generalization, and failure modes of emerging methods. To address this gap, we propose a comprehensive benchmark suite for MFGs (Bench-MFG), focusing on the discrete-time, discrete-space, stationary setting for the sake of clarity. We introduce a taxonomy of problem classes, ranging from no-interaction and monotone games to potential and dynamics-coupled games, and provide prototypical environments for each. Furthermore, we propose MF-Garnets, a method for generating random MFG instances to facilitate rigorous statistical testing. We benchmark a variety of learning algorithms across these environments, including a novel black-box approach (MF-PSO) for exploitability minimization. Based on our extensive empirical results, we propose guidelines to standardize future experimental comparisons. Code available at \href{https://github.com/lorenzomagnino/Bench-MFG}{https://github.com/lorenzomagnino/Bench-MFG}.

</details>


### [128] [Wireless TokenCom: RL-Based Tokenizer Agreement for Multi-User Wireless Token Communications](https://arxiv.org/abs/2602.12338)
*Farshad Zeinali,Mahdi Boloursaz Mashhadi,Dusit Niyato,Rahim Tafazolli*

Main category: cs.LG

TL;DR: TokenCom uses tokens for efficient multimodal communications, requiring agreement on tokenizer models/codebooks via Tokenizer Agreement (TA). The paper proposes a hybrid RL framework for TA and resource allocation in multi-user downlink TokenCom, showing better performance than baselines.


<details>
  <summary>Details</summary>
Motivation: To enable efficient semantic- and goal-oriented communications in wireless networks by establishing a shared semantic latent space through Tokenizer Agreement (TA).

Method: A hybrid RL framework combining DQN for TA/sub-channel assignment and DDPG for beamforming, solving a mixed-integer non-convex problem.

Result: The framework improves semantic quality and resource efficiency, reducing video freezing events by 68% compared to H.265-based schemes.

Conclusion: The proposed RL-based approach effectively optimizes TokenCom performance, demonstrating its superiority over conventional methods.

Abstract: Token Communications (TokenCom) has recently emerged as an effective new paradigm, where tokens are the unified units of multimodal communications and computations, enabling efficient digital semantic- and goal-oriented communications in future wireless networks. To establish a shared semantic latent space, the transmitters/receivers in TokenCom need to agree on an identical tokenizer model and codebook. To this end, an initial Tokenizer Agreement (TA) process is carried out in each communication episode, where the transmitter/receiver cooperate to choose from a set of pre-trained tokenizer models/ codebooks available to them both for efficient TokenCom. In this correspondence, we investigate TA in a multi-user downlink wireless TokenCom scenario, where the base station equipped with multiple antennas transmits video token streams to multiple users. We formulate the corresponding mixed-integer non-convex problem, and propose a hybrid reinforcement learning (RL) framework that integrates a deep Q-network (DQN) for joint tokenizer agreement and sub-channel assignment, with a deep deterministic policy gradient (DDPG) for beamforming. Simulation results show that the proposed framework outperforms baseline methods in terms of semantic quality and resource efficiency, while reducing the freezing events in video transmission by 68% compared to the conventional H.265-based scheme.

</details>


### [129] [Multi-Agent Model-Based Reinforcement Learning with Joint State-Action Learned Embeddings](https://arxiv.org/abs/2602.12520)
*Zhizun Wang,David Meger*

Main category: cs.LG

TL;DR: A novel model-based multi-agent reinforcement learning framework integrates joint state-action representation learning with imaginative roll-outs to improve coordination in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of coordinating many agents in partially observable, dynamic environments by enhancing representation learning and data efficiency.

Method: Uses variational auto-encoders to train a world model, augmented by state-action learned embedding (SALE), which informs both imagination roll-outs and joint action-value estimation.

Result: Empirical studies show consistent gains over baseline algorithms in benchmarks like StarCraft II and Multi-Agent MuJoCo, highlighting improved long-term planning.

Conclusion: The framework effectively leverages joint state-action embeddings within a model-based paradigm to enhance multi-agent coordination and optimization.

Abstract: Learning to coordinate many agents in partially observable and highly dynamic environments requires both informative representations and data-efficient training. To address this challenge, we present a novel model-based multi-agent reinforcement learning framework that unifies joint state-action representation learning with imaginative roll-outs. We design a world model trained with variational auto-encoders and augment the model using the state-action learned embedding (SALE). SALE is injected into both the imagination module that forecasts plausible future roll-outs and the joint agent network whose individual action values are combined through a mixing network to estimate the joint action-value function. By coupling imagined trajectories with SALE-based action values, the agents acquire a richer understanding of how their choices influence collective outcomes, leading to improved long-term planning and optimization under limited real-environment interactions. Empirical studies on well-established multi-agent benchmarks, including StarCraft II Micro-Management, Multi-Agent MuJoCo, and Level-Based Foraging challenges, demonstrate consistent gains of our method over baseline algorithms and highlight the effectiveness of joint state-action learned embeddings within a multi-agent model-based paradigm.

</details>


### [130] [Intrinsic Credit Assignment for Long Horizon Interaction](https://arxiv.org/abs/2602.12342)
*Ilze Amanda Auzina,Joschka Strüber,Sergio Hernández-Gutiérrez,Shashwat Goel,Ameya Prabhu,Matthias Bethge*

Main category: cs.LG

TL;DR: ΔBelief-RL uses intrinsic beliefs from language models to reward intermediate progress, improving long-horizon uncertainty navigation and outperforming outcome-based rewards.


<details>
  <summary>Details</summary>
Motivation: To train agents for navigating uncertainty over long horizons by leveraging intrinsic beliefs for better credit assignment.

Method: Proposes ΔBelief-RL, which assigns rewards based on changes in the agent's belief probability towards the target solution, trained on synthetic interaction data.

Result: Consistently outperforms outcome-based rewards, generalizes to out-of-distribution tasks, and improves with scaled test-time interactions.

Conclusion: Introduces a scalable strategy for long-horizon uncertainty navigation by rewarding intermediate actions intrinsically.

Abstract: How can we train agents to navigate uncertainty over long horizons? In this work, we propose ΔBelief-RL, which leverages a language model's own intrinsic beliefs to reward intermediate progress. Our method utilizes the change in the probability an agent assigns to the target solution for credit assignment. By training on synthetic interaction data, ΔBelief-RL teaches information-seeking capabilities that consistently outperform purely outcome-based rewards for Reinforcement Learning, with improvements generalizing to out-of-distribution applications ranging from customer service to personalization. Notably, the performance continues to improve as we scale test-time interactions beyond the training horizon, with interaction-efficiency increasing even on Pass@k metrics. Overall, our work introduces a scalable training strategy for navigating uncertainty over a long-horizon, by enabling credit assignment to intermediate actions via intrinsic ΔBelief rewards.

</details>


### [131] [TRACE: Temporal Reasoning via Agentic Context Evolution for Streaming Electronic Health Records (EHRs)](https://arxiv.org/abs/2602.12833)
*Zhan Qu,Michael Färber*

Main category: cs.LG

TL;DR: TRACE is a framework improving LLMs' clinical reasoning over patient trajectories without fine-tuning or retrieval, using structured memory and agents for accuracy and safety.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with longitudinal patient data due to evolving states and irregular timing, needing reliable adaptation without computational or privacy issues.

Method: TRACE uses a dual-memory architecture (Global and Individual Protocols) and four agents (Router, Reasoner, Auditor, Steward) for structured temporal reasoning.

Result: TRACE outperforms baselines in next-event prediction, protocol adherence, and safety on MIMIC-IV data, with interpretable reasoning.

Conclusion: TRACE enables efficient, reliable temporal clinical reasoning with LLMs, addressing limitations of existing methods.

Abstract: Large Language Models (LLMs) encode extensive medical knowledge but struggle to apply it reliably to longitudinal patient trajectories, where evolving clinical states, irregular timing, and heterogeneous events degrade performance over time. Existing adaptation strategies rely on fine-tuning or retrieval-based augmentation, which introduce computational overhead, privacy constraints, or instability under long contexts. We introduce TRACE (Temporal Reasoning via Agentic Context Evolution), a framework that enables temporal clinical reasoning with frozen LLMs by explicitly structuring and maintaining context rather than extending context windows or updating parameters. TRACE operates over a dual-memory architecture consisting of a static Global Protocol encoding institutional clinical rules and a dynamic Individual Protocol tracking patient-specific state. Four agentic components, Router, Reasoner, Auditor, and Steward, coordinate over this structured memory to support temporal inference and state evolution. The framework maintains bounded inference cost via structured state compression and selectively audits safety-critical clinical decisions. Evaluated on longitudinal clinical event streams from MIMIC-IV, TRACE significantly improves next-event prediction accuracy, protocol adherence, and clinical safety over long-context and retrieval-augmented baselines, while producing interpretable and auditable reasoning traces.

</details>


### [132] [A Machine Learning Approach to the Nirenberg Problem](https://arxiv.org/abs/2602.12368)
*Gianfranco Cortés,Maria Esteban-Casadevall,Yueqing Feng,Jonas Henkel,Edward Hirst,Tancredi Schettini Gherardini,Alexander G. Stapleton*

Main category: cs.LG

TL;DR: The paper introduces the Nirenberg Neural Network (NNN), a PINN-based approach to solve the Nirenberg problem of prescribing Gaussian curvature on S². It achieves high accuracy for realizable curvatures and distinguishes unrealizable ones.


<details>
  <summary>Details</summary>
Motivation: The work aims to provide a computational tool for exploring longstanding geometric analysis problems, specifically the Nirenberg problem, using neural networks.

Method: A mesh-free PINN directly parametrizes the conformal factor globally, trained with a geometry-aware loss. Consistency checks were done via Gauss-Bonnet theorem and spherical-harmonic expansions.

Result: NNN achieves very low losses ($10^{-7} - 10^{-10}$) for realizable curvatures and higher losses for unrealizable ones, enabling assessment of unknown cases.

Conclusion: Neural solvers like NNN can serve as exploratory tools in geometric analysis, offering quantitative insights into existence questions.

Abstract: This work introduces the Nirenberg Neural Network: a numerical approach to the Nirenberg problem of prescribing Gaussian curvature on $S^2$ for metrics that are pointwise conformal to the round metric. Our mesh-free physics-informed neural network (PINN) approach directly parametrises the conformal factor globally and is trained with a geometry-aware loss enforcing the curvature equation. Additional consistency checks were performed via the Gauss-Bonnet theorem, and spherical-harmonic expansions were fit to the learnt models to provide interpretability.
  For prescribed curvatures with known realisability, the neural network achieves very low losses ($10^{-7} - 10^{-10}$), while unrealisable curvatures yield significantly higher losses. This distinction enables the assessment of unknown cases, separating likely realisable functions from non-realisable ones. The current capabilities of the Nirenberg Neural Network demonstrate that neural solvers can serve as exploratory tools in geometric analysis, offering a quantitative computational perspective on longstanding existence questions.

</details>


### [133] [Policy4OOD: A Knowledge-Guided World Model for Policy Intervention Simulation against the Opioid Overdose Crisis](https://arxiv.org/abs/2602.12373)
*Yijun Ma,Zehong Wang,Weixiang Sun,Zheyuan Zhang,Kaiwen Shi,Nitesh Chawla,Yanfang Ye*

Main category: cs.LG

TL;DR: Policy4OOD is a spatio-temporal world model integrating policy knowledge, spatial dependencies, and socioeconomic data to forecast opioid outcomes, enabling counterfactual analysis and policy optimization via simulation.


<details>
  <summary>Details</summary>
Motivation: Addressing the opioid epidemic requires evaluating policies dynamically; current methods lack integration of forecasting, counterfactual reasoning, and optimization.

Method: Policy4OOD uses a Transformer-based model to encode policy graphs, spatial dependencies, and time series, serving as a simulator for forward forecasting, counterfactual substitution, and Monte Carlo Tree Search-based optimization.

Result: The model improves forecasting accuracy by incorporating spatial dependencies and policy knowledge, validated on a state-level dataset (2019–2024).

Conclusion: Policy4OOD demonstrates the utility of world modeling for dynamic policy evaluation, offering a data-driven tool for public health decision-making.

Abstract: The opioid epidemic remains one of the most severe public health crises in the United States, yet evaluating policy interventions before implementation is difficult: multiple policies interact within a dynamic system where targeting one risk pathway may inadvertently amplify another. We argue that effective opioid policy evaluation requires three capabilities -- forecasting future outcomes under current policies, counterfactual reasoning about alternative past decisions, and optimization over candidate interventions -- and propose to unify them through world modeling. We introduce Policy4OOD, a knowledge-guided spatio-temporal world model that addresses three core challenges: what policies prescribe, where effects manifest, and when effects unfold.Policy4OOD jointly encodes policy knowledge graphs, state-level spatial dependencies, and socioeconomic time series into a policy-conditioned Transformer that forecasts future opioid outcomes.Once trained, the world model serves as a simulator: forecasting requires only a forward pass, counterfactual analysis substitutes alternative policy encodings in the historical sequence, and policy optimization employs Monte Carlo Tree Search over the learned simulator. To support this framework, we construct a state-level monthly dataset (2019--2024) integrating opioid mortality, socioeconomic indicators, and structured policy encodings. Experiments demonstrate that spatial dependencies and structured policy knowledge significantly improve forecasting accuracy, validating each architectural component and the potential of world modeling for data-driven public health decision support.

</details>


### [134] [Value Bonuses using Ensemble Errors for Exploration in Reinforcement Learning](https://arxiv.org/abs/2602.12375)
*Abdul Wahab,Raksha Kumaraswamy,Martha White*

Main category: cs.LG

TL;DR: VBE introduces an algorithm for exploration in RL using ensemble errors to provide first-visit optimism and deep exploration, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitation of existing methods that only retroactively increase value bonuses after seeing higher rewards, VBE aims to encourage first-visit exploration.

Method: VBE maintains an ensemble of random action-value functions (RQFs) and uses their estimation errors to design value bonuses. These bonuses decrease to zero by designing rewards for RQFs appropriately.

Result: VBE outperforms Bootstrap DQN, RND, and ACB on classic environments and scales well to complex environments like Atari.

Conclusion: VBE effectively provides first-visit optimism and deep exploration, demonstrating superior performance and scalability in RL tasks.

Abstract: Optimistic value estimates provide one mechanism for directed exploration in reinforcement learning (RL). The agent acts greedily with respect to an estimate of the value plus what can be seen as a value bonus. The value bonus can be learned by estimating a value function on reward bonuses, propagating local uncertainties around rewards. However, this approach only increases the value bonus for an action retroactively, after seeing a higher reward bonus from that state and action. Such an approach does not encourage the agent to visit a state and action for the first time. In this work, we introduce an algorithm for exploration called Value Bonuses with Ensemble errors (VBE), that maintains an ensemble of random action-value functions (RQFs). VBE uses the errors in the estimation of these RQFs to design value bonuses that provide first-visit optimism and deep exploration. The key idea is to design the rewards for these RQFs in such a way that the value bonus can decrease to zero. We show that VBE outperforms Bootstrap DQN and two reward bonus approaches (RND and ACB) on several classic environments used to test exploration and provide demonstrative experiments that it can scale easily to more complex environments like Atari.

</details>


### [135] [Deep Doubly Debiased Longitudinal Effect Estimation with ICE G-Computation](https://arxiv.org/abs/2602.12379)
*Wenxin Chen,Weishen Pan,Kyra Gan,Fei Wang*

Main category: cs.LG

TL;DR: D3-Net reduces error propagation in ICE G-computation for longitudinal treatment effect estimation by using SDR pseudo-outcomes and a multi-task Transformer, followed by LTMLE for robust final estimates.


<details>
  <summary>Details</summary>
Motivation: Longitudinal treatment effect estimation is challenging due to treatment-confounder feedback and error propagation in ICE G-computation.

Method: D3-Net uses SDR pseudo-outcomes to mitigate error propagation during ICE training and employs a multi-task Transformer with auxiliary supervision. Final estimates are debiased using LTMLE.

Result: D3-Net outperforms existing ICE-based estimators by reducing bias and variance across various scenarios.

Conclusion: The proposed D3-Net framework effectively addresses error propagation and improves robustness in longitudinal treatment effect estimation.

Abstract: Estimating longitudinal treatment effects is essential for sequential decision-making but is challenging due to treatment-confounder feedback. While Iterative Conditional Expectation (ICE) G-computation offers a principled approach, its recursive structure suffers from error propagation, corrupting the learned outcome regression models. We propose D3-Net, a framework that mitigates error propagation in ICE training and then applies a robust final correction. First, to interrupt error propagation during learning, we train the ICE sequence using Sequential Doubly Robust (SDR) pseudo-outcomes, which provide bias-corrected targets for each regression. Second, we employ a multi-task Transformer with a covariate simulator head for auxiliary supervision, regularizing representations against corruption by noisy pseudo-outcomes, and a target network to stabilize training dynamics. For the final estimate, we discard the SDR correction and instead use the uncorrected nuisance models to perform Longitudinal Targeted Minimum Loss-Based Estimation (LTMLE) on the original outcomes. This second-stage, targeted debiasing ensures robustness and optimal finite-sample properties. Comprehensive experiments demonstrate that our model, D3-Net, robustly reduces bias and variance across different horizons, counterfactuals, and time-varying confoundings, compared to existing state-of-the-art ICE-based estimators.

</details>


### [136] [Why Deep Jacobian Spectra Separate: Depth-Induced Scaling and Singular-Vector Alignment](https://arxiv.org/abs/2602.12384)
*Nathanaël Haas,Francçois Gatine,Augustin M Cosse,Zied Bouraoui*

Main category: cs.LG

TL;DR: The paper explores implicit bias in gradient-based training of deep networks by analyzing deep Jacobians, revealing depth-induced exponential scaling and spectral separation, and validating findings experimentally.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand the implicit bias in deep network training, focusing on deep Jacobians' properties like singular-value dynamics and spectral separation.

Method: The paper adopts a fixed-gates view of piecewise-linear networks, providing theoretical proofs and closed-form expressions for singular values and spectral separation, supported by experiments in fixed-gates settings.

Result: Results show exponential scaling of singular values, alignment of singular vectors in matrix products, and decoupled singular-value dynamics, mirroring balanced deep-linear analyses without balancing.

Conclusion: The findings support a mechanistic explanation of low-rank Jacobian structure driving implicit bias, validated empirically.

Abstract: Understanding why gradient-based training in deep networks exhibits strong implicit bias remains challenging, in part because tractable singular-value dynamics are typically available only for balanced deep linear models. We propose an alternative route based on two theoretically grounded and empirically testable signatures of deep Jacobians: depth-induced exponential scaling of ordered singular values and strong spectral separation. Adopting a fixed-gates view of piecewise-linear networks, where Jacobians reduce to products of masked linear maps within a single activation region, we prove the existence of Lyapunov exponents governing the top singular values at initialization, give closed-form expressions in a tractable masked model, and quantify finite-depth corrections. We further show that sufficiently strong separation forces singular-vector alignment in matrix products, yielding an approximately shared singular basis for intermediate Jacobians. Together, these results motivate an approximation regime in which singular-value dynamics become effectively decoupled, mirroring classical balanced deep-linear analyses without requiring balancing. Experiments in fixed-gates settings validate the predicted scaling, alignment, and resulting dynamics, supporting a mechanistic account of emergent low-rank Jacobian structure as a driver of implicit bias.

</details>


### [137] [Rational Neural Networks have Expressivity Advantages](https://arxiv.org/abs/2602.12390)
*Maosen Tang,Alex Townsend*

Main category: cs.LG

TL;DR: Rational activation functions surpass traditional activations like ReLU and Sigmoid in expressiveness and efficiency, requiring fewer parameters for similar error targets, with practical benefits in standard networks.


<details>
  <summary>Details</summary>
Motivation: To demonstrate the superior expressiveness and parameter efficiency of trainable low-degree rational activation functions compared to fixed activations like ReLU and Sigmoid.

Method: The study compares neural networks using rational activations against those with fixed activations, analyzing parameter requirements for achieving a given error target ε. Theoretical separations and practical integrations into standard architectures are evaluated.

Result: Rational-activation networks achieve uniform approximation with poly(loglog(1/ε)) overhead, while fixed activations require Ω(log(1/ε)) parameters. Practically, rational activations perform equally or better under identical conditions.

Conclusion: Trainable rational activations are more efficient and expressive than fixed activations, offering significant advantages in both theory and practice.

Abstract: We study neural networks with trainable low-degree rational activation functions and show that they are more expressive and parameter-efficient than modern piecewise-linear and smooth activations such as ELU, LeakyReLU, LogSigmoid, PReLU, ReLU, SELU, CELU, Sigmoid, SiLU, Mish, Softplus, Tanh, Softmin, Softmax, and LogSoftmax. For an error target of $\varepsilon>0$, we establish approximation-theoretic separations: Any network built from standard fixed activations can be uniformly approximated on compact domains by a rational-activation network with only $\mathrm{poly}(\log\log(1/\varepsilon))$ overhead in size, while the converse provably requires $Ω(\log(1/\varepsilon))$ parameters in the worst case. This exponential gap persists at the level of full networks and extends to gated activations and transformer-style nonlinearities. In practice, rational activations integrate seamlessly into standard architectures and training pipelines, allowing rationals to match or outperform fixed activations under identical architectures and optimizers.

</details>


### [138] [High-dimensional Level Set Estimation with Trust Regions and Double Acquisition Functions](https://arxiv.org/abs/2602.12391)
*Giang Ngo,Dat Phan Trong,Dang Nguyen,Sunil Gupta*

Main category: cs.LG

TL;DR: TRLSE is a novel algorithm for high-dimensional Level Set Estimation (LSE) that uses dual acquisition functions to efficiently refine regions near the threshold boundary, outperforming existing methods in sample efficiency.


<details>
  <summary>Details</summary>
Motivation: High-dimensional LSE is challenging due to exponential growth in search volume with dimensionality. Active learning aims to iteratively acquire informative points for accurate classification.

Method: TRLSE employs dual acquisition functions (global and local) to identify and refine regions near the threshold boundary.

Result: Theoretical analysis and extensive evaluations show TRLSE's superior accuracy and sample efficiency across synthetic and real-world LSE problems.

Conclusion: TRLSE effectively addresses high-dimensional LSE challenges, offering improved performance over existing methods.

Abstract: Level set estimation (LSE) classifies whether an unknown function's value exceeds a specified threshold for given inputs, a fundamental problem in many real-world applications. In active learning settings with limited initial data, we aim to iteratively acquire informative points to construct an accurate classifier for this task. In high-dimensional spaces, this becomes challenging where the search volume grows exponentially with increasing dimensionality. We propose TRLSE, an algorithm for high-dimensional LSE, which identifies and refines regions near the threshold boundary with dual acquisition functions operating at both global and local levels. We provide a theoretical analysis of TRLSE's accuracy and show its superior sample efficiency against existing methods through extensive evaluations on multiple synthetic and real-world LSE problems.

</details>


### [139] [Synthetic Interaction Data for Scalable Personalization in Large Language Models](https://arxiv.org/abs/2602.12394)
*Yuchen Ma,Yue Huang,Wenjie Wang,Xiaonan Luo,Xiangliang Zhang,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: The paper introduces PersonaGym for synthetic personalized LLM interaction data and PPOpt for optimizing user prompts, improving task performance and personalization.


<details>
  <summary>Details</summary>
Motivation: Existing prompt optimization lacks focus on user-specific preferences and constraints due to missing personalized data and reward signals.

Method: Proposes PersonaGym for generating synthetic interaction data (PersonaAtlas) and PPOpt for optimizing prompts using a reason-then-optimize paradigm.

Result: Shows improvements in task performance, personalization quality, and robustness to noise/sparse preferences.

Conclusion: PersonaGym and PPOpt address gaps in personalized LLM deployment, offering scalable solutions for optimizing user prompts.

Abstract: Personalized prompting offers large opportunities for deploying large language models (LLMs) to diverse users, yet existing prompt optimization methods primarily focus on task-level optimization while largely overlooking user-specific preferences and latent constraints of individual users. This gap is primarily due to (i) the absence of high-quality, privacy-sensitive data that capture personalized user-LLM interactions at scale, and (ii) the lack of robust reward signals for individual preferences. To overcome existing data limitations, we introduce a high-fidelity synthetic data generation framework called PersonaGym. Unlike prior work that treats personalization as static persona-preference pairs, PersonaGym models a dynamic preference process via an agentic LLM system to simulate realistic preference behaviors and semantic-aware noise in order to generate personalized multi-turn interaction trajectories. Using PersonaGym, we release PersonaAtlas, a large-scale, high-quality, and diverse synthetic dataset of high-fidelity multi-turn personalized interaction trajectories that closely mirror real-world preference expression and noise patterns. We further propose Personalized Prompt Optimization (PPOpt), a scalable and model-agnostic framework that optimizes user prompts based on interaction histories without modifying the deployed LLM. PPOpt adopts a reason-then-optimize paradigm that infers an explicit user profile and conditions prompt rewriting on the user profile to avoid reward hacking. Our training procedure for PPOpt integrates a cold-start supervised prior with outcome-driven multi-objective reinforcement learning. We present extensive experiments to demonstrate consistent improvements over state-of-the-art baselines in terms of task performance, personalization quality, and robustness to noisy as well as to sparse preference signals.

</details>


### [140] [AstRL: Analog and Mixed-Signal Circuit Synthesis with Deep Reinforcement Learning](https://arxiv.org/abs/2602.12402)
*Felicia B. Guo,Ken T. Ho,Andrei Vladimirescu,Borivoje Nikolic*

Main category: cs.LG

TL;DR: The paper proposes AstRL, a deep reinforcement learning method for automated analog and mixed-signal circuit synthesis, achieving high structural correctness and functionality across diverse design tasks.


<details>
  <summary>Details</summary>
Motivation: The complexity and diversity of analog and mixed-signal circuit designs necessitate a generalized optimization method, which current automation struggles to address.

Method: AstRL frames circuit design as a graph generation problem, using deep reinforcement learning with policy-gradient, behavioral-cloning, and discriminator-based rewards in a simulator-embedded environment.

Result: Experimental results show 100% structural correctness and over 90% functionality in generated designs, outperforming state-of-the-art baselines.

Conclusion: AstRL successfully introduces an expert-aligned paradigm for generalized circuit generation, enabling fine-grained topology synthesis with robust performance.

Abstract: Analog and mixed-signal (AMS) integrated circuits (ICs) lie at the core of modern computing and communications systems. However, despite the continued rise in design complexity, advances in AMS automation remain limited. This reflects the central challenge in developing a generalized optimization method applicable across diverse circuit design spaces, many of which are distinct, constrained, and non-differentiable. To address this, our work casts circuit design as a graph generation problem and introduces a novel method of AMS synthesis driven by deep reinforcement learning (AstRL). Based on a policy-gradient approach, AstRL generates circuits directly optimized for user-specified targets within a simulator-embedded environment that provides ground-truth feedback during training. Through behavioral-cloning and discriminator-based similarity rewards, our method demonstrates, for the first time, an expert-aligned paradigm for generalized circuit generation validated in simulation. Importantly, the proposed approach operates at the level of individual transistors, enabling highly expressive, fine-grained topology generation. Strong inductive biases encoded in the action space and environment further drive structurally consistent and valid generation. Experimental results for three realistic design tasks illustrate substantial improvements in conventional design metrics over state-of-the-art baselines, with 100% of generated designs being structurally correct and over 90% demonstrating required functionality.

</details>


### [141] [Soft Contamination Means Benchmarks Test Shallow Generalization](https://arxiv.org/abs/2602.12413)
*Ari Spiesberger,Juan J. Vazquez,Nicky Pochinkov,Tomáš Gavenčiak,Peli Grietzer,Gavin Leech,Nandi Schoots*

Main category: cs.LG

TL;DR: LLM training data contaminated with benchmark test data leads to biased OOD generalization estimates. Soft contamination via semantic duplicates is widespread and improves benchmark performance, confounding recent gains.


<details>
  <summary>Details</summary>
Motivation: To highlight the issue of data contamination in LLM training, particularly semantic duplicates that traditional n-gram filters miss, and its impact on biased benchmark performance.

Method: Analyzes soft contamination by embedding the Olmo3 training corpus, identifying semantic duplicates, and evaluating their impact on benchmark performance.

Result: Contamination is widespread (78% semantic duplicates in CodeForces, 50% exact duplicates in ZebraLogic). Including duplicates boosts benchmark performance and even improves truly-held-out data performance.

Conclusion: Recent benchmark gains are confounded by soft contamination, reflecting both genuine improvements and test data accumulation in training corpora.

Abstract: If LLM training data is polluted with benchmark test data, then benchmark performance gives biased estimates of out-of-distribution (OOD) generalization. Typical decontamination filters use n-gram matching which fail to detect semantic duplicates: sentences with equivalent (or near-equivalent) content that are not close in string space. We study this soft contamination of training data by semantic duplicates. Among other experiments, we embed the Olmo3 training corpus and find that: 1) contamination remains widespread, e.g. we find semantic duplicates for 78% of CodeForces and exact duplicates for 50% of ZebraLogic problems; 2) including semantic duplicates of benchmark data in training does improve benchmark performance; and 3) when finetuning on duplicates of benchmark datapoints, performance also improves on truly-held-out datapoints from the same benchmark. We argue that recent benchmark gains are thus confounded: the prevalence of soft contamination means gains reflect both genuine capability improvements and the accumulation of test data and effective test data in growing training corpora.

</details>


### [142] [Stabilizing Native Low-Rank LLM Pretraining](https://arxiv.org/abs/2602.12429)
*Paul Janson,Edouard Oyallon,Eugene Belilovsky*

Main category: cs.LG

TL;DR: The paper proposes Spectron, a method to train large language models (LLMs) using exclusively low-rank weights for efficiency, addressing instability issues by controlling spectral norms of weight updates.


<details>
  <summary>Details</summary>
Motivation: The growing parameter counts in foundation models create computational and memory challenges, prompting the need for efficient training methods like low-rank factorization without performance loss.

Method: Introduces Spectron, which dynamically bounds weight updates via spectral renormalization with orthogonalization to stabilize low-rank training.

Result: Enables stable, end-to-end low-rank training with negligible overhead and demonstrates improved inference efficiency over dense models.

Conclusion: Spectron provides a viable solution for training LLMs with low-rank weights, achieving predictable scaling and better efficiency.

Abstract: Foundation models have achieved remarkable success, yet their growing parameter counts pose significant computational and memory challenges. Low-rank factorization offers a promising route to reduce training and inference costs, but the community lacks a stable recipe for training models from scratch using exclusively low-rank weights while matching the performance of the dense model. We demonstrate that Large Language Models (LLMs) can be trained from scratch using exclusively low-rank factorized weights for all non-embedding matrices without auxiliary "full-rank" guidance required by prior methods. While native low-rank training often suffers from instability and loss spikes, we identify uncontrolled growth in the spectral norm (largest singular value) of the weight matrix update as the dominant factor. To address this, we introduce Spectron: Spectral renormalization with orthogonalization, which dynamically bounds the resultant weight updates based on the current spectral norms of the factors. Our method enables stable, end-to-end factorized training with negligible overhead. Finally, we establish compute-optimal scaling laws for natively low-rank transformers, demonstrating predictable power-law behavior and improved inference efficiency relative to dense models.

</details>


### [143] [Safe Reinforcement Learning via Recovery-based Shielding with Gaussian Process Dynamics Models](https://arxiv.org/abs/2602.12444)
*Alexander W. Goodall,Francesco Belardinelli*

Main category: cs.LG

TL;DR: A novel recovery-based shielding framework is introduced for safe reinforcement learning (RL) in non-linear systems, using Gaussian processes (GP) to ensure safety without limiting exploration.


<details>
  <summary>Details</summary>
Motivation: RL lacks provable safety guarantees for critical applications, necessitating a framework that ensures safety while maintaining performance.

Method: Integrates a backup policy (shield) with RL, using GP-based uncertainty quantification to predict and recover from safety violations. Policy optimization is done via model-based sampling.

Result: Effectively ensures safety and strong performance in continuous control environments.

Conclusion: The proposed framework successfully balances safety and exploration, making RL viable for safety-critical applications.

Abstract: Reinforcement learning (RL) is a powerful framework for optimal decision-making and control but often lacks provable guarantees for safety-critical applications. In this paper, we introduce a novel recovery-based shielding framework that enables safe RL with a provable safety lower bound for unknown and non-linear continuous dynamical systems. The proposed approach integrates a backup policy (shield) with the RL agent, leveraging Gaussian process (GP) based uncertainty quantification to predict potential violations of safety constraints, dynamically recovering to safe trajectories only when necessary. Experience gathered by the 'shielded' agent is used to construct the GP models, with policy optimization via internal model-based sampling - enabling unrestricted exploration and sample efficient learning, without compromising safety. Empirically our approach demonstrates strong performance and strict safety-compliance on a suite of continuous control environments.

</details>


### [144] [Computationally sufficient statistics for Ising models](https://arxiv.org/abs/2602.12449)
*Abhijith Jayakumar,Shreya Shukla,Marc Vuffray,Andrey Y. Lokhov,Sidhant Misra*

Main category: cs.LG

TL;DR: The paper explores efficient learning of Gibbs distributions using limited statistics, focusing on the Ising model and demonstrating parameter reconstruction with constrained computational and observational resources.


<details>
  <summary>Details</summary>
Motivation: Gibbs distribution learning is computationally challenging when only sufficient statistics are available. The study aims to find efficient methods under limited observational power.

Method: The study uses the Ising model to demonstrate parameter reconstruction with limited statistics, specifically focusing on observing statistics up to an order of the model's $ℓ_1$ width.

Result: It is feasible to reconstruct model parameters using limited statistics, inferring structure, couplings, and magnetic fields. Prior structural knowledge can further reduce the observational power needed.

Conclusion: Efficient learning of Gibbs distributions is possible with constrained statistics, expanding applicability in physical systems where full samples are impractical.

Abstract: Learning Gibbs distributions using only sufficient statistics has long been recognized as a computationally hard problem. On the other hand, computationally efficient algorithms for learning Gibbs distributions rely on access to full sample configurations generated from the model. For many systems of interest that arise in physical contexts, expecting a full sample to be observed is not practical, and hence it is important to look for computationally efficient methods that solve the learning problem with access to only a limited set of statistics. We examine the trade-offs between the power of computation and observation within this scenario, employing the Ising model as a paradigmatic example. We demonstrate that it is feasible to reconstruct the model parameters for a model with $\ell_1$ width $γ$ by observing statistics up to an order of $O(γ)$. This approach allows us to infer the model's structure and also learn its couplings and magnetic fields. We also discuss a setting where prior information about structure of the model is available and show that the learning problem can be solved efficiently with even more limited observational power.

</details>


### [145] [Regularized Meta-Learning for Improved Generalization](https://arxiv.org/abs/2602.12469)
*Noor Islam S. Mohammad,Md Muntaqim Meherab*

Main category: cs.LG

TL;DR: Regularized meta-learning improves predictive performance and efficiency in deep ensembles by addressing redundancy, multicollinearity, and overfitting.


<details>
  <summary>Details</summary>
Motivation: Deep ensemble methods face issues like redundancy, unstable weighting, and overfitting, limiting their practical utility.

Method: Proposes a four-stage pipeline: redundancy-aware projection, statistical meta-feature augmentation, cross-validated regularized meta-models (Ridge, Lasso, ElasticNet), and inverse-RMSE blending.

Result: Achieves RMSE of 8.582, outperforms simple averaging (8.894) and Ridge stacking (8.627), matches greedy hill climbing (8.603) with faster runtime (4x). Conditioning improves by 53.7%.

Conclusion: Regularized meta-learning offers stable and efficient stacking for high-dimensional ensembles.

Abstract: Deep ensemble methods often improve predictive performance, yet they suffer from three practical limitations: redundancy among base models that inflates computational cost and degrades conditioning, unstable weighting under multicollinearity, and overfitting in meta-learning pipelines. We propose a regularized meta-learning framework that addresses these challenges through a four-stage pipeline combining redundancy-aware projection, statistical meta-feature augmentation, and cross-validated regularized meta-models (Ridge, Lasso, and ElasticNet). Our multi-metric de-duplication strategy removes near-collinear predictors using correlation and MSE thresholds ($τ_{\text{corr}}=0.95$), reducing the effective condition number of the meta-design matrix while preserving predictive diversity. Engineered ensemble statistics and interaction terms recover higher-order structure unavailable to raw prediction columns. A final inverse-RMSE blending stage mitigates regularizer-selection variance. On the Playground Series S6E1 benchmark (100K samples, 72 base models), the proposed framework achieves an out-of-fold RMSE of 8.582, improving over simple averaging (8.894) and conventional Ridge stacking (8.627), while matching greedy hill climbing (8.603) with substantially lower runtime (4 times faster). Conditioning analysis shows a 53.7\% reduction in effective matrix condition number after redundancy projection. Comprehensive ablations demonstrate consistent contributions from de-duplication, statistical meta-features, and meta-ensemble blending. These results position regularized meta-learning as a stable and deployment-efficient stacking strategy for high-dimensional ensemble systems.

</details>


### [146] [Designing RNAs with Language Models](https://arxiv.org/abs/2602.12470)
*Milan Gautam,Ning Dai,Tianshuo Zhou,Bowen Xie,David Mathews,Liang Huang*

Main category: cs.LG

TL;DR: The paper introduces a neural approximator using autoregressive language models and reinforcement learning to improve RNA design, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: RNA design is computationally challenging due to large sequence spaces and competing folds. Traditional methods rely on optimization heuristics, which are inefficient.

Method: Reframes RNA design as conditional sequence generation using a neural approximator (autoregressive LM). Trains with supervised learning on random structure-sequence pairs, then optimizes with RL.

Result: Outperforms state-of-the-art systems on metrics like Boltzmann probability, achieving 1.7x faster performance.

Conclusion: Conditional LM generation offers a scalable, task-agnostic alternative to traditional RNA design methods.

Abstract: RNA design, the task of finding a sequence that folds into a target secondary structure, has broad biological and biomedical impact but remains computationally challenging due to the exponentially large sequence space and exponentially many competing folds. Traditional approaches treat it as an optimization problem, relying on per-instance heuristics or constraint-based search. We instead reframe RNA design as conditional sequence generation and introduce a reusable neural approximator, instantiated as an autoregressive language model (LM), that maps target structures directly to sequences. We first train our model in a supervised setting on random-induced structure-sequence pairs, and then use reinforcement learning (RL) to optimize end-to-end metrics. We also propose methods to select a small subset for RL that greatly improves RL efficiency and quality. Across four datasets, our approach outperforms state-of-the-art systems on key metrics such as Boltzmann probability while being 1.7x faster, establishing conditional LM generation as a scalable, task-agnostic alternative to per-instance optimization for RNA design. Our code and data are available at https://github.com/KuNyaa/RNA-Design-LM.

</details>


### [147] [Tight Bounds for Logistic Regression with Large Stepsize Gradient Descent in Low Dimension](https://arxiv.org/abs/2602.12471)
*Michael Crawshaw,Mingrui Liu*

Main category: cs.LG

TL;DR: The paper refines the analysis of gradient descent (GD) for minimizing logistic loss in binary classification with separable 2D data, showing a tighter convergence rate under a large learning rate, and bounds the transition time from unstable to stable dynamics.


<details>
  <summary>Details</summary>
Motivation: To better understand the dynamics of GD with large learning rates in separable binary classification, particularly focusing on the transition phase between unstable and stable loss behavior.

Method: Analyzes GD's oscillatory dynamics in the subspace orthogonal to the max-margin classifier, providing upper and lower bounds on the transition time (τ) to stable behavior.

Result: Demonstrates GD achieves loss smaller than O(1/(ηT)) under certain conditions and establishes tight bounds on τ, matching upper and lower bounds up to logarithmic factors.

Conclusion: The analysis tightens previous results and clarifies the transient behavior of GD in separable 2D classification, validating the role of large learning rates.

Abstract: We consider the optimization problem of minimizing the logistic loss with gradient descent to train a linear model for binary classification with separable data. With a budget of $T$ iterations, it was recently shown that an accelerated $1/T^2$ rate is possible by choosing a large step size $η= Θ(γ^2 T)$ (where $γ$ is the dataset's margin) despite the resulting non-monotonicity of the loss. In this paper, we provide a tighter analysis of gradient descent for this problem when the data is two-dimensional: we show that GD with a sufficiently large learning rate $η$ finds a point with loss smaller than $\mathcal{O}(1/(ηT))$, as long as $T \geq Ω(n/γ+ 1/γ^2)$, where $n$ is the dataset size. Our improved rate comes from a tighter bound on the time $τ$ that it takes for GD to transition from unstable (non-monotonic loss) to stable (monotonic loss), via a fine-grained analysis of the oscillatory dynamics of GD in the subspace orthogonal to the max-margin classifier. We also provide a lower bound of $τ$ matching our upper bound up to logarithmic factors, showing that our analysis is tight.

</details>


### [148] [Geometric separation and constructive universal approximation with two hidden layers](https://arxiv.org/abs/2602.12482)
*Chanyoung Sung*

Main category: cs.LG

TL;DR: Geometric construction of neural networks separates disjoint compact sets in ℝⁿ, proving universal approximation with 2 hidden layers (or 1 for finite sets) using sigmoidal or ReLU activations.


<details>
  <summary>Details</summary>
Motivation: To provide a constructive and geometric approach for neural networks to separate disjoint compact subsets and achieve universal approximation.

Method: Use geometric construction to design neural networks with two hidden layers (or one for finite sets) employing sigmoidal or ReLU activations for uniform approximation.

Result: Networks with two hidden layers (or one for finite sets) can approximate any continuous function on compact sets in ℝⁿ uniformly.

Conclusion: The study confirms the universality of neural networks with specific architectures and activations for approximating continuous functions.

Abstract: We give a geometric construction of neural networks that separate disjoint compact subsets of $\Bbb R^n$, and use it to obtain a constructive universal approximation theorem. Specifically, we show that networks with two hidden layers and either a sigmoidal activation (i.e., strictly monotone bounded continuous) or the ReLU activation can approximate any real-valued continuous function on an arbitrary compact set $K\subset\Bbb R^n$ to any prescribed accuracy in the uniform norm. For finite $K$, the construction simplifies and yields a sharp depth-2 (single hidden layer) approximation result.

</details>


### [149] [On Robustness and Chain-of-Thought Consistency of RL-Finetuned VLMs](https://arxiv.org/abs/2602.12506)
*Rosie Zhao,Anshul Shah,Xiaoyu Zhu,Xinke Deng,Zhongyu Jiang,Yang Yang,Joerg Liebelt,Arnab Mondal*

Main category: cs.LG

TL;DR: RL fine-tuning boosts LLMs and VLMs but makes them vulnerable to textual perturbations, revealing weaknesses in visual grounding and faithfulness. Faithfulness-aware rewards help, but robustness challenges persist.


<details>
  <summary>Details</summary>
Motivation: To address vulnerabilities in RL-tuned VLMs, such as weak visual grounding and over-reliance on text, by analyzing the impact of textual perturbations and proposing solutions.

Method: Analyze RL fine-tuning dynamics using controlled textual perturbations (misleading captions, incorrect CoT traces) and entropy-based metrics. Test adversarial augmentation and faithfulness-aware rewards.

Result: Fine-tuning improves accuracy but weakens faithfulness and robustness. Faithfulness-aware rewards restore alignment but robustness remains difficult.

Conclusion: Accuracy-only evaluations are insufficient; future protocols must prioritize correctness, robustness, and faithfulness in VLMs.

Abstract: Reinforcement learning (RL) fine-tuning has become a key technique for enhancing large language models (LLMs) on reasoning-intensive tasks, motivating its extension to vision language models (VLMs). While RL-tuned VLMs improve on visual reasoning benchmarks, they remain vulnerable to weak visual grounding, hallucinations, and over-reliance on textual cues. We show that simple, controlled textual perturbations--misleading captions or incorrect chain-of-thought (CoT) traces--cause substantial drops in robustness and confidence, and that these effects are more pronounced when CoT consistency is taken into account across open-source multimodal reasoning models. Entropy-based metrics further show that these perturbations reshape model uncertainty and probability mass on the correct option, exposing model-specific trends in miscalibration. To better understand these vulnerabilities, we further analyze RL fine-tuning dynamics and uncover an accuracy-faithfulness trade-off: fine-tuning raises benchmark accuracy, but can simultaneously erode the reliability of the accompanying CoT and its robustness to contextual shifts. Although adversarial augmentation improves robustness, it does not by itself prevent faithfulness drift. Incorporating a faithfulness-aware reward can restore alignment between answers and reasoning, but when paired with augmentation, training risks collapsing onto shortcut strategies and robustness remains elusive. Together, these findings highlight the limitations of accuracy-only evaluations and motivate training and assessment protocols that jointly emphasize correctness, robustness, and the faithfulness of visually grounded reasoning.

</details>


### [150] [Constraint-Rectified Training for Efficient Chain-of-Thought](https://arxiv.org/abs/2602.12526)
*Qinhang Wu,Sen Lin,Ming Zhang,Yingbin Liang,Ness B. Shroff*

Main category: cs.LG

TL;DR: CRT introduces a principled post-training framework for efficient reasoning in LLMs, balancing reasoning length and accuracy without sacrificing performance.


<details>
  <summary>Details</summary>
Motivation: To address the high inference costs and redundancy (overthinking) in longer reasoning traces of LLMs, while maintaining answer quality.

Method: CRT uses reference-guarded constrained optimization, alternating between minimizing reasoning length and rectifying accuracy, extended with a two-stage training scheme.

Result: CRT reduces token usage while maintaining answer quality, improves reasoning efficiency, and enables fine-grained control over reasoning verbosity.

Conclusion: CRT provides a stable and interpretable solution for efficient reasoning in LLMs, outperforming heuristic-based approaches.

Abstract: Chain-of-Thought (CoT) has significantly enhanced the reasoning capabilities of Large Language Models (LLMs), especially when combined with reinforcement learning (RL) based post-training methods. While longer reasoning traces can improve answer quality and unlock abilities such as self-correction, they also incur high inference costs and often introduce redundant steps, known as overthinking. Recent research seeks to develop efficient reasoning strategies that balance reasoning length and accuracy, either through length-aware reward design or prompt-based calibration. However, these heuristic-based approaches may suffer from severe accuracy drop and be very sensitive to hyperparameters. To address these problems, we introduce CRT (Constraint-Rectified Training), a principled post-training framework based on reference-guarded constrained optimization, yielding a more stable and interpretable formulation for efficient reasoning. CRT alternates between minimizing reasoning length and rectifying accuracy only when performance falls below the reference, enabling stable and effective pruning of redundant reasoning. We further extend CRT with a two-stage training scheme that first discovers the shortest reliable reasoning patterns and then refines accuracy under a learnt length budget, preventing the re-emergence of verbose CoT. Our comprehensive evaluation shows that this framework consistently reduces token usage while maintaining answer quality at a robust and reliable level. Further analysis reveals that CRT improves reasoning efficiency not only by shortening responses but also by reducing internal language redundancy, leading to a new evaluation metric. Moreover, CRT-based training naturally yields a sequence of intermediate checkpoints that span a spectrum of explanation lengths while preserving correctness, enabling fine-grained control over reasoning verbosity without retraining.

</details>


### [151] [Analytical Results for Two Exponential Family Distributions in Hierarchical Dirichlet Processes](https://arxiv.org/abs/2602.12527)
*Naiqi Li*

Main category: cs.LG

TL;DR: The paper extends the Hierarchical Dirichlet Process (HDP) framework to include Gamma-Poisson and Normal-Gamma-Normal conjugate pairs, providing analytical tractability for Poisson and normal distributions.


<details>
  <summary>Details</summary>
Motivation: To broaden the applicability of the HDP beyond Dirichlet-multinomial settings by incorporating exponential family distributions, specifically Poisson and normal distributions.

Method: Derived explicit closed-form expressions for Gamma-Poisson and Normal-Gamma-Normal conjugate pairs within the HDP framework.

Result: Successfully extended the HDP framework to handle Poisson and normal distributions analytically, with detailed proofs.

Conclusion: The work enhances the HDP's versatility, offering practical tools for hierarchical Bayesian nonparametric modeling.

Abstract: The Hierarchical Dirichlet Process (HDP) provides a flexible Bayesian nonparametric framework for modeling grouped data with a shared yet unbounded collection of mixture components. While existing applications of the HDP predominantly focus on the Dirichlet-multinomial conjugate structure, the framework itself is considerably more general and, in principle, accommodates a broad class of conjugate prior-likelihood pairs. In particular, exponential family distributions offer a unified and analytically tractable modeling paradigm that encompasses many commonly used distributions. In this paper, we investigate analytic results for two important members of the exponential family within the HDP framework: the Poisson distribution and the normal distribution. We derive explicit closed-form expressions for the corresponding Gamma-Poisson and Normal-Gamma-Normal conjugate pairs under the hierarchical Dirichlet process construction. Detailed derivations and proofs are provided to clarify the underlying mathematical structure and to demonstrate how conjugacy can be systematically exploited in hierarchical nonparametric models. Our work extends the applicability of the HDP beyond the Dirichlet-multinomial setting and furnishes practical analytic results for researchers employing hierarchical Bayesian nonparametrics.

</details>


### [152] [Flow-Factory: A Unified Framework for Reinforcement Learning in Flow-Matching Models](https://arxiv.org/abs/2602.12529)
*Bowen Ping,Chengyou Jia,Minnan Luo,Hangwei Qian,Ivor Tsang*

Main category: cs.LG

TL;DR: Flow-Factory is a unified framework simplifying reinforcement learning for diffusion and flow-matching models by decoupling algorithms, models, and rewards.


<details>
  <summary>Details</summary>
Motivation: Address fragmented codebases, model-specific implementations, and engineering complexity in aligning models with human preferences.

Method: Modular, registry-based architecture enabling seamless integration of new algorithms and architectures.

Result: Supports GRPO, DiffusionNFT, and AWM across Flux, Qwen-Image, and WAN video models, with minimized implementation overhead.

Conclusion: Empowers rapid prototyping and scaling of innovations, offering production-ready optimizations and training support.

Abstract: Reinforcement learning has emerged as a promising paradigm for aligning diffusion and flow-matching models with human preferences, yet practitioners face fragmented codebases, model-specific implementations, and engineering complexity. We introduce Flow-Factory, a unified framework that decouples algorithms, models, and rewards through through a modular, registry-based architecture. This design enables seamless integration of new algorithms and architectures, as demonstrated by our support for GRPO, DiffusionNFT, and AWM across Flux, Qwen-Image, and WAN video models. By minimizing implementation overhead, Flow-Factory empowers researchers to rapidly prototype and scale future innovations with ease. Flow-Factory provides production-ready memory optimization, flexible multi-reward training, and seamless distributed training support. The codebase is available at https://github.com/X-GenGroup/Flow-Factory.

</details>


### [153] [AMPS: Adaptive Modality Preference Steering via Functional Entropy](https://arxiv.org/abs/2602.12533)
*Zihan Huang,Xintong Li,Rohan Surana,Tong Yu,Rui Wang,Julian McAuley,Jingbo Shang,Junda Wu*

Main category: cs.LG

TL;DR: The paper addresses modality preference imbalance in Multimodal Large Language Models (MLLMs) by introducing an instance-aware metric and adaptive steering strategy, improving accuracy without increasing errors.


<details>
  <summary>Details</summary>
Motivation: MLLMs often favor one modality over another (linguistic or visual), leading to biased results. Prior methods used uniform steering, which is either too strong (causing errors) or too weak (ineffective), and lacks adaptability to individual instances.

Method: The authors propose an instance-aware metric to quantify modality contributions and a learnable module for adaptive steering. This allows dynamic adjustment of steering intensity based on sample-specific susceptibility.

Result: Experiments show their method effectively modulates modality preference while maintaining low error rates compared to traditional steering approaches.

Conclusion: Instance-aware steering provides a more precise and adaptable solution to mitigate modality bias in MLLMs, enhancing performance without compromising inference quality.

Abstract: Multimodal Large Language Models (MLLMs) often exhibit significant modality preference, which is a tendency to favor one modality over another. Depending on the input, they may over-rely on linguistic priors relative to visual evidence, or conversely over-attend to visually salient but facts in textual contexts. Prior work has applied a uniform steering intensity to adjust the modality preference of MLLMs. However, strong steering can impair standard inference and increase error rates, whereas weak steering is often ineffective. In addition, because steering sensitivity varies substantially across multimodal instances, a single global strength is difficult to calibrate. To address this limitation with minimal disruption to inference, we introduce an instance-aware diagnostic metric that quantifies each modality's information contribution and reveals sample-specific susceptibility to steering. Building on these insights, we propose a scaling strategy that reduces steering for sensitive samples and a learnable module that infers scaling patterns, enabling instance-aware control of modality preference. Experimental results show that our instance-aware steering outperforms conventional steering in modulating modality preference, achieving effective adjustment while keeping generation error rates low.

</details>


### [154] [Exploring Accurate and Transparent Domain Adaptation in Predictive Healthcare via Concept-Grounded Orthogonal Inference](https://arxiv.org/abs/2602.12542)
*Pengfei Hu,Chang Lu,Feifan Liu,Yue Ning*

Main category: cs.LG

TL;DR: ExtraCare improves clinical event prediction by decomposing patient representations into invariant and covariant components, offering transparency and better performance.


<details>
  <summary>Details</summary>
Motivation: Clinical EHR models often degrade under different data distributions, and existing DA methods lack transparency, hindering trust in clinical settings.

Method: ExtraCare decomposes representations into invariant and covariant components, supervises their orthogonality, and maps latent dimensions to medical concepts.

Result: ExtraCare outperforms feature alignment models and provides human-understandable explanations, validated on real-world EHR datasets.

Conclusion: ExtraCare enhances prediction accuracy and transparency, making it suitable for clinical practice.

Abstract: Deep learning models for clinical event prediction on electronic health records (EHR) often suffer performance degradation when deployed under different data distributions. While domain adaptation (DA) methods can mitigate such shifts, its "black-box" nature prevents widespread adoption in clinical practice where transparency is essential for trust and safety. We propose ExtraCare to decompose patient representations into invariant and covariant components. By supervising these two components and enforcing their orthogonality during training, our model preserves label information while exposing domain-specific variation at the same time for more accurate predictions than most feature alignment models. More importantly, it offers human-understandable explanations by mapping sparse latent dimensions to medical concepts and quantifying their contributions via targeted ablations. ExtraCare is evaluated on two real-world EHR datasets across multiple domain partition settings, demonstrating superior performance along with enhanced transparency, as evidenced by its accurate predictions and explanations from extensive case studies.

</details>


### [155] [SD-MoE: Spectral Decomposition for Effective Expert Specialization](https://arxiv.org/abs/2602.12556)
*Ruijun Huang,Fang Dong,Xin Zhang,Hengjie Cao,Zhendong Huang,Anrui Chen,Jixian Zhou,Mengyi Chen,Yifeng Yang,Mingzhi Dong,Yujiang Wang,Jinlong Hou,Qin Lv,Robert P. Dick,Yuan Cheng,Fan Yang,Tun Lu,Chun Zhang,Li Shang*

Main category: cs.LG

TL;DR: SD-MoE addresses overlapping expert specialization in Mixture-of-Experts models by decomposing parameters and gradients spectrally, enhancing performance with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Experts in MoE architectures often fail to specialize due to overlapping spectral components and gradient alignment, limiting model capacity and performance.

Method: Proposes Spectral-Decoupled MoE (SD-MoE), which spectrally decomposes parameters and gradients to promote expert specialization.

Result: SD-MoE improves downstream task performance, fosters effective expert specialization, and integrates easily into existing MoE architectures.

Conclusion: SD-MoE effectively mitigates expert overlap issues, enhancing MoE model performance without significant computational cost.

Abstract: Mixture-of-Experts (MoE) architectures scale Large Language Models via expert specialization induced by conditional computation. In practice, however, expert specialization often fails: some experts become functionally similar, while others functioning as de facto shared experts, limiting the effective capacity and model performance. In this work, we analysis from a spectral perspective on parameter and gradient spaces, uncover that (1) experts share highly overlapping dominant spectral components in their parameters, (2) dominant gradient subspaces are strongly aligned across experts, driven by ubiquitous low-rank structure in human corpus, and (3) gating mechanisms preferentially route inputs along these dominant directions, further limiting specialization. To address this, we propose Spectral-Decoupled MoE (SD-MoE), which decomposes both parameter and gradient in the spectral space. SD-MoE improves performance across downstream tasks, enables effective expert specialization, incurring minimal additional computation, and can be seamlessly integrated into a wide range of existing MoE architectures, including Qwen and DeepSeek.

</details>


### [156] [Fractional Order Federated Learning for Battery Electric Vehicle Energy Consumption Modeling](https://arxiv.org/abs/2602.12567)
*Mohammad Partohaghighi,Roummel Marcia,Bruce J. West,YangQuan Chen*

Main category: cs.LG

TL;DR: FO-RI-FedAvg is a lightweight extension of FedAvg that improves stability in federated learning for electric vehicles by using adaptive regularization and non-integer-order optimization, outperforming baselines in accuracy and convergence.


<details>
  <summary>Details</summary>
Motivation: Addressing instability in federated learning for electric vehicles caused by intermittent connectivity, varying client participation, and diverse operating conditions.

Method: Introduces FO-RI-FedAvg with adaptive roughness-informed proximal regularization and non-integer-order local optimization.

Result: Improved accuracy and stable convergence on real-world BEV energy prediction datasets, especially with reduced client participation.

Conclusion: FO-RI-FedAvg effectively stabilizes federated learning under realistic constraints without heavy computational overhead.

Abstract: Federated learning on connected electric vehicles (BEVs) faces severe instability due to intermittent connectivity, time-varying client participation, and pronounced client-to-client variation induced by diverse operating conditions. Conventional FedAvg and many advanced methods can suffer from excessive drift and degraded convergence under these realistic constraints. This work introduces Fractional-Order Roughness-Informed Federated Averaging (FO-RI-FedAvg), a lightweight and modular extension of FedAvg that improves stability through two complementary client-side mechanisms: (i) adaptive roughness-informed proximal regularization, which dynamically tunes the pull toward the global model based on local loss-landscape roughness, and (ii) non-integer-order local optimization, which incorporates short-term memory to smooth conflicting update directions. The approach preserves standard FedAvg server aggregation, adds only element-wise operations with amortizable overhead, and allows independent toggling of each component. Experiments on two real-world BEV energy prediction datasets, VED and its extended version eVED, show that FO-RI-FedAvg achieves improved accuracy and more stable convergence compared to strong federated baselines, particularly under reduced client participation.

</details>


### [157] [VI-CuRL: Stabilizing Verifier-Independent RL Reasoning via Confidence-Guided Variance Reduction](https://arxiv.org/abs/2602.12579)
*Xin-Qiang Cai,Masashi Sugiyama*

Main category: cs.LG

TL;DR: VI-CuRL introduces a verifier-free curriculum reinforcement learning method to address destructive gradient variance in LLMs, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: To overcome scalability limits of external verifiers in RLVR and tackle challenges like destructive gradient variance in verifier-free settings.

Method: VI-CuRL uses intrinsic confidence to create a curriculum, prioritizing high-confidence samples to manage bias-variance trade-off.

Result: The framework ensures asymptotic unbiasedness and outperforms verifier-independent baselines across six benchmarks.

Conclusion: VI-CuRL offers a scalable, stable solution for enhancing LLMs without external verifiers.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a dominant paradigm for enhancing Large Language Models (LLMs) reasoning, yet its reliance on external verifiers limits its scalability. Recent findings suggest that RLVR primarily functions by eliciting latent capabilities, motivating the development of verifier-free algorithms. However, in such settings, standard methods like Group Relative Policy Optimization face a critical challenge: destructive gradient variance that often leads to training collapse. To address this issue, we introduceVerifier-Independent Curriculum Reinforcement Learning (VI-CuRL), a framework that leverages the model's intrinsic confidence to construct a curriculum independent from external verifiers. By prioritizing high-confidence samples, VI-CuRL effectively manages the bias-variance trade-off, specifically targeting the reduction of action and problem variance. We provide a rigorous theoretical analysis, proving that our estimator guarantees asymptotic unbiasedness. Empirically, VI-CuRL promotes stability and consistently outperforms verifier-independent baselines across six challenging benchmarks with/without verifiers.

</details>


### [158] [Multi-Head Attention as a Source of Catastrophic Forgetting in MoE Transformers](https://arxiv.org/abs/2602.12587)
*Anrui Chen,Ruijun Huang,Xin Zhang,Fang Dong,Hengjie Cao,Zhendong Huang,Yifeng Yang,Mengyi Chen,Jixian Zhou,Mingzhi Dong,Yujiang Wang,Jinlong Hou,Qin Lv,Robert P. Dick,Yuan Cheng,Tun Lu,Fan Yang,Li Shang*

Main category: cs.LG

TL;DR: MH-MoE addresses forgetting in MoE Transformers by head-wise routing, reducing composition collisions and improving continual learning performance.


<details>
  <summary>Details</summary>
Motivation: Mixture-of-Experts (MoE) architectures struggle with continual learning due to pre-routing bottlenecks causing feature composition collisions.

Method: Proposes MH-MoE, which routes sub-representations head-wise to enhance granularity and minimize collisions.

Result: Improves continual learning, reducing BWT on Qwen3-0.6B from 11.2% (LoRAMoE) to 4.5%.

Conclusion: MH-MoE effectively mitigates forgetting in MoE Transformers by refining routing granularity.

Abstract: Mixture-of-Experts (MoE) architectures are often considered a natural fit for continual learning because sparse routing should localize updates and reduce interference, yet MoE Transformers still forget substantially even with sparse, well-balanced expert utilization. We attribute this gap to a pre-routing bottleneck: multi-head attention concatenates head-specific signals into a single post-attention router input, forcing routing to act on co-occurring feature compositions rather than separable head channels. We show that this router input simultaneously encodes multiple separately decodable semantic and structural factors with uneven head support, and that different feature compositions induce weakly aligned parameter-gradient directions; as a result, routing maps many distinct compositions to the same route. We quantify this collision effect via a route-wise effective composition number $N_{eff}$ and find that higher $N_{eff}$ is associated with larger old-task loss increases after continual training. Motivated by these findings, we propose MH-MoE, which performs head-wise routing over sub-representations to increase routing granularity and reduce composition collisions. On TRACE with Qwen3-0.6B/8B, MH-MoE effectively mitigates forgetting, reducing BWT on Qwen3-0.6B from 11.2% (LoRAMoE) to 4.5%.

</details>


### [159] [Vehicle behaviour estimation for abnormal event detection using distributed fiber optic sensing](https://arxiv.org/abs/2602.12591)
*Hemant Prasad,Daisuke Ikefuji,Shin Tominaga,Hitoshi Sakurai,Manabu Otani*

Main category: cs.LG

TL;DR: Detects single-lane traffic abnormalities by tracking vehicle paths and lane changes using clustering and spectral centroid analysis, achieving 80% accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of detecting single-lane abnormalities that cause traffic congestion using existing fiber-optic infrastructure.

Method: Tracks individual vehicle paths, estimates positions using clustering, and detects lane changes via spectral centroid shifts in vibrations.

Result: Achieved 80% accuracy in detecting lane changes, indicating abnormalities.

Conclusion: Proposed method effectively identifies single-lane abnormalities through vehicle path and lane change analysis.

Abstract: The distributed fiber-optic sensing (DFOS) system is a cost-effective wide-area traffic monitoring technology that utilizes existing fiber infrastructure to effectively detect traffic congestions. However, detecting single-lane abnormalities, that lead to congestions, is still a challenge. These single-lane abnormalities can be detected by monitoring lane change behaviour of vehicles, performed to avoid congestion along the monitoring section of a road. This paper presents a method to detect single-lane abnormalities by tracking individual vehicle paths and detecting vehicle lane changes along a section of a road. We propose a method to estimate the vehicle position at all time instances and fit a path using clustering techniques. We detect vehicle lane change by monitoring any change in spectral centroid of vehicle vibrations by tracking a reference vehicle along a highway. The evaluation of our proposed method with real traffic data showed 80% accuracy for lane change detection events that represent presence of abnormalities.

</details>


### [160] [Power Interpretable Causal ODE Networks: A Unified Model for Explainable Anomaly Detection and Root Cause Analysis in Power Systems](https://arxiv.org/abs/2602.12592)
*Yue Sun,Likai Wang,Rick S. Blum,Parv Venkitasubramaniam*

Main category: cs.LG

TL;DR: PICODE Networks propose a causality-informed architecture for anomaly detection and explanation in power grids, improving interpretability and reducing reliance on labeled data.


<details>
  <summary>Details</summary>
Motivation: Existing anomaly detection models lack interpretability, providing only binary outputs without explanations for anomalies.

Method: Developed Power Interpretable Causality Ordinary Differential Equation (PICODE) Networks to jointly detect anomalies and explain their causes, including root cause localization and anomaly type classification.

Result: PICODE achieves competitive detection performance with improved interpretability and reduced need for labeled data or external causal graphs.

Conclusion: PICODE offers a unified solution for anomaly detection and root cause analysis in power grids, enhancing transparency and effectiveness.

Abstract: Anomaly detection and root cause analysis (RCA) are critical for ensuring the safety and resilience of cyber-physical systems such as power grids. However, existing machine learning models for time series anomaly detection often operate as black boxes, offering only binary outputs without any explanation, such as identifying anomaly type and origin. To address this challenge, we propose Power Interpretable Causality Ordinary Differential Equation (PICODE) Networks, a unified, causality-informed architecture that jointly performs anomaly detection along with the explanation why it is detected as an anomaly, including root cause localization, anomaly type classification, and anomaly shape characterization. Experimental results in power systems demonstrate that PICODE achieves competitive detection performance while offering improved interpretability and reduced reliance on labeled data or external causal graphs. We provide theoretical results demonstrating the alignment between the shape of anomaly functions and the changes in the weights of the extracted causal graphs.

</details>


### [161] [HyperMLP: An Integrated Perspective for Sequence Modeling](https://arxiv.org/abs/2602.12601)
*Jiecheng Lu,Shihao Yang*

Main category: cs.LG

TL;DR: The paper proposes viewing self-attention as a dynamic two-layer MLP, leading to new architectures (HyperMLP/HyperGLU) that outperform softmax-attention baselines.


<details>
  <summary>Details</summary>
Motivation: To simplify and unify the understanding of self-attention by framing it as a dynamic MLP, enabling more flexible and powerful architectures.

Method: Introduces HyperMLP and HyperGLU, which use dynamic mixing in feature and sequence spaces with a reverse-offset layout for autoregressive tasks.

Result: HyperMLP/HyperGLU outperform softmax-attention baselines under matched parameter budgets.

Conclusion: The dynamic MLP perspective provides a more expressive and unified framework for autoregressive attention, validated by empirical performance.

Abstract: Self-attention is often viewed as probabilistic query-key lookup, motivating designs that preserve normalized attention scores and fixed positional semantics. We advocate a simpler and more unified perspective: an autoregressive attention head can be viewed as a dynamic two-layer MLP whose weights are instantiated from the context history. From this view, attention scores form an ever-growing hidden representation, and standard MLP activations such as ReLU or GLU naturally implement input-conditioned selection over a context-dependent memory pool rather than a probability distribution. Based on this formulation, we introduce HyperMLP and HyperGLU, which learn dynamic mixing in both feature space and sequence space, using a reverse-offset (lag) layout to align temporal mixing with autoregressive semantics. We provide theoretical characterizations of the expressivity and implications of this structure, and empirically show that HyperMLP/HyperGLU consistently outperform strong softmax-attention baselines under matched parameter budgets.

</details>


### [162] [Block-Sample MAC-Bayes Generalization Bounds](https://arxiv.org/abs/2602.12605)
*Matthias Frey,Jingge Zhu,Michael C. Gastpar*

Main category: cs.LG

TL;DR: The paper introduces MAC-Bayes bounds, which generalize PAC-Bayes bounds by focusing on expected generalization error and subsets of training data, potentially yielding tighter results. A numerical example shows their advantage, and high-probability versions are proven impossible under certain conditions.


<details>
  <summary>Details</summary>
Motivation: To improve upon traditional PAC-Bayes bounds by proposing MAC-Bayes bounds that focus on expected generalization error and subsets of training data, potentially offering tighter bounds.

Method: Introduces a family of MAC-Bayes bounds, which generalize expectation versions of PAC-Bayes bounds and incorporate divergence terms dependent on data blocks. A numerical example demonstrates their effectiveness.

Result: MAC-Bayes bounds outperform traditional PAC-Bayes bounds in tightness, as shown numerically. High-probability versions are not generally possible under specified conditions.

Conclusion: MAC-Bayes bounds offer a promising alternative to PAC-Bayes bounds for tighter generalization error bounds, though high-probability versions face limitations.

Abstract: We present a family of novel block-sample MAC-Bayes bounds (mean approximately correct). While PAC-Bayes bounds (probably approximately correct) typically give bounds for the generalization error that hold with high probability, MAC-Bayes bounds have a similar form but bound the expected generalization error instead. The family of bounds we propose can be understood as a generalization of an expectation version of known PAC-Bayes bounds. Compared to standard PAC-Bayes bounds, the new bounds contain divergence terms that only depend on subsets (or \emph{blocks}) of the training data. The proposed MAC-Bayes bounds hold the promise of significantly improving upon the tightness of traditional PAC-Bayes and MAC-Bayes bounds. This is illustrated with a simple numerical example in which the original PAC-Bayes bound is vacuous regardless of the choice of prior, while the proposed family of bounds are finite for appropriate choices of the block size. We also explore the question whether high-probability versions of our MAC-Bayes bounds (i.e., PAC-Bayes bounds of a similar form) are possible. We answer this question in the negative with an example that shows that in general, it is not possible to establish a PAC-Bayes bound which (a) vanishes with a rate faster than $\mathcal{O}(1/\log n)$ whenever the proposed MAC-Bayes bound vanishes with rate $\mathcal{O}(n^{-1/2})$ and (b) exhibits a logarithmic dependence on the permitted error probability.

</details>


### [163] [RelBench v2: A Large-Scale Benchmark and Repository for Relational Data](https://arxiv.org/abs/2602.12606)
*Justin Gu,Rishabh Ranjan,Charilaos Kanatsoulis,Haiming Tang,Martin Jurkovic,Valter Hudovernik,Mark Znidar,Pranshu Chaturvedi,Parth Shroff,Fengyu Li,Jure Leskovec*

Main category: cs.LG

TL;DR: RelBench v2 expands the RDL benchmark with large-scale datasets and autocomplete tasks, outperforming single-table baselines.


<details>
  <summary>Details</summary>
Motivation: To address the need for scalable benchmarks in relational deep learning (RDL) as models grow larger and more complex.

Method: RelBench v2 introduces four new datasets, autocomplete tasks, and integrates external benchmarks for unified evaluation.

Result: RDL models outperform single-table baselines in autocomplete, forecasting, and recommendation tasks.

Conclusion: Modeling relational structure explicitly is crucial for superior performance in relational deep learning tasks.

Abstract: Relational deep learning (RDL) has emerged as a powerful paradigm for learning directly on relational databases by modeling entities and their relationships across multiple interconnected tables. As this paradigm evolves toward larger models and relational foundation models, scalable and realistic benchmarks are essential for enabling systematic evaluation and progress. In this paper, we introduce RelBench v2, a major expansion of the RelBench benchmark for RDL. RelBench v2 adds four large-scale relational datasets spanning scholarly publications, enterprise resource planning, consumer platforms, and clinical records, increasing the benchmark to 11 datasets comprising over 22 million rows across 29 tables. We further introduce autocomplete tasks, a new class of predictive objectives that require models to infer missing attribute values directly within relational tables while respecting temporal constraints, expanding beyond traditional forecasting tasks constructed via SQL queries. In addition, RelBench v2 expands beyond its native datasets by integrating external benchmarks and evaluation frameworks: we translate event streams from the Temporal Graph Benchmark into relational schemas for unified relational-temporal evaluation, interface with ReDeLEx to provide uniform access to 70+ real-world databases suitable for pretraining, and incorporate 4DBInfer datasets and tasks to broaden multi-table prediction coverage. Experimental results demonstrate that RDL models consistently outperform single-table baselines across autocomplete, forecasting, and recommendation tasks, highlighting the importance of modeling relational structure explicitly.

</details>


### [164] [Coden: Efficient Temporal Graph Neural Networks for Continuous Prediction](https://arxiv.org/abs/2602.12613)
*Zulun Zhu,Siqiang Luo*

Main category: cs.LG

TL;DR: Coden is a TGNN model for continuous predictions on dynamic graphs, balancing efficiency and accuracy, outperforming benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing TGNNs struggle with continuous predictions due to computational overhead or quality issues, especially for large graphs.

Method: Introduces Coden, a TGNN model overcoming complexity bottlenecks while maintaining predictive accuracy, with theoretical analyses.

Result: Coden outperforms benchmarks in efficiency and effectiveness across five dynamic datasets.

Conclusion: Coden is a superior solution for continuous predictions in evolving graph environments.

Abstract: Temporal Graph Neural Networks (TGNNs) are pivotal in processing dynamic graphs. However, existing TGNNs primarily target one-time predictions for a given temporal span, whereas many practical applications require continuous predictions, that predictions are issued frequently over time. Directly adapting existing TGNNs to continuous-prediction scenarios introduces either significant computational overhead or prediction quality issues especially for large graphs. This paper revisits the challenge of { continuous predictions} in TGNNs, and introduces {\sc Coden}, a TGNN model designed for efficient and effective learning on dynamic graphs. {\sc Coden} innovatively overcomes the key complexity bottleneck in existing TGNNs while preserving comparable predictive accuracy. Moreover, we further provide theoretical analyses that substantiate the effectiveness and efficiency of {\sc Coden}, and clarify its duality relationship with both RNN-based and attention-based models. Our evaluations across five dynamic datasets show that {\sc Coden} surpasses existing performance benchmarks in both efficiency and effectiveness, establishing it as a superior solution for continuous prediction in evolving graph environments.

</details>


### [165] [Efficient Personalized Federated PCA with Manifold Optimization for IoT Anomaly Detection](https://arxiv.org/abs/2602.12622)
*Xianchao Xiu,Chenyi Huang,Wei Zhang,Wanquan Liu*

Main category: cs.LG

TL;DR: The paper proposes FedEP, a personalized and robust federated PCA method for anomaly detection in IoT networks, outperforming existing methods in accuracy and F1-scores.


<details>
  <summary>Details</summary>
Motivation: IoT networks face security threats, and current federated PCA methods lack personalization and robustness, critical for effective anomaly detection.

Method: FedEP introduces local representations with the ℓ1-norm for personalization and ℓ2,1-norm for robustness, solving the problem via ADMM-based manifold optimization.

Result: FedEP outperforms FedPG, achieving high F1-scores and accuracy in IoT security scenarios.

Conclusion: FedEP is an effective solution for anomaly detection in IoT networks with theoretical convergence guarantees and superior performance.

Abstract: Internet of things (IoT) networks face increasing security threats due to their distributed nature and resource constraints. Although federated learning (FL) has gained prominence as a privacy-preserving framework for distributed IoT environments, current federated principal component analysis (PCA) methods lack the integration of personalization and robustness, which are critical for effective anomaly detection. To address these limitations, we propose an efficient personalized federated PCA (FedEP) method for anomaly detection in IoT networks. The proposed model achieves personalization through introducing local representations with the $\ell_1$-norm for element-wise sparsity, while maintaining robustness via enforcing local models with the $\ell_{2,1}$-norm for row-wise sparsity. To solve this non-convex problem, we develop a manifold optimization algorithm based on the alternating direction method of multipliers (ADMM) with rigorous theoretical convergence guarantees. Experimental results confirm that the proposed FedEP outperforms the state-of-the-art FedPG, achieving excellent F1-scores and accuracy in various IoT security scenarios. Our code will be available at \href{https://github.com/xianchaoxiu/FedEP}{https://github.com/xianchaoxiu/FedEP}.

</details>


### [166] [Formalizing the Sampling Design Space of Diffusion-Based Generative Models via Adaptive Solvers and Wasserstein-Bounded Timesteps](https://arxiv.org/abs/2602.12624)
*Sangwoo Jo,Sungjoon Choi*

Main category: cs.LG

TL;DR: SDM is a framework optimizing diffusion model sampling by aligning solvers with trajectory dynamics and adaptive timesteps, improving efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Current diffusion models face high sampling costs due to static heuristics in solver selection and scheduling.

Method: SDM uses geometric analysis and ODE dynamics to deploy efficient low-order solvers early and higher-order solvers later, with Wasserstein-bounded optimization for timestep scheduling.

Result: SDM achieves state-of-the-art performance (e.g., FID of 1.93 on CIFAR-10) with fewer function evaluations.

Conclusion: SDM offers a principled, efficient solution for diffusion model sampling without extra training or architectural changes.

Abstract: Diffusion-based generative models have achieved remarkable performance across various domains, yet their practical deployment is often limited by high sampling costs. While prior work focuses on training objectives or individual solvers, the holistic design of sampling, specifically solver selection and scheduling, remains dominated by static heuristics. In this work, we revisit this challenge through a geometric lens, proposing SDM, a principled framework that aligns the numerical solver with the intrinsic properties of the diffusion trajectory. By analyzing the ODE dynamics, we show that efficient low-order solvers suffice in early high-noise stages while higher-order solvers can be progressively deployed to handle the increasing non-linearity of later stages. Furthermore, we formalize the scheduling by introducing a Wasserstein-bounded optimization framework. This method systematically derives adaptive timesteps that explicitly bound the local discretization error, ensuring the sampling process remains faithful to the underlying continuous dynamics. Without requiring additional training or architectural modifications, SDM achieves state-of-the-art performance across standard benchmarks, including an FID of 1.93 on CIFAR-10, 2.41 on FFHQ, and 1.98 on AFHQv2, with a reduced number of function evaluations compared to existing samplers. Our code is available at https://github.com/aiimaginglab/sdm.

</details>


### [167] [Dual-Granularity Contrastive Reward via Generated Episodic Guidance for Efficient Embodied RL](https://arxiv.org/abs/2602.12636)
*Xin Liu,Yixuan Li,Yuhui Chen,Yuxing Qin,Haoran Li,Dongbin Zhao*

Main category: cs.LG

TL;DR: DEG is a novel framework for reinforcement learning that generates dense rewards without human annotations or extensive supervision, improving sample efficiency.


<details>
  <summary>Details</summary>
Motivation: The challenge of designing suitable rewards in RL, especially for embodied manipulation, is addressed by DEG to avoid reliance on human-annotated data or expert supervision.

Method: DEG uses large video generation models for domain adaptation, generates episodic guidance, and employs dual-granularity rewards for balanced exploration and matching.

Result: Experiments on 18 diverse tasks show DEG effectively guides RL agents to discover sparse rewards and achieve stable policy convergence.

Conclusion: DEG offers a human-annotation-free, supervision-light solution for dense rewards in RL, enhancing efficiency and performance.

Abstract: Designing suitable rewards poses a significant challenge in reinforcement learning (RL), especially for embodied manipulation. Trajectory success rewards are suitable for human judges or model fitting, but the sparsity severely limits RL sample efficiency. While recent methods have effectively improved RL via dense rewards, they rely heavily on high-quality human-annotated data or abundant expert supervision. To tackle these issues, this paper proposes Dual-granularity contrastive reward via generated Episodic Guidance (DEG), a novel framework to seek sample-efficient dense rewards without requiring human annotations or extensive supervision. Leveraging the prior knowledge of large video generation models, DEG only needs a small number of expert videos for domain adaptation to generate dedicated task guidance for each RL episode. Then, the proposed dual-granularity reward that balances coarse-grained exploration and fine-grained matching, will guide the agent to efficiently approximate the generated guidance video sequentially in the contrastive self-supervised latent space, and finally complete the target task. Extensive experiments on 18 diverse tasks across both simulation and real-world settings show that DEG can not only serve as an efficient exploration stimulus to help the agent quickly discover sparse success rewards, but also guide effective RL and stable policy convergence independently.

</details>


### [168] [Unifying Model-Free Efficiency and Model-Based Representations via Latent Dynamics](https://arxiv.org/abs/2602.12643)
*Jashaswimalya Acharjee,Balaraman Ravindran*

Main category: cs.LG

TL;DR: Unified Latent Dynamics (ULD) is a reinforcement learning algorithm combining model-free efficiency and model-based representation strengths, achieving cross-domain competence with minimal tuning.


<details>
  <summary>Details</summary>
Motivation: To unify the efficiency of model-free methods with the representational strengths of model-based approaches without planning overhead, enabling adaptability across diverse domains.

Method: Embeds state-action pairs into a latent space where the value function is linear, uses synchronized updates of encoder, value, and policy networks, auxiliary losses for predictive dynamics, and reward-scale normalization.

Result: Matches or exceeds performance of specialized model-free and general model-based baselines across 80 environments, with minimal tuning and fewer parameters.

Conclusion: Value-aligned latent representations can provide adaptability and sample efficiency without full model-based planning.

Abstract: We present Unified Latent Dynamics (ULD), a novel reinforcement learning algorithm that unifies the efficiency of model-free methods with the representational strengths of model-based approaches, without incurring planning overhead. By embedding state-action pairs into a latent space in which the true value function is approximately linear, our method supports a single set of hyperparameters across diverse domains -- from continuous control with low-dimensional and pixel inputs to high-dimensional Atari games. We prove that, under mild conditions, the fixed point of our embedding-based temporal-difference updates coincides with that of a corresponding linear model-based value expansion, and we derive explicit error bounds relating embedding fidelity to value approximation quality. In practice, ULD employs synchronized updates of encoder, value, and policy networks, auxiliary losses for short-horizon predictive dynamics, and reward-scale normalization to ensure stable learning under sparse rewards. Evaluated on 80 environments spanning Gym locomotion, DeepMind Control (proprioceptive and visual), and Atari, our approach matches or exceeds the performance of specialized model-free and general model-based baselines -- achieving cross-domain competence with minimal tuning and a fraction of the parameter footprint. These results indicate that value-aligned latent representations alone can deliver the adaptability and sample efficiency traditionally attributed to full model-based planning.

</details>


### [169] [Look Inward to Explore Outward: Learning Temperature Policy from LLM Internal States via Hierarchical RL](https://arxiv.org/abs/2602.13035)
*Yixiao Zhou,Yang Li,Dongzhou Cheng,Hehe Fan,Yu Cheng*

Main category: cs.LG

TL;DR: RLVR introduces Introspective LLM, a framework optimizing sampling temperature dynamically during generation to improve LLM performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on static or heuristic temperature settings, decoupled from task rewards, limiting exploration-exploitation balance.

Method: Hierarchical RL trains LLMs to dynamically select temperatures based on hidden states, jointly optimizing temperature and token policies.

Result: Experiments on math reasoning show learned temperature policies outperform fixed/heuristic baselines, aligning exploration with uncertainty.

Conclusion: Dynamic temperature control enhances LLM performance, offering interpretable exploration behaviors.

Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) trains large language models (LLMs) from sampled trajectories, making decoding strategy a core component of learning rather than a purely inference-time choice. Sampling temperature directly controls the exploration--exploitation trade-off by modulating policy entropy, yet existing methods rely on static values or heuristic adaptations that are decoupled from task-level rewards. We propose Introspective LLM, a hierarchical reinforcement learning framework that learns to control sampling temperature during generation. At each decoding step, the model selects a temperature based on its hidden state and samples the next token from the resulting distribution. Temperature and token policies are jointly optimized from downstream rewards using a coordinate ascent scheme. Experiments on mathematical reasoning benchmarks show that learned temperature policies outperform fixed and heuristic baselines, while exhibiting interpretable exploration behaviors aligned with reasoning uncertainty.

</details>


### [170] [Uncovering spatial tissue domains and cell types in spatial omics through cross-scale profiling of cellular and genomic interactions](https://arxiv.org/abs/2602.12651)
*Rui Yan,Xiaohan Xing,Xun Wang,Zixia Zhou,Md Tauhidul Islam,Lei Xing*

Main category: cs.LG

TL;DR: CellScape is a deep learning framework designed to analyze noisy and complex spatial transcriptomics data, integrating spatial and genomic relationships to uncover biologically meaningful patterns.


<details>
  <summary>Details</summary>
Motivation: Existing computational methods struggle with the noise and complexity of spatial transcriptomics data, limiting the ability to understand cellular interactions and genomic relationships.

Method: CellScape jointly models cellular spatial interactions and genomic relationships using deep learning, integrating these signals for comprehensive analysis.

Result: The framework improves spatial domain segmentation and supports detailed spatial cellular analyses across diverse datasets.

Conclusion: CellScape provides an accurate and versatile tool for deep analysis and interpretation of spatial transcriptomics data.

Abstract: Cellular identity and function are linked to both their intrinsic genomic makeup and extrinsic spatial context within the tissue microenvironment. Spatial transcriptomics (ST) offers an unprecedented opportunity to study this, providing in situ gene expression profiles at single-cell resolution and illuminating the spatial and functional organization of cells within tissues. However, a significant hurdle remains: ST data is inherently noisy, large, and structurally complex. This complexity makes it intractable for existing computational methods to effectively capture the interplay between spatial interactions and intrinsic genomic relationships, thus limiting our ability to discern critical biological patterns. Here, we present CellScape, a deep learning framework designed to overcome these limitations for high-performance ST data analysis and pattern discovery. CellScape jointly models cellular interactions in tissue space and genomic relationships among cells, producing comprehensive representations that seamlessly integrate spatial signals with underlying gene regulatory mechanisms. This technique uncovers biologically informative patterns that improve spatial domain segmentation and supports comprehensive spatial cellular analyses across diverse transcriptomics datasets, offering an accurate and versatile framework for deep analysis and interpretation of ST data.w

</details>


### [171] [Memory-Efficient Structured Backpropagation for On-Device LLM Fine-Tuning](https://arxiv.org/abs/2602.13069)
*Juneyoung Park,Yuri Hong,Seongwan Kim,Jaeho Lee*

Main category: cs.LG

TL;DR: MeSP bridges the gap between high-memory exact gradients (MeBP) and low-memory noisy estimates (MeZO) by exploiting LoRA's low-rank structure, reducing memory usage by 49% while maintaining gradient accuracy.


<details>
  <summary>Details</summary>
Motivation: To enable privacy-preserving personalization of large language models on memory-constrained mobile devices without compromising gradient accuracy.

Method: Memory-efficient Structured Backpropagation (MeSP) manually derives backward passes using LoRA's low-rank structure, recomputing intermediate projections instead of storing them.

Result: MeSP reduces peak memory usage significantly (e.g., from 361MB to 136MB for Qwen2.5-0.5B) and computes mathematically identical gradients, outperforming MeZO's poor gradient estimates.

Conclusion: MeSP enables feasible fine-tuning on memory-constrained devices by balancing memory efficiency with gradient accuracy, overcoming limitations of existing approaches.

Abstract: On-device fine-tuning enables privacy-preserving personalization of large language models, but mobile devices impose severe memory constraints, typically 6--12GB shared across all workloads. Existing approaches force a trade-off between exact gradients with high memory (MeBP) and low memory with noisy estimates (MeZO). We propose Memory-efficient Structured Backpropagation (MeSP), which bridges this gap by manually deriving backward passes that exploit LoRA's low-rank structure. Our key insight is that the intermediate projection $h = xA$ can be recomputed during backward at minimal cost since rank $r \ll d_{in}$, eliminating the need to store it. MeSP achieves 49\% average memory reduction compared to MeBP on Qwen2.5 models (0.5B--3B) while computing mathematically identical gradients. Our analysis also reveals that MeZO's gradient estimates show near-zero correlation with true gradients (cosine similarity $\approx$0.001), explaining its slow convergence. MeSP reduces peak memory from 361MB to 136MB for Qwen2.5-0.5B, enabling fine-tuning scenarios previously infeasible on memory-constrained devices.

</details>


### [172] [SLA2: Sparse-Linear Attention with Learnable Routing and QAT](https://arxiv.org/abs/2602.12675)
*Jintao Zhang,Haoxu Wang,Kai Jiang,Kaiwen Zheng,Youhe Jiang,Ion Stoica,Jianfei Chen,Jun Zhu,Joseph E. Gonzalez*

Main category: cs.LG

TL;DR: SLA2 improves upon SLA by introducing a learnable router, a better sparse-linear attention formulation, and low-bit attention, achieving high sparsity and speedup while maintaining video generation quality.


<details>
  <summary>Details</summary>
Motivation: SLA's heuristic split for sparse and linear attention is suboptimal, and there's a mismatch in its decomposition. SLA2 aims to address these issues for better performance.

Method: SLA2 uses a learnable router, a direct sparse-linear attention formulation, and sparse + low-bit attention via quantization-aware fine-tuning.

Result: SLA2 achieves 97% attention sparsity, 18.6x speedup, and preserves generation quality in video diffusion models.

Conclusion: SLA2 effectively overcomes SLA's limitations, offering improved efficiency and performance in video generation tasks.

Abstract: Sparse-Linear Attention (SLA) combines sparse and linear attention to accelerate diffusion models and has shown strong performance in video generation. However, (i) SLA relies on a heuristic split that assigns computations to the sparse or linear branch based on attention-weight magnitude, which can be suboptimal. Additionally, (ii) after formally analyzing the attention error in SLA, we identify a mismatch between SLA and a direct decomposition into sparse and linear attention. We propose SLA2, which introduces (I) a learnable router that dynamically selects whether each attention computation should use sparse or linear attention, (II) a more faithful and direct sparse-linear attention formulation that uses a learnable ratio to combine the sparse and linear attention branches, and (III) a sparse + low-bit attention design, where low-bit attention is introduced via quantization-aware fine-tuning to reduce quantization error. Experiments show that on video diffusion models, SLA2 can achieve 97% attention sparsity and deliver an 18.6x attention speedup while preserving generation quality.

</details>


### [173] [LCSB: Layer-Cyclic Selective Backpropagation for Memory-Efficient On-Device LLM Fine-Tuning](https://arxiv.org/abs/2602.13073)
*Juneyoung Park,Eunbeen Yoon,Seongwan Kim. Jaeho Lee*

Main category: cs.LG

TL;DR: LCSB improves memory-efficient fine-tuning of LLMs by selectively updating gradients for subsets of layers, reducing computation while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency in MeBP's backward computation through all transformer layers, which contributes significantly to backward time.

Method: Introduces Layer-Cyclic Selective Backpropagation (LCSB), updating gradients only for selected layers per step, leveraging residual connections and AdamW momentum.

Result: Achieves up to 1.40× speedup with minimal quality loss (<2%) and improves stability in quantized settings.

Conclusion: LCSB offers a practical solution for efficient fine-tuning, with theoretical convergence and unexpected regularization benefits.

Abstract: Memory-efficient backpropagation (MeBP) has enabled first-order fine-tuning of large language models (LLMs) on mobile devices with less than 1GB memory. However, MeBP requires backward computation through all transformer layers at every step, where weight decompression alone accounts for 32--42% of backward time. We propose Layer-Cyclic Selective Backpropagation (LCSB), which computes gradients for only a subset of layers per step. Our key insight is that residual connections guarantee gradient flow through identity paths, while AdamW momentum provides implicit updates for non-selected layers. We interpret LCSB as Block Coordinate Descent on the LoRA parameter space, providing theoretical justification for convergence. LCSB achieves up to 1.40$\times$ speedup with less than 2\% quality degradation across five models and three tasks. Surprisingly, in 4-bit quantized settings, LCSB exhibits superior stability: a 3B model that completely diverges under full backpropagation converges smoothly with LCSB, suggesting an implicit regularization effect from selective gradient computation.

</details>


### [174] [Flow Matching from Viewpoint of Proximal Operators](https://arxiv.org/abs/2602.12683)
*Kenji Fukumizu,Wei Huang,Han Bao,Shuntuo Xu,Nisha Chandramoothy*

Main category: cs.LG

TL;DR: OT-CFM is reformulated with an exact proximal expression via an extended Brenier potential, enabling recovery of target points without density assumptions. Convergence and terminal normal hyperbolicity are proven for manifold-supported targets.


<details>
  <summary>Details</summary>
Motivation: To refine OT-CFM by providing exact proximal formulations and analyze its convergence and geometric properties for manifold-supported target distributions.

Method: Reformulate OT-CFM using an extended Brenier potential and derive exact proximal expressions. Analyze convergence of minibatch OT-CFM and prove terminal normal hyperbolicity using second epi-derivatives of convex potentials.

Result: Exact proximal formulation of OT-CFM achieved, with proven convergence and terminal normal hyperbolicity for manifold-supported targets.

Conclusion: OT-CFM's reformulation offers precise dynamical properties, facilitating its application to manifold-supported targets with theoretical guarantees.

Abstract: We reformulate Optimal Transport Conditional Flow Matching (OT-CFM), a class of dynamical generative models, showing that it admits an exact proximal formulation via an extended Brenier potential, without assuming that the target distribution has a density. In particular, the mapping to recover the target point is exactly given by a proximal operator, which yields an explicit proximal expression of the vector field. We also discuss the convergence of minibatch OT-CFM to the population formulation as the batch size increases. Finally, using second epi-derivatives of convex potentials, we prove that, for manifold-supported targets, OT-CFM is terminally normally hyperbolic: after time rescaling, the dynamics contracts exponentially in directions normal to the data manifold while remaining neutral along tangential directions.

</details>


### [175] [Trust the uncertain teacher: distilling dark knowledge via calibrated uncertainty](https://arxiv.org/abs/2602.12687)
*Jeonghyun Kim,SooKyung Kim,Richeng Xuan,Hyunsoo Cho*

Main category: cs.LG

TL;DR: The paper introduces Calibrated Uncertainty Distillation (CUD) to improve knowledge distillation by addressing teacher overconfidence, ensuring students learn from calibrated uncertainty for better accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Conventional cross-entropy training leads teachers to produce overconfident, brittle predictions, limiting effective knowledge transfer to students, especially in complex or uncertain scenarios.

Method: CUD refines the teacher's predictive distribution to balance accuracy and calibration, guiding students to learn from both confident signals and structured uncertainty.

Result: CUD-trained students demonstrate improved accuracy, better calibration under distribution shifts, and enhanced reliability on ambiguous or long-tail inputs.

Conclusion: CUD effectively addresses teacher overconfidence in knowledge distillation, fostering more robust and accurate student models.

Abstract: The core of knowledge distillation lies in transferring the teacher's rich 'dark knowledge'-subtle probabilistic patterns that reveal how classes are related and the distribution of uncertainties. While this idea is well established, teachers trained with conventional cross-entropy often fail to preserve such signals. Their distributions collapse into sharp, overconfident peaks that appear decisive but are in fact brittle, offering little beyond the hard label or subtly hindering representation-level transfer. This overconfidence is especially problematic in high-cardinality tasks, where the nuances among many plausible classes matter most for guiding a compact student. Moreover, such brittle targets reduce robustness under distribution shift, leaving students vulnerable to miscalibration in real-world conditions. To address this limitation, we revisit distillation from a distributional perspective and propose Calibrated Uncertainty Distillation (CUD), a framework designed to make dark knowledge more faithfully accessible. Instead of uncritically adopting the teacher's overconfidence, CUD encourages teachers to reveal uncertainty where it is informative and guides students to learn from targets that are calibrated rather than sharpened certainty. By directly shaping the teacher's predictive distribution before transfer, our approach balances accuracy and calibration, allowing students to benefit from both confident signals on easy cases and structured uncertainty on hard ones. Across diverse benchmarks, CUD yields students that are not only more accurate, but also more calibrated under shift and more reliable on ambiguous, long-tail inputs.

</details>


### [176] [Quantization-Robust LLM Unlearning via Low-Rank Adaptation](https://arxiv.org/abs/2602.13151)
*João Vitor Boer Abitante,Joana Meneguzzo Pasquali,Luan Fonseca Garcia,Ewerton de Oliveira,Thomas da Silva Paula,Rodrigo C. Barros,Lucas S. Kupssinskü*

Main category: cs.LG

TL;DR: The paper addresses the challenge of maintaining unlearning updates in Large Language Models (LLMs) post-quantization by proposing LoRA (low-rank adaptation), which concentrates updates into trainable adapters to preserve effectiveness.


<details>
  <summary>Details</summary>
Motivation: Standard full-parameter fine-tuning often fails to retain unlearning updates under aggressive low-bit post-training quantization (PTQ), leading models to revert to pre-unlearning behavior.

Method: The authors propose quantization-robust unlearning via LoRA, freezing the base model and focusing updates into trainable adapters to ensure updates survive quantization.

Result: LoRA improves 4-bit utility significantly (e.g., 7.93 points increase) and reduces privacy leakage while maintaining strong forgetting performance (VerMem and KnowMem near 0).

Conclusion: LoRA is beneficial for machine unlearning scenarios where quantization is necessary, preserving updates and improving privacy without sacrificing forgetting performance.

Abstract: Large Language Model (LLM) unlearning aims to remove targeted knowledge from a trained model, but practical deployments often require post-training quantization (PTQ) for efficient inference. However, aggressive low-bit PTQ can mask or erase unlearning updates, causing quantized models to revert to pre-unlearning behavior. We show that standard full-parameter fine-tuning often induce parameter changes that are too small to survive 4-bit quantization. We propose quantization-robust unlearning via low-rank adaptation (LoRA): we freeze the base model and concentrate unlearning into trainable adapters so that the effective update is preserved after quantization. On Llama-2-7B evaluated with MUSE dataset (BOOKS and NEWS), LoRA improves 4-bit utility by up to 7.93 points (NPO+GDR on BOOKS: 50.17 to 58.10) and yields higher 4-bit utility on NEWS for GA+GDR (40.06 to 44.82, increase of 4.76). LoRA also substantially reduces privacy leakage under 4-bit PTQ, e.g., for GA+KLR on BOOKS, PrivLeak moves from -25.68 to -5.86 (closer to ideal 0), while maintaining strong forgetting (VerMem and KnowMem near 0). Thus, using LoRA for Machine Unlearning is beneficial for scenarios where quantization is necessary for model deployment.

</details>


### [177] [Leverage-Weighted Conformal Prediction](https://arxiv.org/abs/2602.12693)
*Shreyas Fadnavis*

Main category: cs.LG

TL;DR: LWCP improves conformal prediction by weighting nonconformity scores using leverage, ensuring better coverage without auxiliary models.


<details>
  <summary>Details</summary>
Motivation: Existing methods for adaptive conformal prediction require training additional models, which can be inefficient or impractical. LWCP aims to provide adaptive coverage without this overhead.

Method: LWCP weights nonconformity scores by leveraging the hat matrix's diagonal, adapting intervals based on design matrix geometry rather than auxiliary models.

Result: LWCP maintains marginal validity, achieves optimal conditional coverage asymptotically, and matches classical intervals under Gaussian assumptions while remaining distribution-free.

Conclusion: LWCP efficiently reduces coverage disparities across varying variance regions, requiring minimal computational overhead and no auxiliary models.

Abstract: Split conformal prediction provides distribution-free prediction intervals with finite-sample marginal coverage, but produces constant-width intervals that overcover in low-variance regions and undercover in high-variance regions. Existing adaptive methods require training auxiliary models. We propose Leverage-Weighted Conformal Prediction (LWCP), which weights nonconformity scores by a function of the statistical leverage -- the diagonal of the hat matrix -- deriving adaptivity from the geometry of the design matrix rather than from auxiliary model fitting. We prove that LWCP preserves finite-sample marginal validity for any weight function; achieves asymptotically optimal conditional coverage at essentially no width cost when heteroscedasticity factors through leverage; and recovers the form and width of classical prediction intervals under Gaussian assumptions while retaining distribution-free guarantees. We further establish that randomized leverage approximations preserve coverage exactly with controlled width perturbation, and that vanilla CP suffers a persistent, sample-size-independent conditional coverage gap that LWCP eliminates. The method requires no hyperparameters beyond the choice of weight function and adds negligible computational overhead to vanilla CP. Experiments on synthetic and real data confirm the theoretical predictions, demonstrating substantial reductions in conditional coverage disparity across settings.

</details>


### [178] [SWING: Unlocking Implicit Graph Representations for Graph Random Features](https://arxiv.org/abs/2602.12703)
*Alessandro Manenti,Avinava Dubey,Arijit Sehanobish,Cesare Alippi,Krzysztof Choromanski*

Main category: cs.LG

TL;DR: SWING introduces a new algorithm for implicit graph computations using space walks and Gumbel-softmax sampling, avoiding graph materialization.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of computations on implicitly represented graphs, where edge weights are defined by node features, without needing to materialize the graph.

Method: Uses space walks in continuous embedding spaces and applies Gumbel-softmax sampling with linearized kernels and importance sampling.

Result: SWING efficiently approximates combinatorial calculations and is accelerator-friendly, validated through experiments on various i-graphs.

Conclusion: SWING offers a novel, efficient approach for implicit graph computations, leveraging Fourier analysis and space walks.

Abstract: We propose SWING: Space Walks for Implicit Network Graphs, a new class of algorithms for computations involving Graph Random Features on graphs given by implicit representations (i-graphs), where edge-weights are defined as bi-variate functions of feature vectors in the corresponding nodes. Those classes of graphs include several prominent examples, such as: $ε$-neighborhood graphs, used on regular basis in machine learning. Rather than conducting walks on graphs' nodes, those methods rely on walks in continuous spaces, in which those graphs are embedded. To accurately and efficiently approximate original combinatorial calculations, SWING applies customized Gumbel-softmax sampling mechanism with linearized kernels, obtained via random features coupled with importance sampling techniques. This algorithm is of its own interest. SWING relies on the deep connection between implicitly defined graphs and Fourier analysis, presented in this paper. SWING is accelerator-friendly and does not require input graph materialization. We provide detailed analysis of SWING and complement it with thorough experiments on different classes of i-graphs.

</details>


### [179] [QTabGAN: A Hybrid Quantum-Classical GAN for Tabular Data Synthesis](https://arxiv.org/abs/2602.12704)
*Subhangi Kumari,Rakesh Achutha,Vignesh Sivaraman*

Main category: cs.LG

TL;DR: QTabGAN is a hybrid quantum-classical GAN designed for synthesizing tabular data, especially useful when real data is scarce or private. It leverages quantum circuits' expressive power and classical neural networks, outperforming state-of-the-art models by up to 54.07%.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of synthesizing realistic tabular data due to heterogeneous features and high dimensionality, particularly in scenarios with limited or private data.

Method: QTabGAN combines quantum circuits and classical neural networks. Quantum circuits learn complex data distributions, which are then mapped to tabular features using classical neural networks.

Result: Evaluated on multiple datasets, QTabGAN improves performance by up to 54.07% over state-of-the-art models in classification tasks.

Conclusion: QTabGAN establishes a scalable quantum approach for tabular data synthesis, showcasing the potential of quantum-assisted generative modeling.

Abstract: Synthesizing realistic tabular data is challenging due to heterogeneous feature types and high dimensionality. We introduce QTabGAN, a hybrid quantum-classical generative adversarial framework for tabular data synthesis. QTabGAN is especially designed for settings where real data are scarce or restricted by privacy constraints. The model exploits the expressive power of quantum circuits to learn complex data distributions, which are then mapped to tabular features using classical neural networks. We evaluate QTabGAN on multiple classification and regression datasets and benchmark it against leading state-of-the-art generative models. Experiments show that QTabGAN achieves up to 54.07% improvement across various classification datasets and evaluation metrics, thus establishing a scalable quantum approach to tabular data synthesis and highlighting its potential for quantum-assisted generative modelling.

</details>


### [180] [Physics-Informed Laplace Neural Operator for Solving Partial Differential Equations](https://arxiv.org/abs/2602.12706)
*Heechang Kim,Qianying Cao,Hyomin Shin,Seungchul Lee,George Em Karniadakis,Minseok Choi*

Main category: cs.LG

TL;DR: The paper introduces PILNO, a physics-enhanced neural operator for solving parametric PDEs, improving generalization and data efficiency.


<details>
  <summary>Details</summary>
Motivation: Address limitations of purely data-driven neural operators, especially in small-data regimes and OOD scenarios.

Method: Enhances LNO with physics-informed training, introduces ALNO backbone, virtual inputs, and temporal-causality weighting.

Result: PILNO improves accuracy in small-data settings, reduces variability, and enhances OOD generalization across benchmarks.

Conclusion: PILNO offers a robust, data-efficient solution for parametric PDEs by integrating physics into neural operators.

Abstract: Neural operators have emerged as fast surrogate solvers for parametric partial differential equations (PDEs). However, purely data-driven models often require extensive training data and can generalize poorly, especially in small-data regimes and under unseen (out-of-distribution) input functions that are not represented in the training data. To address these limitations, we propose the Physics-Informed Laplace Neural Operator (PILNO), which enhances the Laplace Neural Operator (LNO) by embedding governing physics into training through PDE, boundary condition, and initial condition residuals. To improve expressivity, we first introduce an Advanced LNO (ALNO) backbone that retains a pole-residue transient representation while replacing the steady-state branch with an FNO-style Fourier multiplier. To make physics-informed training both data-efficient and robust, PILNO further leverages (i) virtual inputs: an unlabeled ensemble of input functions spanning a broad spectral range that provides abundant physics-only supervision and explicitly targets out-of-distribution (OOD) regimes; and (ii) temporal-causality weighting: a time-decaying reweighting of the physics residual that prioritizes early-time dynamics and stabilizes optimization for time-dependent PDEs. Across four representative benchmarks -- Burgers' equation, Darcy flow, a reaction-diffusion system, and a forced KdV equation -- PILNO consistently improves accuracy in small-data settings (e.g., N_train <= 27), reduces run-to-run variability across random seeds, and achieves stronger OOD generalization than purely data-driven baselines.

</details>


### [181] [Mixture of Predefined Experts: Maximizing Data Usage on Vertical Federated Learning](https://arxiv.org/abs/2602.12708)
*Jon Irureta,Gorka Azkune,Jon Imaz,Aizea Lojo,Javier Fernandez-Marques*

Main category: cs.LG

TL;DR: Split-MoPE introduces Split Learning with predefined Mixture of Experts (MoPE) to address sample misalignment in Vertical Federated Learning (VFL), improving data usage and robustness while reducing communication costs.


<details>
  <summary>Details</summary>
Motivation: Existing VFL frameworks assume full sample alignment, which is unrealistic. Split-MoPE aims to bridge this gap by handling misalignment efficiently.

Method: Combines Split Learning with MoPE, using predefined experts for specific data alignments and pretrained encoders to minimize communication rounds.

Result: Outperforms state-of-the-art systems like LASER and Vertical SplitNN in scenarios with high data missingness.

Conclusion: Split-MoPE offers a robust, efficient, and interpretable solution for VFL, especially in real-world settings with partial sample alignment.

Abstract: Vertical Federated Learning (VFL) has emerged as a critical paradigm for collaborative model training in privacy-sensitive domains such as finance and healthcare. However, most existing VFL frameworks rely on the idealized assumption of full sample alignment across participants, a premise that rarely holds in real-world scenarios. To bridge this gap, this work introduces Split-MoPE, a novel framework that integrates Split Learning with a specialized Mixture of Predefined Experts (MoPE) architecture. Unlike standard Mixture of Experts (MoE), where routing is learned dynamically, MoPE uses predefined experts to process specific data alignments, effectively maximizing data usage during both training and inference without requiring full sample overlap. By leveraging pretrained encoders for target data domains, Split-MoPE achieves state-of-the-art performance in a single communication round, significantly reducing the communication footprint compared to multi-round end-to-end training. Furthermore, unlike existing proposals that address sample misalignment, this novel architecture provides inherent robustness against malicious or noisy participants and offers per-sample interpretability by quantifying each collaborator's contribution to each prediction. Extensive evaluations on vision (CIFAR-10/100) and tabular (Breast Cancer Wisconsin) datasets demonstrate that Split-MoPE consistently outperforms state-of-the-art systems such as LASER and Vertical SplitNN, particularly in challenging scenarios with high data missingness.

</details>


### [182] [Can Neural Networks Provide Latent Embeddings for Telemetry-Aware Greedy Routing?](https://arxiv.org/abs/2602.12798)
*Andreas Boltres,Niklas Freymuth,Gerhard Neumann*

Main category: cs.LG

TL;DR: Placer uses Message Passing Networks for telemetry-aware routing, improving efficiency while maintaining explainability.


<details>
  <summary>Details</summary>
Motivation: Recent ML-based routing sacrifices explainability; Placer aims to balance efficacy and transparency.

Method: Employs Message Passing Networks to create latent node embeddings for greedy next-hop routing.

Result: Enables efficient routing without solving all-pairs shortest paths and offers visual insights.

Conclusion: Placer enhances routing efficacy and responsiveness while retaining decision explainability.

Abstract: Telemetry-Aware routing promises to increase efficacy and responsiveness to traffic surges in computer networks. Recent research leverages Machine Learning to deal with the complex dependency between network state and routing, but sacrifices explainability of routing decisions due to the black-box nature of the proposed neural routing modules. We propose \emph{Placer}, a novel algorithm using Message Passing Networks to transform network states into latent node embeddings. These embeddings facilitate quick greedy next-hop routing without directly solving the all-pairs shortest paths problem, and let us visualize how certain network events shape routing decisions.

</details>


### [183] [ADEPT: RL-Aligned Agentic Decoding of Emotion via Evidence Probing Tools -- From Consensus Learning to Ambiguity-Driven Emotion Reasoning](https://arxiv.org/abs/2602.12714)
*Esther Sun,Bo-Hao Su,Abinay Reddy Naini,Shinji Watanabe,Carlos Busso*

Main category: cs.LG

TL;DR: ADEPT improves emotion recognition by combining SLLMs with acoustic tools, shifting from single-pass predictions to evidence-grounded multi-turn reasoning.


<details>
  <summary>Details</summary>
Motivation: Emotion recognition lacks grounding in acoustic evidence and interpretability; ADEPT addresses this by integrating SLLMs with specialized tools.

Method: Reframes emotion recognition as a multi-turn inquiry using semantic/acoustic probing tools, employs GRPO and Evidence Trust Gate.

Result: Boosts primary emotion accuracy and minor emotion characterization with auditable evidence.

Conclusion: ADEPT enhances emotion reasoning by grounding predictions in acoustic and semantic evidence, addressing ambiguity.

Abstract: Speech Large Language Models (SLLMs) enable high-level emotion reasoning but often produce ungrounded, text-biased judgments without verifiable acoustic evidence. In contrast, self-supervised speech encoders such as WavLM provide strong acoustic representations yet remain opaque discriminative models with limited interpretability. To bridge this gap, we introduce ADEPT (Agentic Decoding of Emotion via Evidence Probing Tools), a framework that reframes emotion recognition as a multi-turn inquiry process rather than a single-pass prediction. ADEPT transforms an SLLM into an agent that maintains an evolving candidate emotion set and adaptively invokes dedicated semantic and acoustic probing tools within a structured pipeline of candidate generation, evidence collection, and adjudication. Crucially, ADEPT enables a paradigm shift from consensus learning to ambiguity-driven emotion reasoning. Since human affect exhibits inherent complexity and frequent co-occurrence of emotions, we treat minority annotations as informative perceptual signals rather than discarding them as noise. Finally, we integrate Group Relative Policy Optimization (GRPO) with an Evidence Trust Gate to explicitly couple tool-usage behaviors with prediction quality and enforce evidence-grounded reasoning. Experiments show that ADEPT improves primary emotion accuracy in most settings while substantially improving minor emotion characterization, producing explanations grounded in auditable acoustic and semantic evidence.

</details>


### [184] [Adaptive Structured Pruning of Convolutional Neural Networks for Time Series Classification](https://arxiv.org/abs/2602.12744)
*Javidan Abdullayev,Maxime Devanne,Cyril Meyer,Ali Ismail-Fawaz,Jonathan Weber,Germain Forestier*

Main category: cs.LG

TL;DR: Dynamic Structured Pruning (DSP) is a fully automatic framework for pruning convolution-based TSC models, achieving significant compression (58-75%) while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Deep learning models for Time Series Classification (TSC) are computationally intensive, limiting deployment on resource-constrained devices. Existing pruning methods rely on manual tuning, which lacks scalability.

Method: DSP introduces an instance-wise sparsity loss during training to induce channel-level sparsity, followed by global activation analysis to automatically prune redundant filters without predefined pruning ratios.

Result: Tested on 128 UCR datasets with LITETime and InceptionTime, DSP achieved average compressions of 58% and 75% respectively while preserving classification accuracy. Redundancy analyses confirmed compact and informative representations.

Conclusion: DSP offers a scalable and efficient solution for deploying deep TSC models on resource-constrained devices, addressing computational bottlenecks without manual intervention.

Abstract: Deep learning models for Time Series Classification (TSC) have achieved strong predictive performance but their high computational and memory requirements often limit deployment on resource-constrained devices. While structured pruning can address these issues by removing redundant filters, existing methods typically rely on manually tuned hyperparameters such as pruning ratios which limit scalability and generalization across datasets. In this work, we propose Dynamic Structured Pruning (DSP), a fully automatic, structured pruning framework for convolution-based TSC models. DSP introduces an instance-wise sparsity loss during training to induce channel-level sparsity, followed by a global activation analysis to identify and prune redundant filters without needing any predefined pruning ratio. This work tackles computational bottlenecks of deep TSC models for deployment on resource-constrained devices. We validate DSP on 128 UCR datasets using two different deep state-of-the-art architectures: LITETime and InceptionTime. Our approach achieves an average compression of 58% for LITETime and 75% for InceptionTime architectures while maintaining classification accuracy. Redundancy analyses confirm that DSP produces compact and informative representations, offering a practical path for scalable and efficient deep TSC deployment.

</details>


### [185] [Hierarchical Successor Representation for Robust Transfer](https://arxiv.org/abs/2602.12753)
*Changmin Yu,Máté Lengyel*

Main category: cs.LG

TL;DR: The paper introduces the Hierarchical Successor Representation (HSR) to address limitations of the classical SR, such as policy dependence and spectral diffusion. HSR, combined with NMF, enables stable, sparse state representations for efficient task transfer and exploration in complex environments.


<details>
  <summary>Details</summary>
Motivation: The classical Successor Representation (SR) is limited by policy dependence and poor scalability in complex environments, prompting the need for a more robust framework.

Method: The authors propose HSR, incorporating temporal abstractions and applying non-negative matrix factorisation (NMF) to create stable, sparse state representations.

Result: HSR-NMF produces interpretable topological structures, enabling efficient task transfer and exploration in large, complex environments.

Conclusion: HSR overcomes SR limitations, offering policy-agnostic, hierarchical maps that balance model-free and model-based approaches, enhancing scalability and adaptability.

Abstract: The successor representation (SR) provides a powerful framework for decoupling predictive dynamics from rewards, enabling rapid generalisation across reward configurations. However, the classical SR is limited by its inherent policy dependence: policies change due to ongoing learning, environmental non-stationarities, and changes in task demands, making established predictive representations obsolete. Furthermore, in topologically complex environments, SRs suffer from spectral diffusion, leading to dense and overlapping features that scale poorly. Here we propose the Hierarchical Successor Representation (HSR) for overcoming these limitations. By incorporating temporal abstractions into the construction of predictive representations, HSR learns stable state features which are robust to task-induced policy changes. Applying non-negative matrix factorisation (NMF) to the HSR yields a sparse, low-rank state representation that facilitates highly sample-efficient transfer to novel tasks in multi-compartmental environments. Further analysis reveals that HSR-NMF discovers interpretable topological structures, providing a policy-agnostic hierarchical map that effectively bridges model-free optimality and model-based flexibility. Beyond providing a useful basis for task-transfer, we show that HSR's temporally extended predictive structure can also be leveraged to drive efficient exploration, effectively scaling to large, procedurally generated environments.

</details>


### [186] [GRAIL: Geometry-Aware Retrieval-Augmented Inference with LLMs over Hyperbolic Representations of Patient Trajectories](https://arxiv.org/abs/2602.12828)
*Zhan Qu,Michael Färber*

Main category: cs.LG

TL;DR: GRAIL is a framework for predicting next-visit clinical events from EHRs using structured geometric representations and retrieval, improving accuracy and consistency.


<details>
  <summary>Details</summary>
Motivation: Predicting clinical events from EHRs is difficult due to sparse data, hierarchical vocabularies, and LLM hallucinations. GRAIL addresses these challenges.

Method: GRAIL integrates coding-system hierarchies and temporal associations into a clinical graph, embeds it in hyperbolic space, and uses probabilistic Central Events for visit summarization. It retrieves plausible future events and optionally refines them with an LLM.

Result: GRAIL improves multi-type next-visit prediction and produces hierarchy-consistent forecasts, as demonstrated on MIMIC-IV.

Conclusion: GRAIL effectively addresses EHR prediction challenges by combining structured representations with retrieval and optional LLM refinement.

Abstract: Predicting future clinical events from longitudinal electronic health records (EHRs) is challenging due to sparse multi-type clinical events, hierarchical medical vocabularies, and the tendency of large language models (LLMs) to hallucinate when reasoning over long structured histories. We study next-visit event prediction, which aims to forecast a patient's upcoming clinical events based on prior visits. We propose GRAIL, a framework that models longitudinal EHRs using structured geometric representations and structure-aware retrieval. GRAIL constructs a unified clinical graph by combining deterministic coding-system hierarchies with data-driven temporal associations across event types, embeds this graph in hyperbolic space, and summarizes each visit as a probabilistic Central Event that denoises sparse observations. At inference time, GRAIL retrieves a structured set of clinically plausible future events aligned with hierarchical and temporal progression, and optionally refines their ranking using an LLM as a constrained inference-time reranker. Experiments on MIMIC-IV show that GRAIL consistently improves multi-type next-visit prediction and yields more hierarchy-consistent forecasts.

</details>


### [187] [Closing the Loop: A Control-Theoretic Framework for Provably Stable Time Series Forecasting with LLMs](https://arxiv.org/abs/2602.12756)
*Xingyu Zhang,Hanyun Du,Zeen Song,Jianqi Zhang,Changwen Zheng,Wenwen Qiang*

Main category: cs.LG

TL;DR: F-LLM introduces a closed-loop framework for LLM-based time series forecasting to mitigate error accumulation by using feedback control, outperforming naive autoregressive methods.


<details>
  <summary>Details</summary>
Motivation: Current autoregressive LLMs for time series forecasting suffer from error accumulation due to open-loop inference, leading to inaccurate long-horizon predictions.

Method: Proposes F-LLM, a closed-loop framework with a learnable residual estimator (Observer) and feedback controller to stabilize trajectories.

Result: F-LLM reduces error propagation and achieves improved performance on time series benchmarks.

Conclusion: The closed-loop approach of F-LLM effectively addresses exposure bias in autoregressive forecasting, ensuring bounded errors and better accuracy.

Abstract: Large Language Models (LLMs) have recently shown exceptional potential in time series forecasting, leveraging their inherent sequential reasoning capabilities to model complex temporal dynamics. However, existing approaches typically employ a naive autoregressive generation strategy. We identify a critical theoretical flaw in this paradigm: during inference, the model operates in an open-loop manner, consuming its own generated outputs recursively. This leads to inevitable error accumulation (exposure bias), where minor early deviations cascade into significant trajectory drift over long horizons. In this paper, we reformulate autoregressive forecasting through the lens of control theory, proposing \textbf{F-LLM} (Feedback-driven LLM), a novel closed-loop framework. Unlike standard methods that passively propagate errors, F-LLM actively stabilizes the trajectory via a learnable residual estimator (Observer) and a feedback controller. Furthermore, we provide a theoretical guarantee that our closed-loop mechanism ensures uniformly bounded error, provided the base model satisfies a local Lipschitz constraint. Extensive experiments demonstrate that F-LLM significantly mitigates error propagation, achieving good performance on time series benchmarks.

</details>


### [188] [FLAC: Maximum Entropy RL via Kinetic Energy Regularized Bridge Matching](https://arxiv.org/abs/2602.12829)
*Lei Lv,Yunfei Li,Yu Luo,Fuchun Sun,Xiao Ma*

Main category: cs.LG

TL;DR: FLAC introduces a likelihood-free framework for Maximum Entropy Reinforcement Learning by penalizing kinetic energy of velocity fields, avoiding explicit action density estimation.


<details>
  <summary>Details</summary>
Motivation: Existing iterative generative policies complicate Maximum Entropy Reinforcement Learning due to inaccessible action log-densities.

Method: FLAC formulates policy optimization as a Generalized Schrödinger Bridge problem, using kinetic energy as a proxy for divergence from a high-entropy reference.

Result: FLAC achieves superior or comparable performance on high-dimensional benchmarks without explicit density estimation.

Conclusion: FLAC provides a practical, likelihood-free solution for regulating policy stochasticity in continuous control.

Abstract: Iterative generative policies, such as diffusion models and flow matching, offer superior expressivity for continuous control but complicate Maximum Entropy Reinforcement Learning because their action log-densities are not directly accessible. To address this, we propose Field Least-Energy Actor-Critic (FLAC), a likelihood-free framework that regulates policy stochasticity by penalizing the kinetic energy of the velocity field. Our key insight is to formulate policy optimization as a Generalized Schrödinger Bridge (GSB) problem relative to a high-entropy reference process (e.g., uniform). Under this view, the maximum-entropy principle emerges naturally as staying close to a high-entropy reference while optimizing return, without requiring explicit action densities. In this framework, kinetic energy serves as a physically grounded proxy for divergence from the reference: minimizing path-space energy bounds the deviation of the induced terminal action distribution. Building on this view, we derive an energy-regularized policy iteration scheme and a practical off-policy algorithm that automatically tunes the kinetic energy via a Lagrangian dual mechanism. Empirically, FLAC achieves superior or comparable performance on high-dimensional benchmarks relative to strong baselines, while avoiding explicit density estimation.

</details>


### [189] [Amortized Reasoning Tree Search: Decoupling Proposal and Decision in Large Language Models](https://arxiv.org/abs/2602.12846)
*Zesheng Hong,Jiadong Yu,Hui Pan*

Main category: cs.LG

TL;DR: RLVR suppresses rare but valid reasoning paths, termed 'Normalization Squeeze.' ARTS, a new method, decouples generation from verification to recover performance on rare cases without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: The paper identifies a flaw in RLVR where rare reasoning paths are suppressed, limiting model diversity and performance on complex tasks.

Method: Proposes Amortized Reasoning Tree Search (ARTS) with Flow Matching to decouple generation from verification, enabling robust navigation of sparse reasoning spaces.

Result: ARTS achieves 74.6% on MATH-500, matching fine-tuned policies, and recovers performance on rare cases where RLVR fails.

Conclusion: Disentangling verification from generation (ARTS) offers a more robust solution for complex reasoning tasks compared to traditional RLVR.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has established itself as the dominant paradigm for instilling rigorous reasoning capabilities in Large Language Models. While effective at amplifying dominant behaviors, we identify a critical pathology in this alignment process: the systematic suppression of valid but rare (low-likelihood under the base model distribution) reasoning paths. We theoretically characterize this phenomenon as a "Normalization Squeeze," where the interplay between mode-seeking policy gradients and finite sampling acts as a high-pass likelihood filter, driving the probability of rare correct traces to statistical extinction. To counteract this collapse without discarding the base model's latent diversity, we propose Amortized Reasoning Tree Search (ARTS). Unlike standard approaches that force internalization via parameter updates, ARTS prioritizes deliberation by decoupling generation from verification. We introduce a Flow Matching objective that repurposes the verifier to estimate the conservation of probability flow, enabling robust navigation through sparse, high-entropy search spaces where traditional discriminative objectives fail. Extensive experiments on the MATH-500 benchmark demonstrate that ARTS achieves a performance of 74.6% (BoN@16), effectively matching fully fine-tuned policies (74.7%) without modifying the generative backbone. Crucially, on the long-tail subset where coupled RL optimization collapses to 0% pass@k, ARTS uniquely recovers significant performance, suggesting that disentangling verification from generation offers a more robust pathway for solving complex reasoning tasks.

</details>


### [190] [X-VORTEX: Spatio-Temporal Contrastive Learning for Wake Vortex Trajectory Forecasting](https://arxiv.org/abs/2602.12869)
*Zhan Qu,Michael Färber*

Main category: cs.LG

TL;DR: X-VORTEX is a spatio-temporal contrastive learning framework for tracking aircraft wake vortices using unlabeled LiDAR point cloud sequences. It leverages physics-aware representations to address sensor sparsity and time-varying dynamics, outperforming supervised methods with minimal labeled data.


<details>
  <summary>Details</summary>
Motivation: Wake vortices pose safety and capacity challenges in air traffic management, but tracking them accurately is difficult due to sparse LiDAR scans, fading signatures, and expensive annotations. Current methods ignore temporal structure and don’t scale to large unlabeled datasets.

Method: X-VORTEX uses Augmentation Overlap Theory to learn from unlabeled LiDAR sequences. It pairs weakly perturbed sequences with strongly augmented counterparts via subsampling and masking, aligning representations across missing frames. A geometric encoder extracts per-scan features, and a sequential aggregator models vortex evolution.

Result: Tested on over one million LiDAR scans, X-VORTEX achieves better vortex center localization with only 1% of labeled data compared to supervised baselines. It also supports accurate trajectory forecasting.

Conclusion: X-VORTEX effectively addresses sensor sparsity and vortex dynamics, demonstrating superior performance with minimal labeled data, making it scalable for real-world air traffic management.

Abstract: Wake vortices are strong, coherent air turbulences created by aircraft, and they pose a major safety and capacity challenge for air traffic management. Tracking how vortices move, weaken, and dissipate over time from LiDAR measurements is still difficult because scans are sparse, vortex signatures fade as the flow breaks down under atmospheric turbulence and instabilities, and point-wise annotation is prohibitively expensive. Existing approaches largely treat each scan as an independent, fully supervised segmentation problem, which overlooks temporal structure and does not scale to the vast unlabeled archives collected in practice. We present X-VORTEX, a spatio-temporal contrastive learning framework grounded in Augmentation Overlap Theory that learns physics-aware representations from unlabeled LiDAR point cloud sequences. X-VORTEX addresses two core challenges: sensor sparsity and time-varying vortex dynamics. It constructs paired inputs from the same underlying flight event by combining a weakly perturbed sequence with a strongly augmented counterpart produced via temporal subsampling and spatial masking, encouraging the model to align representations across missing frames and partial observations. Architecturally, a time-distributed geometric encoder extracts per-scan features and a sequential aggregator models the evolving vortex state across variable-length sequences. We evaluate on a real-world dataset of over one million LiDAR scans. X-VORTEX achieves superior vortex center localization while using only 1% of the labeled data required by supervised baselines, and the learned representations support accurate trajectory forecasting.

</details>


### [191] [Transporting Task Vectors across Different Architectures without Training](https://arxiv.org/abs/2602.12952)
*Filippo Rinaldi,Aniello Panariello,Giacomo Salici,Angelo Porrello,Simone Calderara*

Main category: cs.LG

TL;DR: Theseus is a training-free method for transferring task-specific updates across models of different widths by matching functional effects on representations, using orthogonal Procrustes analysis for alignment.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of expensive relearning of task-specific parameter updates for different model variants, especially across models of varying widths.

Method: Theseus characterizes task updates by their functional effects on intermediate representations, formalizing transport as a functional matching problem solved via orthogonal Procrustes analysis.

Result: Theseus improves performance over baselines in vision and language models across widths without additional training or backpropagation.

Conclusion: Task updates can be effectively transferred across architectures when defined functionally, not just parametrically.

Abstract: Adapting large pre-trained models to downstream tasks often produces task-specific parameter updates that are expensive to relearn for every model variant. While recent work has shown that such updates can be transferred between models with identical architectures, transferring them across models of different widths remains largely unexplored. In this work, we introduce Theseus, a training-free method for transporting task-specific updates across heterogeneous models. Rather than matching parameters directly, we characterize a task update by the functional effect it induces on intermediate representations. We formalize task-vector transport as a functional matching problem on observed activations and show that, after aligning representation spaces via orthogonal Procrustes analysis, it admits a stable closed-form solution that preserves the geometry of the update. We evaluate Theseus on vision and language models across different widths, showing consistent improvements over strong baselines without additional training or backpropagation. Our results show that task updates can be meaningfully transferred across architectures when task identity is defined functionally rather than parametrically.

</details>


### [192] [Extending confidence calibration to generalised measures of variation](https://arxiv.org/abs/2602.12975)
*Andrew Thompson,Vivek Desai*

Main category: cs.LG

TL;DR: The paper introduces the Variation Calibration Error (VCE) metric, extending Expected Calibration Error (ECE) to assess calibration of any metric of variation, like Shannon entropy, showing it approaches zero with more data samples.


<details>
  <summary>Details</summary>
Motivation: Existing calibration metrics like ECE focus only on maximum probability, limiting their scope. The authors aim to generalize calibration assessment to any variation metric.

Method: Extends ECE to propose VCE, testing it on synthetic predictions designed to be perfectly calibrated, comparing it to entropy-based UCE.

Result: VCE approaches zero with increasing data samples in perfectly calibrated scenarios, unlike UCE.

Conclusion: VCE effectively generalizes calibration assessment to any variation metric, demonstrating superior properties over UCE in controlled settings.

Abstract: We propose the Variation Calibration Error (VCE) metric for assessing the calibration of machine learning classifiers. The metric can be viewed as an extension of the well-known Expected Calibration Error (ECE) which assesses the calibration of the maximum probability or confidence. Other ways of measuring the variation of a probability distribution exist which have the advantage of taking into account the full probability distribution, for example the Shannon entropy. We show how the ECE approach can be extended from assessing confidence calibration to assessing the calibration of any metric of variation. We present numerical examples upon synthetic predictions which are perfectly calibrated by design, demonstrating that, in this scenario, the VCE has the desired property of approaching zero as the number of data samples increases, in contrast to another entropy-based calibration metric (the UCE) which has been proposed in the literature.

</details>


### [193] [Drift-Aware Variational Autoencoder-based Anomaly Detection with Two-level Ensembling](https://arxiv.org/abs/2602.12976)
*Jin Li,Kleanthis Malialis,Christos G. Panayiotou,Marios M. Polycarpou*

Main category: cs.LG

TL;DR: VAE++ESDD is a novel method combining incremental learning and two-level ensembling (VAEs and drift detectors) to tackle anomaly detection in streaming data with concept drift, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Streaming data are often unlabeled and subject to concept drift, making anomaly detection difficult.

Method: VAE++ESDD uses an ensemble of VAEs for anomaly prediction and statistical-based drift detectors to handle concept drift incrementally.

Result: The method outperforms baselines and state-of-the-art methods on datasets with low anomaly rates and diverse drift characteristics.

Conclusion: VAE++ESDD effectively addresses anomaly detection challenges in nonstationary environments.

Abstract: In today's digital world, the generation of vast amounts of streaming data in various domains has become ubiquitous. However, many of these data are unlabeled, making it challenging to identify events, particularly anomalies. This task becomes even more formidable in nonstationary environments where model performance can deteriorate over time due to concept drift. To address these challenges, this paper presents a novel method, VAE++ESDD, which employs incremental learning and two-level ensembling: an ensemble of Variational AutoEncoder(VAEs) for anomaly prediction, along with an ensemble of concept drift detectors. Each drift detector utilizes a statistical-based concept drift mechanism. To evaluate the effectiveness of VAE++ESDD, we conduct a comprehensive experimental study using real-world and synthetic datasets characterized by severely or extremely low anomalous rates and various drift characteristics. Our study reveals that the proposed method significantly outperforms both strong baselines and state-of-the-art methods.

</details>


### [194] [MAUNet-Light: A Concise MAUNet Architecture for Bias Correction and Downscaling of Precipitation Estimates](https://arxiv.org/abs/2602.12980)
*Sumanta Chandra Mishra Sharma,Adway Mitra,Auroop Ratan Ganguly*

Main category: cs.LG

TL;DR: The paper focuses on developing lightweight neural networks for bias correction and downscaling of precipitation data, introducing MAUNet-Light as a computationally efficient solution.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for bias correction and downscaling are computationally heavy. Deep learning models like MAUNet show promise but are resource-intensive, prompting the need for lighter alternatives.

Method: The study explores teacher-student learning to transfer knowledge from MAUNet to MAUNet-Light, a compact neural network designed for reduced computational demands.

Result: MAUNet-Light achieves comparable accuracy to MAUNet while significantly lowering computational and memory requirements.

Conclusion: Lightweight neural architectures like MAUNet-Light offer efficient solutions for bias correction and downscaling without compromising performance.

Abstract: Satellite-derived data products and climate model simulations of geophysical variables like precipitation, often exhibit systematic biases compared to in-situ measurements. Bias correction and spatial downscaling are fundamental components to develop operational weather forecast systems, as they seek to improve the consistency between coarse-resolution climate model simulations or satellite-based estimates and ground-based observations. In recent years, deep learning-based models have been increasingly replaced traditional statistical methods to generate high-resolution, bias free projections of climate variables. For example, Max-Average U-Net (MAUNet) architecture has been demonstrated for its ability to downscale precipitation estimates. The versatility and adaptability of these neural models make them highly effective across a range of applications, though this often come at the cost of high computational and memory requirements. The aim of this research is to develop light-weight neural network architectures for both bias correction and downscaling of precipitation, for which the teacher-student based learning paradigm is explored. This research demonstrates the adaptability of MAUNet to the task of bias correction, and further introduces a compact, lightweight neural network architecture termed MAUNet-Light.The proposed MAUNet-Light model is developed by transferring knowledge from the trained MAUNet, and it is designed to perform both downscaling and bias correction with reduced computational requirements without any significant loss in accuracy compared to state-of-the-art.

</details>


### [195] [Multi-Dimensional Visual Data Recovery: Scale-Aware Tensor Modeling and Accelerated Randomized Computation](https://arxiv.org/abs/2602.12982)
*Wenjin Qin,Hailin Wang,Jiangjun Peng,Jianjun Wang,Tingwen Huang*

Main category: cs.LG

TL;DR: The paper introduces a generalized nonconvex regularization paradigm using FCTN decomposition for multi-dimensional data recovery, focusing on computational efficiency and scalability. Efficient optimization and randomized compression algorithms are proposed, with theoretical guarantees and experimental validation.


<details>
  <summary>Details</summary>
Motivation: Existing FCTN-based methods for multi-dimensional data recovery lack computational efficiency and modeling capability, prompting the need for improved approaches.

Method: A FCTN-based nonconvex regularization paradigm is proposed, with scalable models and ADMM-based optimization algorithms. Randomized compression techniques enhance computational efficiency.

Result: The proposed method shows superior performance in quantitative metrics, visual quality, and running time compared to state-of-the-art methods.

Conclusion: The framework effectively addresses computational bottlenecks and improves multi-dimensional data recovery, validated by theoretical and experimental results.

Abstract: The recently proposed fully-connected tensor network (FCTN) decomposition has demonstrated significant advantages in correlation characterization and transpositional invariance, and has achieved notable achievements in multi-dimensional data processing and analysis. However, existing multi-dimensional data recovery methods leveraging FCTN decomposition still have room for further enhancement, particularly in computational efficiency and modeling capability. To address these issues, we first propose a FCTN-based generalized nonconvex regularization paradigm from the perspective of gradient mapping. Then, reliable and scalable multi-dimensional data recovery models are investigated, where the model formulation is shifted from unquantized observations to coarse-grained quantized observations. Based on the alternating direction method of multipliers (ADMM) framework, we derive efficient optimization algorithms with convergence guarantees to solve the formulated models. To alleviate the computational bottleneck encountered when processing large-scale multi-dimensional data, fast and efficient randomized compression algorithms are devised in virtue of sketching techniques in numerical linear algebra. These dimensionality-reduction techniques serve as the computational acceleration core of our proposed algorithm framework. Theoretical results on approximation error upper bounds and convergence analysis for the proposed method are derived. Extensive numerical experiments illustrate the effectiveness and superiority of the proposed algorithm over other state-of-the-art methods in terms of quantitative metrics, visual quality, and running time.

</details>


### [196] [Uncertainty in Federated Granger Causality: From Origins to Systemic Consequences](https://arxiv.org/abs/2602.13004)
*Ayush Mohanty,Nazal Mohamed,Nagi Gebraeel*

Main category: cs.LG

TL;DR: This paper introduces a method to quantify uncertainty in federated Granger Causality (GC) frameworks, distinguishing between data noise and model variability, and proves its robustness.


<details>
  <summary>Details</summary>
Motivation: Existing federated GC algorithms lack uncertainty quantification, limiting reliability in distributed settings like smart grids.

Method: The authors classify uncertainty sources, derive closed-form recursions for uncertainty propagation, and analyze convergence conditions.

Result: Steady-state variances depend only on client data statistics, enhancing robustness. Empirical tests confirm improved reliability.

Conclusion: Explicit uncertainty characterization significantly boosts federated causal inference's reliability and interpretability.

Abstract: Granger Causality (GC) provides a rigorous framework for learning causal structures from time-series data. Recent federated variants of GC have targeted distributed infrastructure applications (e.g., smart grids) with distributed clients that generate high-dimensional data bound by data-sovereignty constraints. However, Federated GC algorithms only yield deterministic point estimates of causality and neglect uncertainty. This paper establishes the first methodology for rigorously quantifying uncertainty and its propagation within federated GC frameworks. We systematically classify sources of uncertainty, explicitly differentiating aleatoric (data noise) from epistemic (model variability) effects. We derive closed-form recursions that model the evolution of uncertainty through client-server interactions and identify four novel cross-covariance components that couple data uncertainties with model parameter uncertainties across the federated architecture. We also define rigorous convergence conditions for these uncertainty recursions and obtain explicit steady-state variances for both server and client model parameters. Our convergence analysis demonstrates that steady-state variances depend exclusively on client data statistics, thus eliminating dependence on initial epistemic priors and enhancing robustness. Empirical evaluations on synthetic benchmarks and real-world industrial datasets demonstrate that explicitly characterizing uncertainty significantly improves the reliability and interpretability of federated causal inference.

</details>


### [197] [Prior-Guided Symbolic Regression: Towards Scientific Consistency in Equation Discovery](https://arxiv.org/abs/2602.13021)
*Jing Xiao,Xinhai Chen,Jiaming Peng,Qinglin Wang,Menghan Jia,Zhiquan Lai,Guangping Yu,Dongsheng Li,Tiejun Li,Jie Liu*

Main category: cs.LG

TL;DR: PG-SR is a prior-guided symbolic regression framework that avoids pseudo-equations by incorporating domain priors and a three-stage pipeline, outperforming baselines in robustness and consistency.


<details>
  <summary>Details</summary>
Motivation: Existing symbolic regression methods often produce equations that fit data but violate scientific principles due to reliance on empirical risk minimization without constraints.

Method: PG-SR uses a three-stage pipeline (warm-up, evolution, refinement) with a prior constraint checker and Prior Annealing Constrained Evaluation (PACE) to ensure scientific consistency.

Result: PG-SR reduces Rademacher complexity, provides generalization guarantees, and outperforms baselines in robustness to prior quality, noise, and data scarcity.

Conclusion: PG-SR effectively bridges the gap between empirical performance and scientific consistency in symbolic regression.

Abstract: Symbolic Regression (SR) aims to discover interpretable equations from observational data, with the potential to reveal underlying principles behind natural phenomena. However, existing approaches often fall into the Pseudo-Equation Trap: producing equations that fit observations well but remain inconsistent with fundamental scientific principles. A key reason is that these approaches are dominated by empirical risk minimization, lacking explicit constraints to ensure scientific consistency. To bridge this gap, we propose PG-SR, a prior-guided SR framework built upon a three-stage pipeline consisting of warm-up, evolution, and refinement. Throughout the pipeline, PG-SR introduces a prior constraint checker that explicitly encodes domain priors as executable constraint programs, and employs a Prior Annealing Constrained Evaluation (PACE) mechanism during the evolution stage to progressively steer discovery toward scientifically consistent regions. Theoretically, we prove that PG-SR reduces the Rademacher complexity of the hypothesis space, yielding tighter generalization bounds and establishing a guarantee against pseudo-equations. Experimentally, PG-SR outperforms state-of-the-art baselines across diverse domains, maintaining robustness to varying prior quality, noisy data, and data scarcity.

</details>


### [198] [Machine Learning-Based Classification of Jhana Advanced Concentrative Absorption Meditation (ACAM-J) using 7T fMRI](https://arxiv.org/abs/2602.13008)
*Puneet Kumar,Winson F. Z. Yang,Alakhsimar Singh,Xiaobai Li,Matthew D. Sacchet*

Main category: cs.LG

TL;DR: The study explores whether fMRI-derived ReHo can classify advanced meditation states (ACAM-J) using machine learning. Results show moderate accuracy (66.82%) with prefrontal and anterior cingulate regions playing key roles.


<details>
  <summary>Details</summary>
Motivation: Understanding the neural correlates of ACAM-J can provide insights into consciousness and well-being.

Method: Used fMRI data from 20 meditators to train classifiers and single-case data for evaluation. ReHo maps and feature extraction from brain regions were employed with machine learning models.

Result: Ensemble models achieved 66.82% accuracy in distinguishing ACAM-J from control conditions, with prefrontal and anterior cingulate regions being most influential.

Conclusion: Machine learning shows promise for classifying advanced meditation states, suggesting future research into neuromodulation and mechanistic models.

Abstract: Jhana advanced concentration absorption meditation (ACAM-J) is related to profound changes in consciousness and cognitive processing, making the study of their neural correlates vital for insights into consciousness and well-being. This study evaluates whether functional MRI-derived regional homogeneity (ReHo) can be used to classify ACAM-J using machine-learning approaches. We collected group-level fMRI data from 20 advanced meditators to train the classifiers, and intensive single-case data from an advanced practitioner performing ACAM-J and control tasks to evaluate generalization. ReHo maps were computed, and features were extracted from predefined brain regions of interest. We trained multiple machine learning classifiers using stratified cross-validation to evaluate whether ReHo patterns distinguish ACAM-J from non-meditative states. Ensemble models achieved 66.82% (p < 0.05) accuracy in distinguishing ACAM-J from control conditions. Feature-importance analysis indicated that prefrontal and anterior cingulate areas contributed most to model decisions, aligning with established involvement of these regions in attentional regulation and metacognitive processes. Moreover, moderate agreement reflected in Cohen's kappa supports the feasibility of using machine learning to distinguish ACAM-J from non-meditative states. These findings advocate machine-learning's feasibility in classifying advanced meditation states, future research on neuromodulation and mechanistic models of advanced meditation.

</details>


### [199] [Probabilistic Wind Power Forecasting with Tree-Based Machine Learning and Weather Ensembles](https://arxiv.org/abs/2602.13010)
*Max Bruninx,Diederik van Binsbergen,Timothy Verstraeten,Ann Nowé,Jan Helsen*

Main category: cs.LG

TL;DR: Probabilistic day-ahead wind power forecasts using gradient boosting trees and weather ensembles outperform deterministic methods, with conditional diffusion models yielding the best results.


<details>
  <summary>Details</summary>
Motivation: Accurate wind power forecasts are crucial for integrating renewable energy into the grid.

Method: Comparative analysis of three probabilistic methods (conformalised quantile regression, natural gradient boosting, conditional diffusion models) combined with tree-based ML, validated with Belgian offshore wind farm data.

Result: ML methods reduce mean absolute error by up to 53% vs power curve and 33% vs wake model. Conditional diffusion models perform best. Weather ensembles improve accuracy by 23%.

Conclusion: Machine learning, especially conditional diffusion models with weather ensembles, significantly enhances wind power forecasting accuracy.

Abstract: Accurate production forecasts are essential to continue facilitating the integration of renewable energy sources into the power grid. This paper illustrates how to obtain probabilistic day-ahead forecasts of wind power generation via gradient boosting trees using an ensemble of weather forecasts. To this end, we perform a comparative analysis across three state-of-the-art probabilistic prediction methods-conformalised quantile regression, natural gradient boosting and conditional diffusion models-all of which can be combined with tree-based machine learning. The methods are validated using four years of data for all wind farms present within the Belgian offshore zone. Additionally, the point forecasts are benchmarked against deterministic engineering methods, using either the power curve or an advanced approach incorporating a calibrated analytical wake model. The experimental results show that the machine learning methods improve the mean absolute error by up to 53% and 33% compared to the power curve and the calibrated wake model. Considering the three probabilistic prediction methods, the conditional diffusion model is found to yield the best overall probabilistic and point estimate of wind power generation. Moreover, the findings suggest that the use of an ensemble of weather forecasts can improve point forecast accuracy by up to 23%.

</details>


### [200] [Geometric Manifold Rectification for Imbalanced Learning](https://arxiv.org/abs/2602.13045)
*Xubin Wang,Qing Li,Weijia Jia*

Main category: cs.LG

TL;DR: GMR proposes a framework using geometric priors for imbalanced classification, featuring adaptive local voting and asymmetric cleaning to protect minority samples.


<details>
  <summary>Details</summary>
Motivation: Traditional symmetric undersampling methods fail to handle noise and class overlap effectively, often removing informative minority samples.

Method: GMR introduces geometric confidence estimation with inverse-distance weighted kNN voting and asymmetric cleaning to safeguard minority samples.

Result: Experiments demonstrate GMR's competitiveness with strong sampling baselines on benchmark datasets.

Conclusion: GMR effectively addresses imbalanced classification challenges by leveraging local geometry and asymmetric rules.

Abstract: Imbalanced classification presents a formidable challenge in machine learning, particularly when tabular datasets are plagued by noise and overlapping class boundaries. From a geometric perspective, the core difficulty lies in the topological intrusion of the majority class into the minority manifold, which obscures the true decision boundary. Traditional undersampling techniques, such as Edited Nearest Neighbours (ENN), typically employ symmetric cleaning rules and uniform voting, failing to capture the local manifold structure and often inadvertently removing informative minority samples. In this paper, we propose GMR (Geometric Manifold Rectification), a novel framework designed to robustly handle imbalanced structured data by exploiting local geometric priors. GMR makes two contributions: (1) Geometric confidence estimation that uses inverse-distance weighted kNN voting with an adaptive distance metric to capture local reliability; and (2) asymmetric cleaning that is strict on majority samples while conservatively protecting minority samples via a safe-guarding cap on minority removal. Extensive experiments on multiple benchmark datasets show that GMR is competitive with strong sampling baselines.

</details>


### [201] [Resource-Efficient Gesture Recognition through Convexified Attention](https://arxiv.org/abs/2602.13030)
*Daniel Schwartz,Dario Salvucci,Yusuf Osmanlioglu,Richard Vallett,Genevieve Dion,Ali Shokoufandeh*

Main category: cs.LG

TL;DR: The paper introduces a convexified attention mechanism for wearable e-textile interfaces, reducing parameter count by 97% while achieving 100% gesture recognition accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional deep learning is impractical for wearable e-textile interfaces due to power, computational, and form factor constraints.

Method: Uses Euclidean projection onto the probability simplex and convex loss functions instead of non-convex softmax operations.

Result: Achieves 100% accuracy on tap/swipe gestures with only 120-360 parameters, sub-millisecond inference times, and minimal storage (<7KB).

Conclusion: Demonstrates feasibility of convex optimization for efficient on-device machine learning in textile interfaces; real-world validation needed.

Abstract: Wearable e-textile interfaces require gesture recognition capabilities but face severe constraints in power consumption, computational capacity, and form factor that make traditional deep learning impractical. While lightweight architectures like MobileNet improve efficiency, they still demand thousands of parameters, limiting deployment on textile-integrated platforms. We introduce a convexified attention mechanism for wearable applications that dynamically weights features while preserving convexity through nonexpansive simplex projection and convex loss functions. Unlike conventional attention mechanisms using non-convex softmax operations, our approach employs Euclidean projection onto the probability simplex combined with multi-class hinge loss, ensuring global convergence guarantees. Implemented on a textile-based capacitive sensor with four connection points, our approach achieves 100.00\% accuracy on tap gestures and 100.00\% on swipe gestures -- consistent across 10-fold cross-validation and held-out test evaluation -- while requiring only 120--360 parameters, a 97\% reduction compared to conventional approaches. With sub-millisecond inference times (290--296$μ$s) and minimal storage requirements ($<$7KB), our method enables gesture interfaces directly within e-textiles without external processing. Our evaluation, conducted in controlled laboratory conditions with a single-user dataset, demonstrates feasibility for basic gesture interactions. Real-world deployment would require validation across multiple users, environmental conditions, and more complex gesture vocabularies. These results demonstrate how convex optimization can enable efficient on-device machine learning for textile interfaces.

</details>


### [202] [Diverging Flows: Detecting Extrapolations in Conditional Generation](https://arxiv.org/abs/2602.13061)
*Constantinos Tsakonas,Serena Ivaldi,Jean-Baptiste Mouret*

Main category: cs.LG

TL;DR: Diverging Flows introduces a method for detecting off-manifold inputs in Flow Matching models without sacrificing prediction quality or speed, enabling safer deployment.


<details>
  <summary>Details</summary>
Motivation: Current FM models pose risks in safety-critical applications due to silent failures from off-manifold inputs, necessitating reliable extrapolation detection.

Method: Diverging Flows structurally enforces inefficient transport for off-manifold inputs, allowing simultaneous conditional generation and extrapolation detection.

Result: The method successfully detects extrapolations in synthetic manifolds, style transfer, and weather forecasting without degrading performance or latency.

Conclusion: Diverging Flows enhances FM robustness, making it suitable for trustworthy deployment in critical domains.

Abstract: The ability of Flow Matching (FM) to model complex conditional distributions has established it as the state-of-the-art for prediction tasks (e.g., robotics, weather forecasting). However, deployment in safety-critical settings is hindered by a critical extrapolation hazard: driven by smoothness biases, flow models yield plausible outputs even for off-manifold conditions, resulting in silent failures indistinguishable from valid predictions. In this work, we introduce Diverging Flows, a novel approach that enables a single model to simultaneously perform conditional generation and native extrapolation detection by structurally enforcing inefficient transport for off-manifold inputs. We evaluate our method on synthetic manifolds, cross-domain style transfer, and weather temperature forecasting, demonstrating that it achieves effective detection of extrapolations without compromising predictive fidelity or inference latency. These results establish Diverging Flows as a robust solution for trustworthy flow models, paving the way for reliable deployment in domains such as medicine, robotics, and climate science.

</details>


### [203] [TCRL: Temporal-Coupled Adversarial Training for Robust Constrained Reinforcement Learning in Worst-Case Scenarios](https://arxiv.org/abs/2602.13040)
*Wentao Xu,Zhongming Yao,Weihao Li,Zhenghang Song,Yumeng Song,Tianyi Li,Yushuai Li*

Main category: cs.LG

TL;DR: TCRL is a new adversarial training framework for robust constrained reinforcement learning, addressing temporally coupled perturbations with a dual-constraint mechanism, outperforming existing methods in robustness.


<details>
  <summary>Details</summary>
Motivation: Existing CRL methods lack robustness against temporally coupled perturbations, limiting their effectiveness in safety-critical applications like autonomous driving and robotics.

Method: TCRL introduces a worst-case-perceived cost constraint function and a dual-constraint defense mechanism on rewards to handle temporally coupled adversaries.

Result: Experiments show TCRL outperforms current methods in robustness against temporally coupled perturbation attacks across various CRL tasks.

Conclusion: TCRL provides a more robust solution for CRL in worst-case scenarios by addressing temporally coupled perturbations, enhancing safety in critical applications.

Abstract: Constrained Reinforcement Learning (CRL) aims to optimize decision-making policies under constraint conditions, making it highly applicable to safety-critical domains such as autonomous driving, robotics, and power grid management. However, existing robust CRL approaches predominantly focus on single-step perturbations and temporally independent adversarial models, lacking explicit modeling of robustness against temporally coupled perturbations. To tackle these challenges, we propose TCRL, a novel temporal-coupled adversarial training framework for robust constrained reinforcement learning (TCRL) in worst-case scenarios. First, TCRL introduces a worst-case-perceived cost constraint function that estimates safety costs under temporally coupled perturbations without the need to explicitly model adversarial attackers. Second, TCRL establishes a dual-constraint defense mechanism on the reward to counter temporally coupled adversaries while maintaining reward unpredictability. Experimental results demonstrate that TCRL consistently outperforms existing methods in terms of robustness against temporally coupled perturbation attacks across a variety of CRL tasks.

</details>


### [204] [Bus-Conditioned Zero-Shot Trajectory Generation via Task Arithmetic](https://arxiv.org/abs/2602.13071)
*Shuai Liu,Ning Cao,Yile Chen,Yue Jiang,Gao Cong*

Main category: cs.LG

TL;DR: Paper proposes MobTA for bus-conditioned zero-shot trajectory generation, leveraging source city data and bus timetables without needing target city mobility data.


<details>
  <summary>Details</summary>
Motivation: Mobility trajectory data is scarce, and existing methods assume access to target city data, limiting their use in data-inaccessible scenarios.

Method: MobTA uses task arithmetic to model parameter shifts from bus-timetable-based trajectories to mobility trajectories in source cities, applying this shift to target cities.

Result: MobTA outperforms existing methods and nears performance of models finetuned with target city mobility data.

Conclusion: MobTA enables effective trajectory generation without target city mobility data, broadening applicability.

Abstract: Mobility trajectory data provide essential support for smart city applications. However, such data are often difficult to obtain. Meanwhile, most existing trajectory generation methods implicitly assume that at least a subset of real mobility data from target city is available, which limits their applicability in data-inaccessible scenarios. In this work, we propose a new problem setting, called bus-conditioned zero-shot trajectory generation, where no mobility trajectories from a target city are accessible. The generation process relies solely on source city mobility data and publicly available bus timetables from both cities. Under this setting, we propose MobTA, the first approach to introduce task arithmetic into trajectory generation. MobTA models the parameter shift from bus-timetable-based trajectory generation to mobility trajectory generation in source city, and applies this shift to target city through arithmetic operations on task vectors. This enables trajectory generation that reflects target-city mobility patterns without requiring any real mobility data from it. Furthermore, we theoretically analyze MobTA's stability across base and instruction-tuned LLMs. Extensive experiments show that MobTA significantly outperforms existing methods, and achieves performance close to models finetuned using target city mobility trajectories.

</details>


### [205] [GPTZero: Robust Detection of LLM-Generated Texts](https://arxiv.org/abs/2602.13042)
*George Alexandru Adam,Alexander Cui,Edwin Thomas,Emily Napier,Nazar Shmatko,Jacob Schnell,Jacob Junqi Tian,Alekhya Dronavalli,Edward Tian,Dongwon Lee*

Main category: cs.LG

TL;DR: GPTZero is introduced as an advanced AI detection tool to distinguish human-authored from AI-generated text, addressing concerns like misinformation and skill evaluation undermining.


<details>
  <summary>Details</summary>
Motivation: The rise of LLMs has created challenges in differentiating human and AI-generated text, impacting areas like evaluations and misinformation.

Method: GPTZero uses a hierarchical, multi-task architecture for flexible classification, excels in accuracy across domains, and employs multi-tiered red teaming for robustness.

Result: GPTZero achieves state-of-the-art accuracy, robustness against attacks, and provides explainable detection with educational guidance.

Conclusion: GPTZero ensures reliable, transparent, and fair assessment of text authenticity in the era of LLMs.

Abstract: While historical considerations surrounding text authenticity revolved primarily around plagiarism, the advent of large language models (LLMs) has introduced a new challenge: distinguishing human-authored from AI-generated text. This shift raises significant concerns, including the undermining of skill evaluations, the mass-production of low-quality content, and the proliferation of misinformation. Addressing these issues, we introduce GPTZero a state-of-the-art industrial AI detection solution, offering reliable discernment between human and LLM-generated text. Our key contributions include: introducing a hierarchical, multi-task architecture enabling a flexible taxonomy of human and AI texts, demonstrating state-of-the-art accuracy on a variety of domains with granular predictions, and achieving superior robustness to adversarial attacks and paraphrasing via multi-tiered automated red teaming. GPTZero offers accurate and explainable detection, and educates users on its responsible use, ensuring fair and transparent assessment of text.

</details>


### [206] [EXCODER: EXplainable Classification Of DiscretE time series Representations](https://arxiv.org/abs/2602.13087)
*Yannik Hahn,Antonin Königsfeld,Hasan Tercan,Tobias Meisen*

Main category: cs.LG

TL;DR: Discrete latent representations enhance explainability in time series classification by reducing noise and redundancy, enabling more concise and faithful explanations without compromising performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of explainability in deep learning models for time series classification, especially due to high dimensionality and noise in raw data.

Method: Transforming time series into discrete latent representations using VQ-VAE and DVAE, then applying XAI methods to these compressed representations. Introduces Similar Subsequence Accuracy (SSA) to validate XAI-identified features.

Result: Discrete representations preserve classification performance while enabling compact, interpretable, and computationally efficient explanations. SSA validates the representativeness of XAI-highlighted features.

Conclusion: Discrete latent representations offer a promising solution for improving explainability in time series classification, balancing performance and interpretability.

Abstract: Deep learning has significantly improved time series classification, yet the lack of explainability in these models remains a major challenge. While Explainable AI (XAI) techniques aim to make model decisions more transparent, their effectiveness is often hindered by the high dimensionality and noise present in raw time series data. In this work, we investigate whether transforming time series into discrete latent representations-using methods such as Vector Quantized Variational Autoencoders (VQ-VAE) and Discrete Variational Autoencoders (DVAE)-not only preserves but enhances explainability by reducing redundancy and focusing on the most informative patterns. We show that applying XAI methods to these compressed representations leads to concise and structured explanations that maintain faithfulness without sacrificing classification performance. Additionally, we propose Similar Subsequence Accuracy (SSA), a novel metric that quantitatively assesses the alignment between XAI-identified salient subsequences and the label distribution in the training data. SSA provides a systematic way to validate whether the features highlighted by XAI methods are truly representative of the learned classification patterns. Our findings demonstrate that discrete latent representations not only retain the essential characteristics needed for classification but also offer a pathway to more compact, interpretable, and computationally efficient explanations in time series analysis.

</details>


### [207] [Which Algorithms Can Graph Neural Networks Learn?](https://arxiv.org/abs/2602.13106)
*Solveig Wittig,Antonis Vasileiou,Robert R. Nerem,Timo Stoll,Floris Geerts,Yusu Wang,Christopher Morris*

Main category: cs.LG

TL;DR: The paper develops a theoretical framework for MPNNs to learn and generalize algorithms from small training sets, applies it to various algorithms, and identifies limitations of standard MPNNs while proposing more expressive alternatives.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between empirical and theoretical understanding of MPNNs' ability to learn and generalize discrete algorithms, addressing limitations in existing work.

Method: A general theoretical framework characterizing sufficient conditions for MPNNs to learn algorithms from small training sets and generalize to arbitrary inputs, supported by impossibility results and refined architectures.

Result: The framework applies to algorithms like shortest paths and knapsack problems, with empirical validation and improved bounds for Bellman-Ford.

Conclusion: The work provides formal guarantees for MPNNs' algorithmic reasoning, identifies their limitations, and offers solutions, advancing neural algorithmic reasoning.

Abstract: In recent years, there has been growing interest in understanding neural architectures' ability to learn to execute discrete algorithms, a line of work often referred to as neural algorithmic reasoning. The goal is to integrate algorithmic reasoning capabilities into larger neural pipelines. Many such architectures are based on (message-passing) graph neural networks (MPNNs), owing to their permutation equivariance and ability to deal with sparsity and variable-sized inputs. However, existing work is either largely empirical and lacks formal guarantees or it focuses solely on expressivity, leaving open the question of when and how such architectures generalize beyond a finite training set. In this work, we propose a general theoretical framework that characterizes the sufficient conditions under which MPNNs can learn an algorithm from a training set of small instances and provably approximate its behavior on inputs of arbitrary size. Our framework applies to a broad class of algorithms, including single-source shortest paths, minimum spanning trees, and general dynamic programming problems, such as the $0$-$1$ knapsack problem. In addition, we establish impossibility results for a wide range of algorithmic tasks, showing that standard MPNNs cannot learn them, and we derive more expressive MPNN-like architectures that overcome these limitations. Finally, we refine our analysis for the Bellman-Ford algorithm, yielding a substantially smaller required training set and significantly extending the recent work of Nerem et al. [2025] by allowing for a differentiable regularization loss. Empirical results largely support our theoretical findings.

</details>


### [208] [Quantization-Aware Collaborative Inference for Large Embodied AI Models](https://arxiv.org/abs/2602.13052)
*Zhonghao Lyu,Ming Xiao,Mikael Skoglund,Merouane Debbah,H. Vincent Poor*

Main category: cs.LG

TL;DR: The paper proposes quantization-aware collaborative inference to tackle the computational challenges of large AI models in embodied AI, validating its effectiveness through simulations and real-world tests.


<details>
  <summary>Details</summary>
Motivation: Large AI models' computational demands hinder their use in resource-limited embodied agents, prompting research into efficient inference solutions.

Method: Develops quantization-induced distortion approximation, derives rate-distortion bounds, and formulates joint quantization and computation frequency optimization.

Result: Validates distortion approximation and bounds, showing the design balances inference quality, latency, and energy consumption effectively.

Conclusion: The proposed joint quantization and computation frequency design optimizes embodied AI systems under resource constraints.

Abstract: Large artificial intelligence models (LAIMs) are increasingly regarded as a core intelligence engine for embodied AI applications. However, the massive parameter scale and computational demands of LAIMs pose significant challenges for resource-limited embodied agents. To address this issue, we investigate quantization-aware collaborative inference (co-inference) for embodied AI systems. First, we develop a tractable approximation for quantization-induced inference distortion. Based on this approximation, we derive lower and upper bounds on the quantization rate-inference distortion function, characterizing its dependence on LAIM statistics, including the quantization bit-width. Next, we formulate a joint quantization bit-width and computation frequency design problem under delay and energy constraints, aiming to minimize the distortion upper bound while ensuring tightness through the corresponding lower bound. Extensive evaluations validate the proposed distortion approximation, the derived rate-distortion bounds, and the effectiveness of the proposed joint design. Particularly, simulations and real-world testbed experiments demonstrate the effectiveness of the proposed joint design in balancing inference quality, latency, and energy consumption in edge embodied AI systems.

</details>


### [209] [Backdoor Attacks on Contrastive Continual Learning for IoT Systems](https://arxiv.org/abs/2602.13062)
*Alfous Tim,Kuniyilh Simi D*

Main category: cs.LG

TL;DR: The paper analyzes backdoor attacks in contrastive continual learning (CCL) for IoT systems, highlighting vulnerabilities and proposing mitigation strategies under IoT constraints.


<details>
  <summary>Details</summary>
Motivation: IoT systems rely on continual learning to adapt to dynamic environments, but CCL introduces security risks like backdoor attacks, which exploit embedding alignment and replay mechanisms.

Method: The study formalizes embedding-level attack objectives, examines persistence mechanisms in IoT, and develops a taxonomy. It compares vulnerabilities across learning paradigms and evaluates defenses under IoT constraints.

Result: Findings show CCL enhances IoT adaptability but also increases susceptibility to persistent representation-level threats if security measures are insufficient.

Conclusion: CCL is effective for IoT intelligence but requires robust security to mitigate long-lived backdoor threats.

Abstract: The Internet of Things (IoT) systems increasingly depend on continual learning to adapt to non-stationary environments. These environments can include factors such as sensor drift, changing user behavior, device aging, and adversarial dynamics. Contrastive continual learning (CCL) combines contrastive representation learning with incremental adaptation, enabling robust feature reuse across tasks and domains. However, the geometric nature of contrastive objectives, when paired with replay-based rehearsal and stability-preserving regularization, introduces new security vulnerabilities. Notably, backdoor attacks can exploit embedding alignment and replay reinforcement, enabling the implantation of persistent malicious behaviors that endure through updates and deployment cycles. This paper provides a comprehensive analysis of backdoor attacks on CCL within IoT systems. We formalize the objectives of embedding-level attacks, examine persistence mechanisms unique to IoT deployments, and develop a layered taxonomy tailored to IoT. Additionally, we compare vulnerabilities across various learning paradigms and evaluate defense strategies under IoT constraints, including limited memory, edge computing, and federated aggregation. Our findings indicate that while CCL is effective for enhancing adaptive IoT intelligence, it may also elevate long-lived representation-level threats if not adequately secured.

</details>


### [210] [Unified Multi-Domain Graph Pre-training for Homogeneous and Heterogeneous Graphs via Domain-Specific Expert Encoding](https://arxiv.org/abs/2602.13075)
*Chundong Liang,Yongqi Huang,Dongxiao He,Peiyuan Li,Yawen Li,Di Jin,Weixiong Zhang*

Main category: cs.LG

TL;DR: The paper introduces GPH², a unified graph pre-training method for mixed homogeneous and heterogeneous graphs, addressing challenges like cross-domain discrepancies and lack of universal encoders.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on either homogeneous or heterogeneous graphs, ignoring real-world scenarios where mixed graphs are common. GPH² aims to bridge this gap.

Method: Proposes Unified Multi-View Graph Construction for encoding both graph types, domain-specific expert encoding to handle discrepancies, and Task-oriented Expert Fusion Strategy for downstream tasks.

Result: Extensive experiments show GPH² outperforms existing methods in stable transfer across graph types and domains.

Conclusion: GPH² successfully unifies pre-training for mixed graphs, improving adaptability and performance in downstream tasks.

Abstract: Graph pre-training has achieved remarkable success in recent years, delivering transferable representations for downstream adaptation. However, most existing methods are designed for either homogeneous or heterogeneous graphs, thereby hindering unified graph modeling across diverse graph types. This separation contradicts real-world applications, where mixed homogeneous and heterogeneous graphs are ubiquitous, and distribution shifts between upstream pre-training and downstream deployment are common. In this paper, we empirically demonstrate that a balanced mixture of homogeneous and heterogeneous graph pre-training benefits downstream tasks and propose a unified multi-domain \textbf{G}raph \textbf{P}re-training method across \textbf{H}omogeneous and \textbf{H}eterogeneous graphs ($\mathbf{GPH^{2}}$). To address the lack of a unified encoder for homogeneous and heterogeneous graphs, we propose a Unified Multi-View Graph Construction that simultaneously encodes both without explicit graph-type-specific designs. To cope with the increased cross-domain distribution discrepancies arising from mixed graphs, we introduce domain-specific expert encoding. Each expert is independently pre-trained on a single graph to capture domain-specific knowledge, thereby shielding the pre-training encoder from the adverse effects of cross-domain discrepancies. For downstream tasks, we further design a Task-oriented Expert Fusion Strategy that adaptively integrates multiple experts based on their discriminative strengths. Extensive experiments on mixed graphs demonstrate that $\text{GPH}^{2}$ enables stable transfer across graph types and domains, significantly outperforming existing graph pre-training methods.

</details>


### [211] [R-Diverse: Mitigating Diversity Illusion in Self-Play LLM Training](https://arxiv.org/abs/2602.13103)
*Gengsheng Li,Jinghan He,Shijie Wang,Dan Zhang,Ruiqi Liu,Renrui Zhang,Zijun Yao,Junfeng Fang,Haiyun Guo,Jinqiao Wang*

Main category: cs.LG

TL;DR: R-Diverse improves self-play LLM reasoning by addressing Diversity Illusion with Memory-Augmented Penalty and Skill-Aware Measurement, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Existing self-play frameworks like R-Zero suffer from non-sustained improvement due to Diversity Illusion, where training signals appear diverse but collapse into recurring patterns.

Method: R-Diverse introduces Memory-Augmented Penalty (MAP) to discourage recycling and Skill-Aware Measurement (SAM) to evaluate diversity by reasoning skills.

Result: R-Diverse sustains gains over more iterations and outperforms prior methods across 10 benchmarks.

Conclusion: Addressing Diversity Illusion with MAP and SAM enhances self-play LLM reasoning, leading to sustained performance improvements.

Abstract: Self-play bootstraps LLM reasoning through an iterative Challenger-Solver loop: the Challenger is trained to generate questions that target the Solver's capabilities, and the Solver is optimized on the generated data to expand its reasoning skills. However, existing frameworks like R-Zero often exhibit non-sustained improvement, where early gains degrade as self-play continues. We identify a key failure mode, Diversity Illusion, where the Solver's training signals appear diverse yet collapse into recurring underlying patterns. It manifests as (1) Local Diversity Illusion, where diversity is enforced only within-batch, inducing cross-iteration mode cycling; and (2) Surface Diversity Illusion, where questions vary superficially but require near-identical reasoning skills. To mitigate them, we propose R-Diverse with two aligned innovations: Memory-Augmented Penalty (MAP), which uses a persistent memory bank to discourage recycling across iterations, and Skill-Aware Measurement (SAM), which evaluates diversity by the reasoning skills exercised rather than surface variation of questions. Across 10 math and general reasoning benchmarks, R-Diverse sustains gains over more iterations and consistently outperforms prior self-play methods. Code is available at https://github.com/Gengsheng-Li/R-Diverse.

</details>


### [212] [Eventizing Traditionally Opaque Binary Neural Networks as 1-safe Petri net Models](https://arxiv.org/abs/2602.13128)
*Mohamed Tarraf,Alex Chan,Alex Yakovlev,Rishad Shafik*

Main category: cs.LG

TL;DR: The paper introduces a Petri net-based framework to explain and verify Binary Neural Networks (BNNs), addressing their opacity by modeling their operations as event-driven processes for formal analysis.


<details>
  <summary>Details</summary>
Motivation: BNNs are complex and opaque due to their discrete, non-linear behavior, limiting their use in safety-critical applications where transparency and guarantees are essential.

Method: The authors propose a Petri net (PN) framework to model BNN operations as event-driven processes, creating modular PN blueprints for core components and validating them against software-based BNNs.

Result: The framework enables formal verification of properties like 1-safeness and deadlock-freeness, and assesses scalability and complexity using automated tools.

Conclusion: The PN-based framework provides causal transparency and formal verification capabilities for BNNs, making them more suitable for safety-critical domains.

Abstract: Binary Neural Networks (BNNs) offer a low-complexity and energy-efficient alternative to traditional full-precision neural networks by constraining their weights and activations to binary values. However, their discrete, highly non-linear behavior makes them difficult to explain, validate and formally verify. As a result, BNNs remain largely opaque, limiting their suitability in safety-critical domains, where causal transparency and behavioral guarantees are essential. In this work, we introduce a Petri net (PN)-based framework that captures the BNN's internal operations as event-driven processes. By "eventizing" their operations, we expose their causal relationships and dependencies for a fine-grained analysis of concurrency, ordering, and state evolution. Here, we construct modular PN blueprints for core BNN components including activation, gradient computation and weight updates, and compose them into a complete system-level model. We then validate the composed PN against a reference software-based BNN, verify it against reachability and structural checks to establish 1-safeness, deadlock-freeness, mutual exclusion and correct-by-construction causal sequencing, before we assess its scalability and complexity at segment, component, and system levels using the automated measurement tools in Workcraft. Overall, this framework enables causal introspection of transparent and event-driven BNNs that are amenable to formal reasoning and verification.

</details>


### [213] [Order Matters in Retrosynthesis: Structure-aware Generation via Reaction-Center-Guided Discrete Flow Matching](https://arxiv.org/abs/2602.13136)
*Chenguang Wang,Zihan Zhou,Lei Bai,Tianshu Yu*

Main category: cs.LG

TL;DR: The paper introduces a structure-aware template-free retrosynthesis framework, RetroDiT, which improves learning efficiency and generalization by leveraging atom ordering as a positional inductive bias, achieving state-of-the-art results with minimal data and model size.


<details>
  <summary>Details</summary>
Motivation: Existing template-free methods treat retrosynthesis as black-box sequence generation, limiting efficiency, while semi-template approaches rely on rigid reaction libraries, restricting generalization. The paper aims to bridge this gap.

Method: Proposes RetroDiT, a graph transformer with rotary position embeddings, that encodes chemical reactions' two-stage nature as positional inductive bias. It decouples training from sampling using discrete flow matching, enabling faster generation.

Result: Achieves top-1 performance of 61.2% on USPTO-50k and 51.3% on USPTO-Full (71.1% and 63.4% with oracle centers). Outperforms larger models, with a 280K-parameter model matching a 65M-parameter one.

Conclusion: Structural priors in atom ordering significantly enhance retrosynthesis performance, proving more effective than brute-force scaling, while requiring less data and computational resources.

Abstract: Template-free retrosynthesis methods treat the task as black-box sequence generation, limiting learning efficiency, while semi-template approaches rely on rigid reaction libraries that constrain generalization. We address this gap with a key insight: atom ordering in neural representations matters. Building on this insight, we propose a structure-aware template-free framework that encodes the two-stage nature of chemical reactions as a positional inductive bias. By placing reaction center atoms at the sequence head, our method transforms implicit chemical knowledge into explicit positional patterns that the model can readily capture. The proposed RetroDiT backbone, a graph transformer with rotary position embeddings, exploits this ordering to prioritize chemically critical regions. Combined with discrete flow matching, our approach decouples training from sampling and enables generation in 20--50 steps versus 500 for prior diffusion methods. Our method achieves state-of-the-art performance on both USPTO-50k (61.2% top-1) and the large-scale USPTO-Full (51.3% top-1) with predicted reaction centers. With oracle centers, performance reaches 71.1% and 63.4% respectively, surpassing foundation models trained on 10 billion reactions while using orders of magnitude less data. Ablation studies further reveal that structural priors outperform brute-force scaling: a 280K-parameter model with proper ordering matches a 65M-parameter model without it.

</details>


### [214] [FlashSchNet: Fast and Accurate Coarse-Grained Neural Network Molecular Dynamics](https://arxiv.org/abs/2602.13140)
*Pingzhi Li,Hongxuan Li,Zirui Liu,Xingcheng Lin,Tianlong Chen*

Main category: cs.LG

TL;DR: FlashSchNet is an IO-aware GNN-MD framework that improves GPU utilization through fused operations and quantization, achieving faster simulation speeds than classical force fields while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: GNN potentials like SchNet enhance molecular dynamics (MD) simulation accuracy but suffer from inefficiencies due to fragmented kernels and memory-bound pipelines that underutilize GPUs. The paper aims to address these bottlenecks by making GNN-MD IO-aware.

Method: FlashSchNet introduces four techniques: (1) flash radial basis, (2) flash message passing, (3) flash aggregation, and (4) channel-wise 16-bit quantization to optimize memory usage and computation efficiency.

Result: On an NVIDIA RTX PRO 6000, FlashSchNet achieves 1000 ns/day aggregate throughput for 64 parallel replicas on a CG protein, outperforming baselines by 6.5x with 80% peak memory reduction, while retaining SchNet-level accuracy.

Conclusion: FlashSchNet demonstrates that IO-aware optimizations can significantly accelerate GNN-MD simulations, making them competitive with classical force fields in speed without compromising accuracy.

Abstract: Graph neural network (GNN) potentials such as SchNet improve the accuracy and transferability of molecular dynamics (MD) simulation by learning many-body interactions, but remain slower than classical force fields due to fragmented kernels and memory-bound pipelines that underutilize GPUs. We show that a missing principle is making GNN-MD IO-aware, carefully accounting for reads and writes between GPU high-bandwidth memory (HBM) and on-chip SRAM. We present FlashSchNet, an efficient and accurate IO-aware SchNet-style GNN-MD framework built on four techniques: (1) flash radial basis, which fuses pairwise distance computation, Gaussian basis expansion, and cosine envelope into a single tiled pass, computing each distance once and reusing it across all basis functions; (2) flash message passing, which fuses cutoff, neighbor gather, filter multiplication, and reduction to avoid materializing edge tensors in HBM; (3) flash aggregation, which reformulates scatter-add via CSR segment reduce, reducing atomic writes by a factor of feature dimension and enabling contention-free accumulation in both forward and backward passes; (4) channel-wise 16-bit quantization that exploits the low per-channel dynamic range in SchNet MLP weights to further improve throughput with negligible accuracy loss. On a single NVIDIA RTX PRO 6000, FlashSchNet achieves 1000 ns/day aggregate simulation throughput over 64 parallel replicas on coarse-grained (CG) protein containing 269 beads (6.5x faster than CGSchNet baseline with 80% reduction of peak memory), surpassing classical force fields (e.g. MARTINI) while retaining SchNet-level accuracy and transferability.

</details>


### [215] [Learning to Approximate Uniform Facility Location via Graph Neural Networks](https://arxiv.org/abs/2602.13155)
*Chendi Qian,Christopher Morris,Stefanie Jegelka,Christian Sohler*

Main category: cs.LG

TL;DR: The paper proposes a differentiable MPNN model for UniFL, combining approximation-algorithmic principles without needing supervised data, achieving provable guarantees and outperforming non-learned methods.


<details>
  <summary>Details</summary>
Motivation: Addresses the limitations of existing learning-based approaches (computational overhead, unstable training) and classical approximation algorithms (non-differentiability, lack of adaptability).

Method: Develops a fully differentiable MPNN model embedding approximation-algorithmic principles, avoiding solver supervision or discrete relaxations.

Result: Achieves provable approximation guarantees and outperforms standard non-learned algorithms, bridging the gap with integer linear programming.

Conclusion: Bridges learning-based methods and approximation algorithms for discrete optimization, offering a promising direction for future research.

Abstract: There has been a growing interest in using neural networks, especially message-passing neural networks (MPNNs), to solve hard combinatorial optimization problems heuristically. However, existing learning-based approaches for hard combinatorial optimization tasks often rely on supervised training data, reinforcement learning, or gradient estimators, leading to significant computational overhead, unstable training, or a lack of provable performance guarantees. In contrast, classical approximation algorithms offer such performance guarantees under worst-case inputs but are non-differentiable and unable to adaptively exploit structural regularities in natural input distributions. We address this dichotomy with the fundamental example of Uniform Facility Location (UniFL), a variant of the combinatorial facility location problem with applications in clustering, data summarization, logistics, and supply chain design. We develop a fully differentiable MPNN model that embeds approximation-algorithmic principles while avoiding the need for solver supervision or discrete relaxations. Our approach admits provable approximation and size generalization guarantees to much larger instances than seen during training. Empirically, we show that our approach outperforms standard non-learned approximation algorithms in terms of solution quality, closing the gap with computationally intensive integer linear programming approaches. Overall, this work provides a step toward bridging learning-based methods and approximation algorithms for discrete optimization.

</details>


### [216] [Learning functional components of PDEs from data using neural networks](https://arxiv.org/abs/2602.13174)
*Torkel E. Loman,Yurij Salmaniw,Antonio Leon Villares,Jose A. Carrillo,Ruth E. Baker*

Main category: cs.LG

TL;DR: The paper demonstrates how neural networks embedded in PDEs can approximate unknown functions from data, using nonlocal aggregation-diffusion equations as a case study.


<details>
  <summary>Details</summary>
Motivation: Recovering unknown functions in PDEs is challenging due to measurement difficulties, and the study aims to extend parameter recovery workflows to function recovery.

Method: Neural networks are embedded into PDEs and trained on data to approximate unknown functions, analyzing factors like solution count, sampling density, and noise.

Result: The approach successfully recovers interaction kernels and external potentials from steady state data, leveraging standard workflows.

Conclusion: The method effectively approximates unknown functions in PDEs and can be treated as a standard PDE for predictions.

Abstract: Partial differential equations often contain unknown functions that are difficult or impossible to measure directly, hampering our ability to derive predictions from the model. Workflows for recovering scalar PDE parameters from data are well studied: here we show how similar workflows can be used to recover functions from data. Specifically, we embed neural networks into the PDE and show how, as they are trained on data, they can approximate unknown functions with arbitrary accuracy. Using nonlocal aggregation-diffusion equations as a case study, we recover interaction kernels and external potentials from steady state data. Specifically, we investigate how a wide range of factors, such as the number of available solutions, their properties, sampling density, and measurement noise, affect our ability to successfully recover functions. Our approach is advantageous because it can utilise standard parameter-fitting workflows, and in that the trained PDE can be treated as a normal PDE for purposes such as generating system predictions.

</details>
